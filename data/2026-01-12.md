<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 37]
- [physics.med-ph](#physics.med-ph) [Total: 4]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.HC](#cs.HC) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Bi-Orthogonal Factor Decomposition for Vision Transformers](https://arxiv.org/abs/2601.05328)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: BFD框架通过统计分解和奇异值分解分析Vision Transformer中的注意力机制，揭示了注意力主要通过内容交互，DINOv2在内容-位置耦合上分配更多能量，注意力头表现出功能专门化。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力机制是Vision Transformer的核心计算单元，但缺乏对注意力在token之间交换何种信息（位置、内容或两者）的系统理解。现有注意力图仅显示权重集中位置，无法揭示查询和键交换的具体信息类型。

Method: 提出双正交因子分解（BFD）框架：第一阶段使用ANOVA分解将token激活统计解耦为正交的位置和内容因子；第二阶段对查询-键交互矩阵QK^T进行奇异值分解，揭示这些因子如何介导通信的双正交模式。

Result: 1. 注意力主要通过内容交互，内容-内容交互主导注意力能量，其次是内容-位置耦合；2. DINOv2比监督模型分配更多能量到内容-位置耦合，并在更丰富的模式谱上分布计算；3. 注意力头表现出专门化：分为内容-内容、内容-位置和位置-位置算子；4. DINOv2的优越整体形状处理源于中间层同时保持位置结构并上下文丰富语义内容。

Conclusion: BFD框架揭示了注意力机制中token如何通过位置或语义因子进行交互，为理解Vision Transformer机制提供了实用见解，特别是DINOv2在保持位置结构的同时丰富语义内容的能力。

Abstract: Self-attention is the central computational primitive of Vision Transformers, yet we lack a principled understanding of what information attention mechanisms exchange between tokens. Attention maps describe where weight mass concentrates; they do not reveal whether queries and keys trade position, content, or both. We introduce Bi-orthogonal Factor Decomposition (BFD), a two-stage analytical framework: first, an ANOVA-based decomposition statistically disentangles token activations into orthogonal positional and content factors; second, SVD of the query-key interaction matrix QK^T exposes bi-orthogonal modes that reveal how these factors mediate communication. After validating proper isolation of position and content, we apply BFD to state-of-the-art vision models and uncover three phenomena.(i) Attention operates primarily through content. Content-content interactions dominate attention energy, followed by content-position coupling. DINOv2 allocates more energy to content-position than supervised models and distributes computation across a richer mode spectrum. (ii) Attention mechanisms exhibit specialization: heads differentiate into content-content, content-position, and position-position operators, while singular modes within heads show analogous specialization. (iii) DINOv2's superior holistic shape processing emerges from intermediate layers that simultaneously preserve positional structure while contextually enriching semantic content.
  Overall, BFD exposes how tokens interact through attention and which informational factors - positional or semantic - mediate their communication, yielding practical insights into vision transformer mechanisms.

</details>


### [2] [Coding the Visual World: From Image to Simulation Using Vision Language Models](https://arxiv.org/abs/2601.05344)
*Sagi Eppel*

Main category: cs.CV

TL;DR: 该研究探索视觉语言模型通过Im2Sim方法识别和模拟图像中系统机制的能力，发现VLMs能理解复杂系统但难以复制精细细节


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型是否能够像人类一样构建对图像中描绘系统的心理模型，评估其对复杂系统的理解能力

Method: 使用Im2Sim方法：给VLM提供真实世界系统的自然图像，要求其描述系统并编写模拟代码，执行代码生成合成图像，与原图对比分析

Result: 领先的VLMs（GPT、Gemini）能够理解和建模跨多个抽象层次和广泛领域的复杂多组件系统，但在复制图像中的精细细节和低层次模式排列方面能力有限

Conclusion: VLMs展现出有趣的不对称性：结合了高层次、深入的视觉理解能力，但对精细细节的感知有限，揭示了当前视觉理解模型的优势和局限

Abstract: The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.

</details>


### [3] [MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments](https://arxiv.org/abs/2601.05368)
*Svitlana Morkva,Maximum Wilder-Smith,Michael Oechsle,Alessio Tonioni,Marco Hutter,Vaishakh Patil*

Main category: cs.CV

TL;DR: MOSAIC-GS是一种基于高斯泼溅的单目视频动态场景重建方法，通过多几何线索和运动约束实现高效、高质量的重建


<details>
  <summary>Details</summary>
Motivation: 单目重建由于缺乏多视角约束而具有病态性，难以准确恢复物体几何和时间一致性，需要解决运动推断的模糊性问题

Method: 利用深度、光流、动态物体分割和点跟踪等多几何线索，结合基于刚性的运动约束，在初始化阶段估计初步3D场景动态；将场景分解为静态和动态组件，动态部分的高斯使用时间相关的Poly-Fourier曲线表示轨迹

Result: 相比现有方法，MOSAIC-GS实现了显著更快的优化和渲染速度，同时在标准单目动态场景基准测试中保持了与最先进方法相当的重建质量

Conclusion: MOSAIC-GS通过多几何线索引导的初始化策略和参数高效的运动编码，为单目动态场景重建提供了一种计算高效且高质量的解决方案

Abstract: We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate recovery of object geometry and temporal coherence particularly challenging. To address this, we leverage multiple geometric cues, such as depth, optical flow, dynamic object segmentation, and point tracking. Combined with rigidity-based motion constraints, these cues allow us to estimate preliminary 3D scene dynamics during an initialization stage. Recovering scene dynamics prior to the photometric optimization reduces reliance on motion inference from visual appearance alone, which is often ambiguous in monocular settings. To enable compact representations, fast training, and real-time rendering while supporting non-rigid deformations, the scene is decomposed into static and dynamic components. Each Gaussian in the dynamic part of the scene is assigned a trajectory represented as time-dependent Poly-Fourier curve for parameter-efficient motion encoding. We demonstrate that MOSAIC-GS achieves substantially faster optimization and rendering compared to existing methods, while maintaining reconstruction quality on par with state-of-the-art approaches across standard monocular dynamic scene benchmarks.

</details>


### [4] [Ensemble of radiomics and ConvNeXt for breast cancer diagnosis](https://arxiv.org/abs/2601.05373)
*Jorge Alberto Garza-Abdala,Gerardo Alejandro Fumagal-González,Beatriz A. Bosques-Palomo,Mario Alexis Monsivais Molina,Daly Avedano,Servando Cardona-Huerta,José Gerardo Tamez-Pena*

Main category: cs.CV

TL;DR: 本研究评估了放射组学、深度学习和集成方法在乳腺X线筛查中检测癌症的性能，发现集成方法（AUC 0.87）优于单独的深度学习（0.83）和放射组学（0.80）模型。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌早期诊断对提高生存率至关重要。放射组学和深度学习在辅助放射科医生进行早期癌症检测方面显示出巨大潜力，但需要系统评估这些技术的性能。

Method: 使用两个独立数据集：RSNA 2023乳腺癌检测挑战赛（11,913名患者）和墨西哥TecSalud队列（19,400名患者）。训练ConvNeXtV1-small深度学习模型于RSNA数据集并在TecSalud数据集验证；放射组学模型基于TecSalud数据集开发，采用留一年交叉验证；集成方法通过相同方法一致地组合和校准预测。

Result: 集成方法获得最高的曲线下面积（AUC 0.87），优于ConvNeXtV1-small深度学习模型（AUC 0.83）和放射组学模型（AUC 0.80）。

Conclusion: 结合深度学习和放射组学预测的集成方法显著增强了从乳腺X线片中诊断乳腺癌的能力，为临床实践提供了更准确的辅助诊断工具。

Abstract: Early diagnosis of breast cancer is crucial for improving survival rates. Radiomics and deep learning (DL) have shown significant potential in assisting radiologists with early cancer detection. This paper aims to critically assess the performance of radiomics, DL, and ensemble techniques in detecting cancer from screening mammograms. Two independent datasets were used: the RSNA 2023 Breast Cancer Detection Challenge (11,913 patients) and a Mexican cohort from the TecSalud dataset (19,400 patients). The ConvNeXtV1-small DL model was trained on the RSNA dataset and validated on the TecSalud dataset, while radiomics models were developed using the TecSalud dataset and validated with a leave-one-year-out approach. The ensemble method consistently combined and calibrated predictions using the same methodology. Results showed that the ensemble approach achieved the highest area under the curve (AUC) of 0.87, compared to 0.83 for ConvNeXtV1-small and 0.80 for radiomics. In conclusion, ensemble methods combining DL and radiomics predictions significantly enhance breast cancer diagnosis from mammograms.

</details>


### [5] [FlyPose: Towards Robust Human Pose Estimation From Aerial Views](https://arxiv.org/abs/2601.05747)
*Hassaan Farooq,Marvin Brenner,Peter St\ütz*

Main category: cs.CV

TL;DR: FlyPose是一个轻量级自上而下的人体姿态估计流水线，专门用于无人机视角图像，通过多数据集训练在多个测试集上显著提升了检测和姿态估计性能，并在无人机上实现了实时部署。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在人类密集环境中的应用增加（如包裹配送、交通监控等），需要从空中视角准确感知人体姿态和动作，但现有方法在低分辨率、陡峭视角和遮挡情况下面临挑战，特别是需要实时可行模型的应用场景。

Method: 训练和部署FlyPose轻量级自上而下的人体姿态估计流水线，采用多数据集训练策略，并在Jetson Orin AGX开发套件上实现实时推理，同时创建并发布了FlyPose-104数据集，包含从困难空中视角的手动标注。

Result: 在Manipal-UAV、VisDrone、HIT-UAV等测试集上平均提升了6.8 mAP的人体检测性能；在具有挑战性的UAV-Human数据集上提升了16.3 mAP的2D人体姿态估计性能；在Jetson Orin AGX上实现了约20毫秒的推理延迟（包括预处理），并在四旋翼无人机上成功部署。

Conclusion: FlyPose是一个高效、轻量级的解决方案，能够应对无人机视角下人体姿态估计的挑战，实现了实时性能并在实际飞行实验中成功部署，同时通过发布FlyPose-104数据集为社区提供了有价值的资源。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.

</details>


### [6] [SceneFoundry: Generating Interactive Infinite 3D Worlds](https://arxiv.org/abs/2601.05810)
*ChunTeng Chen,YiChen Hsu,YiWen Liu,WeiFang Sun,TsaiChing Ni,ChunYi Lee,Min Sun,YuanFu Yang*

Main category: cs.CV

TL;DR: SceneFoundry是一个语言引导的扩散框架，用于生成公寓规模的3D环境，包含功能性的可动家具和语义多样的布局，用于机器人训练。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法难以捕捉真实室内环境的功能复杂性，特别是包含对机器人物体操作和导航至关重要的可动部件的物体。自动生成大规模、交互式、物理真实的3D环境对推进机器人学习和具身智能至关重要。

Method: 使用语言引导的扩散框架：1) LLM模块控制楼层布局生成；2) 基于扩散的后验采样从大规模3D资源库中高效填充场景中的可动资产；3) 采用可微分指导函数确保物理可用性，包括调节物体数量、防止关节碰撞、保持足够的机器人可通行空间。

Result: 实验表明该框架能够生成结构有效、语义连贯、功能交互的环境，适用于多种场景类型和条件，支持可扩展的具身AI研究。

Conclusion: SceneFoundry能够生成包含功能性可动家具的大规模3D环境，为机器人训练和具身智能研究提供了可扩展的解决方案。

Abstract: The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.

</details>


### [7] [Multi-Image Super Resolution Framework for Detection and Analysis of Plant Roots](https://arxiv.org/abs/2601.05482)
*Shubham Agarwal,Ofek Nourian,Michael Sidorov,Sharon Chemweno,Ofer Hadar,Naftali Lazarovitch,Jhonathan E. Ephrath*

Main category: cs.CV

TL;DR: 提出了一种用于地下植物根系成像的多图像超分辨率深度学习框架，通过多视角融合提升根系可见性和细节，优于现有超分辨率方法，支持根系性状量化分析。


<details>
  <summary>Details</summary>
Motivation: 地下植物根系成像面临遮挡、土壤湿度变化和低对比度等挑战，传统视觉方法效果有限，需要新的成像技术来改善根系可见性和细节，以支持土壤-植物相互作用、养分吸收和植物健康研究。

Method: 提出了一种地下成像系统，捕获植物根系的多重叠视图，并集成了基于深度学习的多图像超分辨率框架。构建了模拟真实地下成像场景的合成数据集，包含影响图像质量的关键环境因素。MISR算法利用视图间的空间冗余性重建高分辨率图像。

Result: 定量评估表明，该方法优于最先进的超分辨率基线，BRISQUE指标降低了2.3%，CLIP-IQA评分相同，表明图像质量得到改善。能够实现根系性状的准确估计，包括根毛数量和根毛密度。

Conclusion: 该框架为农业和生态研究中的稳健自动地下植物根系成像和性状量化提供了有前景的方向，有助于增强根系表型分析能力。

Abstract: Understanding plant root systems is critical for advancing research in soil-plant interactions, nutrient uptake, and overall plant health. However, accurate imaging of roots in subterranean environments remains a persistent challenge due to adverse conditions such as occlusion, varying soil moisture, and inherently low contrast, which limit the effectiveness of conventional vision-based approaches. In this work, we propose a novel underground imaging system that captures multiple overlapping views of plant roots and integrates a deep learning-based Multi-Image Super Resolution (MISR) framework designed to enhance root visibility and detail. To train and evaluate our approach, we construct a synthetic dataset that simulates realistic underground imaging scenarios, incorporating key environmental factors that affect image quality. Our proposed MISR algorithm leverages spatial redundancy across views to reconstruct high-resolution images with improved structural fidelity and visual clarity. Quantitative evaluations show that our approach outperforms state-of-the-art super resolution baselines, achieving a 2.3 percent reduction in BRISQUE, indicating improved image quality with the same CLIP-IQA score, thereby enabling enhanced phenotypic analysis of root systems. This, in turn, facilitates accurate estimation of critical root traits, including root hair count and root hair density. The proposed framework presents a promising direction for robust automatic underground plant root imaging and trait quantification for agricultural and ecological research.

</details>


### [8] [Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals](https://arxiv.org/abs/2601.05848)
*Nate Gillman,Yinghua Zhou,Zitian Tang,Evan Luo,Arjan Chakravarthy,Daksh Aggarwal,Michael Freeman,Charles Herrmann,Chen Sun*

Main category: cs.CV

TL;DR: Goal Force框架通过力向量和中间动力学定义目标，训练视频生成模型模拟物理交互，实现零样本泛化到复杂现实场景


<details>
  <summary>Details</summary>
Motivation: 现有视频生成世界模型在目标指定方面存在挑战：文本指令过于抽象难以捕捉物理细节，目标图像对于动态任务往往不可行。需要一种更符合人类物理任务概念化的目标定义方式。

Method: 提出Goal Force框架，允许用户通过明确的力向量和中间动力学定义目标。在合成的因果原语数据集（如弹性碰撞、多米诺骨牌倒下）上训练视频生成模型，教会模型在时间和空间中传播力。

Result: 尽管在简单物理数据上训练，模型展现出显著的零样本泛化能力，能够处理复杂现实场景，包括工具操作和多对象因果链。模型表现出作为隐式神经物理模拟器的能力。

Conclusion: 通过将视频生成基于基本物理交互，模型可以作为隐式神经物理模拟器，实现精确的物理感知规划，无需依赖外部物理引擎。这为机器人学和规划任务提供了新的可能性。

Abstract: Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.

</details>


### [9] [Hippocampal Atrophy Patterns Across the Alzheimer's Disease Spectrum: A Voxel-Based Morphometry Analysis](https://arxiv.org/abs/2601.05494)
*Trishna Niraula*

Main category: cs.CV

TL;DR: 使用CAT12/SPM12体素形态测量法分析249名ADNI参与者的基线T1加权MRI，发现AD患者相对于认知正常者和轻度认知障碍者存在显著海马萎缩，海马体积对MCI向AD转化具有中等预测价值，APOE4状态对海马体积无显著影响。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病和轻度认知障碍与灰质损失相关，特别是在内侧颞叶结构。本研究旨在通过体素形态测量法分析灰质体积变化，探索AD进展的关键特征、预测生物标志物和遗传影响。

Method: 使用CAT12/SPM12体素形态测量法分析249名ADNI参与者的基线T1加权MRI扫描（认知正常90人，MCI 129人，AD 30人）。采用一般线性模型分析灰质体积，以诊断组为主要预测因子，年龄和总颅内体积为协变量。统计图阈值设为p<0.001（体素水平），使用家族误差校正进行多重比较校正（p<0.05）。

Result: AD患者相对于认知正常者和MCI患者存在显著海马萎缩（Cohen's d分别为2.03和1.61）。海马体积对MCI向AD转化具有中等预测价值（AUC=0.66）。按APOE4状态分层分析未发现遗传因素对海马体积的显著影响。

Conclusion: 内侧颞叶退行性变是AD进展的关键特征，海马体积可作为AD进展的预测生物标志物，但APOE4状态对海马体积无显著影响，为理解AD的病理机制和预测指标提供了重要见解。

Abstract: Alzheimer's disease (AD) and mild cognitive impairment (MCI) are associated with progressive gray matter loss, particularly in medial temporal structures. In this study, CAT12/SPM12 voxel-based morphometry was applied to baseline T1-weighted MRI scans from 249 ADNI participants (CN = 90, MCI = 129, AD = 30). Gray matter volume was analyzed using a general linear model, with the diagnostic group as primary predictor and age and total intracranial volume as covariates. Statistical maps were thresholded at p < 0.001 (voxelwise) and corrected for multiple comparisons at the cluster level using family-wise error (FWE) correction (p < 0.05). Significant hippocampal atrophy was observed in AD relative to CN and MCI (Cohen's d = 2.03 and 1.61, respectively). Hippocampal volume demonstrated moderate predictive value for conversion from MCI to AD (AUC = 0.66). Stratification by APOE4 status did not reveal significant genetic effects on cross-sectional hippocampal volume. These results support medial temporal degeneration as a key feature of AD progression and provide insights into predictive biomarkers and genetic influences.

</details>


### [10] [MMViR: A Multi-Modal and Multi-Granularity Representation for Long-range Video Understanding](https://arxiv.org/abs/2601.05495)
*Zizhong Li,Haopeng Zhang,Jiawei Zhang*

Main category: cs.CV

TL;DR: MMViR是一种用于长视频理解的新型多模态、多粒度结构化表示方法，通过关键转折点分割视频并构建三级描述，在保持计算效率的同时显著提升长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在处理长视频时面临挑战：直接编码计算成本过高，简单视频转文本方法会产生冗余或碎片化内容，难以处理长视频中的复杂事件、多样场景和长程依赖关系。

Method: 提出MMViR多模态多粒度结构化表示：1）识别关键转折点分割视频；2）构建三级描述耦合全局叙事与细粒度视觉细节；3）支持基于查询的高效检索并具有良好的泛化能力。

Result: 在QA、摘要和检索三个任务上的广泛评估显示，MMViR超越了先前最强方法，在小时级视频理解上实现了19.67%的性能提升，同时将处理延迟降低到原始方法的45.4%。

Conclusion: MMViR通过结构化表示有效解决了长视频理解的计算效率和内容质量难题，为长视频多模态理解提供了高效可行的解决方案。

Abstract: Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.

</details>


### [11] [Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification](https://arxiv.org/abs/2601.05498)
*Samuel E. Johnny,Bernes L. Atabonfack,Israel Alagbe,Assane Gueye*

Main category: cs.CV

TL;DR: 该研究提出了一种基于Segment Anything Model（SAM）的多任务深度学习框架，用于乳腺癌超声图像中的病灶分割和诊断分类，在PRECISE 2025数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌超声（BUS）成像中的肿瘤分割和分类面临低对比度、斑点噪声和病灶形态多样等挑战，需要更准确的分析方法。

Method: 采用多任务深度学习框架，利用SAM视觉编码器的嵌入特征，通过无提示的完全监督方式进行病灶分割和诊断分类。分割分支使用轻量级卷积头或UNet风格解码器，分类分支通过掩码引导注意力机制聚焦病灶相关特征。

Result: 在PRECISE 2025乳腺癌超声数据集上，该方法获得了0.887的Dice相似系数和92.3%的准确率，在PRECISE挑战排行榜中名列前茅。

Conclusion: SAM表示与分割引导学习相结合，能显著改善乳腺癌超声成像中的病灶描绘和诊断预测，为医学图像分析提供了有效解决方案。

Abstract: Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.

</details>


### [12] [Enabling Stroke-Level Structural Analysis of Hieroglyphic Scripts without Language-Specific Priors](https://arxiv.org/abs/2601.05508)
*Fuwen Luo,Zihao Wan,Ziyue Wang,Yaluo Liu,Pau Tong Lin Xu,Xuanjia Qiao,Xiaolong Wang,Peng Li,Yang Liu*

Main category: cs.CV

TL;DR: HieroSA是一个新颖的通用框架，使多模态大语言模型能够从字符位图自动推导笔画级结构，无需手工标注数据，适用于现代表意文字和古代象形文字。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型和多模态大语言模型在处理象形文字等表意文字时存在结构盲区：LLMs将字符处理为文本标记，MLLMs将其视为原始像素网格，两者都无法建模字符笔画的基本逻辑。现有结构分析方法通常是特定于某种文字的且劳动密集型。

Method: 提出Hieroglyphic Stroke Analyzer (HieroSA)框架，将字符图像转换为归一化坐标空间中的显式、可解释的线段表示，实现跨语言泛化，无需语言特定的先验知识。

Result: 大量实验表明，HieroSA能有效捕捉字符内部结构和语义，绕过对语言特定先验的需求。实验结果突显了该工作作为字形分析工具的潜力，用于更深入理解象形文字脚本。

Conclusion: HieroSA为多模态大语言模型提供了一种通用框架，能够自动从字符位图推导笔画级结构，无需手工数据，为表意文字的结构分析提供了新的解决方案。

Abstract: Hieroglyphs, as logographic writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and Multimodal LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to model the underlying logic of character strokes. Furthermore, existing structural analysis methods are often script-specific and labor-intensive. In this paper, we propose Hieroglyphic Stroke Analyzer (HieroSA), a novel and generalizable framework that enables MLLMs to automatically derive stroke-level structures from character bitmaps without handcrafted data. It transforms modern logographic and ancient hieroglyphs character images into explicit, interpretable line-segment representations in a normalized coordinate space, allowing for cross-lingual generalization. Extensive experiments demonstrate that HieroSA effectively captures character-internal structures and semantics, bypassing the need for language-specific priors. Experimental results highlight the potential of our work as a graphematics analysis tool for a deeper understanding of hieroglyphic scripts. View our code at https://github.com/THUNLP-MT/HieroSA.

</details>


### [13] [GaussianSwap: Animatable Video Face Swapping with 3D Gaussian Splatting](https://arxiv.org/abs/2601.05511)
*Xuan Cheng,Jiahao Rao,Chengyang Li,Wenhao Wang,Weilin Chen,Lvqing Yang*

Main category: cs.CV

TL;DR: GaussianSwap是一个基于3D高斯溅射的视频人脸交换框架，通过构建目标视频的3D人脸化身并转移源图像身份，实现高质量、可交互的人脸交换。


<details>
  <summary>Details</summary>
Motivation: 传统视频人脸交换框架仅限于生成基于像素的面部表示，交换后的人脸只是一组无结构的像素，缺乏动画或交互操作能力。需要从传统的基于像素的视频生成转向创建具有交换人脸的高保真化身。

Method: 框架首先预处理目标视频提取FLAME参数、相机姿态和分割掩码，然后将3D高斯溅射绑定到跨帧的FLAME模型上，实现动态面部控制。为保持身份一致性，提出从三个最先进的人脸识别模型构建复合身份嵌入用于化身微调。最后将交换后的化身渲染到背景帧上得到最终视频。

Result: 实验结果表明，GaussianSwap在身份保持、视觉清晰度和时间一致性方面表现优异，同时实现了以前无法实现的交互应用。

Conclusion: GaussianSwap实现了从传统像素级视频生成到创建高保真交换人脸化身的范式转变，提供了更好的身份保持、视觉质量和交互能力。

Abstract: We introduce GaussianSwap, a novel video face swapping framework that constructs a 3D Gaussian Splatting based face avatar from a target video while transferring identity from a source image to the avatar. Conventional video swapping frameworks are limited to generating facial representations in pixel-based formats. The resulting swapped faces exist merely as a set of unstructured pixels without any capacity for animation or interactive manipulation. Our work introduces a paradigm shift from conventional pixel-based video generation to the creation of high-fidelity avatar with swapped faces. The framework first preprocesses target video to extract FLAME parameters, camera poses and segmentation masks, and then rigs 3D Gaussian splats to the FLAME model across frames, enabling dynamic facial control. To ensure identity preserving, we propose an compound identity embedding constructed from three state-of-the-art face recognition models for avatar finetuning. Finally, we render the face-swapped avatar on the background frames to obtain the face-swapped video. Experimental results demonstrate that GaussianSwap achieves superior identity preservation, visual clarity and temporal consistency, while enabling previously unattainable interactive applications.

</details>


### [14] [DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion](https://arxiv.org/abs/2601.05538)
*Yiming Sun,Zifan Ye,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: DIFF-MF：一种基于差异驱动的通道-空间状态空间模型的多模态图像融合方法，通过模态间特征差异图指导特征提取，在通道和空间维度进行融合，在保持线性计算复杂度的同时有效整合互补信息。


<details>
  <summary>Details</summary>
Motivation: 现有基于状态空间模型的多模态图像融合方法存在两个问题：要么过度优先考虑红外强度而牺牲可见光细节，要么保留可见光结构但降低热目标显著性。需要克服这些挑战以实现更好的融合效果。

Method: 提出DIFF-MF方法：1）利用模态间特征差异图指导特征提取；2）通道维度上使用通道交换模块，通过交叉注意力双状态空间建模增强通道间交互；3）空间维度上使用空间交换模块，通过跨模态状态空间扫描实现全面空间融合。

Result: 在驾驶场景和低空无人机数据集上的实验结果表明，该方法在视觉质量和定量评估方面均优于现有方法，同时保持线性计算复杂度。

Conclusion: DIFF-MF通过差异驱动的通道-空间状态空间模型，有效解决了现有方法在红外强度与可见光细节平衡方面的问题，实现了高质量的多模态图像融合。

Abstract: Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.

</details>


### [15] [MoGen: A Unified Collaborative Framework for Controllable Multi-Object Image Generation](https://arxiv.org/abs/2601.05546)
*Yanfeng Li,Yue Sun,Keren Fu,Sio-Kei Im,Xiaoming Liu,Guangtao Zhai,Xiaohong Liu,Tao Tan*

Main category: cs.CV

TL;DR: MoGen：一种用户友好的多对象图像生成方法，通过区域语义锚定模块实现语言描述与图像区域的精确对齐，并引入自适应多模态引导模块整合多源控制信号，提升生成质量、数量一致性和细粒度控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有多对象图像生成方法难以实现语言描述与图像区域的精确对齐，常导致对象数量不一致和属性混淆。主流方法依赖外部控制信号约束空间布局，但输入格式僵化，无法适应用户异构资源条件和多样化约束需求。

Method: 1. 设计区域语义锚定（RSA）模块，在生成过程中将语言描述中的短语单元精确锚定到对应图像区域，实现遵循对象数量规格的文本到图像生成。2. 引入自适应多模态引导（AMG）模块，自适应解析和整合多源控制信号的多种组合，形成结构化意图，指导对场景布局和对象属性的选择性约束。

Result: 实验结果表明，MoGen在生成质量、数量一致性和细粒度控制方面显著优于现有方法，同时展现出优越的可访问性和控制灵活性。

Conclusion: MoGen通过区域语义锚定和自适应多模态引导，解决了多对象图像生成中的对齐和控制问题，提供了一种用户友好且灵活的解决方案，代码已开源。

Abstract: Existing multi-object image generation methods face difficulties in achieving precise alignment between localized image generation regions and their corresponding semantics based on language descriptions, frequently resulting in inconsistent object quantities and attribute aliasing. To mitigate this limitation, mainstream approaches typically rely on external control signals to explicitly constrain the spatial layout, local semantic and visual attributes of images. However, this strong dependency makes the input format rigid, rendering it incompatible with the heterogeneous resource conditions of users and diverse constraint requirements. To address these challenges, we propose MoGen, a user-friendly multi-object image generation method. First, we design a Regional Semantic Anchor (RSA) module that precisely anchors phrase units in language descriptions to their corresponding image regions during the generation process, enabling text-to-image generation that follows quantity specifications for multiple objects. Building upon this foundation, we further introduce an Adaptive Multi-modal Guidance (AMG) module, which adaptively parses and integrates various combinations of multi-source control signals to formulate corresponding structured intent. This intent subsequently guides selective constraints on scene layouts and object attributes, achieving dynamic fine-grained control. Experimental results demonstrate that MoGen significantly outperforms existing methods in generation quality, quantity consistency, and fine-grained control, while exhibiting superior accessibility and control flexibility. Code is available at: https://github.com/Tear-kitty/MoGen/tree/master.

</details>


### [16] [VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck](https://arxiv.org/abs/2601.05547)
*Feiran Zhang,Yixin Wu,Zhenghua Wang,Xiaohua Wang,Changze Lv,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.CV

TL;DR: VIB-Probe：基于变分信息瓶颈理论的视觉语言模型幻觉检测与缓解框架，通过分析内部注意力头信号来识别和减少文本生成中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态任务中表现出色，但容易产生幻觉（生成文本与视觉内容不符）。现有方法主要依赖输出logits或外部验证工具，忽略了内部机制。研究发现特定注意力头携带真实生成的主要信号，但直接探测这些高维状态很困难，因为视觉语言语法和噪声纠缠在一起。

Method: 提出VIB-Probe框架，利用变分信息瓶颈理论提取跨层和跨头的判别模式，同时通过信息瓶颈原理过滤语义噪声。利用VIB探针的梯度识别对幻觉有强因果影响的注意力头，并引入推理时干预策略来缓解幻觉。

Result: 在多个基准测试上的广泛实验表明，VIB-Probe在幻觉检测和缓解两个设置上都显著优于现有基线方法。

Conclusion: VIB-Probe通过分析视觉语言模型的内部注意力机制，提供了一种有效的幻觉检测和缓解方法，优于现有方法，代码将公开提供。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal tasks, but remain susceptible to hallucinations, where generated text deviates from the underlying visual content. Existing hallucination detection methods primarily rely on output logits or external verification tools, often overlooking their internal mechanisms. In this work, we investigate the outputs of internal attention heads, postulating that specific heads carry the primary signals for truthful generation.However, directly probing these high-dimensional states is challenging due to the entanglement of visual-linguistic syntax and noise. To address this, we propose VIB-Probe, a novel hallucination detection and mitigation framework leveraging the Variational Information Bottleneck (VIB) theory. Our method extracts discriminative patterns across layers and heads while filtering out semantic nuisances through the information bottleneck principle. Furthermore, by leveraging the gradients of our VIB probe, we identify attention heads with strong causal influence on hallucinations and introduce an inference-time intervention strategy for hallucination mitigation. Extensive experiments across diverse benchmarks demonstrate that VIB-Probe significantly outperforms existing baselines in both settings. Our code will be made publicly available.

</details>


### [17] [One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection](https://arxiv.org/abs/2601.05552)
*Bin-Bin Gao,Chengjie Wang*

Main category: cs.CV

TL;DR: UniADet提出了一种极其简单的通用视觉异常检测框架，通过解耦分类和分割任务以及跨层特征，仅需学习少量参数，在零样本/少样本设置下超越现有方法


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言基础模型的异常检测方法存在复杂提示工程、精细适配模块和困难训练策略等问题，限制了方法的灵活性和通用性。本文重新思考视觉-语言模型在异常检测中的基本机制，旨在解决这些问题。

Method: 首先发现语言编码器在异常分类和分割中仅用于生成决策权重，证明其在通用异常检测中不必要。然后提出简单方法完全解耦分类和分割任务，并解耦跨层特征，即为不同任务和层次特征学习独立权重。

Result: UniADet在14个真实世界异常检测基准测试中（涵盖工业和医疗领域）大幅超越最先进的零样本/少样本方法，甚至首次超越了全样本异常检测方法。该方法仅需0.002M可学习参数，具有高度简单性、参数效率和通用性。

Conclusion: UniADet通过解耦设计提供了一个极其简单、通用且有效的通用视觉异常检测框架，无需复杂提示工程或精细适配，在多个领域基准测试中表现出色，为异常检测研究提供了新的思路。

Abstract: Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.

</details>


### [18] [Semi-Supervised Facial Expression Recognition based on Dynamic Threshold and Negative Learning](https://arxiv.org/abs/2601.05556)
*Zhongpeng Cai,Jun Yu,Wei Xu,Tianyu Liu,Jianqing Sun,Jiaen Liang*

Main category: cs.CV

TL;DR: 提出基于动态阈值调整和选择性负学习的半监督面部表情识别算法，在RAF-DB和AffectNet数据集上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 面部表情识别是人机交互和情感计算的关键任务，但获取大量标注数据成本高昂，因此需要设计能够充分利用标注和未标注数据的半监督算法

Method: 1) 特征提取阶段采用局部注意力增强和特征图随机丢弃策略；2) 引入动态阈值调整方法适应半监督学习框架；3) 通过选择性负学习策略从低置信度未标注样本中挖掘有用信息

Result: 在RAF-DB和AffectNet数据集上取得最先进性能，即使不使用完整数据集也能超越全监督方法

Conclusion: 提出的半监督面部表情识别算法通过动态阈值调整和选择性负学习有效利用了未标注数据，证明了方法的有效性

Abstract: Facial expression recognition is a key task in human-computer interaction and affective computing. However, acquiring a large amount of labeled facial expression data is often costly. Therefore, it is particularly important to design a semi-supervised facial expression recognition algorithm that makes full use of both labeled and unlabeled data. In this paper, we propose a semi-supervised facial expression recognition algorithm based on Dynamic Threshold Adjustment (DTA) and Selective Negative Learning (SNL). Initially, we designed strategies for local attention enhancement and random dropout of feature maps during feature extraction, which strengthen the representation of local features while ensuring the model does not overfit to any specific local area. Furthermore, this study introduces a dynamic thresholding method to adapt to the requirements of the semi-supervised learning framework for facial expression recognition tasks, and through a selective negative learning strategy, it fully utilizes unlabeled samples with low confidence by mining useful expression information from complementary labels, achieving impressive results. We have achieved state-of-the-art performance on the RAF-DB and AffectNet datasets. Our method surpasses fully supervised methods even without using the entire dataset, which proves the effectiveness of our approach.

</details>


### [19] [What's Left Unsaid? Detecting and Correcting Misleading Omissions in Multimodal News Previews](https://arxiv.org/abs/2601.05563)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Dayang Li,Herun Wan,Wei Zhou,Min-Yen Kan*

Main category: cs.CV

TL;DR: 该研究开发了一个多阶段管道来检测社交媒体新闻预览中的选择性省略误导，构建了MM-Misleading基准，并提出了OMGuard系统来改进多模态误导性检测和内容修正。


<details>
  <summary>Details</summary>
Motivation: 社交媒体新闻预览（图像-标题对）即使事实正确，也可能通过选择性省略关键上下文导致读者形成与完整文章不同的判断，这种"解释漂移"的隐性危害比显性虚假信息更难检测且研究不足。

Method: 开发多阶段管道来解构和模拟预览理解与上下文理解，构建MM-Misleading基准；提出OMGuard系统，包括(1)解释感知微调改进多模态误导性检测，(2)理性引导的误导内容修正，使用明确推理指导标题重写。

Result: OMGuard将8B模型的检测准确率提升到与235B LVLM相当的水平，并显著增强端到端修正能力；分析显示误导性通常源于局部叙事转变而非全局框架变化，并识别出纯文本修正失败而需要视觉干预的场景。

Conclusion: 选择性省略导致的社交媒体新闻预览误导是一个重要但研究不足的问题，需要专门的多模态检测和修正方法；OMGuard系统在检测和修正方面都表现出色，揭示了误导机制的特征和视觉干预的必要性。

Abstract: Even when factually correct, social-media news previews (image-headline pairs) can induce interpretation drift: by selectively omitting crucial context, they lead readers to form judgments that diverge from what the full article conveys. This covert harm is harder to detect than explicit misinformation yet remains underexplored. To address this gap, we develop a multi-stage pipeline that disentangles and simulates preview-based versus context-based understanding, enabling construction of the MM-Misleading benchmark. Using this benchmark, we systematically evaluate open-source LVLMs and uncover pronounced blind spots to omission-based misleadingness detection. We further propose OMGuard, which integrates (1) Interpretation-Aware Fine-Tuning, which used to improve multimodal misleadingness detection and (2) Rationale-Guided Misleading Content Correction, which uses explicit rationales to guide headline rewriting and reduce misleading impressions. Experiments show that OMGuard lifts an 8B model's detection accuracy to match a 235B LVLM and delivers markedly stronger end-to-end correction. Further analysis reveals that misleadingness typically stems from local narrative shifts (e.g., missing background) rather than global frame changes, and identifies image-driven scenarios where text-only correction fails, highlighting the necessity of visual interventions.

</details>


### [20] [Towards Generalized Multi-Image Editing for Unified Multimodal Models](https://arxiv.org/abs/2601.05572)
*Pengcheng Xu,Peng Tang,Donghao Luo,Xiaobin Hu,Weichu Cui,Qingdong He,Zhennan Chen,Jiangning Zhang,Charles Ling,Boyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的多图像编辑框架，通过可学习的潜在分离器和正弦索引编码技术，解决了统一多模态模型在多图像编辑中视觉一致性和细节区分的问题。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型在多图像编辑中存在视觉一致性不足和视觉线索区分困难的问题，特别是在处理多个输入图像时难以准确区分图像身份和保持细节一致性。

Method: 提出了两种创新算法：1) 可学习的潜在分离器，在潜在空间中明确区分每个参考图像，实现准确解耦的条件控制；2) 正弦索引编码，为同一图像的视觉标记分配连续的正弦索引嵌入，提供明确的图像身份标识，同时支持可变数量输入的泛化和外推。

Result: 实验表明，在多样化的多图像编辑任务上，相比先前基线方法，在语义一致性、视觉保真度和跨图像整合方面都有明显改进，验证了方法在一致性和泛化能力上的优势。

Conclusion: 提出的可扩展多图像编辑框架通过创新的潜在分离和索引编码技术，有效解决了UMMs在多图像编辑中的视觉一致性和细节区分问题，为多图像编辑任务提供了更好的解决方案。

Abstract: Unified Multimodal Models (UMMs) integrate multimodal understanding and generation, yet they are limited to maintaining visual consistency and disambiguating visual cues when referencing details across multiple input images. In this work, we propose a scalable multi-image editing framework for UMMs that explicitly distinguishes image identities and generalizes to variable input counts. Algorithmically, we introduce two innovations: 1) The learnable latent separators explicitly differentiate each reference image in the latent space, enabling accurate and disentangled conditioning. 2) The sinusoidal index encoding assigns visual tokens from the same image a continuous sinusoidal index embedding, which provides explicit image identity while allowing generalization and extrapolation on a variable number of inputs. To facilitate training and evaluation, we establish a high-fidelity benchmark using an inverse dataset construction methodology to guarantee artifact-free, achievable outputs. Experiments show clear improvements in semantic consistency, visual fidelity, and cross-image integration over prior baselines on diverse multi-image editing tasks, validating our advantages on consistency and generalization ability.

</details>


### [21] [Orient Anything V2: Unifying Orientation and Rotation Understanding](https://arxiv.org/abs/2601.05573)
*Zehan Wang,Ziang Zhang,Jiayang Xu,Jialei Wang,Tianyu Pang,Chao Du,HengShuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: Orient Anything V2是一个增强的基础模型，用于从单张或配对图像中统一理解物体的3D方向和旋转，相比V1版本能处理更多样的旋转对称性并直接估计相对旋转。


<details>
  <summary>Details</summary>
Motivation: 现有方向估计模型在处理具有不同旋转对称性的物体时存在局限性，需要扩展模型能力以处理更广泛的物体类别和更复杂的旋转场景。

Method: 通过四个关键创新：1) 使用生成模型合成可扩展的3D资产；2) 高效模型在环标注系统识别有效前表面；3) 对称感知的周期性分布拟合目标；4) 多帧架构直接预测相对物体旋转。

Result: 在11个广泛使用的基准测试中，Orient Anything V2在方向估计、6DoF姿态估计和物体对称性识别方面实现了最先进的零样本性能。

Conclusion: 该模型展示了强大的泛化能力，显著拓宽了方向估计在各种下游任务中的适用性。

Abstract: This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.

</details>


### [22] [Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection](https://arxiv.org/abs/2601.05580)
*Hanyi Wang,Jun Lan,Yaoyu Kang,Huijia Zhu,Weiqiang Wang,Zhuosheng Zhang,Shilin Wang*

Main category: cs.CV

TL;DR: 提出三阶段领域持续学习框架，用于持续适应不断演化的AI生成图像检测，在27个生成模型基准上取得优异性能


<details>
  <summary>Details</summary>
Motivation: AI生成图像的恶意滥用和广泛传播威胁在线信息真实性，现有检测方法难以泛化到未见过的生成模型，且生成技术快速演进加剧了这一挑战，缺乏适应性的检测模型在现实应用中可能失效

Method: 三阶段领域持续学习框架：1) 参数高效微调构建可迁移的离线检测模型；2) 集成未见数据流到持续学习过程，使用渐进复杂度数据增强链和K-FAC方法缓解灾难性遗忘；3) 基于线性模式连接的线性插值策略捕捉不同生成模型的共性

Result: 离线检测器在平均精度上超越领先基线+5.51%，持续学习策略达到92.20%的平均准确率，优于最先进方法

Conclusion: 提出的三阶段框架能有效应对AI生成图像检测中的模型泛化和持续适应挑战，在模拟真实世界场景的27个生成模型基准上表现出色

Abstract: The malicious misuse and widespread dissemination of AI-generated images pose a significant threat to the authenticity of online information. Current detection methods often struggle to generalize to unseen generative models, and the rapid evolution of generative techniques continuously exacerbates this challenge. Without adaptability, detection models risk becoming ineffective in real-world applications. To address this critical issue, we propose a novel three-stage domain continual learning framework designed for continuous adaptation to evolving generative models. In the first stage, we employ a strategic parameter-efficient fine-tuning approach to develop a transferable offline detection model with strong generalization capabilities. Building upon this foundation, the second stage integrates unseen data streams into a continual learning process. To efficiently learn from limited samples of novel generated models and mitigate overfitting, we design a data augmentation chain with progressively increasing complexity. Furthermore, we leverage the Kronecker-Factored Approximate Curvature (K-FAC) method to approximate the Hessian and alleviate catastrophic forgetting. Finally, the third stage utilizes a linear interpolation strategy based on Linear Mode Connectivity, effectively capturing commonalities across diverse generative models and further enhancing overall performance. We establish a comprehensive benchmark of 27 generative models, including GANs, deepfakes, and diffusion models, chronologically structured up to August 2024 to simulate real-world scenarios. Extensive experiments demonstrate that our initial offline detectors surpass the leading baseline by +5.51% in terms of mean average precision. Our continual learning strategy achieves an average accuracy of 92.20%, outperforming state-of-the-art methods.

</details>


### [23] [GS-DMSR: Dynamic Sensitive Multi-scale Manifold Enhancement for Accelerated High-Quality 3D Gaussian Splatting](https://arxiv.org/abs/2601.05584)
*Nengbo Lu,Minghua Pan,Shaohua Sun,Yizhou Liang*

Main category: cs.CV

TL;DR: GS-DMSR方法通过自适应梯度聚焦和多尺度流形增强模块，在3D动态场景重建中平衡收敛速度与渲染质量，实现96 FPS的高帧率并减少存储和训练开销。


<details>
  <summary>Details</summary>
Motivation: 3D动态场景重建领域长期面临模型收敛速度与渲染质量平衡的挑战，特别是在复杂动态运动场景的高精度建模中，这一问题亟待解决。

Method: 提出GS-DMSR方法：1）通过定量分析高斯属性动态演化过程实现自适应梯度聚焦，动态识别高斯模型运动状态差异并应用差异化优化策略；2）集成多尺度流形增强模块，利用隐式非线性解码器和显式变形场的协同优化增强复杂变形场景建模效率。

Result: 在合成数据集上达到96 FPS的帧率，同时有效减少了存储开销和训练时间。

Conclusion: 该方法成功解决了3D动态场景重建中收敛速度与渲染质量的平衡问题，为复杂动态运动场景的高效建模提供了有效解决方案。

Abstract: In the field of 3D dynamic scene reconstruction, how to balance model convergence rate and rendering quality has long been a critical challenge that urgently needs to be addressed, particularly in high-precision modeling of scenes with complex dynamic motions. To tackle this issue, this study proposes the GS-DMSR method. By quantitatively analyzing the dynamic evolution process of Gaussian attributes, this mechanism achieves adaptive gradient focusing, enabling it to dynamically identify significant differences in the motion states of Gaussian models. It then applies differentiated optimization strategies to Gaussian models with varying degrees of significance, thereby significantly improving the model convergence rate. Additionally, this research integrates a multi-scale manifold enhancement module, which leverages the collaborative optimization of an implicit nonlinear decoder and an explicit deformation field to enhance the modeling efficiency for complex deformation scenes. Experimental results demonstrate that this method achieves a frame rate of up to 96 FPS on synthetic datasets, while effectively reducing both storage overhead and training time.Our code and data are available at https://anonymous.4open.science/r/GS-DMSR-2212.

</details>


### [24] [SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes](https://arxiv.org/abs/2601.05600)
*Chuhan Wang,Xintong Li,Jennifer Yuntong Zhang,Junda Wu,Chengkai Huang,Lina Yao,Julian McAuley,Jingbo Shang*

Main category: cs.CV

TL;DR: SceneAlign框架利用场景图进行结构化视觉干预，通过构建对比样本优化多模态大语言模型的视觉推理忠实度


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂视觉场景中经常出现推理不忠实的问题，表现为幻觉实体、错误关系、跳过步骤和过度指定推理。现有基于偏好的方法依赖文本扰动或答案条件化推理，无法解决这一问题，因为模型可以利用语言先验绕过视觉基础。

Method: 提出SceneAlign框架，利用场景图作为结构化视觉信息进行可控的结构干预。通过识别推理关键节点，采用四种针对性策略扰动这些节点，模拟典型的视觉基础失败情况，构建语言上合理但视觉事实不准确的硬负样本推理。这些对比样本用于直接偏好优化，引导模型进行细粒度、结构忠实的推理。

Result: 在七个视觉推理基准测试中，SceneAlign持续提高了答案准确性和推理忠实度，证明了基于视觉基础的对齐方法在多模态推理中的有效性。

Conclusion: SceneAlign通过结构化视觉干预和对比学习，有效解决了多模态大语言模型在复杂视觉场景中的推理不忠实问题，为提升视觉推理的准确性和可靠性提供了新方法。

Abstract: Multimodal large language models often struggle with faithful reasoning in complex visual scenes, where intricate entities and relations require precise visual grounding at each step. This reasoning unfaithfulness frequently manifests as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning. Existing preference-based approaches, typically relying on textual perturbations or answer-conditioned rationales, fail to address this challenge as they allow models to exploit language priors to bypass visual grounding. To address this, we propose SceneAlign, a framework that leverages scene graphs as structured visual information to perform controllable structural interventions. By identifying reasoning-critical nodes and perturbing them through four targeted strategies that mimic typical grounding failures, SceneAlign constructs hard negative rationales that remain linguistically plausible but are grounded in inaccurate visual facts. These contrastive pairs are used in Direct Preference Optimization to steer models toward fine-grained, structure-faithful reasoning. Across seven visual reasoning benchmarks, SceneAlign consistently improves answer accuracy and reasoning faithfulness, highlighting the effectiveness of grounding-aware alignment for multimodal reasoning.

</details>


### [25] [Learning Geometric Invariance for Gait Recognition](https://arxiv.org/abs/2601.05604)
*Zengbin Wang,Junjie Li,Saihui Hou,Xu Liu,Chunshui Cao,Yongzhen Huang,Muyi Sun,Siye Wang,Man Zhang*

Main category: cs.CV

TL;DR: 论文提出RRS-Gait框架，通过几何变换视角实现步态识别，探索反射、旋转、缩放三种几何变换的近似特征等变性，最终实现身份不变性学习。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别研究大多通过数据驱动方式隐式学习不同步态条件下的共同特征，但较少显式探索不同步态条件之间的内在关系。本文试图建立不同步态条件之间的连接，将步态变化视为几何变换的组合。

Method: 提出RRS-Gait框架，探索反射、旋转、缩放三种常见几何变换。首先基于特定几何变换灵活调整卷积核以实现近似特征等变性，然后将这三种等变性感知特征分别输入全局池化操作进行最终的不变性感知学习。

Result: 在四个主流步态数据集（Gait3D、GREW、CCPG、SUSTech1K）上的大量实验表明，该方法在各种步态条件下都表现出优越性能。

Conclusion: 通过将步态变化视为几何变换组合的新视角，实现了几何不变性学习，从而自然获得身份不变性。RRS-Gait框架为步态识别提供了有效的几何变换处理方法。

Abstract: The goal of gait recognition is to extract identity-invariant features of an individual under various gait conditions, e.g., cross-view and cross-clothing. Most gait models strive to implicitly learn the common traits across different gait conditions in a data-driven manner to pull different gait conditions closer for recognition. However, relatively few studies have explicitly explored the inherent relations between different gait conditions. For this purpose, we attempt to establish connections among different gait conditions and propose a new perspective to achieve gait recognition: variations in different gait conditions can be approximately viewed as a combination of geometric transformations. In this case, all we need is to determine the types of geometric transformations and achieve geometric invariance, then identity invariance naturally follows. As an initial attempt, we explore three common geometric transformations (i.e., Reflect, Rotate, and Scale) and design a $\mathcal{R}$eflect-$\mathcal{R}$otate-$\mathcal{S}$cale invariance learning framework, named ${\mathcal{RRS}}$-Gait. Specifically, it first flexibly adjusts the convolution kernel based on the specific geometric transformations to achieve approximate feature equivariance. Then these three equivariant-aware features are respectively fed into a global pooling operation for final invariance-aware learning. Extensive experiments on four popular gait datasets (Gait3D, GREW, CCPG, SUSTech1K) show superior performance across various gait conditions.

</details>


### [26] [SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving](https://arxiv.org/abs/2601.05640)
*Jingyu Li,Junjie Wu,Dongnan Hu,Xiangkai Huang,Bin Sun,Zhihui Hao,Xianpeng Lang,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: SGDrive提出了一种基于视觉语言模型的层次化知识结构框架，通过场景-智能体-目标三层分解来增强自动驾驶规划能力，在NAVSIM基准测试中取得了相机方法的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的端到端自动驾驶方法缺乏对驾驶特定推理的专业理解，特别是在3D空间和时间维度上。这些通用模型难以建立捕捉几何关系、场景上下文和运动模式的结构化时空表示，而这些对于安全轨迹规划至关重要。

Method: SGDrive在预训练的视觉语言模型基础上，将驾驶理解分解为场景-智能体-目标三层层次结构，模拟人类驾驶认知：首先感知整体环境（场景上下文），然后关注安全关键智能体及其行为，最后制定短期目标并执行动作。这种层次分解提供了通用视觉语言模型所缺乏的结构化时空表示。

Result: 在NAVSIM基准测试中，SGDrive在PDMS和EPDMS两个指标上都取得了相机方法中的最先进性能，验证了层次化知识结构对于将通用视觉语言模型适配到自动驾驶领域的有效性。

Conclusion: 通过将驾驶理解分解为场景-智能体-目标层次结构，SGDrive能够为通用视觉语言模型提供必要的结构化时空表示，显著提升了自动驾驶规划能力，为视觉语言模型在自动驾驶领域的专业化应用提供了有效途径。

Abstract: Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM's representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.

</details>


### [27] [SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More](https://arxiv.org/abs/2601.05688)
*Muye Huang,Lingling Zhang,Yifei Li,Yaqiang Wu,Jun Liu*

Main category: cs.CV

TL;DR: SketchVL是一个通过FinePO算法进行细粒度信用分配的多模态大语言模型，通过在图像上绘制中间推理步骤作为标记，并使用细粒度过程奖励模型对每个绘图动作评分，显著提升了图表理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在图表理解上面临挑战，特别是通过强化学习训练的模型存在信用分配问题，无法区分单个生成响应中正确和错误的推理步骤，这限制了模型进行精确复杂视觉推理的能力。

Method: 提出SketchVL模型，采用FinePO强化学习算法进行细粒度信用分配。模型在图像上绘制中间推理步骤作为标记，并将标注后的图像反馈给自身，形成多步推理过程。FinePO算法利用FinePRM对轨迹中的每个绘图动作进行评分，精确分配每个步骤的信用。

Result: 实验表明SketchVL能够将其步骤级行为与FinePRM对齐，在图表数据集、自然图像数据集和数学任务上，相比基础模型平均性能提升7.23%。

Conclusion: SketchVL通过细粒度信用分配机制显著提升了多模态大语言模型的推理能力，为训练强大的推理模型提供了有前景的新方向。

Abstract: Charts are high-density visual carriers of complex data and medium for information extraction and analysis. Due to the need for precise and complex visual reasoning, automated chart understanding poses a significant challenge to existing Multimodal Large Language Models (MLLMs). Many MLLMs trained with reinforcement learning (RL) face the challenge of credit assignment. Their advantage estimation, typically performed at the trajectory level, cannot distinguish between correct and incorrect reasoning steps within a single generated response. To address this limitation, we introduce SketchVL, a novel MLLM that optimized with FinePO, a new RL algorithm designed for fine-grained credit assignment within each trajectory. SketchVL's methodology involves drawing its intermediate reasoning steps as markers on the image and feeding the annotated image back to itself, creating a robust, multi-step reasoning process. During training, the FinePO algorithm leverages a Fine-grained Process Reward Model (FinePRM) to score each drawing action within a trajectory, thereby precisely assigning credit for each step. This mechanism allows FinePO to more strongly reward correct tokens when a trajectory is globally successful, and more heavily penalize incorrect tokens when the trajectory is globally suboptimal, thus achieving fine-grained reinforcement signals. Experiments show that SketchVL learns to align its step-level behavior with the FinePRM, achieving an average performance gain of 7.23\% over its base model across chart datasets, natural image datasets, and mathematics, providing a promising new direction for training powerful reasoning models.

</details>


### [28] [Rotate Your Character: Revisiting Video Diffusion Models for High-Quality 3D Character Generation](https://arxiv.org/abs/2601.05722)
*Jin Wang,Jianxiang Lu,Comi Chen,Guangzheng Xu,Haoyu Yang,Peng Chen,Na Zhang,Yifan Xu,Longhuang Wu,Shuai Shao,Qinglin Lu,Ping Luo*

Main category: cs.CV

TL;DR: RCM是一个先进的图像到视频扩散框架，专门用于高质量的新视角合成和3D角色生成，能够处理复杂姿势并生成高分辨率轨道视频。


<details>
  <summary>Details</summary>
Motivation: 从单张图像生成高质量3D角色在数字内容创作中面临重大挑战，特别是由于复杂的身体姿势和自遮挡问题。现有方法难以处理这些复杂情况。

Method: RCM是一个图像到视频扩散框架，具有以下关键技术：1) 将任意复杂姿势的角色转移到规范姿势；2) 生成1024x1024分辨率的高分辨率轨道视频；3) 支持不同初始相机姿态的可控观察位置；4) 支持最多4张输入图像的多视角条件。

Result: 大量实验表明，RCM在新视角合成和3D生成质量方面均优于现有最先进方法。

Conclusion: RCM框架为解决单图像3D角色生成中的复杂姿势和自遮挡问题提供了有效解决方案，在多个关键指标上表现出优越性能。

Abstract: Generating high-quality 3D characters from single images remains a significant challenge in digital content creation, particularly due to complex body poses and self-occlusion. In this paper, we present RCM (Rotate your Character Model), an advanced image-to-video diffusion framework tailored for high-quality novel view synthesis (NVS) and 3D character generation. Compared to existing diffusion-based approaches, RCM offers several key advantages: (1) transferring characters with any complex poses into a canonical pose, enabling consistent novel view synthesis across the entire viewing orbit, (2) high-resolution orbital video generation at 1024x1024 resolution, (3) controllable observation positions given different initial camera poses, and (4) multi-view conditioning supporting up to 4 input images, accommodating diverse user scenarios. Extensive experiments demonstrate that RCM outperforms state-of-the-art methods in both novel view synthesis and 3D generation quality.

</details>


### [29] [TAGRPO: Boosting GRPO on Image-to-Video Generation with Direct Trajectory Alignment](https://arxiv.org/abs/2601.05729)
*Jin Wang,Jianxiang Lu,Guangzheng Xu,Comi Chen,Haoyu Yang,Linqing Wang,Peng Chen,Mingtao Chen,Zhichao Hu,Longhuang Wu,Shuai Shao,Qinglin Lu,Ping Luo*

Main category: cs.CV

TL;DR: TAGRPO：一种基于对比学习的图像到视频生成模型后训练框架，通过改进的GRPO损失和记忆库机制，在相同初始噪声生成的视频上实现更有效的奖励优化。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在文本到图像/视频生成中有效，但直接应用于图像到视频(I2V)模型时奖励提升不一致。需要专门针对I2V特性的优化框架。

Method: 提出TAGRPO框架：1) 利用相同初始噪声生成的视频作为优化指导；2) 在中间潜在空间应用改进的GRPO损失，使高奖励轨迹对齐、低奖励轨迹远离；3) 引入视频记忆库增强多样性并降低计算开销。

Result: TAGRPO在I2V生成任务中相比DanceGRPO取得了显著改进，证明了该框架的有效性。

Conclusion: TAGRPO为I2V模型提供了一种简单而有效的后训练优化框架，通过对比学习思想改进了现有GRPO方法在图像到视频生成中的应用效果。

Abstract: Recent studies have demonstrated the efficacy of integrating Group Relative Policy Optimization (GRPO) into flow matching models, particularly for text-to-image and text-to-video generation. However, we find that directly applying these techniques to image-to-video (I2V) models often fails to yield consistent reward improvements. To address this limitation, we present TAGRPO, a robust post-training framework for I2V models inspired by contrastive learning. Our approach is grounded in the observation that rollout videos generated from identical initial noise provide superior guidance for optimization. Leveraging this insight, we propose a novel GRPO loss applied to intermediate latents, encouraging direct alignment with high-reward trajectories while maximizing distance from low-reward counterparts. Furthermore, we introduce a memory bank for rollout videos to enhance diversity and reduce computational overhead. Despite its simplicity, TAGRPO achieves significant improvements over DanceGRPO in I2V generation.

</details>


### [30] [FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time](https://arxiv.org/abs/2601.05738)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 提出了一种实时跟踪SLAM系统，将高效相机跟踪与基于3D高斯泼溅的光真实感特征丰富建图相结合，通过密集特征栅格化实现超越RGB-D输入的强语义能力。


<details>
  <summary>Details</summary>
Motivation: 传统语义SLAM方法通常嵌入预定义的类别标签，限制了其应用范围。本文旨在开发一个能够实现自由视点、开放集分割的SLAM系统，同时提升跟踪稳定性和地图保真度。

Method: 将密集特征栅格化集成到新视角合成中，并与视觉基础模型对齐。使用3D高斯泼溅技术进行光真实感建图，实现实时相机跟踪和特征丰富的映射。

Result: 在标准基准测试中实现实时跟踪，与最先进系统相当。相比最近的固定集SLAM基线，姿态误差降低9%，建图精度提高8%。同时提供与离线3DGS模型相当的语义和语言掩码结果。

Conclusion: 实时特征嵌入SLAM不仅能够支持新的下游应用，还能提升底层跟踪和建图子系统的性能，在保持最先进跟踪、深度和RGB渲染的同时，实现开放集语义理解能力。

Abstract: We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\% lower pose error and 8\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.

</details>


### [31] [ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers](https://arxiv.org/abs/2601.05741)
*Guray Ozgur,Eduarda Caldeira,Tahar Chettaoui,Jan Niklas Kolf,Marco Huber,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 提出ViTNT-FIQA方法，通过分析Vision Transformer中间层patch嵌入的演化稳定性来评估人脸图像质量，无需训练且只需单次前向传播


<details>
  <summary>Details</summary>
Motivation: 现有FIQA方法主要利用最终层表示，而无训练方法需要多次前向传播或反向传播，需要更高效、无需训练的质量评估方法

Method: 通过计算Vision Transformer连续块间L2归一化patch嵌入的欧氏距离，测量patch嵌入演化的稳定性，并将这些距离聚合成图像级质量分数

Result: 在八个基准测试（LFW、AgeDB-30、CFP-FP、CALFW、Adience、CPLFW、XQLFW、IJB-C）上表现出与最先进方法竞争的性能，同时保持计算效率

Conclusion: ViTNT-FIQA提供了一种无需训练、计算高效的人脸图像质量评估方法，可直接应用于任何预训练的ViT人脸识别模型，通过分析中间层特征演化稳定性实现质量评估

Abstract: Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.

</details>


### [32] [Adaptive Disentangled Representation Learning for Incomplete Multi-View Multi-Label Classification](https://arxiv.org/abs/2601.05785)
*Quanjiang Li,Zhiming Liu,Tianxiang Xu,Tingjin Luo,Chenping Hou*

Main category: cs.CV

TL;DR: ADRL方法通过自适应解耦表示学习解决多视图多标签学习中的特征缺失和标注不完整问题，采用特征级传播、随机掩码策略和原型特定特征选择等技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 多视图多标签学习面临特征缺失和标注不完整的问题，现有方法在特征恢复、表示解耦和标签语义建模方面存在局限性，需要更有效的解决方案。

Method: 提出自适应解耦表示学习方法(ADRL)，包括：1)基于邻域感知的跨模态特征级亲和力传播实现鲁棒视图补全；2)随机掩码策略增强重建效果；3)跨标签分布传播类别级关联以捕获相互依赖的标签原型；4)基于互信息的目标函数促进共享表示一致性；5)原型特定特征选择和伪标签生成；6)利用伪标签空间结构指导视图融合的判别性权衡。

Result: 在公共数据集和实际应用中的大量实验表明ADRL具有优越性能。

Conclusion: ADRL通过综合解决特征恢复、表示解耦和标签语义建模问题，为多视图多标签学习中的特征缺失和标注不完整问题提供了有效的解决方案。

Abstract: Multi-view multi-label learning frequently suffers from simultaneous feature absence and incomplete annotations, due to challenges in data acquisition and cost-intensive supervision. To tackle the complex yet highly practical problem while overcoming the existing limitations of feature recovery, representation disentanglement, and label semantics modeling, we propose an Adaptive Disentangled Representation Learning method (ADRL). ADRL achieves robust view completion by propagating feature-level affinity across modalities with neighborhood awareness, and reinforces reconstruction effectiveness by leveraging a stochastic masking strategy. Through disseminating category-level association across label distributions, ADRL refines distribution parameters for capturing interdependent label prototypes. Besides, we formulate a mutual-information-based objective to promote consistency among shared representations and suppress information overlap between view-specific representation and other modalities. Theoretically, we derive the tractable bounds to train the dual-channel network. Moreover, ADRL performs prototype-specific feature selection by enabling independent interactions between label embeddings and view representations, accompanied by the generation of pseudo-labels for each category. The structural characteristics of the pseudo-label space are then exploited to guide a discriminative trade-off during view fusion. Finally, extensive experiments on public datasets and real-world applications demonstrate the superior performance of ADRL.

</details>


### [33] [LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting](https://arxiv.org/abs/2601.05853)
*Yinghan Xu,John Dingliana*

Main category: cs.CV

TL;DR: 提出LayerGS框架，将任意姿态的人体分解为可动画的多层3D人体化身，分离身体和服装，使用2D高斯表示各层，通过扩散模型修复遮挡区域，实现高质量渲染和虚拟试穿。


<details>
  <summary>Details</summary>
Motivation: 传统单层重建方法将服装锁定在特定身份上，而现有多层方法在处理遮挡区域时存在困难。需要一种能够准确分解人体和服装、处理遮挡区域、支持虚拟试穿的高质量3D人体重建方法。

Method: 1) 使用2D高斯集合编码每个层，实现精确几何和逼真渲染；2) 通过预训练的2D扩散模型和分数蒸馏采样(SDS)修复隐藏区域；3) 三阶段训练策略：先通过单层重建获得粗略规范服装，然后进行多层训练联合恢复内层身体和外层服装细节。

Result: 在两个3D人体基准数据集(4D-Dress, Thuman2.0)上的实验表明，该方法在渲染质量和层分解/重组方面优于现有最优方法，支持新颖视角和姿态下的逼真虚拟试穿。

Conclusion: LayerGS框架能够创建高质量、可动画的多层3D人体资产，推进了沉浸式应用中高保真3D人体资源的实际创建，代码已开源。

Abstract: We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at https://github.com/RockyXu66/LayerGS

</details>


### [34] [Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation](https://arxiv.org/abs/2601.05855)
*Kaiwen Huang,Yizhe Zhang,Yi Zhou,Tianyang Xu,Tao Zhou*

Main category: cs.CV

TL;DR: 提出BCSI框架用于半监督医学图像分割，通过语义空间扰动、通道选择性路由和双向通道交互解决现有方法错误累积、结构复杂和数据流交互不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法（如均值教师、双流一致性学习）存在错误累积、模型结构复杂以及忽略标记与未标记数据流之间交互的问题，需要更有效的解决方案。

Method: 1) 语义空间扰动(SSP)：使用两种强增强操作扰动数据，利用弱增强的伪标签进行无监督学习，并对两种强增强的预测进行一致性约束；2) 通道选择性路由(CR)：动态选择最相关通道进行信息交换，减少噪声；3) 双向通道交互(BCI)：补充额外语义信息并增强重要通道表示。

Result: 在多个基准3D医学数据集上的实验结果表明，该方法优于现有的半监督方法。

Conclusion: 提出的BCSI框架通过创新的扰动机制、选择性通道路由和双向交互策略，有效解决了半监督医学图像分割中的关键挑战，取得了优越的性能。

Abstract: Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.

</details>


### [35] [Context-Aware Decoding for Faithful Vision-Language Generation](https://arxiv.org/abs/2601.05939)
*Mehrdad Fazli,Bowen Wei,Ziwei Zhu*

Main category: cs.CV

TL;DR: 该论文研究了大型视觉语言模型中的幻觉问题，通过分析层间生成动态发现真实token比幻觉token更早积累概率质量，并提出了无需训练的上下文嵌入注入方法来缓解幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在图像描述和视觉推理等开放任务中存在幻觉问题（生成与视觉输入不一致的响应），这是当前模型的关键限制。

Method: 使用Logit Lens分析LVLMs在解码器层间构建下一个token分布的方式，发现"承诺深度差距"现象。基于此提出上下文嵌入注入方法，利用最后一个输入token的隐藏状态作为接地信号，在解码过程中保持视觉保真度。

Result: 在CHAIR、AMBER和MMHal-Bench基准测试中（最大token长度为512），CEI方法在三个LVLMs上都优于现有最先进的基线方法，其动态变体实现了最低的整体幻觉率。

Conclusion: 通过结合新的机制洞察和可扩展的干预措施，这项工作推进了LVLMs中幻觉问题的缓解，为训练免费的幻觉缓解提供了有效方案。

Abstract: Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth gap: truthful tokens accumulate probability mass on their final candidates earlier than hallucinatory ones. Drawing on this discovery, we introduce Context Embedding Injection (CEI), a lightweight method that harnesses the hidden state of the last input token-the context embedding-as a grounding signal to maintain visual fidelity throughout decoding and curb hallucinations. Evaluated on the CHAIR, AMBER, and MMHal-Bench benchmarks (with a maximum token length of 512), CEI outperforms state-of-the-art baselines across three LVLMs, with its dynamic variant yielding the lowest overall hallucination rates. By integrating novel mechanistic insights with a scalable intervention, this work advances the mitigation of hallucinations in LVLMs.

</details>


### [36] [WaveRNet: Wavelet-Guided Frequency Learning for Multi-Source Domain-Generalized Retinal Vessel Segmentation](https://arxiv.org/abs/2601.05942)
*Chanchan Wang,Yuanfang Wang,Qing Xu,Guanxin Chen*

Main category: cs.CV

TL;DR: WaveRNet是一个基于小波引导频率学习的框架，用于鲁棒的多源域泛化视网膜血管分割，通过频谱引导域调制器、频率自适应域融合和分层掩码提示细化器解决光照和对比度变化下的域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割面临域偏移的挑战，特别是光照不均匀和对比度变化导致的性能下降。现有基于SAM的方法忽视了频率域信息，且直接上采样会丢失细血管细节，需要更鲁棒的域泛化解决方案。

Method: 1. 频谱引导域调制器(SDM)：结合小波分解和可学习域令牌，分离光照鲁棒的低频结构和高频血管边界，生成域特定特征。2. 频率自适应域融合(FADF)：通过小波频率相似度进行智能测试时域选择和软加权融合。3. 分层掩码提示细化器(HMPR)：通过粗到细的细化和长程依赖建模克服SAM上采样限制。

Result: 在四个公共视网膜数据集上采用Leave-One-Domain-Out协议进行广泛实验，WaveRNet实现了最先进的泛化性能。

Conclusion: WaveRNet通过小波引导的频率学习框架有效解决了视网膜血管分割中的域泛化问题，在光照和对比度变化下表现出优异的鲁棒性，为自动化眼科诊断提供了可靠解决方案。

Abstract: Domain-generalized retinal vessel segmentation is critical for automated ophthalmic diagnosis, yet faces significant challenges from domain shift induced by non-uniform illumination and varying contrast, compounded by the difficulty of preserving fine vessel structures. While the Segment Anything Model (SAM) exhibits remarkable zero-shot capabilities, existing SAM-based methods rely on simple adapter fine-tuning while overlooking frequency-domain information that encodes domain-invariant features, resulting in degraded generalization under illumination and contrast variations. Furthermore, SAM's direct upsampling inevitably loses fine vessel details. To address these limitations, we propose WaveRNet, a wavelet-guided frequency learning framework for robust multi-source domain-generalized retinal vessel segmentation. Specifically, we devise a Spectral-guided Domain Modulator (SDM) that integrates wavelet decomposition with learnable domain tokens, enabling the separation of illumination-robust low-frequency structures from high-frequency vessel boundaries while facilitating domain-specific feature generation. Furthermore, we introduce a Frequency-Adaptive Domain Fusion (FADF) module that performs intelligent test-time domain selection through wavelet-based frequency similarity and soft-weighted fusion. Finally, we present a Hierarchical Mask-Prompt Refiner (HMPR) that overcomes SAM's upsampling limitation through coarse-to-fine refinement with long-range dependency modeling. Extensive experiments under the Leave-One-Domain-Out protocol on four public retinal datasets demonstrate that WaveRNet achieves state-of-the-art generalization performance. The source code is available at https://github.com/Chanchan-Wang/WaveRNet.

</details>


### [37] [VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction](https://arxiv.org/abs/2601.05966)
*Longbin Ji,Xiaoxiong Liu,Junyuan Shang,Shuohuan Wang,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CV

TL;DR: VideoAR是首个大规模视觉自回归视频生成框架，通过多尺度下一帧预测和自回归建模，在保持高质量的同时显著提升计算效率，缩小了自回归与扩散模型之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成领域主要被扩散模型和流匹配模型主导，这些模型虽然能产生高质量结果，但计算密集且难以扩展。需要一种更高效、可扩展的视频生成方法。

Method: 提出VideoAR框架，结合多尺度下一帧预测和自回归建模，使用3D多尺度分词器编码时空动态。引入多尺度时序RoPE、跨帧错误校正和随机帧掩码来提升长期一致性。采用多阶段预训练流程逐步对齐不同分辨率和时长的时空学习。

Result: 在自回归模型中达到新的SOTA：UCF-101的FVD从99.5提升到88.6，推理步骤减少10倍以上，VBench得分达到81.74，与规模大一个数量级的扩散模型相当。

Conclusion: VideoAR缩小了自回归与扩散范式之间的性能差距，为未来视频生成研究提供了一个可扩展、高效且时序一致的基础框架。

Abstract: Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [38] [On the dynamical stability of skeletal muscle](https://arxiv.org/abs/2601.05275)
*Javier A. Almonacid,Nilima Nigam,James M. Wakeling*

Main category: physics.med-ph

TL;DR: 本文重新探讨了骨骼肌在长度大于最优长度时的动态稳定性问题，提出了一个稳定的1D Hill型肌肉模型，该模型结合了3D变形特性，解决了传统模型在力-长度曲线下降区域的发散问题。


<details>
  <summary>Details</summary>
Motivation: 70多年来，关于活性骨骼肌在长度大于最优长度时是否具有动态稳定性一直存在争议。计算肌肉模型的稳定性直接影响我们模拟肌肉在不同操作长度下变形的能力，特别是在模型预测不稳定但肌肉仍能保持功能的长度区域。

Method: 首先使用特征值分析和无量纲化等分析工具结合数值模拟，验证传统Hill型肌肉模型在力-长度曲线下降区域的发散动态。然后提出一个稳定的1D Hill型肌肉模型，该模型结合了骨骼肌3D变形的特性，形成完全凸的力-长度关系。

Result: 传统Hill型肌肉模型在力-长度曲线的下降区域确实会表现出发散动态。提出的稳定模型能够提供完全凸的力-长度关系，为数值模拟带来鲁棒性，同时保持了1D模型的计算效率。

Conclusion: 肌肉的激活无关内在力学特性足以稳定收缩，即使在力-长度曲线的下降区域也能保持功能完整性，这为理解肌肉在主动拉伸过程中如何维持功能完整性提供了新见解。

Abstract: There has been debate for over 70-years about whether active skeletal muscle is dynamically stable at lengths greater than its optimal length. The stability of computational muscle models is a critical issue, as it directly affects our ability to simulate muscle deformation across different operating lengths, especially at lengths where muscles are known to remain functional despite model-predicted instabilities.
  In this study, we revisit the question of dynamical stability of ODE-based models of skeletal muscle. In particular, we investigate whether activation-independent tissue properties can provide stability to contractions along the dip region of the total force-length curve. First, using a combination of analytical tools (eigenvalue analysis and non-dimensionalization) and numerical simulations, we confirm that traditional Hill-type muscle models can display divergent dynamics in this region. Then, we propose a stabilized version of a 1D Hill-type muscle model that incorporates the 3D nature of skeletal muscle deformation. This results in a completely convex force-length relationship that can bring robustness to numerical simulations, while preserving the computational efficiency of 1D models. Our findings suggest that activation-independent intrinsic mechanical properties of muscle are sufficient to stabilize contractions even in the dip region, offering new insight into how muscles maintain functional integrity during active stretch.

</details>


### [39] [Effect of Right Ventricular Outflow Tract Material Properties on Simulated Transcatheter Pulmonary Placement](https://arxiv.org/abs/2601.05410)
*Jalaj Maheshwari,Wensi Wu,Christopher N. Zelonis,Steve A. Maas,Kyle Sunderland,Yuval Barak-Corren,Stephen Ching,Patricia Sabin,Andras Lasso,Matthew J. Gillespie,Jeffrey A. Weiss,Matthew A. Jolley*

Main category: physics.med-ph

TL;DR: 该研究通过有限元模拟分析了经导管肺动脉瓣置换术中右心室流出道材料特性不确定性和跨瓣环补片对瓣膜部署的影响，发现基质剪切模量、纤维模量和纤维取向角度对组织应力影响最大，而补片位置和刚度会影响局部应力分布。


<details>
  <summary>Details</summary>
Motivation: 当前经导管肺动脉瓣置换术的有限元模拟通常假设右心室流出道和相邻组织的材料特性，但这些材料特性的变化对瓣膜部署的敏感性未知，同时跨瓣环补片的刚度和位置对模拟结果的影响也尚未探索。

Method: 使用FEBioUncertainSCI对患者特异性右心室流出道进行材料特性敏感性分析，材料模型采用非耦合HGO模型；同时分析跨瓣环补片的影响，考虑两种补片位置和四种补片刚度；使用SlicerHeart和FEBio实现的自定义指标进行结果可视化和量化。

Result: 敏感性分析显示：基质剪切模量(c)、纤维模量(k1)和纤维平均取向角度(gamma)对95%应力影响最大，而只有c对95%拉格朗日应变影响最大；一阶敏感性指数对总阶敏感性指数贡献最大。跨瓣环补片模拟显示：峰值应力和应变依赖于补片位置；随着补片刚度增加，补片与右心室流出道连接处的应力增加，补片本身的应力增加而应变减小；TPV装置的总封闭体积在所有模拟补片情况下保持不变。

Conclusion: 虽然组织材料特性的不确定性和补片位置可能影响功能结果，但有限元模拟为评估经导管肺动脉瓣置换术中的这些结果提供了可靠框架。

Abstract: Finite element (FE) simulations emulating transcatheter pulmonary valve (TPV) system deployment in patient-specific right ventricular outflow tracts (RVOT) assume material properties for the RVOT and adjacent tissues. Sensitivity of the deployment to variation in RVOT material properties is unknown. Moreover, the effect of a transannular patch stiffness and location on simulated TPV deployment has not been explored. A sensitivity analysis on the material properties of a patient-specific RVOT during TPV deployment, modeled as an uncoupled HGO material, was conducted using FEBioUncertainSCI. Further, the effects of a transannular patch during TPV deployment were analyzed by considering two patch locations and four patch stiffnesses. Visualization of results and quantification were performed using custom metrics implemented in SlicerHeart and FEBio. Sensitivity analysis revealed that the shear modulus of the ground matrix (c), fiber modulus (k1), and fiber mean orientation angle (gamma) had the greatest effect on 95th %ile stress, whereas only c had the greatest effect on 95th %ile Lagrangian strain. First-order sensitivity indices contributed the greatest to the total-order sensitivity indices. Simulations using a transannular patch revealed that peak stress and strain were dependent on patch location. As stiffness of the patch increased, greater stress was observed at the interface connecting the patch to the RVOT, and stress in the patch itself increased while strain decreased. The total enclosed volume by the TPV device remained unchanged across all simulated patch cases. This study highlights that while uncertainties in tissue material properties and patch locations may influence functional outcomes, FE simulations provide a reliable framework for evaluating these outcomes in TPVR.

</details>


### [40] [Group-kernel auto-calibration and group-patch k-space reconstruction: Fast MRI with time-variant B0 kernels partitioned into time-invariant subsets](https://arxiv.org/abs/2601.05469)
*Rui Tian,Martin Uecker,Oliver Holder,Pavel Povolni,Theodor Steffen,Klaus Scheffler*

Main category: physics.med-ph

TL;DR: 该论文提出了一种连续场校准方法和高效的k空间重建技术，用于解决MRI加速中B0调制带来的校准误差和重建时间增加问题，实现了2D 8倍和3D 14.6倍的最大加速因子。


<details>
  <summary>Details</summary>
Motivation: 提高MRI扫描速度需要单位时间内捕获更多空间编码信息，通常通过在过采样读出期间叠加额外的场调制来实现。但这会引入校准误差并增加重建时间，因此需要更有效的校准和重建方法。

Method: 提出连续场校准方法，将GRAPPA核推广到显式提取连续B0调制核，解决两个仅在不同场调制的ACS区域之间的插值关系。将共享相同瞬时图像空间调制的k空间位置分组，分别估计时间不变核子集。同时提出k空间子区域重建技术作为传统混合空间重建的高效替代方案。

Result: 在9.4T下测试，通过局部B0线圈阵列和Wave-CAIPI加速的FLASH序列实现了无伪影图像重建，最大加速因子达到2D 8倍和3D 14.6倍。非线性梯度调制方案与线性梯度调制达到相似的采样效率，提出的重建方法显示出减少重建时间的潜力。

Conclusion: 快速B0调制和广泛采用的并行成像可以共享共同的数学框架，从而实现类似鲁棒的重建。对于使用动态B0和静态RF核的扫描，不仅信号编码，而且自动校准和重建都可以在k空间中执行，为稳健消除涡流电流和探索更复杂的B0调制策略以实现终极MRI速度铺平道路。

Abstract: Purpose: Pushing MRI speed further demands more spatially-encoded information captured per unit time, e.g., by superimposing additional field modulations during oversampled readout. However, this can introduce calibration errors and increase reconstruction time. Thus, we propose a continuous field calibration approach and an efficient k-space reconstruction technique.
  Theory and Methods: Our auto-calibration generalizes GRAPPA kernels to explicitly extract continuous B0 modulation kernels, solving interpolation relationships between two ACS regions differing only in the extra field modulation. The k-space locations sharing the same instantaneous image-space modulation are grouped, so that subsets of time-invariant kernels can be separately estimated, as a generalized solution for Wave-CAIPI/FRONSAC-type scans. This view further inspires a k-space subregion-wise reconstruction technique, as an efficient alternative to conventional hybrid-space reconstruction. At 9.4T, FLASH accelerated by a local B0 coil array and Wave-CAIPI were tested with retrospective undersampling.
  Results: Artifact-free images were reconstructed, under diverse rapid B0 modulation schemes, reaching maximum acceleration factors of 8-fold in 2D and 14.6-fold in 3D. Some nonlinear gradients modulation schemes reach similar sampling efficiency as linear gradients modulation. The proposed reconstruction shows potentials in reducing reconstruction time.
  Conclusion: Rapid B0 modulations and widely-adopted parallel imaging can share a common mathematical framework, and consequently, achieve similarly-robust reconstructions. Specifically, for scans using dynamic B0 and static RF kernels, not only signal encoding, but also auto-calibration and reconstruction can be performed in k-space. This paves the way to robustly remove eddy currents, and explore more complex B0 modulation strategies towards ultimate MRI speed.

</details>


### [41] [Inclusion of Inter-crystal Scattering in PET: Analytical Models and Dedicated Reconstruction](https://arxiv.org/abs/2601.05717)
*Jorge Roser,Hong Phuc Vo,Rebecca Kantorek,Steven Seeger,Magdalena Rafecas*

Main category: physics.med-ph

TL;DR: 该研究提出了一种基于物理模型的PET晶体间散射事件处理方法，通过解析表达式计算灵敏度和系统矩阵，无需识别原始响应线，能够将ICS事件与传统PET事件联合重建，提高图像质量。


<details>
  <summary>Details</summary>
Motivation: PET中的晶体间散射通常被视为降低图像空间分辨率的负面效应，但同时也被认为是提高PET灵敏度的潜在途径，特别是在小动物成像等灵敏度受限的场景中。现有ICS事件恢复方法存在成功率有限、计算负担重或数据训练依赖性强等问题。

Method: 提出基于物理的ICS事件模型，推导出灵敏度图像和系统矩阵的解析表达式，无需识别原始响应线。开发了将ICS事件与传统PET事件整合的联合图像重建算法（基于列表模式MLEM），并建立了相应的解析模型。使用MERMAID小鱼类PET扫描仪原型进行蒙特卡洛模拟和实验验证。

Result: 模拟和实验结果表明，虽然恢复系数值略有下降，但包含ICS事件能显著降低统计噪声并改善均匀性。图像质量体模的数据验证了该方法的有效性。

Conclusion: 提出的物理模型方法能够有效整合PET中的晶体间散射事件，在保持图像质量的同时提高系统灵敏度，为小动物PET成像等灵敏度受限场景提供了实用解决方案。

Abstract: Inter-crystal scattering (ICS) in Positron Emission Tomography (PET) is commonly regarded as a degradation effect that might compromise the image spatial resolution. In parallel, the inclusion of ICS events has also been recognized as a potential approach to increase PET sensitivity, which could be especially beneficial in scenarios where the latter is a limiting factor, such as very small animal imaging. Several methods for the recovery of ICS events have been proposed, many of which aim to locate the first interaction, i.e., the Compton scattering site, usually limited by their success rate, computational burden or data and training dependency. Conversely, this work proposes a physics-based model for ICS events, leading to analytical expressions of the sensitivity image and the system matrix (required by statistical reconstruction algorithms), without the need to identify the original line of response. After validating the model, the work shows how ICS events can be integrated into a joint image reconstruction algorithm (based on list-mode MLEM) together with conventional PET events, for which dedicated analytical models are also developed. To assess the performance of the proposed approach, Monte-Carlo simulated and experimental data of an image quality phantom were obtained with the MERMAID small-fish PET scanner prototype. Both simulation and experimental results indicate that, while slightly decreasing the recovery coefficient values, the inclusion of ICS clearly reduces statistical noise and improves uniformity.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [42] [Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models](https://arxiv.org/abs/2601.05336)
*Tracey Yee Hsin Tay,Xu Yan,Jonathan Ouyang,Daniel Wu,William Jiang,Jonathan Kao,Yuchen Cui*

Main category: cs.RO

TL;DR: GAMMA系统利用眼动追踪和视觉语言模型，通过注视点推断用户意图，实现机器人自主操作，无需任务特定训练


<details>
  <summary>Details</summary>
Motivation: 设计直观的机器人控制界面是促进人机交互的关键挑战，特别是在辅助护理场景中。眼动注视作为一种快速、非侵入性且意图丰富的输入方式，是传达用户目标的理想通道

Method: GAMMA系统结合自我中心眼动追踪和视觉语言模型，通过将注视点置于场景上下文中，将视觉注意力映射到高级语义理解，实现技能选择和参数化，无需任务特定训练

Result: 在桌面操作任务评估中，GAMMA相比无推理的基线眼动控制方法表现出更鲁棒、直观和可泛化的控制能力

Conclusion: GAMMA展示了结合基础模型和眼动追踪实现自然、可扩展机器人自主控制的潜力，为直观的人机交互提供了有效解决方案

Abstract: Designing intuitive interfaces for robotic control remains a central challenge in enabling effective human-robot interaction, particularly in assistive care settings. Eye gaze offers a fast, non-intrusive, and intent-rich input modality, making it an attractive channel for conveying user goals. In this work, we present GAMMA (Gaze Assisted Manipulation for Modular Autonomy), a system that leverages ego-centric gaze tracking and a vision-language model to infer user intent and autonomously execute robotic manipulation tasks. By contextualizing gaze fixations within the scene, the system maps visual attention to high-level semantic understanding, enabling skill selection and parameterization without task-specific training. We evaluate GAMMA on a range of table-top manipulation tasks and compare it against baseline gaze-based control without reasoning. Results demonstrate that GAMMA provides robust, intuitive, and generalizable control, highlighting the potential of combining foundation models and gaze for natural and scalable robot autonomy. Project website: https://gamma0.vercel.app/

</details>


### [43] [Assembling Solar Panels by Dual Robot Arms Towards Full Autonomous Lunar Base Construction](https://arxiv.org/abs/2601.05491)
*Luca Nunziante,Kentaro Uno,Gustavo H. Diaz,Shreya Santra,Alessandro De Luca,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文开发了用于月球基地建设的双机器人系统，通过视觉、控制和硬件集成实现太阳能电池板模块的自主装配。


<details>
  <summary>Details</summary>
Motivation: 随着人类重返月球计划的推进，需要建立月球前哨站，其中太阳能发电塔等基础设施的建设至关重要。类似于国际空间站的建造方式，通过模块化运输和现场组装是实用方案，需要自主机器人系统来完成这些任务。

Method: 设计并测试了专门用于太阳能电池板模块装配的感知与控制流程，包括定制硬件开发、模块化太阳能电池板模拟、主动-被动连接器设计，并将抓取夹具控制集成到整个流程中。

Result: 成功实现了双机器人操纵器对任意位置放置的太阳能电池板进行有效连接，展示了视觉、控制和硬件系统在复杂空间应用中的无缝集成。

Conclusion: 该研究证明了双机器人系统能够有效执行太阳能电池板模块的自主装配任务，为未来月球基地建设中的机器人应用提供了可行的技术方案。

Abstract: Since the successful Apollo program, humanity is once again aiming to return to the Moon for scientific discovery, resource mining, and inhabitation. Upcoming decades focus on building a lunar outpost, with robotic systems playing a crucial role to safely and efficiently establish essential infrastructure such as solar power generating towers. Similar to the construction of the International Space Station (ISS), shipping necessary components via modules and assembling them in situ should be a practical scenario. In this context, this paper focuses on the integration of vision, control, and hardware systems within an autonomous sequence for a dual-arm robot system. We explore a perception and control pipeline specifically designed for assembling solar panel modules, one of the benchmark tasks. Ad hoc hardware was designed and tested in real-world experiments. A mock-up of modular solar panels and active-passive connectors are employed, with the control of this grappling fixture integrated into the proposed pipeline. The successful implementation of our method demonstrates that the two robot manipulators can effectively connect arbitrarily placed panels, highlighting the seamless integration of vision, control, and hardware systems in complex space applications.

</details>


### [44] [TOSC: Task-Oriented Shape Completion for Open-World Dexterous Grasp Generation from Partial Point Clouds](https://arxiv.org/abs/2601.05499)
*Weishang Wu,Yifei Shi,Zhiping Cai*

Main category: cs.RO

TL;DR: 该论文提出了一种面向任务的形状补全方法，专注于补全潜在的接触区域而非整个形状，通过预训练基础模型生成多个候选，使用3D判别自编码器评估并优化最合理的候选，最后通过FlowGrasp模型生成面向任务的灵巧抓取。


<details>
  <summary>Details</summary>
Motivation: 在严重部分观测条件下，机器人操作开放世界物体时，任务导向的灵巧抓取仍然具有挑战性。大量缺失数据使得通用形状补全方法失效，因此需要一种专门针对抓取任务的形状补全方法。

Method: 1. 利用多个预训练基础模型的零样本能力生成多个面向任务的形状补全候选；2. 提出3D判别自编码器评估每个候选的合理性，并从全局角度优化最合理的候选；3. 开发名为FlowGrasp的条件流匹配模型，从优化后的形状生成面向任务的灵巧抓取。

Result: 在任务导向灵巧抓取和任务导向形状补全方面达到最先进性能，将抓取位移和Chamfer距离分别比现有最佳方法提高了16.17%和55.26%。特别在严重缺失数据的物体抓取方面表现出良好能力，并在处理开放集类别和任务方面展示出良好的泛化性。

Conclusion: 面向任务的形状补全方法通过专注于补全潜在接触区域而非整个形状，有效解决了严重部分观测条件下的机器人抓取问题，结合预训练基础模型和判别自编码器优化，显著提升了任务导向抓取的性能。

Abstract: Task-oriented dexterous grasping remains challenging in robotic manipulations of open-world objects under severe partial observation, where significant missing data invalidates generic shape completion. In this paper, to overcome this limitation, we study Task-Oriented Shape Completion, a new task that focuses on completing the potential contact regions rather than the entire shape. We argue that shape completion for grasping should be explicitly guided by the downstream manipulation task. To achieve this, we first generate multiple task-oriented shape completion candidates by leveraging the zero-shot capabilities of object functional understanding from several pre-trained foundation models. A 3D discriminative autoencoder is then proposed to evaluate the plausibility of each generated candidate and optimize the most plausible one from a global perspective. A conditional flow-matching model named FlowGrasp is developed to generate task-oriented dexterous grasps from the optimized shape. Our method achieves state-of-the-art performance in task-oriented dexterous grasping and task-oriented shape completion, improving the Grasp Displacement and the Chamfer Distance over the state-of-the-art by 16.17\% and 55.26%, respectively. In particular, it shows good capabilities in grasping objects with severe missing data. It also demonstrates good generality in handling open-set categories and tasks.

</details>


### [45] [Learning specifications for reactive synthesis with safety constraints](https://arxiv.org/abs/2601.05533)
*Kandai Watanabe,Nicholas Renninger,Sriram Sankaranarayanan,Morteza Lahijanian*

Main category: cs.RO

TL;DR: 本文提出了一种从演示中学习的新方法，使机器人能够在动态环境中自主执行复杂任务。该方法通过概率形式语言建模潜在任务，并引入定制的反应式合成框架来平衡机器人成本与用户任务偏好。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态环境中难以同时保证任务完成、安全约束、机器人成本优化和用户偏好满足。需要一种能够从演示中学习安全约束任务规范，并生成平衡多方目标的执行策略的框架。

Method: 1) 将潜在任务建模为概率形式语言；2) 提出安全约束学习框架，将任务规范推断为概率确定性有限自动机(PDFA)；3) 改进证据驱动的状态合并算法，在学习过程中融入安全要求；4) 引入多目标反应式合成算法，将交互建模为机器人与环境的双人博弈，生成满足PDFA任务且平衡用户偏好与机器人成本的确定性策略；5) 提出计算可行的值迭代算法生成帕累托前沿。

Result: 实验结果表明：1) 学习的PDFA从不包含不安全行为；2) 合成的策略始终能完成任务，同时满足机器人成本和用户偏好要求；3) 算法在不同机器人和任务上均表现出有效性；4) 能够生成帕累托最优解集供用户选择。

Conclusion: 该方法成功实现了从演示中学习安全约束任务规范，并通过多目标优化生成平衡用户偏好与机器人成本的执行策略，为动态环境中的机器人自主任务执行提供了有效解决方案。

Abstract: This paper presents a novel approach to learning from demonstration that enables robots to autonomously execute complex tasks in dynamic environments. We model latent tasks as probabilistic formal languages and introduce a tailored reactive synthesis framework that balances robot costs with user task preferences. Our methodology focuses on safety-constrained learning and inferring formal task specifications as Probabilistic Deterministic Finite Automata (PDFA). We adapt existing evidence-driven state merging algorithms and incorporate safety requirements throughout the learning process to ensure that the learned PDFA always complies with safety constraints. Furthermore, we introduce a multi-objective reactive synthesis algorithm that generates deterministic strategies that are guaranteed to satisfy the PDFA task while optimizing the trade-offs between user preferences and robot costs, resulting in a Pareto front of optimal solutions. Our approach models the interaction as a two-player game between the robot and the environment, accounting for dynamic changes. We present a computationally-tractable value iteration algorithm to generate the Pareto front and the corresponding deterministic strategies. Comprehensive experimental results demonstrate the effectiveness of our algorithms across various robots and tasks, showing that the learned PDFA never includes unsafe behaviors and that synthesized strategies consistently achieve the task while meeting both the robot cost and user-preference requirements.

</details>


### [46] [EvoQRE: Modeling Bounded Rationality in Safety-Critical Traffic Simulation via Evolutionary Quantal Response Equilibrium](https://arxiv.org/abs/2601.05653)
*Phu-Hoa Pham,Chi-Nguyen Tran,Duy-Minh Dao-Sy,Phu-Quy Nguyen-Lam,Trung-Kiet Huynh*

Main category: cs.RO

TL;DR: EvoQRE：一个基于量子响应均衡和演化博弈动力学的交通交互建模框架，用于模拟人类驾驶员的有限理性行为，在安全关键场景中实现更真实的交通仿真。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶交通仿真框架通常依赖模仿学习或博弈论方法，这些方法假设完全理性的智能体，但人类驾驶员实际上表现出有限理性，在认知和感知约束下做出近似最优决策。

Method: 提出EvoQRE框架，将安全关键交通交互建模为一般和马尔可夫博弈，通过量子响应均衡和演化博弈动力学求解。框架集成预训练生成世界模型与熵正则化复制器动力学，捕捉随机人类行为同时保持均衡结构。

Result: 理论证明所提动力学在弱单调性假设下以O(log k / k^{1/3})收敛率收敛到Logit-QRE。在Waymo Open Motion Dataset和nuPlan基准测试中，EvoQRE实现了最先进的真实感、改进的安全指标，并通过可解释的理性参数可控生成多样化安全关键场景。

Conclusion: EvoQRE为有限理性人类驾驶行为建模提供了原则性框架，在交通仿真中实现了更真实的安全关键交互，为自动驾驶系统测试和评估提供了重要工具。

Abstract: Existing traffic simulation frameworks for autonomous vehicles typically rely on imitation learning or game-theoretic approaches that solve for Nash or coarse correlated equilibria, implicitly assuming perfectly rational agents. However, human drivers exhibit bounded rationality, making approximately optimal decisions under cognitive and perceptual constraints. We propose EvoQRE, a principled framework for modeling safety-critical traffic interactions as general-sum Markov games solved via Quantal Response Equilibrium (QRE) and evolutionary game dynamics. EvoQRE integrates a pre-trained generative world model with entropy-regularized replicator dynamics, capturing stochastic human behavior while maintaining equilibrium structure. We provide rigorous theoretical results, proving that the proposed dynamics converge to Logit-QRE under a two-timescale stochastic approximation with an explicit convergence rate of O(log k / k^{1/3}) under weak monotonicity assumptions. We further extend QRE to continuous action spaces using mixture-based and energy-based policy representations. Experiments on the Waymo Open Motion Dataset and nuPlan benchmark demonstrate that EvoQRE achieves state-of-the-art realism, improved safety metrics, and controllable generation of diverse safety-critical scenarios through interpretable rationality parameters.

</details>


### [47] [Motion Compensation for Real Time Ultrasound Scanning in Robotically Assisted Prostate Biopsy Procedures](https://arxiv.org/abs/2601.05661)
*Matija Markulin,Luka Matijević,Luka Siktar,Janko Jurdana,Branimir Caran,Marko Švaco,Filip Šuligoj,Bojan Šekoranja*

Main category: cs.RO

TL;DR: 开发用于前列腺超声检查的机器人辅助系统，通过协作机器人臂自主扫描前列腺模型，保持超声探头与前列腺相对位置恒定，实现快速准确的前列腺3D重建，为活检规划提供支持。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌是男性常见癌症，活检诊断对医生技术要求高且结果依赖操作者经验。开发机器人辅助超声检查系统可降低操作难度，实现更快、更准确、更普及的前列腺活检。

Method: 开发实验室设置，使用协作机器人臂自主扫描前列腺模型，模型连接到模拟患者运动的医疗机器人臂。系统保持超声探头与前列腺相对位置恒定，通过分割每个切片生成前列腺轮廓，转换为3D点云用于活检规划。

Result: 前列腺平均扫描时间30秒，3D重建平均耗时3秒。在四种运动场景（静止、水平、垂直、组合）下测试，ICP配准结果显示：S-H配准平均拟合度83.2%、RMSE 0.35mm；S-V配准84.1%、RMSE 0.37mm；S-C配准79.4%、RMSE 0.37mm。最大机器人跟踪误差3mm，运动补偿最大延迟0.5秒。

Conclusion: 开发的机器人辅助超声系统能够有效补偿患者运动，保持超声探头与前列腺的相对位置，实现快速准确的前列腺3D重建。3mm的跟踪误差在医学文献中足以满足前列腺活检要求，系统有望提高前列腺活检的效率和准确性。

Abstract: Prostate cancer is one of the most common types of cancer in men. Its diagnosis by biopsy requires a high level of expertise and precision from the surgeon, so the results are highly operator-dependent. The aim of this work is to develop a robotic system for assisted ultrasound (US) examination of the prostate, a prebiopsy step that could reduce the dexterity requirements and enable faster, more accurate and more available prostate biopsy. We developed and validated a laboratory setup with a collaborative robotic arm that can autonomously scan a prostate phantom and attached the phantom to a medical robotic arm that mimics the patient's movements. The scanning robot keeps the relative position of the US probe and the prostate constant, ensuring a consistent and robust approach to reconstructing the prostate. To reconstruct the prostate, each slice is segmented to generate a series of prostate contours converted into a 3D point cloud used for biopsy planning. The average scan time of the prostate was 30 s, and the average 3D reconstruction of the prostate took 3 s. We performed four motion scenarios: the phantom was scanned in a stationary state (S), with horizontal motion (H), with vertical motion (V), and with a combination of the two (C). System validation is performed by registering the prostate point cloud reconstructions acquired during different motions (H, V, C) with those obtained in the stationary state. ICP registration with a threshold of 0.8 mm yields mean 83.2\% fitness and 0.35 mm RMSE for S-H registration, 84.1\% fitness and 0.37 mm RMSE for S-V registration and 79.4\% fitness and 0.37 mm RMSE for S-C registration. Due to the elastic and soft material properties of the prostate phantom, the maximum robot tracking error was 3 mm, which can be sufficient for prostate biopsy according to medical literature. The maximum delay in motion compensation was 0.5 s.

</details>


### [48] [InsSo3D: Inertial Navigation System and 3D Sonar SLAM for turbid environment inspection](https://arxiv.org/abs/2601.05805)
*Simon Archieri,Ahmet Cinar,Shu Pan,Jonatan Scharff Willners,Michele Grimald,Ignacio Carlucho,Yvan Petillot*

Main category: cs.RO

TL;DR: InsSo3D是一种使用3D声纳和惯性导航系统进行大规模3D SLAM的准确高效方法，能在浑浊水下环境中实现厘米级精度的定位与建图。


<details>
  <summary>Details</summary>
Motivation: 传统声纳只能提供2D图像（包含距离和方位信息但缺乏高程信息），存在高程模糊问题。3D声纳能生成3D点云，避免了高程模糊，但需要专门适配的SLAM框架来处理3D声纳数据并利用INS作为先验。

Method: 提出一个鲁棒且现代的SLAM框架，专门适配3D声纳数据，使用惯性导航系统作为先验，检测闭环并进行位姿图优化。

Result: 在测试水箱（有地面真值数据）和室外淹没采石场进行评估。与水下运动跟踪系统和视觉SFM获得的参考轨迹和地图比较显示，InsSo3D能有效校正里程计漂移。在50分钟任务中平均轨迹误差低于21厘米，生成10m×20m地图，平均重建误差为9厘米。

Conclusion: InsSo3D能够在浑浊水下条件下实现厘米级精度的定位与建图，为自然或人工水下结构的安全检查提供了可行方案。

Abstract: This paper presents InsSo3D, an accurate and efficient method for large-scale 3D Simultaneous Localisation and Mapping (SLAM) using a 3D Sonar and an Inertial Navigation System (INS). Unlike traditional sonar, which produces 2D images containing range and azimuth information but lacks elevation information, 3D Sonar produces a 3D point cloud, which therefore does not suffer from elevation ambiguity. We introduce a robust and modern SLAM framework adapted to the 3D Sonar data using INS as prior, detecting loop closure and performing pose graph optimisation. We evaluated InsSo3D performance inside a test tank with access to ground truth data and in an outdoor flooded quarry. Comparisons to reference trajectories and maps obtained from an underwater motion tracking system and visual Structure From Motion (SFM) demonstrate that InsSo3D efficiently corrects odometry drift. The average trajectory error is below 21cm during a 50-minute-long mission, producing a map of 10m by 20m with a 9cm average reconstruction error, enabling safe inspection of natural or artificial underwater structures even in murky water conditions.

</details>


### [49] [Modular Autonomy with Conversational Interaction: An LLM-driven Framework for Decision Making in Autonomous Driving](https://arxiv.org/abs/2601.05806)
*Marvin Seegert,Korbinian Moller,Johannes Betz*

Main category: cs.RO

TL;DR: 该论文提出了一种基于大语言模型的自动驾驶系统自然语言交互框架，通过三层架构实现从人类语言到结构化动作空间的映射，确保安全性和透明度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的进展为自动驾驶系统提供了超越刚性输入的自然语言交互机会，但需要解决人类语言复杂性到结构化动作空间的映射挑战。

Method: 提出集成LLM交互层与Autoware的框架，包含三个关键组件：交互类别分类、面向应用的领域特定语言（DSL）用于命令翻译、安全保护验证层，采用两阶段LLM架构确保透明度。

Result: 评估确认系统的时间效率和翻译鲁棒性，仿真成功验证了所有五个交互类别的命令执行，为可扩展的DSL辅助交互提供了基础。

Conclusion: 该工作为模块化和安全意识的自动驾驶堆栈中的可扩展、DSL辅助交互奠定了基础，实现了从高级命令到结构化动作的有效映射。

Abstract: Recent advancements in Large Language Models (LLMs) offer new opportunities to create natural language interfaces for Autonomous Driving Systems (ADSs), moving beyond rigid inputs. This paper addresses the challenge of mapping the complexity of human language to the structured action space of modular ADS software. We propose a framework that integrates an LLM-based interaction layer with Autoware, a widely used open-source software. This system enables passengers to issue high-level commands, from querying status information to modifying driving behavior. Our methodology is grounded in three key components: a taxonomization of interaction categories, an application-centric Domain Specific Language (DSL) for command translation, and a safety-preserving validation layer. A two-stage LLM architecture ensures high transparency by providing feedback based on the definitive execution status. Evaluation confirms the system's timing efficiency and translation robustness. Simulation successfully validated command execution across all five interaction categories. This work provides a foundation for extensible, DSL-assisted interaction in modular and safety-conscious autonomy stacks.

</details>


### [50] [Intelligent Singularity Avoidance in UR10 Robotic Arm Path Planning Using Hybrid Fuzzy Logic and Reinforcement Learning](https://arxiv.org/abs/2601.05836)
*Sheng-Kai Chen,Jyh-Horng Wu*

Main category: cs.RO

TL;DR: 该论文提出了一种结合模糊逻辑安全系统和强化学习算法的UR10机械臂路径规划方法，用于检测和避免奇异点，实现了90%的成功率。


<details>
  <summary>Details</summary>
Motivation: 机械臂操作中的奇异点会导致控制失效和设备损坏，这是一个关键挑战。现有方法在实时检测和避免奇异配置方面存在不足，需要更智能的解决方案来确保安全可靠的机器人操作。

Method: 提出了一种混合方法：1）使用可操作性度量、条件数分析进行实时奇异点检测；2）模糊逻辑决策系统进行安全评估；3）稳定的强化学习框架进行自适应路径规划；4）集成PyBullet仿真训练和URSim连接实现实际部署。

Result: 实验结果显示，系统在达到目标位置的同时保持与奇异配置的安全距离方面取得了90%的成功率，证明了该方法在奇异点检测和避免方面的有效性。

Conclusion: 该研究成功开发了一个综合的奇异点检测和避免系统，结合模糊逻辑和强化学习，为机械臂安全操作提供了有效的解决方案，具有实际部署的潜力。

Abstract: This paper presents a comprehensive approach to singularity detection and avoidance in UR10 robotic arm path planning through the integration of fuzzy logic safety systems and reinforcement learning algorithms. The proposed system addresses critical challenges in robotic manipulation where singularities can cause loss of control and potential equipment damage. Our hybrid approach combines real-time singularity detection using manipulability measures, condition number analysis, and fuzzy logic decision-making with a stable reinforcement learning framework for adaptive path planning. Experimental results demonstrate a 90% success rate in reaching target positions while maintaining safe distances from singular configurations. The system integrates PyBullet simulation for training data collection and URSim connectivity for real-world deployment.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [51] [Protosampling: Enabling Free-Form Convergence of Sampling and Prototyping through Canvas-Driven Visual AI Generation](https://arxiv.org/abs/2601.05401)
*Alicia Guo,David Ledo,George Fitzmaurice,Fraser Anderson*

Main category: cs.HC

TL;DR: 论文提出"原型采样"概念，结合生成式AI实现即时生成和混搭新媒体的创作过程，并通过Atelier系统实现这一理念。


<details>
  <summary>Details</summary>
Motivation: 传统创意过程依赖对现有媒体的采样和原型制作，而生成式AI使创作者能够超越现有媒体，即时生成和混搭新内容。需要一种新的框架和工具来支持这种融合了生成式AI的创意工作流程。

Method: 提出"原型采样"概念，基于现有文献进行定义和操作化。开发Atelier系统，这是一个画布式系统，利用多种生成式图像和视频模型进行视觉创作。系统特点包括：1) 融合思考和创作空间；2) 提供封装的技术工作流程；3) 通过交互式可视化、智能搜索和收藏支持涌现式导航。

Result: 成功定义了"原型采样"概念并开发了Atelier系统，该系统能够支持生成式AI时代的创意工作流程，将看似不相关的想法紧密交织成最终解决方案。

Conclusion: 原型采样作为一种视角，重新构建了创意工作，强调过程本身以及看似不相关的想法如何紧密交织成最终解决方案。Atelier系统为这种新型创意实践提供了技术实现框架。

Abstract: As an emergent process, creativity relies on explorations via sampling and prototyping for problem construction. These activities compile knowledge, provide a context enveloping the solution, and answer questions. With Generative AI, practitioners can go beyond sampling existing media towards instantly generating and remixing new ones. We refer to this convergence as 'protosampling'. Using existing literature we ground a definition for protosampling and operationalize it through Atelier, a canvas-like system that leverages a variety of generative image and video models for visual creation. Atelier: (1) blends the spaces for thinking and creation, where both references and generated assets co-exist in one space, (2) provides various encapsulated technical workflows that focus on the activity at hand, and (3) enables navigating emergence through interactive visualizations, smart search, and collections. Protosampling as a lens reframes creative work to emphasize the process itself and how seemingly disjointed thoughts can tightly interweave into a final solution.

</details>


### [52] [Secure Text Entry using a Virtual Radial Keyboard with Dynamically Resized Keys and Non-Intrusive Randomization](https://arxiv.org/abs/2601.05516)
*Yuxuan Huang,Qiao Jin,Tongyu Nie,Victoria Interrante,Evan Suma Rosenberg*

Main category: cs.HC

TL;DR: 本文针对VR文本输入的安全性问题，提出了一种新型虚拟径向键盘设计，通过环形字母排列和随机旋转机制，在安全性和可用性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟现实的广泛应用，安全高效的文本输入需求日益重要。现有最先进的VR安全文本输入方法存在漏洞，需要在安全性和可用性之间找到更好的平衡。

Method: 设计了一种虚拟径向键盘：1）按键按字母顺序环形排列；2）通过控制器旋转选择按键；3）按键动态扩展以方便精确选择；4）随机旋转机制在每次击键后移动键盘，保持相对位置但破坏绝对空间映射。

Result: 通过30人参与的用户研究，与先前安全方法和标准QWERTY键盘比较。结果显示径向键盘显著提高了抗击键预测攻击的能力，但由于非QWERTY布局不熟悉，在输入速度和主观工作量方面存在权衡。定量趋势和定性反馈表明通过练习有很强的性能提升潜力。

Conclusion: 径向键盘在安全性和可用性之间取得了良好平衡，为VR安全文本输入提供了有前景的解决方案。未来工作包括布局变化、视觉增强等界面改进方向。

Abstract: As virtual reality (VR) becomes more widely adopted, secure and efficient text entry is an increasingly critical need. In this paper, we identify a vulnerability in a state-of-the-art secure VR text entry method and introduce a novel virtual radial keyboard designed to achieve a balance between security with usability. Keys are arranged alphabetically in a circular layout, with each key selected by controller rotation and dynamically expanding to facilitate precise selection. A randomized rotation mechanism shifts the keyboard after each keystroke, preserving relative key positions while disrupting absolute spatial mappings to protect against inference attacks. We conducted a within-subject study (N=30) comparing our method with the prior secure technique and a standard QWERTY keyboard. Results showed that the radial keyboard significantly improves resistance to keystroke prediction attacks while incurring a tradeoff in entry speed and subjective workload due to the unfamiliar non-QWERTY layout. However, both quantitative trends and qualitative feedback indicate strong potential for performance improvements with practice. We also discuss design implications, possible interface refinements, and directions for future work, including layout variations and visual enhancements.

</details>


### [53] [Advancing credit mobility through stakeholder-informed AI design and adoption](https://arxiv.org/abs/2601.05666)
*Yerin Kwak,Siddharth Adelkar,Zachary A. Pardos*

Main category: cs.HC

TL;DR: 该研究应用人工智能技术为美国纽约州立大学系统开发课程衔接预测方法，通过利益相关者参与设计，显著提高了学分转移效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 从两年制大学转入四年制大学对学生的社会经济流动至关重要，但学分转移过程中课程衔接问题导致学生面临进度延迟和额外成本。目前的人工审核方式耗时耗力，虽然已有AI应用探索，但在实际衔接实践中仍有限制。

Method: 首先调查衔接工作人员和教师，评估基线算法推荐采纳率并收集反馈；基于这些洞察开发监督对齐方法，解决课程目录描述中的表面匹配和机构偏见问题。

Result: 新方法比先前方法准确度提高了5.5倍；基于该方法预测和61%的平均采纳率，预计可实现12倍的有效学分流动机会增长。

Conclusion: 利益相关者参与设计的AI方法能够扩展学生学分流动性，帮助重塑当前机构在课程衔接方面的决策过程，为高等教育管理提供可扩展支持。

Abstract: Transferring from a 2-year to a 4-year college is crucial for socioeconomic mobility, yet students often face challenges ensuring their credits are fully recognized, leading to delays in their academic progress and unexpected costs. Determining whether courses at different institutions are equivalent (i.e., articulation) is essential for successful credit transfer, as it minimizes unused credits and increases the likelihood of bachelor's degree completion. However, establishing articulation agreements remains time- and resource-intensive, as all candidate articulations are reviewed manually. Although recent efforts have explored the use of artificial intelligence to support this work, its use in articulation practice remains limited. Given these challenges and the need for scalable support, this study applies artificial intelligence to suggest articulations between institutions in collaboration with the State University of New York system, one of the largest systems of higher education in the US. To develop our methodology, we first surveyed articulation staff and faculty to assess adoption rates of baseline algorithmic recommendations and gather feedback on perceptions and concerns about these recommendations. Building on these insights, we developed a supervised alignment method that addresses superficial matching and institutional biases in catalog descriptions, achieving a 5.5-fold improvement in accuracy over previous methods. Based on articulation predictions of this method and a 61% average surveyed adoption rate among faculty and staff, these findings project a 12-fold increase in valid credit mobility opportunities that would otherwise remain unrealized. This study suggests that stakeholder-informed design of AI in higher education administration can expand student credit mobility and help reshape current institutional decision-making in course articulation.

</details>


### [54] [SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces](https://arxiv.org/abs/2601.05789)
*Tianwang Jia,Xiaoqing Chen,Dongrui Wu*

Main category: cs.HC

TL;DR: SAFE是一种基于联邦学习的脑电信号解码算法，通过本地数据训练保护隐私，采用批量归一化改善泛化，并通过对抗训练增强鲁棒性，在多个数据集上优于14种现有方法。


<details>
  <summary>Details</summary>
Motivation: 脑电信号解码算法面临泛化能力不足、对抗攻击脆弱性和隐私泄露三大挑战，需要一种能同时解决这些问题的方法。

Method: 提出SAFE联邦学习方法：1) 数据本地化保护隐私；2) 本地批量归一化处理跨被试特征分布偏移；3) 输入空间和参数空间的双重对抗训练增强鲁棒性。

Result: 在5个脑电数据集上测试，SAFE在解码准确率和对抗鲁棒性上均优于14种现有方法，甚至超过不考虑隐私保护的集中式训练方法。

Conclusion: SAFE是首个无需目标被试校准数据就能同时实现高解码精度、强对抗鲁棒性和可靠隐私保护的算法，适用于实际脑机接口应用。

Abstract: Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) are widely adopted due to their efficiency and portability; however, their decoding algorithms still face multiple challenges, including inadequate generalization, adversarial vulnerability, and privacy leakage. This paper proposes Secure and Accurate FEderated learning (SAFE), a federated learning-based approach that protects user privacy by keeping data local during model training. SAFE employs local batch-specific normalization to mitigate cross-subject feature distribution shifts and hence improves model generalization. It further enhances adversarial robustness by introducing perturbations in both the input space and the parameter space through federated adversarial training and adversarial weight perturbation. Experiments on five EEG datasets from motor imagery (MI) and event-related potential (ERP) BCI paradigms demonstrated that SAFE consistently outperformed 14 state-of-the-art approaches in both decoding accuracy and adversarial robustness, while ensuring privacy protection. Notably, it even outperformed centralized training approaches that do not consider privacy protection at all. To our knowledge, SAFE is the first algorithm to simultaneously achieve high decoding accuracy, strong adversarial robustness, and reliable privacy protection without using any calibration data from the target subject, making it highly desirable for real-world BCIs.

</details>


### [55] [Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI](https://arxiv.org/abs/2601.05825)
*Lucija Mihić Zidar,Philipp Wicke,Praneel Bhatia,Rosa Lutz,Marius Klug,Thorsten O. Zander*

Main category: cs.HC

TL;DR: 研究探讨了将脑电图（EEG）分类器从受控任务转移到口语人机对话中的可行性，建立了端到端处理流程，验证了工作负荷解码的跨范式迁移，并识别了隐式同意解码的局限性。


<details>
  <summary>Details</summary>
Motivation: 被动脑机接口为大型语言模型对齐提供了潜在的隐式反馈源，但大多数心理状态解码都在受控任务中进行，需要验证这些方法能否应用于实际口语对话场景。

Method: 引入两种对话范式（拼字游戏和句子补全任务），开发端到端流程进行转录、标注，并将词级对话事件与连续EEG分类器输出对齐，在试点研究中测试工作负荷和隐式同意解码。

Result: 工作负荷解码在口语互动中显示出可解释的趋势，支持跨范式迁移；隐式同意解码实现了连续应用和精确时间对齐，但发现了构造迁移和基于事件分类器异步应用的局限性。

Conclusion: 研究证实了将被动BCI信号整合到对话AI系统中的可行性，同时明确了相关约束条件，为未来应用奠定了基础。

Abstract: Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.

</details>


### [56] [How to Analyse Interviews: A Documentary Method of Interpretation](https://arxiv.org/abs/2601.05871)
*Andy Crabtree*

Main category: cs.HC

TL;DR: 本文提出了一种新颖的纪录片解释方法（DMI），用于分析访谈记录中的内生主题，该方法不需要分析师具备定性分析专业知识，仅需掌握自然语言能力。


<details>
  <summary>Details</summary>
Motivation: 访谈在HCI研究中非常普遍，但现有定性分析方法（如内容分析、扎根理论、解释现象学分析、主题分析等）通常需要分析师具备专业知识和理论背景。本文旨在开发一种更易访问的分析方法，让不具备专业定性分析技能的研究者也能有效分析访谈数据。

Method: 提出了纪录片解释方法（DMI），这是一种内生主题分析方法，专注于发现访谈记录集合中固有的、由参与者集体推理形成的主题。该方法强调分析那些内生于数据本身、反映参与者对研究相关议题集体理解的主题。

Result: DMI与现有定性分析方法形成鲜明对比：它不需要分析师精通定性分析技术，也不需要扎实的理论和方法知识基础。DMI是一种"成员方法"而非社会科学方法，主要依赖自然语言的掌握能力——这是大多数人具备的能力。

Conclusion: 纪录片解释方法为HCI领域的访谈分析提供了一种新颖、易用的替代方案，降低了定性分析的门槛，使更多研究者能够有效分析访谈数据中的内生主题和集体推理过程。

Abstract: Interviews are commonplace in HCI. This paper presents a novel documentary method of interpretation that supports analysis of the topics contained within a collection of transcripts, topics that are endogenous to it and which elaborate participants collective reasoning about issues of relevance to research. We contrast endogenous topic analysis with established qualitative approaches, including content analysis, grounded theory, interpretative phenomenological analysis, and thematic analysis, to draw out the distinctive character of the documentary method of interpretation. Unlike established methods, the DMI does not require that the analyst be proficient in qualitative analysis, or have sound knowledge of underlying theories and methods. The DMI is a members method, not a social science method, that relies on mastery of natural language; a competence most people possess.

</details>


### [57] [A Framework for Optimizing Human-Machine Interaction in Classification Systems](https://arxiv.org/abs/2601.05974)
*Goran Muric,Steven Minton*

Main category: cs.HC

TL;DR: 论文提出了一个基于双阈值策略优化人机协同分类系统的实用框架，通过自动处理置信案例和人工审核模糊案例来平衡系统准确性与人工工作量。


<details>
  <summary>Details</summary>
Motivation: 自动化决策系统在不确定情况下越来越依赖人工监督来确保准确性。现有系统通常使用单一决策阈值，无法有效平衡系统自动化程度与人工干预需求，需要更精细的策略来优化人机协同效率。

Method: 采用双阈值策略框架：定义下限和上限两个阈值，自动接受置信度高的案例，自动拒绝置信度低的案例，将处于两个阈值之间的模糊案例路由给人工审核。将该问题形式化为优化任务，平衡系统准确性与人工审核工作量，并通过大量蒙特卡洛模拟验证框架行为。

Result: 量化了不同概率分数分布对人工干预效率的影响，识别了边际收益递减区域（即额外审核带来最小收益的区域）。框架为需要选择性人工验证的任何决策流程提供了通用、可重复的可靠性改进方法。

Conclusion: 双阈值策略框架能够有效优化人机协同分类系统，在保持准确性的同时合理分配人工审核资源。该框架适用于实体解析、欺诈检测、医疗分诊、内容审核等多种需要选择性人工验证的应用场景。

Abstract: Automated decision systems increasingly rely on human oversight to ensure accuracy in uncertain cases. This paper presents a practical framework for optimizing such human-in-the-loop classification systems using a double-threshold policy. Instead of relying on a single decision cutoff, the system defines two thresholds (a lower and an upper) to automatically accept or reject confident cases while routing ambiguous ones for human review. We formalize this problem as an optimization task that balances system accuracy against human review workload and demonstrate its behavior through extensive Monte Carlo simulations. Our results quantify how different probability score distributions affect the efficiency of human intervention and identify the regions of diminishing returns where additional review yields minimal benefit. The framework provides a general, reproducible method for improving reliability in any decision pipeline requiring selective human validation, including applications in entity resolution, fraud detection, medical triage, and content moderation.

</details>
