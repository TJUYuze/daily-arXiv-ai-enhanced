<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 163]
- [cs.HC](#cs.HC) [Total: 16]
- [physics.med-ph](#physics.med-ph) [Total: 5]
- [cs.RO](#cs.RO) [Total: 52]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices](https://arxiv.org/abs/2512.05969)
*Hokin Deng*

Main category: cs.CV

TL;DR: 视频生成模型现在可以进行推理任务，在象棋、迷宫、数独、心理旋转和瑞文矩阵等任务上，Sora-2等领先模型达到60%的成功率。研究者建立了基于"任务对"设计的实验范式，并开发了支持该范式的代码框架。


<details>
  <summary>Details</summary>
Motivation: 探索视频生成模型是否具备推理能力，建立可扩展的评估范式来系统测试模型在各种推理任务上的表现。

Method: 采用"任务对"设计作为核心实验范式，构建了包含39个模型的代码框架VMEvalKit，支持自动评估并与人类判断高度相关。

Result: 在象棋、迷宫、数独、心理旋转和瑞文矩阵等推理任务上，Sora-2等领先模型达到60%的成功率，自动评估与人类判断强相关。

Conclusion: 视频生成模型已具备一定推理能力，建立的评估范式具有高度可扩展性，为通过强化学习提升视频模型推理能力提供了机会。

Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.

</details>


### [2] [Adaptive Dataset Quantization: A New Direction for Dataset Pruning](https://arxiv.org/abs/2512.05987)
*Chenyue Yu,Jianyu Yu*

Main category: cs.CV

TL;DR: 提出一种针对边缘设备大规模数据集存储和通信成本的新颖数据集量化方法，通过减少样本内冗余来压缩数据，相比传统方法在相同压缩比下保持更好的模型训练性能。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限的边缘设备在大规模数据集存储和通信方面面临的成本挑战。传统的数据集剪枝和蒸馏方法主要关注样本间冗余，而本文关注样本内冗余的压缩。

Method: 提出数据集量化方法：1) 对每个样本应用线性对称量化获取初始量化范围和尺度；2) 引入自适应量化分配算法，根据不同样本的精度需求分配不同量化比例，同时保持总压缩比恒定。

Result: 在CIFAR-10、CIFAR-100和ImageNet-1K数据集上的实验表明，该方法在保持模型训练性能的同时实现了显著的数据集压缩，在相同压缩比下优于传统量化和数据集剪枝基线方法。

Conclusion: 该方法首次使用有限比特表示数据集以降低存储需求，引入具有自适应比例分配的数据集级量化算法，为边缘设备的大规模数据集管理提供了有效的解决方案。

Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

</details>


### [3] [EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head](https://arxiv.org/abs/2512.05991)
*Chang Liu,Tianjiao Jing,Chengcheng Ma,Xuanqi Zhou,Zhengxuan Lian,Qin Jin,Hongliang Yuan,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: EmoDiffTalk：首个支持连续多模态情感编辑的3D高斯泼溅说话头生成框架，通过情感感知高斯扩散和文本到动作单元控制器实现精细面部动画和动态情感编辑


<details>
  <summary>Details</summary>
Motivation: 当前基于3D高斯泼溅的真实感3D说话头在情感表达操控方面存在显著不足，特别是在使用多模态控制进行细粒度和扩展性动态情感编辑方面

Method: 提出EmoDiffTalk框架，核心是情感感知高斯扩散：包括动作单元提示高斯扩散过程用于细粒度面部动画，以及精确的文本到动作单元情感控制器，通过文本输入提供准确且扩展的动态情感编辑

Result: 在公开的EmoTalk3D和RenderMe-360数据集上实验表明，EmoDiffTalk在情感细微度、唇形同步保真度和可控性方面优于先前工作，建立了高质量、扩散驱动、多模态可编辑3D说话头合成的原则性路径

Conclusion: EmoDiffTalk是首批支持在基于动作单元的表情空间内进行连续多模态情感编辑的3D高斯泼溅说话头生成框架之一，为高质量可编辑3D说话头合成提供了新方向

Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.

</details>


### [4] [Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology](https://arxiv.org/abs/2512.05993)
*Ruchika Verma,Shrishtee Kandoi,Robina Afzal,Shengjia Chen,Jannes Jegminat,Michael W. Karlovich,Melissa Umphlett,Timothy E. Richardson,Kevin Clare,Quazi Hossain,Jorge Samanamud,Phyllis L. Faust,Elan D. Louis,Ann C. McKee,Thor D. Stein,Jonathan D. Cherry,Jesse Mez,Anya C. McGoldrick,Dalilah D. Quintana Mora,Melissa J. Nirenberg,Ruth H. Walker,Yolfrankcis Mendez,Susan Morgello,Dennis W. Dickson,Melissa E. Murray,Carlos Cordon-Cardo,Nadejda M. Tsankova,Jamie M. Walker,Diana K. Dangoor,Stephanie McQuillan,Emma L. Thorn,Claudia De Sanctis,Shuying Li,Thomas J. Fuchs,Kurt Farrell,John F. Crary,Gabriele Campanella*

Main category: cs.CV

TL;DR: NeuroFM是一个专门针对神经病理学领域训练的基础模型，相比通用病理学模型在神经退行性疾病分析中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型主要基于外科病理数据训练，这些数据富含非神经组织，过度代表了非神经系统疾病。神经病理学具有独特的细胞类型、细胞结构和疾病特征，这种领域不匹配限制了通用模型捕捉神经退行性疾病关键形态学模式的能力。

Method: 开发了NeuroFM，这是一个专门在涵盖多种神经退行性病理的脑组织全切片图像上训练的基础模型。

Result: NeuroFM在多个神经病理学特定下游任务中表现出优于通用模型的性能，包括混合性痴呆疾病分类、海马区域分割以及神经退行性共济失调识别（涵盖小脑性特发性震颤和脊髓小脑性共济失调亚型）。

Conclusion: 针对脑组织训练的领域专业化基础模型比通用外科病理数据集训练的模型能更好地捕捉神经病理学特定特征。NeuroFM通过针对神经退行性疾病的独特形态学景观定制基础模型，为脑疾病诊断和研究提供了更准确可靠的AI分析，为数字病理学专业领域的领域特定模型开发树立了先例。

Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.

</details>


### [5] [FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting](https://arxiv.org/abs/2512.05996)
*Yi Liu,Jingyu Song,Vedanth Kallakuri,Katherine A. Skinner*

Main category: cs.CV

TL;DR: FishDetector-R1是一个基于MLLM的弱监督框架，用于水下鱼类检测、分割和计数，在DeepFish数据集上显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类图像分析对生态监测至关重要，但由于视觉质量下降和标注成本高昂，这一问题仍然具有挑战性。

Method: 提出统一的MLLM框架，包含两个关键组件：1) 新颖的detect-to-count提示机制，确保空间一致的检测和计数；2) 基于可验证奖励的强化学习(RLVR)，利用稀疏点标签的可扩展范式。

Result: 在DeepFish数据集上，AP提升20%，mIoU提升10%，MAE降低30%，GAME降低35%。消融研究验证了奖励设计的有效性，且改进能很好地泛化到其他水下数据集，显示出强大的跨域鲁棒性。

Conclusion: FishDetector-R1通过弱监督为准确的海洋视觉理解提供了可靠且可扩展的解决方案。

Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.

</details>


### [6] [High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing](https://arxiv.org/abs/2512.06012)
*Emmanuel Akeweje,Conall Kirk,Chi-Wai Chan,Denis Dowling,Mimi Zhang*

Main category: cs.CV

TL;DR: 开发了一个基于机器学习的自动化框架，用于大规模分析金属粉末形态，通过高分辨率成像和聚类算法识别粉末形状特征，为SLM工艺的原料质量控制提供支持。


<details>
  <summary>Details</summary>
Motivation: 选择性激光熔化(SLM)工艺的零件质量严重依赖原料粉末形态，但传统粉末表征方法效率低、定性化，无法捕捉工业规模批次的异质性，需要开发高效自动化分析工具。

Method: 提出自动化机器学习框架，结合高通量成像、形状提取和聚类分析。开发并评估三种聚类流程：自编码器流程、形状描述符流程和函数数据流程。在约126,000个粉末图像数据集上进行验证。

Result: 内部有效性指标显示傅里叶描述符+k-means流程效果最佳，获得最低的Davies-Bouldin指数和最高的Calinski-Harabasz分数，同时在标准工作站上保持每个颗粒亚毫秒级的运行时间。

Conclusion: 该无监督学习框架实现了粉末形态的快速自动化评估，支持跟踪粉末在重复使用周期中的形状演变，为SLM工作流程中的实时原料监控提供了可行路径。

Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.

</details>


### [7] [VAT: Vision Action Transformer by Unlocking Full Representation of ViT](https://arxiv.org/abs/2512.06013)
*Wenhao Li,Chengwei Ma,Weixin Mao*

Main category: cs.CV

TL;DR: VAT是一种基于Vision Transformer的新型架构，通过利用ViT所有层的特征层次结构，实现感知与动作生成的深度渐进融合，在机器人模仿学习中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习中，Vision Transformers（ViTs）是视觉感知的标准方法，但大多数方法只使用最后一层的特征，丢弃了有价值的信息。作者认为这提供了不足的表示，需要充分利用ViT的完整特征层次结构。

Method: 提出Vision Action Transformer（VAT），这是一种从ViT扩展而来的新型架构。VAT处理专门的动作令牌，在所有Transformer层中与视觉特征进行交互，实现感知与动作生成的深度渐进融合。

Result: 在一系列模拟操作任务中，VAT在四个LIBERO基准测试中实现了98.15%的平均成功率，超越了OpenVLA-OFT等先前方法，建立了新的最先进水平。

Conclusion: VAT不仅为模仿学习提供了一个强大的模型，还证明了利用视觉模型的完整"表示轨迹"对于推进机器人策略至关重要。这项工作展示了充分利用ViT特征层次结构的重要性。

Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.

</details>


### [8] [Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets](https://arxiv.org/abs/2512.06014)
*Jiho Shin,Dominic Marshall,Matthieu Komorowski*

Main category: cs.CV

TL;DR: 该研究对两个大规模胸部X光嵌入模型（CXR-Foundation和MedImageInsight）在公开数据集上进行标准化基准测试，评估其在医学图像表示学习中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学图像表示学习方面表现出色，但它们在跨数据集上的比较行为尚未得到充分探索。本研究旨在建立标准化的评估框架，为医学基础模型提供可复现的基准。

Method: 使用统一的预处理流程和固定的下游分类器，在两个公开胸部X光数据集（MIMIC-CR和NIH ChestX-ray14）上评估两个模型。直接从预训练编码器提取嵌入，使用轻量级LightGBM分类器训练多个疾病标签，报告平均AUROC和F1分数及95%置信区间。

Result: MedImageInsight在大多数任务中表现略优，而CXR-Foundation展现出更强的跨数据集稳定性。MedImageInsight嵌入的无监督聚类进一步揭示了与定量结果一致的疾病特异性结构。

Conclusion: 研究结果强调了医学基础模型标准化评估的必要性，并为未来多模态和临床整合研究建立了可复现的基准。

Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.

</details>


### [9] [PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation](https://arxiv.org/abs/2512.06020)
*Wenyi Mo,Tianyu Zhang,Yalong Bai,Ligong Han,Ying Ba,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 提出一个多模态框架，利用MLLM提取用户偏好表示并注入扩散模型，通过偏好导向的视觉问答和两种判别任务来捕捉细粒度语义线索，实现个性化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法捕捉细微的用户偏好，要么缺乏有效的个性化视觉信号编码机制，需要开发能更好反映个人审美选择的图像生成方法。

Method: 1) 训练MLLM进行偏好导向的视觉问答来捕捉细粒度语义线索；2) 引入两种互补的探测任务：用户间判别（区分不同用户）和用户内判别（区分喜欢与不喜欢的内容）；3) 设计基于最大均值差异的对齐损失来弥合模态差距；4) 将得到的嵌入用于条件化生成器。

Result: 大量实验表明，该方法在图像质量和偏好对齐方面显著优于强基线，突出了表示提取和对齐在个性化生成中的有效性。

Conclusion: 提出的多模态框架通过有效提取用户偏好表示并将其与扩散模型对齐，实现了既能忠实于文本提示又能符合用户偏好的个性化图像生成。

Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.

</details>


### [10] [Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying](https://arxiv.org/abs/2512.06190)
*Shichen Li,Ahmadreza Eslaminia,Chenhui Shao*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态颜色轨迹预测方法，通过整合高维时序颜色信息和干燥工艺参数，实现准确且数据高效的颜色轨迹预测，在未见过的干燥条件下显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 食品干燥中颜色演变是产品质量的重要指标，现有研究主要依赖低维颜色特征，无法完全捕捉食品样本复杂的动态颜色轨迹，且现有建模方法缺乏对未见工艺条件的泛化能力。

Method: 开发了一种新颖的多模态颜色轨迹预测方法，整合高维时序颜色信息与干燥工艺参数，实现准确且数据高效的颜色轨迹预测。

Result: 在未见干燥条件下，模型在饼干干燥中达到RMSE 2.12，苹果干燥中达到RMSE 1.29，相比基线模型误差减少超过90%。

Conclusion: 实验结果表明该模型具有优越的准确性、鲁棒性和广泛适用性，能够有效预测食品干燥过程中的颜色轨迹变化。

Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.

</details>


### [11] [The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation](https://arxiv.org/abs/2512.06032)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.CV

TL;DR: SAM2和SAM3存在根本性断裂：SAM2是基于空间提示的纯视觉-时序分割模型，而SAM3是统一视觉语言架构，具备开放词汇推理、语义基础、对比对齐和基于示例的概念理解能力。


<details>
  <summary>Details</summary>
Motivation: 分析SAM2和SAM3之间的根本性差异，解释为什么SAM2在基于提示的分割方面的专业知识无法迁移到SAM3的多模态概念驱动范式中，阐明SAM3代表了分割基础模型的新类别。

Method: 通过五个核心组件进行结构化分析：1) 基于提示与基于概念的分割之间的概念断裂；2) 架构差异；3) 数据集和标注差异；4) 训练和超参数区别；5) 评估、指标和失败模式。

Result: SAM2和SAM3在概念、架构、数据、训练和评估方面存在根本性差异，SAM2专注于空间提示和几何时序分割，而SAM3引入了多模态概念驱动范式，具备开放词汇推理和语义理解能力。

Conclusion: SAM3代表了分割基础模型的新类别，标志着从基于提示的分割向概念驱动分割时代的转变，为新兴的概念驱动分割领域指明了未来发展方向。

Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.

</details>


### [12] [EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing](https://arxiv.org/abs/2512.06065)
*Runjia Li,Moayed Haji-Ali,Ashkan Mirzaei,Chaoyang Wang,Arpit Sahni,Ivan Skorokhodov,Aliaksandr Siarohin,Tomas Jakab,Junlin Han,Sergey Tulyakov,Philip Torr,Willi Menapace*

Main category: cs.CV

TL;DR: 提出EgoEdit生态系统，用于解决第一人称视频编辑的独特挑战，包括快速自我运动和频繁的手物交互，支持实时流式推理


<details>
  <summary>Details</summary>
Motivation: 现有AI视频编辑器在第三人称视频上表现良好，但第一人称视角存在独特挑战（快速自我运动、频繁手物交互），导致显著的领域差距。现有离线编辑流程延迟高，限制了实时交互

Method: 构建EgoEditData数据集（专门为第一人称编辑场景设计，包含丰富手物交互并明确保留手部）；开发EgoEdit指令跟随第一人称视频编辑器（支持单GPU实时流式推理）；引入EgoEditBench评估套件（针对指令忠实度、手部和交互保留、自我运动下的时间稳定性）

Result: EgoEdit在时间稳定性、指令忠实度和交互延迟方面表现优异。在第一人称编辑基准上取得明显优势（现有方法在此表现不佳），同时在通用编辑任务上保持与最强基线相当的性能

Conclusion: 提出了完整的EgoEdit生态系统，解决了第一人称视频编辑的独特挑战，实现了实时交互式编辑，数据集和评估基准将公开供研究社区使用

Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit

</details>


### [13] [Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light](https://arxiv.org/abs/2512.06080)
*Tzofi Klinghoffer,Siddharth Somasundaram,Xiaoyu Xiang,Yuchen Fan,Christian Richardt,Akshat Dave,Ramesh Raskar,Rakesh Ranjan*

Main category: cs.CV

TL;DR: 利用单光子激光雷达通过多弹跳光信息恢复遮挡区域和镜面反射场景的3D重建，提出数据驱动方法处理多路照明下的复杂光传输问题


<details>
  <summary>Details</summary>
Motivation: 单视角3D场景重建在存在遮挡区域和镜面材料（如镜子）时具有挑战性，传统单光子激光雷达方法只能逐点照明，而实际应用中需要同时照明多个场景点

Method: 提出数据驱动方法处理单光子激光雷达中的光传输逆问题，创建首个大规模室内场景激光雷达瞬态数据集（约10万个样本），学习复杂光传输先验，将测量的两弹跳光分解为每个激光点的贡献

Result: 实验证明分解后的光可用于从单次测量中推断具有遮挡和镜子的场景的3D几何形状，实现了在复杂场景下的3D重建

Conclusion: 通过数据驱动方法处理多路照明下的单光子激光雷达光传输问题，能够有效恢复遮挡几何和镜面反射场景的密集深度信息，为实际应用提供了可行方案

Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

</details>


### [14] [BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving](https://arxiv.org/abs/2512.06096)
*Karthik Mohan,Sonam Singh,Amit Arvind Kale*

Main category: cs.CV

TL;DR: BeLLA是一个端到端架构，将统一的360°鸟瞰图表示与大型语言模型连接，用于自动驾驶问答任务，在需要空间推理的问题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自动驾驶研究中存在局限性：单视角编码器无法利用多摄像头系统的空间结构，而聚合多视角特征缺乏统一的空间表示，难以进行自我中心方向、物体关系和上下文推理。

Method: 提出BeLLA端到端架构，连接统一的360°鸟瞰图表示与大型语言模型，用于自动驾驶场景的问答任务。

Result: 在NuScenes-QA和DriveLM基准测试中，BeLLA在需要空间推理的问题上（如相对物体定位和附近物体行为理解）持续优于现有方法，某些任务获得高达+9.3%的绝对提升，在其他类别中也表现有竞争力。

Conclusion: BeLLA通过结合统一的鸟瞰图表示和语言模型，显著提升了自动驾驶场景中空间推理能力，能够处理多样化的问答任务。

Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.

</details>


### [15] [Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation](https://arxiv.org/abs/2512.06105)
*Junwen Zheng,Xinran Xu,Li Rong Wang,Chang Cai,Lucinda Siyun Tan,Dingyuan Wang,Hong Liang Tey,Xiuyi Fan*

Main category: cs.CV

TL;DR: 提出CEFM框架，通过对比学习将临床ABC诊断标准映射到视觉特征空间，生成结构化文本解释，提升黑色素瘤分类的可解释性和临床信任度。


<details>
  <summary>Details</summary>
Motivation: 深度学习在黑色素瘤分类中已达到专家级性能，但模型不透明和缺乏可解释性阻碍了临床采用。临床医生难以信任黑盒模型的决策过程。

Method: 提出跨模态可解释框架CEFM，使用对比学习作为核心机制。将临床诊断标准（不对称性、边界、颜色）通过双投影头映射到Vision Transformer嵌入空间，对齐临床语义与视觉特征，然后通过自然语言生成转换为结构化文本解释。

Result: 在公开数据集上达到92.79%准确率和0.961的AUC，在多个可解释性指标上有显著提升。定性分析显示学习到的嵌入空间排列与临床医生应用ABC规则一致。

Conclusion: CEFM框架有效连接了高性能分类与临床信任，通过将临床诊断标准与视觉特征对齐并生成可解释的文本输出，为黑色素瘤诊断提供了透明可靠的AI辅助工具。

Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.

</details>


### [16] [Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation](https://arxiv.org/abs/2512.06158)
*Su Sun,Cheng Zhao,Himangi Mittal,Gaurav Mittal,Rohith Kukkala,Yingjie Victor Chen,Mei Chen*

Main category: cs.CV

TL;DR: Track4DGen：一个两阶段框架，通过结合多视角视频扩散模型、基础点追踪器和混合4D高斯泼溅重建器，从稀疏输入生成动态4D对象，解决了外观和运动一致性保持的问题。


<details>
  <summary>Details</summary>
Motivation: 从稀疏输入生成动态4D对象很困难，因为需要同时保持跨视角和时间的外观与运动一致性，同时抑制伪影和时间漂移。现有方法主要依赖像素或潜在空间的视频扩散损失监督，缺乏显式的、时间感知的特征级追踪指导。

Method: Track4DGen采用两阶段框架：第一阶段在多视角视频扩散生成器中强制密集的特征级点对应关系，产生时间一致的特征；第二阶段使用混合运动编码重建4D高斯泼溅，将扩散特征（携带追踪先验）与Hex-plane特征拼接，并用4D球谐函数增强动态建模。

Result: Track4DGen在多视角视频生成和4D生成基准测试中均超越基线方法，产生时间稳定、可文本编辑的4D资产。此外还创建了Sketchfab28数据集，用于基准测试和未来研究。

Conclusion: 通过显式注入追踪器导出的运动先验到中间特征表示中，Track4DGen能够生成具有更好时间一致性和跨视角一致性的动态4D对象，为4D生成领域提供了有效解决方案。

Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.

</details>


### [17] [Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection](https://arxiv.org/abs/2512.06171)
*Jessica Plassmann,Nicolas Schuler,Michael Schuth,Georg von Freymann*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自动化工作流程，用于从剪切散斑测量中生成缺陷标注，减少人工标注工作量，支持可扩展的数据集创建。


<details>
  <summary>Details</summary>
Motivation: 剪切散斑技术虽然对表面位移梯度敏感，能有效检测安全关键部件的亚表面缺陷，但其工业应用受到高质量标注数据集缺乏的限制。人工标注劳动密集、主观性强且难以标准化。

Method: 引入自动化工作流程，利用深度学习从剪切散斑测量中生成缺陷标注，包括高分辨率分割和边界框标签。

Result: 与专家标注数据对比评估显示，该方法具有足够准确性，能够支持弱监督训练，减少人工工作量，并为稳健缺陷检测提供可扩展的数据集创建支持。

Conclusion: 提出的自动化标注工作流程解决了剪切散斑技术工业应用中的关键瓶颈，通过减少对人工标注的依赖，支持大规模、标准化缺陷检测数据集的创建。

Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.

</details>


### [18] [Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction](https://arxiv.org/abs/2512.06174)
*Shilin Hu,Jingyi Xu,Akshat Dave,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 该论文提出了一种将显式物理建模（几何和光照）嵌入到基于深度学习的阴影生成中的新框架，通过单目RGB图像获取3D几何和主导光照方向，基于阴影形成物理原理生成初始阴影，再用扩散模型进行精细化，在DESOBAV2数据集上训练，生成既真实又物理一致的阴影。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的阴影生成方法很少利用显式的物理建模，而阴影形成的物理原理表明，遮挡物会阻挡来自光源的光线，形成跟随遮挡物轮廓的阴影。论文旨在将这种物理建模嵌入到深度学习框架中，以生成更真实且物理一致的阴影。

Method: 首先从单目RGB图像获取密集点云形式的近似3D几何，并预测主导光照方向。基于这些信号，利用阴影形成的物理原理恢复相对准确的阴影位置和形状。然后将这个基于物理的初始估计集成到扩散框架中，对阴影进行精细化处理，生成真实、高保真的外观，同时确保与场景几何和光照的一致性。

Result: 在DESOBAV2数据集上训练后，该模型生成的阴影既视觉真实又物理一致，优于现有方法，特别是在具有复杂几何或模糊光照的场景中表现更佳。

Conclusion: 该研究成功地将显式物理建模（几何和光照）嵌入到基于深度学习的阴影生成框架中，通过结合物理原理和扩散模型，能够生成既真实又物理一致的阴影，在复杂场景中表现优于现有方法。

Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.

</details>


### [19] [Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction](https://arxiv.org/abs/2512.06179)
*Shilin Hu,Jingyi Xu,Sagnik Das,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 提出首个联合检测投射阴影和附着阴影的框架，通过光照-几何推理循环优化阴影分割和光照估计，在自建数据集上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有阴影检测方法主要针对投射阴影，缺乏专门的附着阴影检测数据集和模型。附着阴影对理解物体三维结构和场景理解至关重要，需要填补这一研究空白。

Method: 提出包含阴影检测模块和光照估计模块的联合框架：1) 阴影检测模块分别预测两种阴影类型；2) 光照估计模块从检测到的阴影推断光照方向；3) 结合表面法线生成几何一致的部分遮挡图；4) 通过闭环推理过程迭代优化阴影预测和光照估计。

Result: 实验结果表明，该方法显著改善了附着阴影的检测性能，BER（平衡错误率）至少降低33%，同时保持了良好的整体阴影和投射阴影检测性能。

Conclusion: 通过光照-几何推理的迭代闭环框架能够有效联合检测投射阴影和附着阴影，填补了该领域的研究空白，为三维场景理解提供了重要工具。

Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.

</details>


### [20] [The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning](https://arxiv.org/abs/2512.06206)
*Akis Linardos,Sarthak Pati,Ujjwal Baid,Brandon Edwards,Patrick Foley,Kevin Ta,Verena Chung,Micah Sheller,Muhammad Irfan Khan,Mojtaba Jafaritadi,Elina Kontio,Suleiman Khan,Leon Mächler,Ivan Ezhov,Suprosanna Shit,Johannes C. Paetzold,Gustav Grimberg,Manuel A. Nickel,David Naccache,Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni,Daewoon Kim,Leonard L. Klausmann,Prashant Shah,Bjoern Menze,Dimitrios Makris,Spyridon Bakas*

Main category: cs.CV

TL;DR: MICCAI FeTS 2024挑战赛专注于联邦学习在胶质瘤亚区分割中的应用，评估了新的权重聚合方法以提升鲁棒性和效率。基于PID控制器的方法在分割性能和通信效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 该挑战赛旨在推进联邦学习在医学影像中的应用，特别是在多中心胶质瘤MRI数据的分割任务中，评估新的权重聚合方法以提高联邦学习的鲁棒性和通信效率。

Method: 使用标准化的联邦学习设置和多机构数据集（来自BraTS基准，共1,251个训练病例、219个验证病例和570个隐藏测试病例）。采用累积评分系统，综合考虑分割性能（Dice相似系数和95%Hausdorff距离）和通信效率（收敛分数）。

Result: 基于PID控制器的方法获得最高排名，ET、TC、WT的平均DSC分别为0.733、0.761、0.751，对应的HD95值分别为33.922mm、33.623mm、32.309mm，通信效率也最高（收敛分数0.764）。该方法超越了以往挑战赛的最佳方法。

Conclusion: PID控制器是稳定和优化联邦学习中权重聚合的有效机制，该挑战赛推动了医学影像中联邦学习的发展，代码已开源。

Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.

</details>


### [21] [Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study](https://arxiv.org/abs/2512.06221)
*Alena Makarova*

Main category: cs.CV

TL;DR: 对一篇结合SVD和WDR的图像压缩方法进行独立可重复性研究，发现原论文声称的优于JPEG2000和WDR的结果无法复现，实际性能并未超越JPEG2000，SSIM仅部分改进


<details>
  <summary>Details</summary>
Motivation: 验证原论文声称的SVD+WDR图像压缩技术优于JPEG2000和WDR的结论是否可重复，并识别原方法描述中的模糊之处对可重复性的影响

Method: 重新实现原论文的SVD+WDR方法，仔细填补缺失的实现细节，尽可能复现原始实验，并在新图像上进行额外实验，使用PSNR和SSIM作为评估指标

Result: 与原论文声称相反，SVD+WDR技术在PSNR方面通常未超越JPEG2000或WDR，仅在SSIM方面相对于JPEG2000有部分改进，揭示了原方法描述中的模糊细节（如量化和阈值初始化）对性能报告的显著影响

Conclusion: 该可重复性研究表明原论文的SVD+WDR方法并未如声称的那样优于JPEG2000和WDR，强调了方法描述清晰度对科学可重复性的重要性，模糊的实现细节可能导致性能评估偏差

Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.

</details>


### [22] [Opinion: Learning Intuitive Physics May Require More than Visual Data](https://arxiv.org/abs/2512.06232)
*Ellen Su,Solim Legris,Todd M. Gureckis,Mengye Ren*

Main category: cs.CV

TL;DR: 在SAYCam发育现实数据集上预训练的V-JEPA模型，在IntPhys2直觉物理基准测试中未表现出显著性能提升，表明仅靠发育现实数据不足以让当前架构学习直觉物理表征。


<details>
  <summary>Details</summary>
Motivation: 人类通过基于直觉物理理解的丰富内部模型熟练导航世界，而尽管在大规模互联网视频数据上训练，最先进的深度学习模型在直觉物理基准测试中仍达不到人类水平。本研究探讨数据分布（而非数据量）是否是学习这些物理原理的关键。

Method: 在SAYCam数据集上预训练视频联合嵌入预测架构（V-JEPA）模型，该数据集是发育现实的、自我中心的视频数据集，部分捕捉了三名儿童的日常视觉体验，数据量仅为最先进模型训练数据的0.01%。

Result: 在SAYCam数据集上训练并未导致IntPhys2基准测试性能的显著改善，表明仅训练于发育现实数据集不足以让当前架构学习支持直觉物理的表征。

Conclusion: 仅改变视觉数据量和分布可能不足以构建具有人工直觉物理的系统，需要更深入的方法来使模型学习物理原理。

Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.

</details>


### [23] [Language-driven Fine-grained Retrieval](https://arxiv.org/abs/2512.06255)
*Shijie Wang,Xin Yu,Yadan Luo,Zijian Wang,Pengfei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: LaFG是一个语言驱动的细粒度图像检索框架，通过大语言模型和视觉语言模型将类别名称转换为属性级监督，以解决传统方法在未见类别上泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度图像检索方法使用基于类别名称的稀疏one-hot标签作为监督，虽然对已见类别有效，但忽略了类别名称中丰富的语义信息，导致跨类别细节可比性建模不足，限制了模型对未见类别的泛化能力。

Method: 1. 使用大语言模型将类别名称转换为详细的属性导向描述；2. 利用冻结的视觉语言模型将描述投影到视觉对齐空间，聚类形成数据集级属性词汇表；3. 通过全局提示模板选择类别相关属性，聚合成类别特定的语言原型；4. 使用这些原型监督检索模型。

Result: 该方法能够更好地建模跨类别细节的可比性，提升细粒度图像检索模型在未见类别上的泛化能力。

Conclusion: LaFG通过语言驱动的属性级监督，有效解决了传统细粒度图像检索方法在语义建模和未见类别泛化方面的局限性，为细粒度视觉理解提供了新思路。

Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer

</details>


### [24] [Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs](https://arxiv.org/abs/2512.06258)
*Chaoyang Wang,Yangfan He,Yiyang Zhou,Yixuan Wang,Jiaqi Liu,Peng Xia,Zhengzhong Tu,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

TL;DR: 论文揭示大型视觉语言模型存在路径选择偏差问题：即使知道正确答案，也常通过错误推理路径得出，导致结果不稳定。作者提出PSO两阶段优化框架，通过路径选择优化提升推理性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型存在一个关键但未被充分探索的缺陷：即使模型知道正确答案，也经常通过错误的推理路径得出结果。核心问题不是缺乏知识，而是在庞大的推理搜索空间中的路径选择偏差。模型虽然能够采样正确的解决方案轨迹，但不成比例地倾向于不稳定或逻辑不一致的路径，导致结果不稳定和不可靠。

Method: 提出PSO（路径选择优化）两阶段后训练框架：第一阶段使用带模板和答案奖励的组相对策略优化来培养结构化、逐步推理；第二阶段进行在线偏好优化，模型从GRPO生成的数据中采样推理路径，自我评估，并向优选轨迹对齐。错误或次优路径同时存储在负向回放记忆中作为硬负样本，定期回顾以防止模型重复先前错误并促进持续推理改进。

Result: 大量实验表明，PSO有效修剪无效推理路径，显著提高推理准确性（平均提升7.4%），并产生更稳定一致的思维链。

Conclusion: PSO框架成功解决了大型视觉语言模型的路径选择偏差问题，通过优化推理路径选择，显著提升了模型的推理性能和稳定性，为改进LVLMs的可靠性提供了有效方法。

Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.

</details>


### [25] [TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06269)
*Quan Tran,Tuan Dang*

Main category: cs.CV

TL;DR: 本文提出了一种通过多视角三角测量约束来改进3D高斯泼溅重建质量的方法，解决了传统方法因仅依赖光度损失导致的几何不一致问题，在DTU数据集上取得了0.50mm的平均倒角距离。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然能实时合成新视角且渲染逼真，但仅依赖光度损失会导致重建不一致，产生"漂浮物"伪影和非结构化几何，无法提取高质量表面。需要引入几何一致性约束来解决这一问题。

Method: 提出了一种通过约束多视角三角测量来增强全局几何一致性的方法。利用多个估计视角在物理世界中达成3D表示共识，通过自监督方式从相邻视角束重新三角化得到鲁棒共识点，并惩罚渲染3D点与共识点的偏差来优化过程。

Result: 在多个数据集上验证了方法的有效性，取得了最先进的结果。在DTU数据集上获得了0.50mm的平均倒角距离，优于同类显式方法。

Conclusion: 通过引入多视角三角测量约束，显著改善了3D高斯泼溅的重建质量，解决了几何不一致问题，为高质量表面提取提供了有效解决方案。将开源代码以促进社区验证和可复现性。

Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.

</details>


### [26] [FacePhys: State of the Heart Learning](https://arxiv.org/abs/2512.06275)
*Kegang Wang,Jiankai Tang,Yuntao Wang,Xin Liu,Yuxuan Fan,Jiatong Ji,Yuanchun Shi,Daniel McDuff*

Main category: cs.CV

TL;DR: FacePhys是一种基于时空状态空间对偶性的内存高效rPPG算法，解决了模型可扩展性、跨数据集泛化和实时操作的三难问题，在误差减少49%的同时实现了3.6MB内存占用和9.46ms每帧延迟。


<details>
  <summary>Details</summary>
Motivation: 基于摄像头的生命体征测量技术（特别是远程光电容积描记术rPPG）为舒适、普适的健康监测提供了机会，但实际部署受到前端设备计算限制和数据压缩传输导致信号质量下降的制约。

Method: 提出FacePhys算法，基于时空状态空间对偶性构建，利用可转移的心脏状态捕捉视频帧间的细微周期性变化，同时保持最小计算开销，支持扩展视频序列训练和低延迟推理。

Result: FacePhys实现了49%的误差减少，达到新的最先进水平；内存占用仅3.6MB，每帧延迟9.46ms，比现有方法提升83%到99%；在实际部署中实现可靠的实时性能。

Conclusion: FacePhys通过创新的时空状态空间对偶性设计，成功解决了rPPG技术在实际部署中的计算限制和精度问题，为实时、高效的远程生命体征监测提供了可行的解决方案。

Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.

</details>


### [27] [RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension](https://arxiv.org/abs/2512.06276)
*Tianyi Gao,Hao Li,Han Fang,Xin Wei,Xiaodong Dong,Hongbo Sun,Ye Yuan,Zhongjiang He,Jinglin Xu,Jingmin Xin,Hao Sun*

Main category: cs.CV

TL;DR: RefBench-PRO是一个新的指称表达理解基准，将任务分解为感知和推理两个核心维度，包含六个渐进挑战性子任务，并提出了基于强化学习的Ref-R1方法来提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有REC基准主要评估感知能力，缺乏可解释的评分机制，无法揭示多模态大语言模型在不同认知能力上的接地能力。为了解决这一局限性，需要一个新的基准来全面评估MLLM的指称表达理解能力。

Method: 1) 引入RefBench-PRO基准，将指称表达分解为感知和推理两个维度，进一步细分为六个渐进挑战性子任务：属性、位置、交互、常识、关系和拒绝；2) 开发全自动数据生成管道，为六个子维度生成多样化的指称表达；3) 提出Ref-R1学习方案，采用基于动态IoU的GRPO强化学习方法来提高复杂推理条件下的定位精度。

Result: 大量实验表明，RefBench-PRO能够对MLLM在指称表达理解方面进行可解释的评估，在感知和推理维度上都提出了更大的挑战。Ref-R1方法为REC建立了更强的基线。

Conclusion: RefBench-PRO是一个全面的REC基准，通过分解指称表达为感知和推理维度，提供了更细粒度的评估框架。提出的Ref-R1学习方案通过强化学习提高了定位精度，为未来的REC研究建立了新的基准。

Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

</details>


### [28] [A Sleep Monitoring System Based on Audio, Video and Depth Information](https://arxiv.org/abs/2512.06282)
*Lyn Chao-ling Chen,Kuan-Wen Chen,Yi-Ping Hung*

Main category: cs.CV

TL;DR: 开发基于事件方法的非侵入式睡眠监测系统，通过红外深度传感器、RGB摄像头和四麦克风阵列检测运动、开关灯和噪音三类事件，用于家庭环境下的睡眠障碍定量评估。


<details>
  <summary>Details</summary>
Motivation: 需要一种非侵入式的方法来定量评估睡眠障碍，特别是在家庭环境中，传统方法可能干扰正常睡眠或无法提供客观的量化数据。

Method: 使用红外深度传感器、RGB摄像头和四麦克风阵列设备，在低光照环境下监测睡眠。建立深度信号背景模型检测运动幅度，建立彩色图像背景模型检测光照变化，采用事件检测算法从三类传感器处理数据中检测事件发生。

Result: 系统在睡眠条件下进行了测试，实验结果验证了系统的可靠性，能够有效检测和分类三种类型的睡眠干扰事件。

Conclusion: 该非侵入式睡眠监测系统通过多传感器融合和事件检测方法，能够有效定量评估家庭环境中的睡眠障碍，为睡眠质量评估提供了可靠的技术方案。

Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.

</details>


### [29] [ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models](https://arxiv.org/abs/2512.06328)
*Jiahao Li,Yusheng Luo,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: ReCAD是一个强化学习框架，通过引导预训练大模型生成精确的参数化CAD模型，利用其内在生成能力，显著提升了几何精度并支持复杂CAD操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖监督微调注入知识，对可编辑性支持有限，未能充分利用预训练大模型的强大生成先验。需要一种能够生成精确参数化CAD模型并支持复杂操作的方法。

Method: 1) 微调视觉语言模型使其具备基本CAD生成能力，将CAD脚本重写为参数化代码用于生成准确的文本描述进行监督；2) 提出新颖的强化学习策略，以参数化代码为指导增强模型在复杂问题上的推理能力；3) 采用分层基元学习过程，在统一奖励函数下逐步教授结构化组合技能，确保几何精度和语义保真度。

Result: 在文本到CAD和图像到CAD任务中均达到最先进水平，显著提升了几何精度。在图像到CAD任务中，将平均Chamfer距离从73.47降至29.61（分布内）和从272.06降至80.23（分布外），大幅超越现有基线。

Conclusion: ReCAD框架成功利用预训练大模型的生成能力，通过强化学习引导生成精确参数化CAD模型，支持复杂CAD操作，在几何精度和语义保真度方面取得显著改进，为CAD生成任务提供了新的有效解决方案。

Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.

</details>


### [30] [S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening](https://arxiv.org/abs/2512.06330)
*Haoyu Zhang,Junhan Luo,Yugang Cao,Siran Peng,Jie Huang,Liangjian-Deng*

Main category: cs.CV

TL;DR: S2WMamba提出了一种新的全色锐化方法，通过2D/1D小波变换显式解耦频率信息，结合Mamba跨模态交互和动态门融合，有效分离空间细节与光谱保真度。


<details>
  <summary>Details</summary>
Motivation: 全色锐化中联合处理PAN和MS图像往往导致空间细节与光谱保真度纠缠，难以同时保持两者。现有方法在处理这种跨模态信息融合时存在局限性。

Method: 1) 对PAN应用2D Haar DWT定位空间边缘和纹理；2) 对MS应用通道级1D Haar DWT分离光谱高低频分量；3) 建立光谱分支（注入空间细节）和空间分支（用光谱细化PAN）；4) 基于Mamba的跨调制实现长程依赖建模；5) 多尺度动态门（乘性+加性）自适应融合分支输出。

Result: 在WV3、GF2和QB数据集上，S2WMamba匹配或超越现有强基线（FusionMamba、CANNet、U2Net、ARConv），PSNR提升最高达0.23 dB，在WV3全分辨率上达到HQNR 0.956。消融实验验证了2D/1D DWT位置、并行双分支和融合门的有效性。

Conclusion: 通过显式频率解耦和轻量级跨模态交互，S2WMamba有效解决了全色锐化中空间细节与光谱保真度的纠缠问题，在多个数据集上取得了优越性能。

Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.

</details>


### [31] [Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate](https://arxiv.org/abs/2512.06344)
*Kaile Wang,Lijun He,Haisheng Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: MTGC框架通过多模态引导和任务感知语义压缩，在超低比特率下提升生成式图像压缩的语义一致性和感知质量


<details>
  <summary>Details</summary>
Motivation: 当前生成式图像压缩在超低比特率（bpp < 0.05）下存在语义偏差问题，由生成幻觉引起，限制了其在6G语义通信场景中的可靠部署

Method: 提出MTGC框架：1）集成三种引导模态：简洁文本描述（全局语义）、高压缩图像（低层视觉信息）、语义伪词（细粒度任务相关语义）；2）设计任务感知语义压缩模块（TASCM）生成SPWs；3）设计多模态引导扩散解码器（MGDD）采用双路径协同引导机制，结合交叉注意力和ControlNet残差注入三种引导

Result: 实验表明MTGC显著提升语义一致性（如DIV2K数据集上DISTS下降10.59%），同时在超低比特率下实现感知质量和像素级保真度的显著提升

Conclusion: MTGC框架通过多模态引导和任务感知语义压缩，有效解决了超低比特率下生成式图像压缩的语义偏差问题，为6G语义通信提供了可靠解决方案

Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

</details>


### [32] [CLUENet: Cluster Attention Makes Neural Networks Have Eyes](https://arxiv.org/abs/2512.06345)
*Xiangshuai Song,Jun-Jie Huang,Tianrui Liu,Ke Liang,Chang Tang*

Main category: cs.CV

TL;DR: CLUENet是一种基于聚类范式的透明深度架构，通过全局软聚合硬分配、温度缩放余弦注意力、门控残差连接、硬共享特征调度和改进的聚类池化策略，在视觉语义理解任务中实现了准确性、效率和可解释性的平衡。


<details>
  <summary>Details</summary>
Motivation: 卷积和注意力模型在视觉任务中虽然成功，但其固定的感受野和复杂架构限制了建模不规则空间模式的能力，并且可解释性不足，对需要高模型透明度的任务构成挑战。聚类范式虽然提供可解释性和灵活语义建模，但存在准确性有限、效率低和训练中梯度消失的问题。

Method: 提出了CLUENet（CLUster attEntion Network），包含三个关键创新：1）全局软聚合硬分配，结合温度缩放余弦注意力和门控残差连接以增强局部建模；2）块间硬共享特征调度；3）改进的聚类池化策略。

Result: 在CIFAR-100和Mini-ImageNet数据集上的实验表明，CLUENet超越了现有聚类方法和主流视觉模型，在分类性能和视觉可解释性方面都有显著提升。

Conclusion: CLUENet为视觉语义理解提供了一种透明深度架构，在准确性、效率和透明度之间实现了令人信服的平衡，解决了现有聚类方法在准确性、效率和训练稳定性方面的局限性。

Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.

</details>


### [33] [TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search](https://arxiv.org/abs/2512.06353)
*Kaicheng Yang,Kaisen Yang,Baiting Wu,Xun Zhang,Qianrui Yang,Haotong Qin,He Zhang,Yulun Zhang*

Main category: cs.CV

TL;DR: TreeQ是一个统一的DiT量化框架，通过树结构搜索、环境噪声引导和通用Monarch分支解决DiT量化中的关键挑战，首次在DiT模型上实现接近无损的4位PTQ性能。


<details>
  <summary>Details</summary>
Motivation: DiT在图像生成中表现出色但计算和内存需求高，混合精度量化在U-Net上成功但DiT量化研究不足，需要解决搜索效率、目标对齐和信息瓶颈等挑战。

Method: 提出TreeQ框架：1) 树结构搜索(TSS)利用DiT线性特性在O(n)时间内遍历解空间；2) 环境噪声引导(ENG)统一PTQ和QAT优化目标；3) 通用Monarch分支(GMB)防止超低位量化中的信息损失。

Result: 在DiT-XL/2模型的W3A3和W4A4 PTQ/PEFT设置下实现SOTA性能，首次在DiT模型上实现接近无损的4位PTQ性能。

Conclusion: TreeQ为DiT量化提供了有效的统一框架，解决了搜索效率、目标对齐和信息瓶颈等关键问题，推动了DiT在实际部署中的应用。

Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ

</details>


### [34] [Rectifying Latent Space for Generative Single-Image Reflection Removal](https://arxiv.org/abs/2512.06358)
*Mingjia Li,Jin Hu,Hainuo Wang,Qiming Hu,Jiarui Wang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜在扩散模型的单图像反射去除方法，通过重构潜在空间使其与反射形成的线性物理特性对齐，解决了现有方法在处理复合图像时的局限性。


<details>
  <summary>Details</summary>
Motivation: 单图像反射去除是一个高度不适定问题，现有方法难以推理被破坏区域的组成，导致在真实场景中恢复和泛化能力不足。本文发现问题的核心在于语义编码器的潜在空间缺乏解释复合图像作为其组成层线性叠加的内在结构。

Method: 方法包含三个协同组件：1）反射等变VAE，将潜在空间与反射形成的线性物理特性对齐；2）可学习的任务特定文本嵌入，提供精确指导并绕过模糊语言；3）深度引导的早期分支采样策略，利用生成随机性获得有希望的结果。

Result: 在多个基准测试中取得了新的SOTA性能，并且在具有挑战性的真实世界案例中表现出良好的泛化能力。

Conclusion: 通过重构编辑目的的潜在扩散模型，使其能够有效感知和处理高度模糊的分层图像输入，从而产生高质量的输出，为单图像反射去除问题提供了有效的解决方案。

Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.

</details>


### [35] [Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection](https://arxiv.org/abs/2512.06363)
*Jiabao Guo,Yadian Wang,Hui Ma,Yuhao Fu,Ju Jia,Hui Liu,Shengeng Tang,Lechao Cheng,Yunfeng Diao,Ajian Liu*

Main category: cs.CV

TL;DR: 本文提出SPL-UAD框架，通过解耦物理攻击和数字攻击的优化分支，解决统一攻击检测中优化方向冲突的问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的人脸识别系统同时面临物理呈现攻击和数字伪造攻击的威胁，现有方法使用CLIP加正则化约束来增强模型泛化能力，但存在物理攻击和数字攻击检测在相同类别提示空间下优化方向冲突的问题。

Method: 提出SPL-UAD框架：1）构建可学习的并行提示分支，通过自适应欺骗上下文提示生成实现物理和数字攻击的独立优化控制；2）设计线索感知增强，利用双提示机制在数据上生成具有挑战性的样本挖掘任务。

Result: 在大规模UniAttackDataPlus数据集上的实验表明，该方法在统一攻击检测任务中取得了显著的性能提升。

Conclusion: SPL-UAD框架通过解耦物理和数字攻击的优化分支，有效解决了统一攻击检测中的优化冲突问题，显著增强了模型对未见攻击类型的鲁棒性。

Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.

</details>


### [36] [VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2512.06373)
*Yuji Wang,Wenlong Liu,Jingxuan Niu,Haoji Zhang,Yansong Tang*

Main category: cs.CV

TL;DR: VG-Refiner：首个工具精炼的指代接地推理框架，通过两阶段思考-再思考机制和精炼奖励，解决现有工具集成视觉推理中不可靠工具输出的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成视觉推理范式主要关注通过强化学习整合各种视觉工具，但忽视了设计有效响应机制来处理不可靠或错误的工具输出。这一局限在指代和接地任务中尤为突出，不准确的检测工具预测经常误导TiVR模型产生幻觉推理。

Method: 提出VG-Refiner框架，引入两阶段思考-再思考机制，使模型能够明确分析和响应工具反馈；设计精炼奖励以鼓励对不良工具结果进行有效修正；提出两个新指标并建立公平评估协议来系统衡量模型精炼能力；使用少量任务特定数据增强VG-Refiner的精炼能力。

Result: VG-Refiner在指代和推理接地基准测试中实现了准确性和修正能力的显著提升，同时保持了预训练模型的通用能力。

Conclusion: VG-Refiner是首个针对工具精炼指代接地推理的框架，通过有效的工具反馈响应机制，显著提升了多模态问题解决中工具集成视觉推理的可靠性和准确性。

Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

</details>


### [37] [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376)
*Xinhao Xiang,Abhijeet Rastogi,Jiawei Zhang*

Main category: cs.CV

TL;DR: 该论文研究了AI生成驾驶视频(AIGVs)在自动驾驶模型训练和评估中的可靠性，提出了诊断框架、失败模式分类、基准测试ADGV-Bench和评估器ADGVE，发现盲目使用AIGVs会降低感知性能，但经过ADGVE筛选后能成为真实数据的有效补充。


<details>
  <summary>Details</summary>
Motivation: 文本到视频模型能够生成高分辨率驾驶场景，AI生成驾驶视频(AIGVs)为自动驾驶提供了低成本、可扩展的替代数据源。但关键问题是：这些视频能否可靠地支持自动驾驶模型的训练和评估？需要系统研究AIGVs在自动驾驶应用中的可靠性。

Method: 1. 提出AIGV失败模式分类法，包括视觉伪影、物理上不可信的运动和交通语义违规；2. 构建ADGV-Bench基准测试，包含人工质量标注和多个感知任务的密集标签；3. 提出ADGVE评估器，结合静态语义、时序线索、车道遵守信号和视觉语言模型引导的推理，为每个视频片段生成单一质量分数。

Result: 实验表明：盲目添加原始AIGVs会降低感知性能（对象检测、跟踪和实例分割）；使用ADGVE筛选AIGVs能持续改善通用视频质量评估指标和下游自动驾驶模型性能；经过筛选的AIGVs能成为真实世界数据的有益补充。

Conclusion: 研究揭示了AIGVs在自动驾驶中的风险和潜力，提供了实用工具来安全利用大规模视频生成技术。ADGVE评估器能有效识别高质量AIGVs，使其成为真实驾驶数据的有效补充，为未来自动驾驶管道中安全使用生成视频提供了方法论。

Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.

</details>


### [38] [Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement](https://arxiv.org/abs/2512.06400)
*Jing Tao,Yonghong Zong,Banglei Guana,Pengju Sun,Taihang Lei,Yang Shanga,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出基于区域感知的红外与可见光图像融合框架，使用空间变化曝光相机结合多曝光与多模态成像，在极端环境下保持几何精度和热辐射信息


<details>
  <summary>Details</summary>
Motivation: 在摄影测量中，准确融合红外和可见光谱同时保持可见特征的几何保真度并纳入热辐射信息是一个重大挑战，特别是在极端条件下。现有方法常常会损害可见图像质量，影响测量精度。

Method: 提出基于区域感知的融合框架，使用空间变化曝光相机结合多曝光和多模态成像。首先进行基于区域感知的特征融合以确保精确的多模态配准，然后进行自适应融合与对比度增强。通过区域显著性图引导的结构相似性补偿机制优化红外-可见光谱集成。该框架还能适应单曝光场景，实现不同条件下的鲁棒融合。

Result: 在合成数据和真实世界数据上的实验表明，该方法在图像清晰度和性能方面优于现有最先进方法，定量和视觉评估均证明了其优越性。

Conclusion: 提出的基于区域感知的融合框架能够有效解决极端环境下红外与可见光图像融合的挑战，在保持几何精度的同时整合热辐射信息，为摄影测量应用提供了更可靠的解决方案。

Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.

</details>


### [39] [Rethinking Training Dynamics in Scale-wise Autoregressive Generation](https://arxiv.org/abs/2512.06421)
*Gengze Zhou,Chongjian Ge,Hao Tan,Feng Liu,Yicong Hong*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SAR（自回归细化）的后训练方法，通过交错尺度展开机制和对比学生强制损失，解决了尺度自回归模型中的曝光偏差问题，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 尺度自回归模型存在曝光偏差问题，这会损害生成质量。作者识别出两个主要原因：1）训练-测试不匹配，模型在推理时必须依赖自身不完美的预测；2）尺度学习难度不平衡，某些尺度的优化复杂度过高。

Method: 提出自回归细化（SAR）方法，包含两个核心组件：1）交错尺度展开（SSR）机制，执行轻量级自回归展开，使模型接触自身中间预测，对齐训练-测试模式；2）对比学生强制损失（CSFL），为自生成上下文提供充分监督，确保训练稳定。

Result: 实验表明，将SAR应用于预训练的自回归模型能持续提升生成质量，且计算开销最小。例如，在ImageNet 256上训练的FlexVAR-d16模型，SAR在10个epoch内（32xA100 GPU上5小时）实现了5.2%的FID降低。

Conclusion: SAR凭借其高效性、可扩展性和有效性，有望成为视觉自回归生成的可靠后训练方法，能够显著缓解曝光偏差问题并提升生成质量。

Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.

</details>


### [40] [When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition](https://arxiv.org/abs/2512.06426)
*Nzakiese Mbongo,Kailash A. Hambarde,Hugo Proença*

Main category: cs.CV

TL;DR: 提出双路径Transformer框架，结合CLIP的视觉和属性线索进行远距离性别识别，在U-DetAGReID数据集上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 远距离图像中的性别识别面临空间分辨率低、视角变化大、面部线索缺失等挑战，需要更鲁棒的解决方案

Method: 双路径Transformer框架：1) 直接视觉路径通过选择性微调CLIP图像编码器上层；2) 属性中介路径通过软生物特征提示（发型、服装、配饰）在CLIP文本-图像空间中推断性别；使用空间通道注意力模块增强判别性定位

Result: 在构建的U-DetAGReID数据集上，该方法在宏观F1、准确率、AUC等多个指标上超越最先进的人物属性和重识别基线方法，对距离、角度和高度变化具有一致鲁棒性

Conclusion: 语言引导的双路径学习为无约束远距离场景中的负责任性别识别提供了原则性、可扩展的基础

Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

</details>


### [41] [Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening](https://arxiv.org/abs/2512.06434)
*Lucas R. Mareque,Ricardo L. Armentano,Leandro J. Cymberknop*

Main category: cs.CV

TL;DR: 提出基于深度学习的全自动方法，从2D合成人体图像估计5项关键人体测量指标，用于运动员心血管风险评估


<details>
  <summary>Details</summary>
Motivation: 运动员心血管筛查中传统手动测量方法劳动密集、依赖操作者、难以规模化，需要自动化解决方案来支持大规模筛查

Method: 使用从3D身体网格生成的10万张合成图像数据集，训练并评估VGG19、ResNet50和DenseNet121模型进行回归预测

Result: 所有模型达到亚厘米级精度，ResNet50表现最佳，平均MAE为0.668厘米

Conclusion: 深度学习可提供准确、可扩展的人体测量数据，作为运动员筛查的实用工具，未来将在真实图像上验证以扩展应用

Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.

</details>


### [42] [Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion](https://arxiv.org/abs/2512.06504)
*Andrii Lysyi,Anatoliy Sachenko,Pavlo Radiuk,Mykola Lysyi,Oleksandr Melnychenko,Diana Zahorodnia*

Main category: cs.CV

TL;DR: 提出一个智能集成框架，用于光伏基础设施的自动化检测，通过多模态融合、自适应重采集和地理空间去重，解决了传统方法的局限性，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统光伏检测方法存在热成像调色板偏差、数据冗余和高通信带宽需求等关键缺陷，需要开发一个全面的自动化监测系统来提升电站安全性和运行效率。

Method: 采用协同架构：1) 学习调色板不变的热成像嵌入表示；2) 通过门控机制融合对比度归一化的RGB流；3) 使用罗德里格斯更新的闭环自适应重采集控制器确认模糊异常；4) 基于DBSCAN和半正矢距离的地理空间去重模块。

Result: 在公开PVF-10基准测试中达到0.903的mAP@0.5，比单模态基线提升12-15%；现场验证召回率达96%；去重过程减少重复导致的假阳性15-20%；相关性遥测减少空中数据传输60-70%。

Conclusion: 建立了一个主动式光伏检测的新范式，系统性能优异，具备现场应用准备度，显著提升了检测精度、减少了误报和数据传输需求。

Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.

</details>


### [43] [Towards Stable Cross-Domain Depression Recognition under Missing Modalities](https://arxiv.org/abs/2512.06447)
*Jiuyi Chen,Mingkui Tan,Haifeng Lu,Qiuna Xu,Zhihua Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 提出基于多模态大语言模型的稳定跨域抑郁识别框架SCD-MLLM，通过统一处理异构抑郁数据并增强对缺失模态的稳定性，在多个公开数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 抑郁症带来严重的公共健康风险，包括自杀风险，急需及时且可扩展的筛查方法。现有的多模态自动抑郁检测方法缺乏统一的通用框架，对现实世界中常见的缺失模态情况稳定性有限。

Method: 提出SCD-MLLM框架，包含两个关键组件：1) 多源数据输入适配器(MDIA)，使用掩码机制和任务特定提示将异构抑郁相关输入转换为统一标记序列；2) 模态感知自适应融合模块(MAFM)，通过共享投影机制自适应整合音频和视觉特征。

Result: 在五个公开抑郁数据集(CMDC、AVEC2014、DAIC-WOZ、DVlog、EATD)上进行多数据集联合训练实验，在完整和部分模态设置下均优于现有SOTA模型及领先商业LLM(Gemini和GPT)，展现出优越的跨域泛化能力和对缺失模态的稳定性。

Conclusion: SCD-MLLM提供了一个统一的框架，能够有效处理来自不同来源的异构抑郁数据，在现实应用中具有更强的稳定性和泛化能力，为多模态自动抑郁检测提供了有前景的解决方案。

Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.

</details>


### [44] [More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery](https://arxiv.org/abs/2512.07596)
*Wenzhen Dong,Jieming Yu,Yiming Huang,Hongqiu Wang,Lei Zhu,Albert C. S. Chung,Hongliang Ren,Long Bai*

Main category: cs.CV

TL;DR: SAM 3在机器人辅助手术中的评估显示，相比SAM和SAM 2在图像和视频分割方面有显著改进，语言提示在手术领域表现欠佳，3D重建能力展示潜力但仍有局限。


<details>
  <summary>Details</summary>
Motivation: 评估SAM 3在机器人辅助手术中的性能，特别是其零样本分割能力（点、边界框和语言提示）以及3D感知能力，探索其在动态手术场景中的适用性。

Method: 在MICCAI EndoVis 2017和2018基准上进行综合测试，评估SAM 3的图像和视频分割性能；使用SCARED、StereoMIS和EndoNeRF数据集进行零样本评估，测试单目深度估计和3D器械重建能力。

Result: SAM 3在空间提示下的图像和视频分割明显优于SAM和SAM 2；语言提示在手术领域表现不理想；3D重建能力展示出处理手术场景数据和从2D图像重建3D解剖结构的能力，但在复杂动态手术场景中仍有局限。

Conclusion: SAM 3在机器人辅助手术中展现出显著进步，特别是空间提示下的分割性能，但语言提示需要领域特定训练，3D重建能力有潜力但需进一步改进以适应复杂动态手术环境。

Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.

</details>


### [45] [Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction](https://arxiv.org/abs/2512.06485)
*Kush Revankar,Shreyas Deshpande,Araham Sayeed,Ansh Tandale,Sarika Bobde*

Main category: cs.CV

TL;DR: Sanvaad是一个轻量级多模态无障碍框架，支持聋人、视障用户和普通听力人群之间的实时双向通信，结合计算机视觉和语音处理技术。


<details>
  <summary>Details</summary>
Motivation: 当前聋人、视障用户与普通听力人群之间的通信工具通常只支持单向交互，存在局限性。需要一种支持实时双向通信的无障碍解决方案。

Method: 1. 针对聋人用户：基于MediaPipe地标构建印度手语识别模块，支持语音到手语的转换（语音检测映射到预定义短语，生成GIF或字母可视化）
2. 针对视障用户：提供无屏幕语音界面，集成多语言语音识别、文本摘要和文本转语音生成
3. 整体框架：使用Streamlit构建界面，支持桌面和移动环境，采用轻量级计算机视觉和语音处理工具

Result: 开发了一个实用的无障碍通信框架，能够在边缘设备上流畅运行，无需专用硬件，支持实时双向通信。

Conclusion: Sanvaad通过结合轻量级计算机视觉和语音处理工具，为包容性通信提供了实用且可访问的途径，解决了当前单向通信工具的局限性。

Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.

</details>


### [46] [sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only](https://arxiv.org/abs/2512.07698)
*Arslan Artykov,Corentin Sautier,Vincent Lepetit*

Main category: cs.CV

TL;DR: 首个从单目视频中联合预测部件分割和关节参数的数据驱动方法，仅用合成数据训练就能泛化到真实物体


<details>
  <summary>Details</summary>
Motivation: 理解铰接物体是机器人和数字孪生创建中的基础挑战，需要同时恢复部件分割和关节参数。先前工作主要依赖多视图系统、物体扫描或静态相机，缺乏从自由移动相机拍摄的单目视频中理解铰接物体的方法。

Method: 提出首个数据驱动方法，从自由移动相机拍摄的单目视频中联合预测部件分割和关节参数。仅使用合成数据进行训练，能够直接处理随意录制的视频。

Result: 方法在仅使用合成数据训练的情况下，展现出对真实世界物体的强大泛化能力，为铰接物体理解提供了可扩展且实用的解决方案。

Conclusion: 该方法适用于动态环境中的实时应用，为铰接物体理解提供了从单目视频中联合预测部件分割和关节参数的有效解决方案。

Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/

</details>


### [47] [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](https://arxiv.org/abs/2512.07756)
*Mayank Anand,Ujair Alam,Surya Prakash,Priya Shukla,Gora Chand Nandi,Domenec Puig*

Main category: cs.CV

TL;DR: UltrasODM是一个双流框架，通过每帧不确定性校准、显著性诊断和可操作提示来辅助超声医师采集，减少重建误差，提高临床信任度。


<details>
  <summary>Details</summary>
Motivation: 临床超声采集高度依赖操作者，快速探头运动和亮度波动常导致重建误差，降低临床信任度和实用性。需要一种能提供实时反馈和不确定性评估的系统来辅助操作者。

Method: UltrasODM采用双流框架：1)对比排序模块按运动相似性分组帧；2)光流流与Dual-Mamba时间模块融合，用于鲁棒的6-DoF姿态估计；3)人机交互层结合贝叶斯不确定性、临床校准阈值和显著性图。当不确定性超过阈值时，系统发出非侵入性警报，建议纠正措施。

Result: 在临床自由手超声数据集上评估，相比UltrasOM，UltrasODM将漂移减少15.2%，距离误差减少12.1%，Hausdorff距离减少10.1%，同时生成每帧不确定性和显著性输出。

Conclusion: UltrasODM通过强调透明度和临床医生反馈，提高了重建可靠性，支持更安全、更可信的临床工作流程。代码已公开。

Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.

</details>


### [48] [On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization](https://arxiv.org/abs/2512.06530)
*Mohammed Wattad,Tamir Shor,Alex Bronstein*

Main category: cs.CV

TL;DR: 该研究探索了学习型k空间采集模式在加速磁共振成像中的跨域泛化能力，提出通过引入采集不确定性来增强模型对扫描仪和成像条件变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要针对单一数据集或模态优化k空间采集模式，缺乏对其跨成像域可迁移性的考虑。本研究旨在探索学习型k空间采样在跨域设置中的泛化能力。

Method: 提出一种增强域鲁棒性的新方法：在训练过程中引入采集不确定性，通过随机扰动k空间轨迹来模拟不同扫描仪和成像条件的变化。

Result: 系统评估显示，使用学习型采样模式训练的模型在跨域设置中表现出更好的泛化性能。引入采集不确定性的方法有效提升了模型对域变化的鲁棒性。

Conclusion: k空间轨迹设计不应仅被视为加速机制，而应作为改善MRI重建中域泛化能力的重要自由度。学习型采样模式具有跨域可迁移性，为实际临床应用提供了新思路。

Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.

</details>


### [49] [SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities](https://arxiv.org/abs/2512.06562)
*Dung Thuy Nguyen,Quang Nguyen,Preston K. Robinette,Eli Jiang,Taylor T. Johnson,Kevin Leach*

Main category: cs.CV

TL;DR: SUGAR是一个可扩展的生成式遗忘框架，用于从3D感知生成模型中移除特定人物身份，无需重新训练整个模型，支持同时或顺序移除多个身份。


<details>
  <summary>Details</summary>
Motivation: 随着3D感知生成模型在人物身份合成方面取得进展，引发了关于用户同意和从模型输出空间中移除特定个体的紧迫问题。现有方法要么将不需要的身份投影到不现实的输出，要么依赖静态模板人脸，存在局限性。

Method: SUGAR为每个身份学习个性化的替代潜在表示，将重建引导到视觉连贯的替代方案，同时保持模型质量和多样性。还引入了持续效用保持目标，防止随着更多身份被遗忘而导致的性能退化。

Result: SUGAR在移除多达200个身份方面实现了最先进的性能，与现有基线相比，保留效用提高了700%。

Conclusion: SUGAR提供了一个有效的框架，用于从生成模型中移除特定人物身份，解决了用户同意问题，同时保持了模型的整体质量和多样性。

Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.

</details>


### [50] [GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation](https://arxiv.org/abs/2512.06565)
*Xiujin Liu*

Main category: cs.CV

TL;DR: GNC-Pose是一种完全无需学习的单目6D物体姿态估计方法，通过渲染初始化、几何感知对应点加权和鲁棒GNC优化实现，在YCB数据集上达到与学习方法和无学习方法相当的精度。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计方法通常依赖学习特征或类别特定先验，需要大量训练数据。本文旨在开发一种完全无需学习的解决方案，仅使用几何一致性先验，提供简单、鲁棒且实用的姿态估计方法。

Method: 方法包含三个主要部分：1）基于特征匹配和渲染对齐的粗2D-3D对应点初始化；2）基于GNC原则的几何感知聚类加权机制，根据3D结构一致性分配点级置信度；3）最终LM细化提高精度。该方法不依赖学习特征、训练数据或类别特定先验。

Result: 在YCB物体和模型集上测试，尽管完全无需学习，GNC-Pose在严重异常值污染下仍能稳定优化，达到与基于学习方法和无学习方法相当的竞争性精度。

Conclusion: GNC-Pose为无学习6D姿态估计提供了一个简单、鲁棒且实用的解决方案，仅依赖几何一致性先验，无需训练数据或类别特定知识，在YCB数据集上表现出色。

Abstract: We present GNC--Pose, a fully learning--free monocular 6D object pose estimation pipeline for textured objects that combines rendering--based initialization, geometry--aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D--3D correspondences obtained through feature matching and rendering--based alignment, our method builds upon the Graduated Non--Convexity (GNC) principle and introduces a geometry--aware, cluster--based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC--Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC--Pose achieves competitive accuracy compared with both learning-based and learning--free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.

</details>


### [51] [MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding](https://arxiv.org/abs/2512.06581)
*Yuhao Su,Anwesa Choudhuri,Zhongpai Gao,Benjamin Planche,Van Nguyen Nguyen,Meng Zheng,Yuhan Shen,Arun Innanje,Terrence Chen,Ehsan Elhamifar,Ziyan Wu*

Main category: cs.CV

TL;DR: 本文提出了MedVidBench基准数据集和MedGRPO强化学习框架，用于解决医学视频理解中的空间精度、时序推理和临床语义问题。通过跨数据集奖励归一化和医学LLM评判器，实现了多数据集平衡训练，显著提升了医学视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医学视频理解方面存在困难，特别是在空间精度、时序推理和临床语义方面。现有方法难以处理医学视频的多层次任务（视频级、片段级、帧级），且标准强化学习在多数据集训练中因奖励尺度不平衡而失效。

Method: 1. 构建MedVidBench基准数据集：包含531,850个视频-指令对，来自8个医学来源，涵盖视频级、片段级和帧级任务，采用专家引导提示和双模型验证的质量保证流程。
2. 提出MedGRPO强化学习框架：包含跨数据集奖励归一化（将各数据集的中位数性能映射到共同奖励值）和医学LLM评判器（通过比较相似性评分在五个临床维度上评估字幕质量）。

Result: 1. 在MedVidBench上监督微调Qwen2.5-VL-7B模型，在所有任务上显著优于GPT-4.1和Gemini-2.5-Flash。
2. MedGRPO框架在SFT基线基础上进一步提升了定位和字幕生成任务的性能。
3. 标准强化学习因数据集间奖励尺度不平衡而导致训练崩溃，而MedGRPO解决了这一问题。

Conclusion: 本文为医学领域的视觉语言模型建立了基础性基准和稳健的训练方法。MedVidBench基准和MedGRPO框架共同推动了医学视频理解的发展，为解决医学视觉语言任务中的关键挑战提供了有效解决方案。

Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.

</details>


### [52] [Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics](https://arxiv.org/abs/2512.06612)
*Kazuya Nishimura,Haruka Hirose,Ryoma Bise,Kaito Shiku,Yasuhiro Kojima*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基因表达估计方法STRank，通过学习相对表达模式而非绝对表达值，以应对RNA测序中的随机噪声和批次效应。


<details>
  <summary>Details</summary>
Motivation: 从病理图像估计基因表达可以降低RNA测序成本，但由于测序技术复杂性和细胞内在变异性，观察到的基因表达包含随机噪声和批次效应，准确估计绝对表达值仍然是一个重大挑战。

Method: 提出学习相对表达模式而非绝对表达水平的新目标，假设基因的相对表达水平在独立实验中表现出一致模式。基于此假设，提出名为STRank的新型损失函数，该函数对噪声和批次效应具有鲁棒性。

Result: 在合成数据集和真实数据集上的实验证明了所提方法的有效性。

Conclusion: 通过专注于学习相对表达模式而非绝对表达值，STRank方法能够有效应对基因表达估计中的噪声和批次效应问题，为从病理图像进行基因表达估计提供了更稳健的解决方案。

Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.

</details>


### [53] [Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution](https://arxiv.org/abs/2512.06642)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.CV

TL;DR: 使用掩码自编码器（MAE）在模拟强引力透镜图像上进行预训练，学习可泛化表示，用于暗物质模型分类和图像超分辨率任务，相比从头训练获得更好性能。


<details>
  <summary>Details</summary>
Motivation: 强引力透镜可以揭示暗物质子结构的影响，但从噪声、低分辨率图像中分析这些效应具有挑战性。需要开发能够同时处理分类和超分辨率任务的通用表示学习方法。

Method: 提出基于模拟强引力透镜图像的MAE预训练策略：1）在DeepLense ML4SCI基准的模拟图像上使用掩码图像建模目标预训练Vision Transformer编码器；2）针对两个下游任务分别微调编码器：暗物质模型分类（冷暗物质、轴子样或无子结构）和低分辨率图像超分辨率（16×16到64×64）；3）研究不同掩码比例对性能的影响。

Result: 1）分类任务：90%掩码比例下，微调分类器获得宏观AUC 0.968和准确率88.65%，优于从头训练的基线（AUC 0.957，准确率82.46%）；2）超分辨率任务：MAE预训练模型重建图像PSNR约33 dB，SSIM 0.961，略优于从头训练；3）掩码比例分析显示一致权衡：更高掩码比例改善分类但轻微降低重建保真度。

Conclusion: 在物理丰富的模拟数据上进行MAE预训练提供了灵活、可重用的编码器，适用于多个强引力透镜分析任务，证明了自监督预训练在天体物理应用中的价值。

Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

</details>


### [54] [Personalized Image Descriptions from Attention Sequences](https://arxiv.org/abs/2512.06662)
*Ruoyu Xue,Hieu Le,Jingyi Xu,Sounak Mondal,Abe Leite,Gregory Zelinsky,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: DEPER模型通过同时建模个性化语言风格和观看行为，显著提升了图像描述生成的质量和人类对齐度


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像描述模型只关注语言风格，忽略了个人观看模式（注意力分布）的差异。不同人观看同一图像时会关注不同区域、对象和细节，并以不同顺序和语言风格进行描述，导致描述存在显著差异。

Method: 提出DEPER方法，学习一个主体嵌入，同时捕获语言风格和观看行为，通过辅助注意力预测任务进行指导。使用轻量级适配器将这些嵌入与冻结的视觉语言模型对齐，实现少样本个性化而无需重新训练。

Result: 在四个数据集上（涵盖不同观看任务以及简短和详细描述），DEPER平均提升了24%的性能，表明建模个性化注意力能产生更符合人类习惯和更高质量的描述。

Conclusion: 理解人们如何观看有助于预测他们会说什么；在多媒体系统中建模人类感知的多样性可以提高性能和人类对齐度。

Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.

</details>


### [55] [1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673)
*Shida Gao,Feng Xue,Xiangfeng Wang,Anlong Ming,Teng Long,Yihua Shao,Haozhe Wang,Zhaowen Lin,Wei Wang,Nicu Sebe*

Main category: cs.CV

TL;DR: DEViL是一个结合视频大语言模型和开放词汇检测器的系统，通过参考语义令牌实现端到端学习，解决时空定位中自回归空间解码导致的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在时空定位任务中采用自回归方式生成边界框作为文本令牌，导致输出序列过长、空间误差随时间累积以及定位结果在视频中逐渐漂移的问题。

Method: 提出DEViL系统，将视频大语言模型与开放词汇检测器通过参考语义令牌连接，该令牌既作为控制信号又替代检测器的文本嵌入。同时提出管状挖掘时间正则化技术，确保检测器生成时间一致的目标查询。

Result: 实验表明DEViL在多种细粒度视频理解任务上表现优异，特别是在时空视觉定位和基于视觉问答的定位任务上取得显著效果。

Conclusion: DEViL通过结合大语言模型和检测器，并引入参考语义令牌和时间正则化，有效解决了时空定位中的误差累积问题，实现了更好的时空关联和定位精度。

Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.

</details>


### [56] [RunawayEvil: Jailbreaking the Image-to-Video Generative Models](https://arxiv.org/abs/2512.06674)
*Songping Wang,Rufan Qian,Yueming Lyu,Qinglong Liu,Linzhuang Zou,Jie Qin,Songhua Liu,Caifeng Shan*

Main category: cs.CV

TL;DR: RunawayEvil：首个具有动态进化能力的多模态越狱框架，针对图像到视频生成模型，通过"策略-战术-行动"范式实现自我放大的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 当前图像到视频生成系统的安全性研究不足，特别是对越狱攻击的脆弱性缺乏深入探索。多模态系统面临的安全威胁尚未得到充分研究。

Method: 采用"策略-战术-行动"范式，包含三个核心组件：策略感知命令单元（通过强化学习和LLM实现策略自进化）、多模态战术规划单元（生成协调的文本越狱指令和图像篡改指南）、战术行动单元（执行和评估多模态协调攻击）。

Result: 在商业I2V模型（如Open-Sora 2.0和CogVideoX）上实现了最先进的攻击成功率，在COCO2017数据集上比现有方法高出58.5%到79%。

Conclusion: 该工作为I2V模型的漏洞分析提供了关键工具，为构建更鲁棒的视频生成系统奠定了基础，揭示了多模态系统安全性的重要研究空白。

Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.

</details>


### [57] [EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy](https://arxiv.org/abs/2512.06684)
*Yumeng He,Zanwei Zhou,Yekun Zheng,Chen Liang,Yunbo Wang,Xiaokang Yang*

Main category: cs.CV

TL;DR: EMGauss：基于高斯溅射的3D重建框架，用于从平面扫描的2D切片重建各向异性体积电子显微镜数据，无需大规模预训练


<details>
  <summary>Details</summary>
Motivation: 体积电子显微镜(vEM)存在各向异性分辨率限制，现有基于各向同性假设的深度学习方法在处理形态各向异性结构时失效

Method: 将切片到3D重建重新定义为基于高斯溅射的3D动态场景渲染问题，轴向切片进展建模为2D高斯点云的时间演化，并引入教师-学生自举机制增强数据稀疏区域保真度

Result: 相比基于扩散和GAN的重建方法，EMGauss显著提升插值质量，实现连续切片合成，无需大规模预训练

Conclusion: EMGauss为体积电子显微镜提供了一种通用的切片到3D重建解决方案，可推广到其他成像领域

Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.

</details>


### [58] [Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation](https://arxiv.org/abs/2512.06689)
*Jisoo Park,Seonghak Lee,Guisik Kim,Taewoo Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: UniVoiceLite是一个轻量级无监督音视频框架，统一了语音增强和语音分离任务，利用唇部运动和面部身份线索引导语音提取，无需配对噪声-干净数据。


<details>
  <summary>Details</summary>
Motivation: 现实世界音频通常同时包含背景噪声和重叠说话人，需要统一解决方案。现有方法多为多阶段架构，参数复杂且依赖监督训练，限制了可扩展性和泛化能力。

Method: 提出UniVoiceLite框架：1) 利用唇部运动和面部身份线索引导语音提取；2) 使用Wasserstein距离正则化稳定潜在空间；3) 采用无监督训练，无需配对噪声-干净数据。

Result: 实验结果表明，UniVoiceLite在噪声和多说话人场景下均表现出色，实现了效率与鲁棒泛化能力的结合。

Conclusion: UniVoiceLite提供了一个轻量级、无监督的统一框架，有效解决了语音增强和语音分离任务，具有良好性能和泛化能力。

Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.

</details>


### [59] [The Role of Entropy in Visual Grounding: Analysis and Optimization](https://arxiv.org/abs/2512.06726)
*Shuo Li,Jiajun Sun,Zhihao Zhang,Xiaoran Fan,Senjie Jin,Hui Li,Yuming Yang,Junjie Ye,Lixing Shen,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 本文提出ECVGPO算法，通过熵控制优化多模态大语言模型在视觉定位任务中的强化学习微调，平衡探索与利用，在多个基准测试中取得广泛改进。


<details>
  <summary>Details</summary>
Motivation: 虽然基于强化学习的多模态大语言模型微调在熵控制技术方面取得了显著进展，但熵在视觉定位等感知导向任务中的作用和特性，以及有效的控制策略仍未被充分探索。

Method: 聚焦视觉定位任务，分析熵在感知任务与推理任务中的差异特性，并基于这些发现提出ECVGPO（熵控制视觉定位策略优化）算法，这是一个用于有效熵调节的可解释算法。

Result: 通过熵控制，探索与利用之间的权衡得到更好平衡。实验表明，ECVGPO在多个基准测试和模型上实现了广泛的性能改进。

Conclusion: ECVGPO算法通过有效的熵控制机制，显著提升了多模态大语言模型在视觉定位任务中的性能，为感知导向任务的强化学习微调提供了新的优化策略。

Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.

</details>


### [60] [FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation](https://arxiv.org/abs/2512.06738)
*M Yashwanth,Sampath Koti,Arunabh Singh,Shyam Marjit,Anirban Chakraborty*

Main category: cs.CV

TL;DR: FedSCAl是一个联邦学习框架，通过服务器-客户端对齐机制解决联邦源自由域自适应问题，在存在显著客户端间域差异的情况下提升伪标签准确性


<details>
  <summary>Details</summary>
Motivation: 解决联邦源自由域自适应问题中的挑战，包括客户端间显著的域差异、无法访问源数据集、以及现有方法因数据异构性导致的客户端漂移和不可靠伪标签问题

Method: 提出FedSCAl框架，采用服务器-客户端对齐机制，通过对齐客户端和服务器模型的预测来正则化客户端更新，从而减轻客户端漂移

Result: 在基准视觉数据集上的实验表明，FedSCAl在联邦源自由域自适应设置中持续优于最先进的联邦学习方法，显著提升了伪标签准确性

Conclusion: FedSCAl通过服务器-客户端对齐机制有效解决了联邦源自由域自适应中的客户端漂移问题，为处理客户端间域差异和源数据集不可访问的挑战提供了有效解决方案

Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.

</details>


### [61] [Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2512.06746)
*Ruoxin Chen,Jiahui Gao,Kaiqing Lin,Keyue Zhang,Yandan Zhao,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

TL;DR: 论文提出AlignGemini检测器，通过任务-模型对齐原则，将AI生成图像检测分解为语义一致性检查和像素伪影检测两个互补任务，分别用VLM和像素伪影专家处理，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的AI生成图像检测方法需要大量资源，且存在严重幻觉问题。研究发现VLM对语义敏感但对像素伪影不敏感，而传统像素伪影检测器则缺乏语义意识，存在任务-模型不匹配问题。

Method: 提出任务-模型对齐原则，将AI生成图像检测形式化为两个互补任务：语义一致性检查和像素伪影检测。构建AlignGemini双分支检测器，一个分支用纯语义监督微调VLM，另一个分支用纯像素伪影监督训练像素伪影专家。

Result: 在五个野外基准测试中，AlignGemini实现了平均准确率+9.5%的提升，证明了任务-模型对齐原则在可泛化AI生成图像检测中的有效性。

Conclusion: 任务-模型对齐是提升AI生成图像检测性能的有效途径，通过将检测分解为互补任务并匹配相应模型，可以克服单一模型的局限性，实现更好的泛化能力。

Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.

</details>


### [62] [UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement](https://arxiv.org/abs/2512.06750)
*Weiqi Li,Xuanyu Zhang,Bin Chen,Jingfen Xie,Yan Wang,Kexin Zhang,Junlin Li,Li Zhang,Jian Zhang,Shijie Zhao*

Main category: cs.CV

TL;DR: UARE是首个统一图像质量评估、修复和增强的视觉语言模型，通过两阶段训练框架将质量评估与修复任务结合，利用质量评估信号指导修复过程。


<details>
  <summary>Details</summary>
Motivation: 尽管图像质量评估（IQA）和图像修复在概念上紧密相关，但现有工作大多将它们分开处理。统一的多模态理解-生成模型的最新进展表明，更强的理解能力可以提升生成性能，这促使研究者探索如何用单一模型统一IQA和修复任务，并研究IQA如何指导修复过程。

Method: UARE基于预训练的统一理解和生成模型，采用两阶段训练框架：1）渐进式从单一类型失真扩展到高阶混合退化的训练计划，使模型能处理多种退化；2）使用交错文本-图像数据进行统一微调，将IQA信号与修复目标对齐。通过多任务协同训练，UARE利用IQA提升修复和增强性能。

Result: 在IQA、修复和增强任务上的广泛实验证明了UARE的有效性。模型能够统一处理图像质量评估、修复和增强任务，并利用质量评估信号提升修复性能。

Conclusion: UARE是首个统一图像质量评估、修复和增强的视觉语言模型，通过将理解与生成任务结合，证明了IQA可以有效地指导图像修复和增强过程，为低层视觉任务提供了新的统一框架。

Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.

</details>


### [63] [VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors](https://arxiv.org/abs/2512.06759)
*Wenbo Lyu,Yingjun Du,Jinglin Zhao,Xianton Zhen,Ling Shao*

Main category: cs.CV

TL;DR: VisChainBench是一个大规模基准测试，用于评估大型视觉语言模型在多图像、多轮场景中的视觉推理能力，包含1,457个任务和20,000多张图像，覆盖日常场景和工程故障排除等三个领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注静态或水平比较，严重依赖语言线索，忽视了渐进式、上下文相关的推理和视觉到视觉的推理挑战。需要评估LVLMs在顺序、相互依赖任务中进行多步视觉推理的能力。

Method: 使用多智能体生成流水线构建基准测试，确保高视觉多样性和受控的语言偏差。包含三个不同领域的任务，模拟真实世界的决策过程。

Result: 创建了VisChainBench基准测试，包含1,457个任务，涵盖20,000多张图像，数据可通过Hugging Face访问。

Conclusion: VisChainBench填补了现有基准测试的空白，能够严格评估LVLMs在多图像、多轮场景中的视觉推理能力，为模型评估提供了新的标准。

Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench

</details>


### [64] [Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding](https://arxiv.org/abs/2512.06769)
*Hang Yin,Xiaomin He,PeiWen Yuan,Yiwei Li,Jiayi Shi,Wenxiao Fan,Shaoxiong Feng,Kan Li*

Main category: cs.CV

TL;DR: 提出Stitch and Tell方法，通过拼接图像并生成空间感知的文本对，无需标注即可增强视觉语言模型的空间理解能力，减少空间幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型存在空间幻觉问题，即错误描述图像中物体的相对位置。作者认为这主要源于图像和文本之间的不对称特性。

Method: 提出Stitch and Tell方法，通过沿空间轴拼接图像，基于拼接图像的布局生成空间感知的标题或问答对，无需依赖昂贵的高级模型或人工参与。

Result: 在三种架构、两个训练数据集和八个基准测试上评估，SiTe显著提升了空间理解任务（MME_Position +5.50%，Spatial-MM +4.19%），同时保持或提升了一般视觉语言基准的性能。

Conclusion: 将空间感知结构显式注入训练数据是缓解空间幻觉、提高空间理解能力的有效方法，同时能保持一般的视觉语言能力。

Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.

</details>


### [65] [Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos](https://arxiv.org/abs/2512.06783)
*Tobias Leuthold,Michele Xiloyannis,Yves Zimmermann*

Main category: cs.CV

TL;DR: 提出一种实时后处理算法，融合BlazePose的3D和2D估计，通过加权优化结合骨骼长度和生物力学约束，提升姿态估计的解剖学一致性。


<details>
  <summary>Details</summary>
Motivation: 现有实时姿态估计模型（如BlazePose）缺乏解剖学约束，在物理治疗等自动化教练应用中需要更准确、更符合解剖学的姿态估计。

Method: 采用加权优化融合BlazePose的3D和2D估计，惩罚与预期骨骼长度和生物力学模型的偏差；使用自适应测量信任的卡尔曼滤波器精炼个体解剖学的骨骼长度估计。

Result: 在Physio2.2M数据集上，相比BlazePose 3D估计，3D MPJPE降低10.2%，身体段间角度误差减少16.6%。

Conclusion: 该方法提供了一种基于计算高效视频到3D姿态估计的鲁棒、解剖学一致的姿态估计，适用于消费级笔记本电脑和移动设备上的自动化物理治疗、医疗保健和运动教练。

Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.

</details>


### [66] [VDOT: Efficient Unified Video Creation via Optimal Transport Distillation](https://arxiv.org/abs/2512.06802)
*Yutong Wang,Haiyu Zhang,Tianfan Xue,Yu Qiao,Yaohui Wang,Chang Xu,Xinyuan Chen*

Main category: cs.CV

TL;DR: VDOT是一个高效统一的视频生成模型，通过分布匹配蒸馏和最优传输技术，在4步生成中达到或超过100步基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型要么只能处理少数特定条件，要么因推理复杂导致生成时间过长，难以在实际应用中部署。需要开发既高效又通用的视频生成解决方案。

Method: 采用分布匹配蒸馏范式，使用最优传输技术优化真实与生成分数分布之间的差异，避免KL散度蒸馏中的零强迫或梯度崩溃问题。同时集成判别器感知真实视频数据，并开发了自动化视频数据标注过滤管道和统一测试基准UVCBench。

Result: 实验表明，仅需4步生成的VDOT模型在性能上达到或超过了需要100步去噪的基线模型，显著提升了生成效率。

Conclusion: VDOT通过最优传输增强的分布匹配蒸馏，实现了高效稳定的视频生成，为实际应用提供了可行的解决方案，并通过统一基准促进了该领域的发展。

Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.

</details>


### [67] [MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.06810)
*Yueqian Wang,Songxiang Liu,Disong Wang,Nuo Xu,Guanglu Wan,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 本文提出MMDuet2，一种基于文本到文本方法的主动视频多模态大语言模型，能够自主决定在视频播放过程中何时回应，无需手动调整响应阈值或精确标注回复时间。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态大语言模型大多采用回合制交互，只能在用户回合结束后回复。为满足实时应用需求，需要模型能够在视频播放过程中主动决定何时回应，但现有方法存在需要手动调整响应决策阈值和标注精确回复时间的问题。

Method: 提出基于文本到文本的主动交互方法，模型根据对话历史和当前帧的视觉上下文自主决定回应或保持沉默。引入多回合强化学习训练方法，鼓励及时准确的回应，无需精确的回复时间标注。在包含52k视频和两种对话类型的数据集上通过监督微调和强化学习训练MMDuet2模型。

Result: MMDuet2在响应时机和质量上优于现有主动视频MLLM基线模型，在ProactiveVideoQA基准测试中达到最先进的性能。

Conclusion: 提出的文本到文本主动交互方法和多回合强化学习训练策略有效解决了视频多模态大语言模型在实时应用中的主动响应问题，无需手动调整阈值或精确时间标注，实现了更好的响应时机控制。

Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

</details>


### [68] [RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2512.06811)
*Xiang Lin,Weixin Li,Shu Guo,Lihong Wang,Di Huang*

Main category: cs.CV

TL;DR: RMAdapter是一种新颖的重建式多模态适配器，通过双分支架构在少样本场景下平衡视觉语言模型的特定任务适应性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前预训练视觉语言模型在少样本微调中存在平衡特定任务适应和泛化能力的挑战，且现有研究主要关注基于提示的方法，基于适配器的方法研究不足且性能存在差距。

Method: 提出重建式多模态适配器，采用双分支架构：1）适应分支通过参数高效微调注入任务特定知识；2）重建分支通过将潜在空间特征重建回原始特征空间来保留通用知识。通过局部计算重建损失和共享投影模块保持轻量化，并加入一致性约束来调节判别性和泛化性之间的权衡。

Result: 在不依赖数据增强或重复提示设计的情况下，RMAdapter在三个代表性任务（新类别泛化、新目标数据集泛化、领域泛化）中全面优于最先进方法。

Conclusion: RMAdapter通过创新的双分支架构有效解决了视觉语言模型在少样本微调中平衡特定任务适应性和泛化能力的难题，为多模态适配器研究提供了新方向。

Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.

</details>


### [69] [MeshSplatting: Differentiable Rendering with Opaque Meshes](https://arxiv.org/abs/2512.06818)
*Jan Held,Sanghyun Son,Renaud Vandeghen,Daniel Rebain,Matheus Gadelha,Yi Zhou,Anthony Cioppa,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: MeshSplatting：一种基于网格的重建方法，通过可微分渲染联合优化几何和外观，将神经渲染与交互式3D图形连接起来，实现实时场景交互。


<details>
  <summary>Details</summary>
Motivation: 现有的基于基元的溅射方法（如3D高斯溅射）虽然实现了实时渲染的新视角合成，但其基于点的表示与AR/VR和游戏引擎中基于网格的流程不兼容。需要一种能够创建平滑、高质量网格的方法，以便在实时3D引擎中高效渲染。

Method: MeshSplatting通过可微分渲染联合优化几何和外观。使用受限Delaunay三角剖分强制连接性，并通过细化表面一致性来创建端到端平滑的网格。该方法训练速度更快，内存使用更少。

Result: 在Mip-NeRF360数据集上，MeshSplatting比当前最先进的基于网格的新视角合成方法MiLo提升了+0.69 dB的PSNR，同时训练速度快2倍，内存使用减少2倍。

Conclusion: MeshSplatting成功创建了端到端平滑、视觉高质量的网格，能够在实时3D引擎中高效渲染，为神经渲染和交互式3D图形之间架起了桥梁，实现了无缝的实时场景交互。

Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.

</details>


### [70] [SparseCoop: Cooperative Perception with Kinematic-Grounded Queries](https://arxiv.org/abs/2512.06838)
*Jiahao Wang,Zhongwei Jiang,Wenchao Sun,Jiaru Zhong,Haibao Yu,Yuner Zhang,Chenyang Lu,Chuang Zhang,Lei He,Shaobing Xu,Jianqiang Wang*

Main category: cs.CV

TL;DR: SparseCoop是一个完全稀疏的协同感知框架，用于3D检测和跟踪，完全摒弃了中间BEV表示，通过实例查询、粗到细聚合和协同实例去噪实现高效协同感知。


<details>
  <summary>Details</summary>
Motivation: 当前协同感知方法存在通信成本高、灵活性差、对齐不精确等问题。基于密集BEV特征共享的方法通信成本呈二次方增长，缺乏跨异步或不同视角的精确对齐灵活性。而稀疏查询方法则存在几何表示不足、融合策略次优和训练不稳定等问题。

Method: 提出SparseCoop框架，包含三个创新：1) 基于运动学的实例查询，使用包含3D几何和速度的显式状态向量进行精确时空对齐；2) 粗到细聚合模块实现鲁棒融合；3) 协同实例去噪任务加速和稳定训练。完全摒弃中间BEV表示。

Result: 在V2X-Seq和Griffin数据集上实现了最先进的性能。具有优越的计算效率、低传输成本和强大的通信延迟鲁棒性。

Conclusion: SparseCoop通过完全稀疏的协同感知框架，解决了当前方法的通信成本、对齐精度和训练稳定性问题，在保持高性能的同时实现了高效计算和低通信开销。

Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.

</details>


### [71] [CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840)
*Satoshi Hashimoto,Tatsuya Konishi,Tomoya Kaichi,Kazunori Matsumoto,Mori Kurokawa*

Main category: cs.CV

TL;DR: 该论文首次将持续学习与弱监督视频异常检测相结合，提出CADE方法解决领域漂移和遗忘问题，在多场景数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视频异常检测方法主要处理静态数据集，忽视了数据领域可能变化的问题。当数据领域发生漂移时，仅用新数据训练会导致对先前数据的性能下降（遗忘）。因此需要从持续学习角度解决这一问题。

Method: 提出CADE方法：1）使用双生成器处理数据不平衡和标签不确定性；2）提出多判别器集成方法，通过多个模型捕捉因遗忘而错过的异常模式，解决模型偏向特定异常模式导致的"不完整性"问题。

Result: 在ShanghaiTech和Charlotte Anomaly等多场景视频异常检测数据集上的大量实验表明，CADE显著优于现有的视频异常检测方法。

Conclusion: 该研究首次将持续学习与弱监督视频异常检测相结合，提出的CADE方法能有效应对领域漂移和遗忘问题，在多场景异常检测中表现出色。

Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.

</details>


### [72] [Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845)
*Satoshi Hashimoto,Hitoshi Nishimura,Yanan Wang,Mori Kurokawa*

Main category: cs.CV

TL;DR: PA-VAD：一种无需真实异常视频的生成驱动视频异常检测方法，通过合成伪异常视频与真实正常视频配对训练，在标准弱监督设置下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 实际部署视频异常检测面临真实异常视频稀缺和收集成本高的问题。本文旨在不依赖任何真实异常视频的情况下，在标准弱监督设置下训练检测器，提供可扩展部署的实用路径。

Method: 1. 合成阶段：使用CLIP选择类别相关的初始图像，通过视觉语言模型优化文本提示以提高保真度和场景一致性，然后调用视频扩散模型生成伪异常视频。2. 训练阶段：通过域对齐正则化模块缓解合成异常中过度的时空幅度，该模块结合域对齐和内存使用感知更新。

Result: 在ShanghaiTech数据集上达到98.2%，在UCF-Crime数据集上达到82.5%，分别超过最强真实异常方法0.6%和UVAD SOTA方法1.9%。

Conclusion: 无需收集真实异常即可获得高精度异常检测，为可扩展部署提供了实用路径。生成驱动的方法在标准弱监督设置下表现出色，证明了不依赖真实异常视频的可行性。

Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.

</details>


### [73] [Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT](https://arxiv.org/abs/2512.06849)
*Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik Möller*

Main category: cs.CV

TL;DR: 提出一种弱监督方法，仅使用椎体级别的健康/恶性标签（无需病变掩码），通过扩散自编码器和像素差异图生成候选病变区域，再通过Hide-and-Seek Attribution机制筛选真实恶性区域，实现CT中椎体转移瘤的准确分割。


<details>
  <summary>Details</summary>
Motivation: CT中椎体转移瘤的准确分割在临床上很重要，但由于体素级标注稀缺，且溶骨性和成骨性病变常与良性退行性改变相似，难以规模化应用。需要开发仅使用弱标签（椎体级别健康/恶性标签）的方法来生成可靠的病变分割。

Method: 方法结合扩散自编码器（DAE）生成椎体的健康编辑版本，通过像素差异图提出候选病变区域。引入Hide-and-Seek Attribution机制：依次揭示每个候选区域同时隐藏其他区域，将编辑后的图像通过DAE投影回数据流形，使用潜在空间分类器量化该组件的独立恶性贡献。高评分区域形成最终的溶骨性或成骨性分割。

Result: 在保留的放射科医生标注上，尽管没有掩码监督，仍取得了强健的成骨性/溶骨性性能（F1: 0.91/0.85; Dice: 0.87/0.78），显著超过基线方法（F1: 0.79/0.67; Dice: 0.74/0.55）。

Conclusion: 椎体级别标签可以转化为可靠的病变掩码，表明生成编辑与选择性遮挡相结合能够支持CT中准确的弱监督分割。

Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.

</details>


### [74] [Omni-Referring Image Segmentation](https://arxiv.org/abs/2512.06862)
*Qiancheng Zheng,Yunhang Shen,Gen Luo,Baiyang Song,Xing Sun,Xiaoshuai Sun,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出OmniRIS任务，支持文本指令和带掩码/框/涂鸦的参考图像作为全模态提示，实现高度通用化的图像分割


<details>
  <summary>Details</summary>
Motivation: 现有单模态条件分割任务（如RIS和视觉RIS）存在局限性，需要更通用的图像分割方法，能够同时利用文本和视觉模态的优势

Method: 提出OmniRIS任务框架，构建OmniRef数据集（186,939个全模态提示对应30,956张图像），设计OmniSegNet基线模型处理全模态提示编码等关键挑战

Result: 实验验证了OmniSegNet能够有效遵循全模态指令，并展示了OmniRIS在高度通用化图像分割方面的优越性

Conclusion: OmniRIS通过支持文本和视觉全模态提示，实现了更通用、更实用的图像分割，为高度通用化图像分割研究提供了新方向

Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.

</details>


### [75] [Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training](https://arxiv.org/abs/2512.06864)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS是一个无监督视频实例分割框架，通过质量引导的自训练方法，在无需人工标注的情况下实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割面临像素级掩码和时间一致性标注的双重挑战。现有的无监督方法如VideoCutLER虽然通过合成数据消除了光流依赖，但仍受限于合成到真实域的差距。

Method: 提出AutoQ-VIS框架，通过质量引导的自训练方法，在伪标签生成和自动质量评估之间建立闭环系统，实现从合成视频到真实视频的渐进式适应。

Result: 在YouTubeVIS-2019验证集上达到52.6 AP50，比之前的SOTA方法VideoCutLER提升了4.4%，且完全不需要人工标注。

Conclusion: 证明了质量感知自训练在无监督视频实例分割中的可行性，为克服合成到真实域差距提供了有效解决方案。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.

</details>


### [76] [Spatial Retrieval Augmented Autonomous Driving](https://arxiv.org/abs/2512.06865)
*Xiaosong Jia,Chenhe Zhang,Yule Jiang,Songbur Wong,Zhiyuan Zhang,Chen Chen,Shaofeng Zhang,Xuanhe Zhou,Xue Yang,Junchi Yan,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出空间检索范式，通过引入离线检索的地理图像作为额外输入，增强自动驾驶系统的环境感知能力，特别是在视野受限、遮挡或极端天气条件下。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统依赖车载传感器进行环境感知，但受限于实时感知范围，在视野受限、遮挡、黑暗或雨天等极端条件下表现不佳。人类驾驶员能够在能见度差时回忆道路结构，因此希望赋予模型这种"回忆"能力。

Method: 提出空间检索范式，从离线缓存（如Google Maps或存储的自动驾驶数据集）中检索地理图像作为额外输入。扩展nuScenes数据集，通过Google Maps API检索地理图像并与自车轨迹对齐。在五个核心自动驾驶任务上建立基线。

Result: 扩展的模态能够增强某些任务的性能。将开源数据集整理代码、数据和基准，供进一步研究这一新的自动驾驶范式。

Conclusion: 空间检索范式为自动驾驶系统提供了"回忆"能力，通过离线检索的地理图像增强环境感知，特别是在视野受限条件下，且无需额外传感器，可作为现有任务的即插即用扩展。

Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.

</details>


### [77] [Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior](https://arxiv.org/abs/2512.06866)
*Yulin Li,Haokun Gui,Ziyang Fan,Junjie Wang,Bin Kang,Bin Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: DyToK：一种无需训练的VLLM动态令牌压缩方法，利用注意力机制中的关键帧先验动态调整每帧令牌保留比例，提升长视频处理效率


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在处理长视频时面临二次计算复杂度增长问题，现有关键帧采样方法在特征编码前引入额外计算成本，且二元帧选择范式不够优化

Method: 提出DyToK（Dynamic Token compression via LLM-guided Keyframe prior），利用VLLM注意力层自然编码的查询条件关键帧先验，动态调整每帧令牌保留比例，优先保留语义丰富的帧并抑制冗余

Result: DyToK实现了最先进的效率-准确性权衡，与现有压缩方法（如VisionZip和FastV）兼容，在LLaVA-OneVision和Qwen2.5-VL等多个VLLM上实现4.3倍推理加速同时保持准确性

Conclusion: DyToK提供了一种无需训练的即插即用动态令牌压缩范式，有效解决了VLLM处理长视频时的效率瓶颈问题

Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .

</details>


### [78] [Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective](https://arxiv.org/abs/2512.06870)
*Wangkai Li,Rui Sun,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: ECOCSeg提出了一种基于纠错输出码的语义分割方法，通过将类别分解为属性并处理部分错误比特，提升伪标签学习的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 伪标签学习在语义分割中广泛应用，特别是在标签稀缺的场景如无监督域适应和半监督学习中。然而，这种范式会产生错误的伪标签，并且由于使用one-hot编码，这些错误在训练过程中会被进一步放大。

Method: 提出了ECOCSeg方法，利用纠错输出码为每个类别创建细粒度编码。包括：1）引入基于ECOC的分类器，将类别分解为属性并处理部分不准确的比特；2）开发比特级标签去噪机制，生成更高质量的伪标签。

Result: ECOCSeg可以轻松集成到现有方法中，在多个UDA和SSL基准测试中，跨不同分割架构都表现出显著的性能提升。

Conclusion: ECOCSeg通过纠错输出码提供了一种新颖的语义分割视角，能够有效处理伪标签中的错误，提高模型的稳定性和泛化能力，在标签稀缺场景下具有重要应用价值。

Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.

</details>


### [79] [SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification](https://arxiv.org/abs/2512.06877)
*Mohammed Q. Alkhatib,Ali Jamali,Swalpa Kumar Roy*

Main category: cs.CV

TL;DR: 提出基于卷积混合器范式的轻量级架构，用于遥感场景分类，在AID和EuroSAT数据集上取得良好精度与效率平衡


<details>
  <summary>Details</summary>
Motivation: 遥感场景分类对地球观测至关重要，但现有CNN和ViT模型在空间分辨率、视角、方向和背景条件变化下泛化能力有限，需要更高效的解决方案

Method: 提出轻量级卷积混合器架构，通过多尺度深度卷积进行空间混合，通过逐点操作进行通道混合，高效提取局部和上下文信息，同时保持低参数量和计算量

Result: 在AID数据集上获得74.7%总体精度、74.57%平均精度和73.79 Kappa值；在EuroSAT数据集上获得93.90%总体精度、93.93%平均精度和93.22 Kappa值

Conclusion: 所提方法在精度和效率之间取得了良好平衡，优于广泛使用的CNN和transformer模型，代码将公开提供

Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer

</details>


### [80] [Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion](https://arxiv.org/abs/2512.06882)
*Yu Zhu,Naoya Chiba,Koichi Hashimoto*

Main category: cs.CV

TL;DR: 提出分层图像引导的3D分割框架，通过从实例级到部件级的渐进式细化，解决工业场景中遮挡、尺度差异和语义不一致问题，无需昂贵标注。


<details>
  <summary>Details</summary>
Motivation: 工业环境中密集布局和多尺度物体导致可靠3D分割困难：严重遮挡削弱几何边界，大尺度差异使端到端模型难以同时捕捉粗细节和细细节。现有方法需要昂贵标注或存在跨视图语义不一致问题。

Method: 分层图像引导框架：1) 实例分割：渲染俯视图，用YOLO-World提示SAM生成掩码，反向投影到3D点云；2) 部件级分割：对每个实例渲染多视角图像，在每个视角应用相同2D分割和反向投影，通过贝叶斯更新融合确保跨视图语义一致性。

Result: 在真实工厂数据实验中有效处理遮挡和结构复杂性，获得一致高的每类mIoU分数。在公共数据集上的额外评估证实了框架的泛化能力，突显其鲁棒性、标注效率和适应不同3D环境的能力。

Conclusion: 提出的分层图像引导3D分割框架能够有效解决工业场景中的遮挡、尺度差异和语义不一致问题，无需昂贵标注，具有鲁棒性、高效性和良好泛化能力。

Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.

</details>


### [81] [JoPano: Unified Panorama Generation via Joint Modeling](https://arxiv.org/abs/2512.06885)
*Wancheng Feng,Chen An,Zhenliang He,Meina Kan,Shiguang Shan,Lukun Wang*

Main category: cs.CV

TL;DR: JoPano提出了一种基于DiT的统一全景图生成方法，通过联合面适配器和条件切换机制，在单个模型中同时处理文本到全景图和视图到全景图两种任务，显著提升了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有全景图生成方法面临两大挑战：1）基于U-Net的架构限制了生成全景图的视觉质量；2）通常将文本到全景图和视图到全景图两个核心任务独立处理，导致建模冗余和效率低下。

Method: 提出基于DiT的联合面全景图生成方法，包括：1）基于立方体贴图表示的联合面适配器，将预训练DiT的生成能力迁移到全景图领域；2）泊松融合减少立方体面边界的不一致性；3）条件切换机制统一两种任务于单个模型；4）引入Seam-SSIM和Seam-Sobel指标量化评估接缝一致性。

Result: JoPano在文本到全景图和视图到全景图生成任务中都能生成高质量全景图，在FID、CLIP-FID、IS和CLIP-Score等指标上达到最先进性能。

Conclusion: JoPano通过统一的DiT架构和创新的适配器设计，成功解决了现有全景图生成方法的局限性，实现了高质量、高效率的全景图生成，并为全景图生成领域提供了新的评估指标和方法框架。

Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

</details>


### [82] [Balanced Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2512.06886)
*Wangkai Li,Rui Sun,Bohao Liao,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: BLDA提出了一种平衡学习域适应方法，通过分析预测logits分布来识别过预测和欠预测类别，使用共享锚分布对齐不同类别的logits分布，并在自训练中引入logits校正项，以解决语义分割UDA中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自训练方法在无监督域适应语义分割中，由于数据分布偏移和标签空间差异，难以平衡地学习各个类别，导致类别预测偏差问题。

Method: 1) 通过分析预测logits分布识别过预测和欠预测类别；2) 使用共享锚分布对logits分布进行后处理对齐；3) 在线估计logits分布并在损失函数中加入logits校正项；4) 利用累积密度作为域共享的结构知识连接源域和目标域。

Result: 在两个标准UDA语义分割基准上的广泛实验表明，BLDA能够持续提升性能，特别是对于欠预测类别，并且可以集成到多种现有方法中。

Conclusion: BLDA通过直接评估和缓解类别偏差，无需先验分布知识，有效解决了UDA语义分割中的类别不平衡问题，显著提升了模型性能。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.

</details>


### [83] [Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation](https://arxiv.org/abs/2512.06888)
*Liyang Song,Hardik Bishnoi,Sai Kumar Reddy Manne,Sarah Ostadabbas,Briana J. Taylor,Michael Wan*

Main category: cs.CV

TL;DR: 开发首个可重复的婴儿呼吸监测计算机视觉系统，包括新数据集AIR-400和婴儿专用算法，填补了婴儿无接触呼吸监测的技术空白


<details>
  <summary>Details</summary>
Motivation: 婴儿呼吸异常与神经发育障碍和婴儿猝死综合征相关，但现有呼吸监测技术主要针对成人，缺乏婴儿专用的视频数据集和可重复算法

Method: 创建包含400个视频的AIR-400数据集，开发基于婴儿特定感兴趣区域检测和时空神经处理的算法，结合光流输入增强处理

Result: 建立了首个可重复的婴儿呼吸估计基准，提供了公开可用的数据集、代码库和训练模型

Conclusion: 该研究填补了婴儿无接触呼吸监测的技术空白，为早期检测和治疗呼吸异常提供了重要工具，所有资源已公开以促进该领域发展

Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.

</details>


### [84] [Scaling Zero-Shot Reference-to-Video Generation](https://arxiv.org/abs/2512.06905)
*Zijian Zhou,Shikun Liu,Haozhe Liu,Haonan Qiu,Zhaochong An,Weiming Ren,Zhiheng Liu,Xiaoke Huang,Kam Woh Ng,Tian Xie,Xiao Han,Yuren Cong,Hang Li,Chuyan Zhu,Aditya Patel,Tao Xiang,Sen He*

Main category: cs.CV

TL;DR: Saber是一个无需显式参考图像-视频-文本三元组数据的零样本参考到视频生成框架，通过掩码训练策略和注意力模型设计实现身份一致性和参考感知表示


<details>
  <summary>Details</summary>
Motivation: 当前参考到视频生成方法依赖昂贵的显式参考图像-视频-文本三元组数据，这些数据的构建成本高且难以扩展，需要绕过这一瓶颈

Method: 提出Saber框架，仅使用视频-文本对进行训练，采用掩码训练策略和定制的基于注意力的模型设计，学习身份一致和参考感知的表示，并集成掩码增强技术减少复制粘贴伪影

Result: 在OpenS2V-Eval基准测试中表现优于使用R2V数据训练的方法，展示了在不同数量参考图像下的出色泛化能力

Conclusion: Saber通过零样本框架成功绕过了对昂贵R2V数据的依赖，实现了可扩展的参考到视频生成，在保持身份一致性的同时减少了伪影

Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.

</details>


### [85] [NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921)
*Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 本文介绍了NeuroABench——首个专门评估神经外科解剖理解的多模态基准，包含9小时标注视频，覆盖89种手术和68个解剖结构。测试显示当前MLLMs在解剖识别任务中最高准确率仅40.87%，远低于神经外科学员平均46.5%的水平。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在手术视频理解方面的研究主要关注手术流程和工作流，而忽视了临床实践中至关重要的解剖理解能力。外科医生依赖精确的解剖知识来解读、回顾和学习手术视频，但目前缺乏专门评估解剖理解能力的基准。

Method: 作者提出了NeuroABench基准，包含9小时标注的神经外科视频，覆盖89种不同手术。采用新颖的多模态标注流程和多重审查周期，评估模型对68个临床解剖结构的识别能力。对10多个最先进的MLLMs进行了实验评估，并与4名神经外科学员进行了对比测试。

Result: 实验结果显示，最佳MLLM在解剖识别任务中仅达到40.87%的准确率。神经外科学员测试中，最佳学员准确率为56%，最低为28%，平均为46.5%。最佳MLLM仅与最低分学员相当，显著低于学员平均水平。

Conclusion: NeuroABench填补了手术视频理解中解剖评估的空白，揭示了当前MLLMs在解剖理解方面的显著局限性。虽然MLLMs已取得一定进展，但与人类水平相比仍有巨大差距，为未来研究提供了重要的评估基准和改进方向。

Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.

</details>


### [86] [Selective Masking based Self-Supervised Learning for Image Semantic Segmentation](https://arxiv.org/abs/2512.06981)
*Yuemin Wang,Ian Stavness*

Main category: cs.CV

TL;DR: 提出一种基于选择性掩码图像重建的自监督学习方法，用于语义分割预训练，相比随机掩码和ImageNet监督预训练，在通用和杂草分割数据集上分别提升2.9%和2.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统掩码图像建模方法使用随机掩码，未能充分利用训练模型的知识；需要更有效的自监督预训练方法来提升语义分割性能，特别是在计算资源受限的场景下。

Method: 提出选择性掩码图像重建方法，通过迭代步骤选择重建损失最高的图像块进行掩码，利用已训练模型的知识指导掩码选择，替代传统的随机掩码策略。

Result: 在Pascal VOC、Cityscapes两个通用数据集和两个杂草分割数据集上，选择性掩码方法比随机掩码和ImageNet监督预训练分别提升2.9%和2.5%的下游分割准确率，特别改善了最低性能类别的准确率。

Conclusion: 选择性掩码图像重建为语义分割工作流提供了有效实用的自监督预训练方案，特别适合需要有限模型容量以满足推理速度和计算资源要求的场景。

Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.

</details>


### [87] [Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues](https://arxiv.org/abs/2512.07034)
*Tuan-Anh Vu,Hai Nguyen-Truong,Ziqiang Zheng,Binh-Son Hua,Qing Guo,Ivor Tsang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: TransCues是一个用于透明物体分割的框架，通过边界特征增强和反射特征增强模块，在多个基准数据集上显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 玻璃等透明物体在日常中普遍存在，但由于其透明性和反射特性，现有分割方法难以将其与不透明材料区分开来。人类感知依赖边界和反射物体特征来识别玻璃物体，但现有文献在处理透明物体时未能充分捕捉这两种特性。

Method: 提出TransCues框架，采用金字塔式transformer编码器-解码器架构，通过边界特征增强模块和反射特征增强模块以互利的方式结合这两种强大的视觉线索。

Result: 在两个模块的有效协同下，在多个基准数据集上取得显著性能提升：Trans10K-v2 (+4.2% mIoU)、MSD (+5.6% mIoU)、RGBD-Mirror (+10.1% mIoU)、TROSD (+13.1% mIoU)、Stanford2D3D (+8.3% mIoU)。

Conclusion: 通过同时利用边界特征和反射特征，TransCues框架能够有效分割透明物体，在多个数据集上大幅超越现有最先进方法，证明了该方法处理玻璃物体的有效性。

Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.

</details>


### [88] [Evaluating and Preserving High-level Fidelity in Super-Resolution](https://arxiv.org/abs/2512.07037)
*Josep M. Rocafort,Shaolin Su,Javier Vazquez-Corral,Alexandra Gomez-Villa*

Main category: cs.CV

TL;DR: 本文提出衡量超分辨率模型高级语义保真度的重要性，构建首个带保真度评分的数据集，分析现有质量指标与保真度的相关性，并展示通过保真度反馈微调模型可同时提升语义保真度和感知质量。


<details>
  <summary>Details</summary>
Motivation: 当前超分辨率模型虽然能重建细节并产生视觉愉悦的输出，但其强大的生成能力有时会产生幻觉，改变图像内容。这种高级别的内容变化容易被人类识别，但现有低级别图像质量指标未能很好衡量。需要建立衡量SR模型高级保真度的补充标准，以揭示生成式SR模型的可靠性。

Method: 1) 构建首个带有不同SR模型保真度评分的标注数据集；2) 评估SOTA SR模型在保持高级保真度方面的实际表现；3) 分析现有图像质量指标与保真度测量的相关性；4) 展示基础模型能更好地处理这一高级任务；5) 基于保真度反馈微调SR模型。

Result: 建立了首个SR保真度标注数据集，评估了SOTA SR模型的保真度表现，发现现有质量指标与保真度相关性有限，基础模型更适合处理高级保真度任务，通过保真度反馈微调可同时提升语义保真度和感知质量。

Conclusion: 高级保真度测量作为补充标准对评估和优化SR模型具有重要价值。提出的保真度标准不仅可用于模型评估，还能指导模型优化，实现语义保真度和感知质量的双重提升。将发布数据集、代码和模型。

Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.

</details>


### [89] [DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation](https://arxiv.org/abs/2512.07051)
*Adnan Munir,Shujaat Khan*

Main category: cs.CV

TL;DR: DAUNet是一种轻量级UNet变体，结合可变形卷积V2和无参数注意力SimAM，在保持模型复杂度不变的情况下提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在自动化诊断和治疗规划中至关重要，但现有模型在处理几何变化和上下文感知特征融合方面存在不足，同时需要保持轻量级以适应资源受限的临床环境。

Method: 提出DAUNet架构：1）瓶颈层使用动态可变形卷积核处理几何变化；2）解码器和跳跃连接路径集成SimAM注意力模块进行显著性感知细化；3）整体保持轻量级设计。

Result: 在两个挑战性数据集（FH-PS-AoP超声和FUMPE CT）上，DAUNet在Dice分数、HD95和ASD指标上优于最先进模型，同时保持优越的参数效率。消融研究验证了可变形卷积和SimAM注意力的各自贡献。

Conclusion: DAUNet对缺失上下文和低对比度区域具有鲁棒性，适合在实时和资源受限的临床环境中部署，为医学图像分割提供了高效解决方案。

Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.

</details>


### [90] [RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting](https://arxiv.org/abs/2512.07052)
*Hoang-Nhat Tran,Francesco Di Sario,Gabriele Spadaro,Giuseppe Valenzise,Enzo Tartaglione*

Main category: cs.CV

TL;DR: 提出了一种灵活的3D高斯泼溅压缩方案，支持在预定边界内的任意速率插值，无需重新训练即可适应不同带宽和设备限制。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）虽然能实现实时逼真渲染，但存在内存需求大和训练成本高的问题。现有压缩方法只能在固定速率下工作，无法适应变化的带宽和设备限制。

Method: 提出了一种灵活的3DGS压缩方案，支持在预定边界内的任意速率插值。该方法计算轻量，无需为不同速率重新训练，能在广泛的操作点保持渲染质量。

Result: 实验表明该方法实现了高效、高质量的压缩，同时提供动态速率控制，适合在沉浸式应用中实际部署。

Conclusion: 该方法解决了3DGS压缩的灵活性限制问题，为沉浸式应用提供了实用的动态速率控制解决方案，代码将在接受后开源。

Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.

</details>


### [91] [$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction](https://arxiv.org/abs/2512.07062)
*Changliang Xia,Chengyou Jia,Minnan Luo,Zhuohang Dang,Xin Shen,Bowen Ping*

Main category: cs.CV

TL;DR: D³-Predictor：一种无噪声的确定性框架，通过重构预训练扩散模型消除随机性，将时间步依赖的视觉专家集成到一个完整的几何先验中，用于密集预测任务。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为密集预测骨干存在核心限制：扩散采样的随机噪声与需要确定性图像到几何映射的密集预测任务本质不匹配。这种随机噪声会破坏细粒度空间线索，将模型推向时间步特定的噪声目标，从而破坏有意义的几何结构映射。

Method: 提出D³-Predictor，通过重构预训练扩散模型消除随机性噪声，将其视为时间步依赖的视觉专家集合，自监督地聚合这些异构先验到一个单一、干净、完整的几何先验中，同时利用任务特定监督无缝适应密集预测任务。

Result: 在各种密集预测任务上的广泛实验表明，D³-Predictor在多样化场景中实现了竞争性或最先进的性能，同时所需训练数据不到先前方法的一半，并能以单步推理高效执行。

Conclusion: D³-Predictor成功解决了扩散模型中随机噪声与密集预测任务之间的不匹配问题，通过无噪声确定性框架实现了优异的性能表现，为扩散模型在密集预测任务中的应用提供了新思路。

Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.

</details>


### [92] [Context-measure: Contextualizing Metric for Camouflage](https://arxiv.org/abs/2512.07076)
*Chen-Yang Wang,Gepeng Ji,Song Shao,Ming-Ming Cheng,Deng-Ping Fan*

Main category: cs.CV

TL;DR: 提出Context-measure评估范式，通过概率像素感知相关框架考虑空间依赖性和像素级伪装量化，比现有上下文无关指标更可靠


<details>
  <summary>Details</summary>
Motivation: 当前伪装场景评估指标忽视上下文依赖性这一关键因素，这些指标原本为评估一般或显著对象设计，假设空间上下文不相关，无法准确反映伪装场景的真实评估需求

Method: 提出新的上下文化评估范式Context-measure，基于概率像素感知相关框架，通过纳入空间依赖性和像素级伪装量化，使评估更符合人类感知

Result: 在三个具有挑战性的伪装对象分割数据集上的广泛实验表明，Context-measure比现有的上下文无关指标提供更可靠的评估结果

Conclusion: Context-measure可为涉及伪装模式的各种计算机视觉应用提供基础评估基准，包括农业、工业和医疗场景，代码已开源

Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.

</details>


### [93] [COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision](https://arxiv.org/abs/2512.07107)
*Jaeyoon Lee,Hojoon Jung,Sungtae Hwang,Jihyong Oh,Jongwon Choi*

Main category: cs.CV

TL;DR: COREA：首个联合学习可重光照3D高斯和SDF的统一框架，实现精确几何重建和真实重光照


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法虽然扩展到网格重建和基于物理的渲染，但其几何仍从2D渲染学习，导致表面粗糙和BRDF-光照分解不可靠

Method: 引入粗到细的双向3D到3D对齐策略，让几何信号直接在3D空间学习；深度提供粗对齐，深度梯度和法线细化精细结构；密度控制机制稳定高斯增长

Result: 在标准基准测试中，COREA在新视角合成、网格重建和PBR方面均取得优越性能

Conclusion: COREA通过联合学习可重光照3D高斯和SDF，在统一框架内实现了精确几何重建和真实重光照，解决了现有方法的局限性

Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.

</details>


### [94] [Training-free Clothing Region of Interest Self-correction for Virtual Try-On](https://arxiv.org/abs/2512.07126)
*Shengjie Lu,Zhibin Wan,Jiejie Liu,Quan Zhang,Mingjie Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的虚拟试穿方法，通过能量函数约束注意力机制，使生成结果更好地保留目标服装细节，并设计了新的评估指标VTID来全面评估生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法在生成服装时存在模式、纹理和边界方面的差异，无法准确保留目标服装细节，且现有评估指标只关注图像真实性而忽略了与目标元素的匹配度。

Method: 提出使用能量函数约束生成过程中的注意力图，使注意力更集中于服装感兴趣区域，从而生成更符合目标服装细节的结果。同时设计了新的评估指标VTID来全面评估生成质量。

Result: 在VITON-HD和DressCode数据集上，该方法在LPIPS、FID、KID和VTID指标上分别优于先前SOTA方法1.4%、2.3%、12.3%和5.8%。在CC-Reid下游任务中，在LTCC、PRCC、VC-Clothes数据集上Rank-1指标分别提升2.5%、1.1%和1.6%。

Conclusion: 提出的能量函数约束注意力机制能有效提升虚拟试穿的质量，新设计的VTID指标能更全面地评估生成结果，方法在多个数据集和下游任务中均表现出优越性能。

Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.

</details>


### [95] [MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP](https://arxiv.org/abs/2512.07128)
*Chau Truong,Hieu Ta Quang,Dung D. Le*

Main category: cs.CV

TL;DR: MulCLIP提出了一种端到端多级对齐框架，通过全局对比对齐、局部特征校准和子标题聚合补丁对齐，解决CLIP模型处理长文本描述时性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型如CLIP在处理简短描述时表现良好，但在处理长文本详细描述时性能下降。虽然已有方法利用区域建议信息来映射视觉区域与长文本句子，但部署成本较高。需要一种更高效的方法来处理长文本与图像的细粒度对齐。

Method: 1. 保持图像与摘要及长标题的全局对比对齐，同时扩展位置嵌入以支持更长文本序列
2. 提出局部校准特征的token重建对齐，增强单词与图像补丁之间的语义连接
3. 提出子标题聚合补丁对齐，自动提取和聚合每个子标题的上下文丰富补丁

Result: 在多个基准测试中，MulCLIP方法持续提升了下游任务性能。消融研究证实其多尺度对齐是驱动比区域建议辅助方法更好细粒度能力的关键因素。

Conclusion: MulCLIP通过多级对齐框架有效解决了CLIP模型处理长文本描述的问题，在保持部署效率的同时提升了细粒度理解能力，特别适合多样化的实际应用场景。

Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.

</details>


### [96] [A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning](https://arxiv.org/abs/2512.07136)
*Siyang Jiang,Mu Yuan,Xiang Ji,Bufang Yang,Zeyu Liu,Lilin Xu,Yang Li,Yuting He,Liran Dong,Wenrui Lu,Zhenyu Yan,Xiaofan Jiang,Wei Gao,Hongkai Chen,Guoliang Xing*

Main category: cs.CV

TL;DR: CUHK-X是一个大规模多模态数据集和基准套件，用于人类动作识别、理解和推理，包含58,445个样本覆盖40种动作，通过基于提示的场景创建方法生成逻辑一致的文本描述。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理非RGB模态（如深度、IMU、毫米波）时存在困难，且现有数据集主要提供粗糙的数据标签注释，不足以捕捉细粒度的动作动态，无法满足人类动作理解和推理的需求。

Method: 提出CUHK-X数据集，包含58,445个样本覆盖40种动作，由30名参与者在两个室内环境中完成。采用基于提示的场景创建方法，利用LLM生成逻辑连贯的活动序列，然后进行人工验证，确保文本描述的逻辑和时空一致性。

Result: 实验结果显示平均准确率：HAR为76.52%，HAU为40.76%，HARn为70.25%。数据集包含三个基准和六个评估任务，为社区提供了数据密集型学习方法的基础。

Conclusion: CUHK-X数据集和基准套件旨在使研究社区能够应用和开发数据密集型学习方法，实现鲁棒的多模态人类活动分析，填补了现有数据集在细粒度动作理解和推理方面的空白。

Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.

</details>


### [97] [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/abs/2512.07141)
*Fenghua Weng,Chaochao Lu,Xia Hu,Wenqi Shao,Wenjie Wang*

Main category: cs.CV

TL;DR: TRR（Think-Reflect-Revise）是一个三阶段训练框架，通过策略引导的自我反思增强大型视觉语言模型的安全对齐能力，将安全响应率从42.8%提升到87.7%。


<details>
  <summary>Details</summary>
Motivation: 现有单次推理范式（think-then-answer）容易受到上下文或视觉越狱攻击，因为单次推理可能忽略自身输出中的显性有害内容。关键洞察是利用被浪费的信号——通过反思利用第一轮推理中揭示的恶意内容，实现真正的自我修正。

Method: 1）构建包含5000个样例的Reflective Safety Reasoning数据集，遵循think-reflect-revise流程；2）使用ReSafe数据集微调目标模型以初始化反思行为；3）通过强化学习强化策略引导的反思。

Result: TRR显著提升了LVLMs的安全性能：在Qwen2.5-VL-7B上，整体安全响应率从42.8%提高到87.7%；在安全感知基准和越狱攻击评估中均有显著改进；在MMMU和MMStar等通用基准上保持稳定性能。

Conclusion: TRR框架通过引入反思阶段，有效利用单次推理中被忽略的有害内容信号，实现了真正的自我修正，显著增强了LVLMs的安全对齐能力，同时保持了通用能力。

Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

</details>


### [98] [CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics](https://arxiv.org/abs/2512.07155)
*Dahyeon Kye,Jeahun Sung,MinKyu Jeon,Jihyong Oh*

Main category: cs.CV

TL;DR: CHIMERA是一个零样本扩散模型框架，通过自适应缓存注入和语义锚点提示实现平滑的图像变形，解决了现有方法在结构和语义对齐方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像变形方面存在挑战，常常产生突兀的过渡或过饱和的外观，主要原因是缺乏自适应的结构和语义对齐机制。

Method: 提出CHIMERA框架，将变形定义为缓存反转引导的去噪过程。采用自适应缓存注入（ACI）在DDIM反转期间缓存输入特征并在去噪时自适应重新注入，以及语义锚点提示（SAP）利用视觉语言模型生成共享锚点提示作为语义桥梁。

Result: 实验和用户研究表明，CHIMERA比现有方法实现了更平滑和语义对齐的过渡，在图像变形领域建立了新的最先进水平。同时提出了全局-局部一致性评分（GLCS）作为变形导向的评估指标。

Conclusion: CHIMERA通过创新的缓存注入和语义锚点机制，成功解决了扩散模型在图像变形中的对齐问题，实现了更自然平滑的过渡效果。

Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.

</details>


### [99] [TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration](https://arxiv.org/abs/2512.07171)
*Shravan Venkatraman,Rakesh Raj Madavan,Pavan Kumar S,Muthu Subash Kavitha*

Main category: cs.CV

TL;DR: TIDE是一个两阶段水下图像恢复框架，通过专门先验分解显式建模退化特征，针对性地处理空间变化的水下退化问题。


<details>
  <summary>Details</summary>
Motivation: 水下图像恢复对海洋应用至关重要，但现有方法通常对整个图像应用统一的恢复策略，难以处理空间变化且同时发生的多种退化问题。

Method: TIDE将水下退化分解为四个关键因素（颜色失真、雾霾、细节丢失和噪声），为每个因素设计专门的恢复专家，生成专门的恢复假设，然后基于局部退化模式自适应融合，最后通过渐进细化阶段纠正残留伪影。

Result: 在标准基准和挑战性浑浊水域条件下的广泛实验表明，TIDE在基于参考的保真度指标上具有竞争力，同时在非参考感知质量指标上优于最先进方法，在颜色校正和对比度增强方面有显著改进。

Conclusion: TIDE通过显式建模退化特征和专门先验分解，有效解决了水下图像恢复中空间变化的复杂退化问题，实现了更自然的恢复效果。

Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.

</details>


### [100] [START: Spatial and Textual Learning for Chart Understanding](https://arxiv.org/abs/2512.07186)
*Zhuoming Liu,Xiaofeng Gao,Feiyang Niu,Qiaozi Gao,Liu Liu,Robinson Piramuthu*

Main category: cs.CV

TL;DR: START是一个用于图表理解的多模态大语言模型，通过空间和文本学习增强对图表视觉布局和底层数据表示的联合理解，在多个基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 图表理解对MLLM在实际场景中的应用至关重要。与自然图像不同，图表同时包含结构化视觉布局（空间属性）和底层数据表示（文本属性），需要同时理解两者才能进行精确的细粒度图表推理。

Method: 提出START方法，包含两个核心组件：(1) 图表元素定位增强空间理解，(2) 图表到代码生成增强数据细节理解。通过创新的数据生成流程创建START-Dataset，先利用MLLM将真实图表图像转换为可执行图表代码，再通过LLM演化代码以确定图表元素位置。

Result: START在不同模型规模和基准测试上都带来了持续的性能提升，明显超越了先前的最先进方法。同时提出了Chart Spatial understanding Benchmark (CS-Bench)来评估模型对图表空间结构的理解能力。

Conclusion: 通过空间和文本学习的联合训练，START显著提升了多模态大语言模型在图表理解任务上的性能，填补了图表空间理解评估的空白，为实际应用场景中的图表分析提供了有效解决方案。

Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.

</details>


### [101] [SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting](https://arxiv.org/abs/2512.07197)
*Seokhyun Youn,Soohyun Lee,Geonho Kim,Weeyoung Kwon,Sung-Ho Bae,Jihyong Oh*

Main category: cs.CV

TL;DR: 该论文是关于高效高斯泼溅技术的综述，系统性地回顾了3D和4D高斯泼溅的压缩方法，旨在解决存储和计算需求过大的问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然能实现实时高保真3D重建和新视角合成，但其存储和渲染数百万高斯分布所需的内存和计算需求巨大，在4D动态场景中问题更加严重，限制了实际应用。

Method: 将现有高效3D和4D高斯泼溅技术系统性地分为两大方向：参数压缩和结构压缩，并全面总结每个类别中的核心思想和方法趋势。

Result: 提供了首个统一的高效3D和4D高斯泼溅技术概述，涵盖了广泛使用的数据集、评估指标和代表性基准比较。

Conclusion: 讨论了当前局限性，并概述了面向静态和动态3D场景表示的可扩展、紧凑、实时高斯泼溅的有前景研究方向。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.

</details>


### [102] [Generating Storytelling Images with Rich Chains-of-Reasoning](https://arxiv.org/abs/2512.07198)
*Xiujie Song,Qi Jia,Shota Watanabe,Xiaoyi Pang,Ruijie Chen,Mengyue Wu,Kenny Q. Zhu*

Main category: cs.CV

TL;DR: 该论文提出StorytellingPainter两阶段管道，结合LLMs的推理能力和T2I模型的视觉合成能力，生成具有丰富语义连接的故事性图像，并开发了专门的评估框架。


<details>
  <summary>Details</summary>
Motivation: 故事性图像通过丰富的视觉线索传达逻辑连接的故事，具有广泛的应用价值，但由于其复杂的语义性质，这类图像难以创建且相对稀缺。需要探索如何利用生成式AI模型来创建这类图像。

Method: 提出StorytellingPainter两阶段管道：1）利用大型语言模型（LLMs）进行创造性推理生成故事；2）使用文本到图像（T2I）模型进行视觉合成。同时开发了包含语义复杂性评估器、KNN多样性评估器和故事-图像对齐评估器的评估框架。针对开源和专有LLMs之间的性能差距，进一步探索定制化训练策略，开发了Mini-Storytellers系列轻量级模型。

Result: 实验结果表明，所提方法的可行性和有效性得到了验证。通过结合LLMs的推理能力和T2I模型的视觉合成能力，成功生成了具有丰富语义连接的故事性图像。

Conclusion: 该研究成功解决了故事性图像生成的挑战，通过创新的两阶段管道和专门的评估框架，为生成具有复杂语义连接的故事性图像提供了有效解决方案，并开发了轻量级模型来缩小开源与专有LLMs之间的性能差距。

Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.

</details>


### [103] [MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning](https://arxiv.org/abs/2512.07203)
*Xuhui Zheng,Kang An,Ziliang Wang,Yuhang Wang,Faqiang Qian,Yichao Wu*

Main category: cs.CV

TL;DR: MMRPT是一个基于掩码多模态强化预训练框架，通过强化学习增强多模态大模型的视觉推理能力，解决传统图像-文本对预训练中的描述性偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统多模态预训练受限于图像-文本对的描述性偏差，导致模型更倾向于依赖表面语言线索而非真实的视觉理解。需要一种能够强化视觉推理能力的新预训练方法。

Method: MMRPT首次将强化学习直接融入大规模视觉语言模型的预训练中。通过注意力机制估计句子级别的视觉依赖度，掩码高度依赖视觉的文本片段，模型在语义-视觉奖励的指导下通过视觉推理重建这些片段。

Result: 实验显示MMRPT在多个基准测试中实现一致的零样本性能提升，在监督微调下显著提高了模型的鲁棒性，证明强化驱动的掩码推理为多模态模型提供了更可靠和可泛化的预训练目标。

Conclusion: 强化学习驱动的掩码多模态预训练能够有效增强多模态大模型的视觉推理能力，提供比传统基于描述性配对的预训练更可靠和可泛化的学习目标。

Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.

</details>


### [104] [AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT](https://arxiv.org/abs/2512.07206)
*Boyang Pan,Zeyu Zhang,Hongyu Meng,Bin Cui,Yingying Zhang,Wenli Hou,Junhao Li,Langdi Zhong,Xiaoxiao Chen,Xiaoyu Xu,Changjin Zuo,Chao Cheng,Nan-Jie Gong*

Main category: cs.CV

TL;DR: 开发了名为AutoLugano的全自动深度学习系统，能够从基线FDG-PET/CT扫描中实现淋巴瘤的端到端分类，包括病灶分割、解剖定位和自动化Lugano分期。


<details>
  <summary>Details</summary>
Motivation: 开发一个全自动、端到端的系统，将单个基线FDG-PET/CT扫描转化为完整的Lugano分期，以协助初始分期、治疗分层和临床决策支持。

Method: 系统包含三个顺序模块：1) 解剖信息病灶分割（3D nnU-Net模型）；2) 基于图谱的解剖定位（使用TotalSegmentator工具包将病灶映射到21个预定义淋巴结区域）；3) 自动化Lugano分期（将受累区域的空间分布转化为Lugano分期和治疗组）。在autoPET数据集（n=1,007）上训练，在独立队列（67名患者）上进行外部验证。

Result: 在外部验证集上，区域受累检测的总体准确率为88.31%，敏感性74.47%，特异性94.21%，F1分数80.80%。对于关键的治疗分层任务（局限期vs.晚期），准确率达到85.07%，特异性90.48%，敏感性82.61%，优于基线模型。

Conclusion: AutoLugano是首个全自动、端到端的管道系统，能够将单个基线FDG-PET/CT扫描转化为完整的Lugano分期。该系统在初始分期、治疗分层和临床决策支持方面具有强大潜力。

Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.

</details>


### [105] [VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation](https://arxiv.org/abs/2512.07215)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: 本文系统比较了基于CLIP和DINOv2的视觉基础模型在手持物体抓取场景中的3D姿态估计性能，发现两者具有互补优势：CLIP在语义理解方面表现优异，而DINOv2在几何特征提取方面更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型和视觉语言模型已经通过提供丰富的语义和几何表示革新了计算机视觉领域。本研究旨在全面比较基于CLIP和DINOv2的方法在手持物体抓取场景中的3D姿态估计性能，为机器人操作和抓取应用提供模型选择指导。

Method: 在手持物体抓取场景中对基于CLIP和DINOv2的方法进行全面的视觉比较，评估两种模型在6D物体姿态估计任务上的表现。通过基准数据集上的大量实验，分析两种模型在语义一致性和几何精度方面的差异。

Result: 实验结果表明，基于CLIP的方法在语义一致性方面表现更好，而基于DINOv2的方法在几何精度方面具有竞争优势。两种模型展现出互补优势：CLIP通过语言基础实现优异的语义理解，DINOv2提供更优的密集几何特征。

Conclusion: 本研究为机器人操作和抓取应用中选择合适的视觉模型提供了重要见解。CLIP和DINOv2在语义理解和几何特征提取方面各有优势，应根据具体应用需求选择合适的模型或结合两者的优势。

Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.

</details>


### [106] [ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery](https://arxiv.org/abs/2512.07229)
*Fang Zhou,Zhiqiang Chen,Martin Pavlovski,Yizhong Zhang*

Main category: cs.CV

TL;DR: ReLKD是一个用于广义类别发现（GCD）的端到端框架，通过利用隐式的类间关系来增强新类别的分类性能，特别是在标注数据有限的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 广义类别发现（GCD）面临对包含已知类别和新类别的未标注数据进行分类的挑战，而现有方法通常独立处理每个类别，忽略了固有的类间关系。在现实场景中直接获取这些类间关系具有显著挑战性。

Method: ReLKD包含三个关键模块：1）目标粒度模块用于学习判别性表示；2）粗粒度模块用于捕获层次化的类间关系；3）蒸馏模块用于将粗粒度模块的知识转移到目标粒度模块，以优化表示学习。

Result: 在四个数据集上的广泛实验证明了ReLKD的有效性，特别是在标注数据有限的场景下表现突出。

Conclusion: ReLKD通过有效利用隐式类间关系并将其知识转移到目标粒度表示学习中，显著提升了广义类别发现中新类别的分类性能。

Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.

</details>


### [107] [STRinGS: Selective Text Refinement in Gaussian Splatting](https://arxiv.org/abs/2512.07230)
*Abhinav Raundhal,Gaurav Behera,P J Narayanan,Ravi Kiran Sarvadevabhatla,Makarand Tapaswi*

Main category: cs.CV

TL;DR: STRinGS是一个针对3D高斯泼溅(3DGS)的文本感知选择性优化框架，通过分离处理文本和非文本区域，显著提升3D重建中文本的可读性


<details>
  <summary>Details</summary>
Motivation: 现实场景中的文本（如标志、标签、指示牌）包含重要上下文信息，但现有3D表示方法（如3DGS）难以保留细粒度文本细节，小的重建错误会导致显著的语义损失

Method: 提出STRinGS框架，将文本和非文本区域分开处理：先优化文本区域，再与优化后的非文本区域合并进行全场景优化，实现文本感知的选择性细化

Result: STRinGS在仅7K次迭代下，相比3DGS在文本区域实现了63.6%的相对改进；引入了OCR字符错误率(CER)作为文本可读性评估指标；创建了STRinGS-360数据集

Conclusion: STRinGS方法和数据集共同推动了文本丰富环境中3D场景理解的发展，为更鲁棒的文本感知重建方法铺平了道路

Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.

</details>


### [108] [Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models](https://arxiv.org/abs/2512.07234)
*Biao Chen,Lin Zuo,Mengmeng Jing,Kunbin He,Yuchen Wang*

Main category: cs.CV

TL;DR: 提出Dropout Prompt Learning方法，通过评估文本和视觉分支中token的重要性，为每个token分配灵活的dropout概率，并结合残差熵正则化来提升视觉语言模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Dropout作为广泛使用的正则化技术能提升模型泛化能力，但传统的dropout方法在视觉语言模型中应用有限。作者希望将dropout应用于提升视觉语言模型的鲁棒性，特别是在低样本学习、长尾分类和分布外泛化等挑战性场景中。

Method: 提出Dropout Prompt Learning方法：1）在文本和视觉分支的token上应用dropout；2）考虑模态内上下文和模态间对齐来评估token重要性，为每个token分配灵活的dropout概率；3）提出残差熵正则化，在保持语义对齐的同时鼓励dropout引入的多样化表示。

Result: 在15个基准测试上验证了方法的有效性，在低样本学习、长尾分类和分布外泛化等挑战性场景中表现优异。在基础到新类别的泛化任务中，超越了包括KgCoOp（提升5.10%）和PromptSRC（提升2.13%）在内的正则化方法。

Conclusion: Dropout Prompt Learning通过智能的token级dropout和残差熵正则化，有效提升了视觉语言模型的鲁棒性和泛化能力，在多种挑战性场景中取得了显著性能提升。

Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

</details>


### [109] [Unified Camera Positional Encoding for Controlled Video Generation](https://arxiv.org/abs/2512.07237)
*Cheng Zhang,Boying Li,Meng Wei,Yan-Pei Cao,Camilo Cruz Gambardella,Dinh Phung,Jianfei Cai*

Main category: cs.CV

TL;DR: 本文提出UCPE（统一相机位置编码），一种几何一致的相机表示方法，通过相对光线编码和绝对方向编码实现精确的相机控制，在视频生成任务中仅需增加不到1%的可训练参数就能达到最先进的相机可控性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有相机编码方法通常基于简化的针孔模型假设，限制了在真实世界多样化相机内参和镜头畸变情况下的泛化能力。Transformer在3D感知、视频生成等任务中需要理解相机几何以将视觉观察与三维空间关联。

Method: 提出相对光线编码（Relative Ray Encoding）统一表示完整的相机信息（6自由度位姿、内参和镜头畸变），并识别俯仰和滚转为绝对方向编码的有效组件。UCPE通过轻量级空间注意力适配器集成到预训练的视频扩散Transformer中。

Result: 构建了涵盖广泛相机运动和镜头类型的大规模视频数据集。实验验证UCPE在相机可控视频生成中的有效性，仅增加不到1%的可训练参数就实现了最先进的相机可控性和视觉保真度。

Conclusion: UCPE作为一种通用的相机表示方法，在Transformer中具有广泛应用潜力，可用于未来的多视图、视频和3D任务。该方法为相机几何理解提供了统一的解决方案。

Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.

</details>


### [110] [Zero-Shot Textual Explanations via Translating Decision-Critical Features](https://arxiv.org/abs/2512.07245)
*Toshinori Yamauchi,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: TEXTER是一种新的零样本图像分类器解释方法，通过识别决策关键特征并将其映射到CLIP特征空间来生成更忠实、可解释的文本解释。


<details>
  <summary>Details</summary>
Motivation: 现有零样本解释方法通常将全局图像特征与语言对齐，只能描述可见内容而非驱动预测的关键因素。大型视觉语言模型虽然能生成字幕，但并非专门为分类器特定推理设计。

Method: TEXTER首先识别对预测贡献最大的神经元，强调这些神经元编码的决策关键特征，然后将这些强调的特征映射到CLIP特征空间以检索反映模型推理的文本解释。稀疏自编码器进一步提高了Transformer架构的可解释性。

Result: 大量实验表明，TEXTER生成的解释比现有方法更忠实、更可解释。

Conclusion: TEXTER通过隔离决策关键特征再进行对齐的方法，成功克服了现有零样本解释方法的局限性，能够生成更准确反映图像分类器推理过程的文本解释。

Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.

</details>


### [111] [AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing](https://arxiv.org/abs/2512.07247)
*Ziming Hong,Tianyu Huang,Runnan Chen,Shanshan Ye,Mingming Gong,Bo Han,Tongliang Liu*

Main category: cs.CV

TL;DR: AdLift是首个针对3D高斯泼溅（3DGS）的编辑保护方法，通过将严格有界的2D对抗扰动提升到3D高斯表示中，防止任意视角和维度的指令驱动编辑。


<details>
  <summary>Details</summary>
Motivation: 虽然基于扩散模型的指令驱动2D图像编辑技术已扩展到3DGS，极大地促进了3D内容创作，但也使这些资产面临未经授权编辑和恶意篡改的严重风险。现有的2D图像对抗扰动保护方法难以直接应用于3DGS，面临视角泛化保护和平衡不可见性与保护能力两大挑战。

Method: 提出AdLift方法，通过提升严格有界的2D对抗扰动到3D高斯表示的保护机制。采用定制的Lifted PGD进行渐进式优化：首先在渲染图像上对编辑模型进行梯度截断，应用投影梯度严格约束图像级扰动；然后通过图像到高斯的拟合操作将扰动反向传播到保护高斯参数；交替进行梯度截断和图像-高斯拟合，实现跨不同视角的一致保护性能。

Result: 实验结果表明，AdLift能有效抵御最先进的指令驱动2D图像和3DGS编辑方法，在保护能力和不可见性之间取得了良好平衡。

Conclusion: AdLift是首个针对3DGS的编辑保护框架，成功解决了视角泛化保护和平衡不可见性与保护能力的挑战，为3DGS资产提供了有效的安全保障。

Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.

</details>


### [112] [DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement](https://arxiv.org/abs/2512.07253)
*Handing Xu,Zhenguo Nie,Tairan Peng,Huimin Pan,Xin-Jun Liu*

Main category: cs.CV

TL;DR: 提出了一种基于退化感知的实时内窥镜视频增强框架，通过对比学习提取退化表示并在帧间传播，实现高质量实时增强


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术依赖术中视频，但视频常受不均匀照明、组织散射、遮挡和运动模糊等退化因素影响，现有深度学习方法计算量大难以实时应用

Method: 提出退化感知框架：1) 使用对比学习从图像中提取退化表示；2) 引入融合机制，用退化表示调制图像特征指导单帧增强模型；3) 在退化与恢复图像间采用循环一致性约束训练以提高鲁棒性和泛化性

Result: 实验表明该框架在性能与效率之间取得了优越平衡，相比多种先进方法表现更佳

Conclusion: 退化感知建模对实时内窥镜视频增强有效，隐式学习和传播退化表示为临床应用提供了实用途径

Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.

</details>


### [113] [A graph generation pipeline for critical infrastructures based on heuristics, images and depth data](https://arxiv.org/abs/2512.07269)
*Mike Diessner,Yannick Tarant*

Main category: cs.CV

TL;DR: 提出基于摄影测量学的图生成流水线，使用立体相机获取RGB图像和深度数据，通过深度学习进行物体检测和实例分割，结合启发式规则推断关系，为关键基础设施创建虚拟表示。


<details>
  <summary>Details</summary>
Motivation: 传统基于激光扫描的3D点云方法成本高且需要专业知识，需要更经济高效的方法为关键基础设施创建虚拟表示以进行仿真和数字孪生。

Method: 基于摄影测量学的图生成流水线：使用立体相机获取RGB图像和深度数据；采用深度学习进行物体检测和实例分割；结合用户定义的启发式规则或规则来推断物体间关系。

Result: 在两个液压系统上的实验结果表明，该方法能够生成接近真实情况的图，同时具有灵活性可针对特定应用定制，透明度高适用于关键基础设施的高风险决策。

Conclusion: 提出的基于摄影测量学的图生成方法比传统激光扫描更经济高效，能够为关键基础设施创建准确的虚拟表示，具有实际应用价值。

Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.

</details>


### [114] [RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2512.07273)
*Zhi Rao,Yucheng Zhou,Benjia Zhou,Yiqing Huang,Sergio Escalera,Jun Wan*

Main category: cs.CV

TL;DR: 提出RVLF框架解决无注释手语翻译的两大挑战：通过融合骨架运动与视觉特征改进手语表示，结合GRPO强化学习优化句子级语义对齐，在多个数据集上显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前无注释手语翻译面临两个关键问题：1) 手语表示不足，难以捕捉细微视觉线索；2) 基于LLM的方法存在句子级语义错位，限制了翻译质量。

Method: 提出三阶段强化视觉语言框架RVLF：1) 构建专门的手语大视觉语言模型，融合骨架运动线索和DINOv2提取的视觉特征；2) 通过指令调优获得SLT-SFT基线模型；3) 引入GRPO优化策略，结合BLEU和ROUGE奖励函数微调模型，获得SLT-GRPO优化模型。

Result: 在CSL-Daily、PHOENIX-2014T、How2Sign和OpenASL数据集上，BLEU-4分数分别提升+5.1、+1.11、+1.4和+1.61，无需外部大规模手语数据集预训练，显著改善翻译质量和语义一致性。

Conclusion: RVLF框架有效解决了无注释手语翻译的表示不足和语义错位问题，首次将GRPO引入手语翻译领域，通过强化学习优化显著提升了翻译性能。

Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.

</details>


### [115] [Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery](https://arxiv.org/abs/2512.07276)
*Mai Tsujimoto,Junjue Wang,Weihao Xuan,Naoto Yokoya*

Main category: cs.CV

TL;DR: Geo3DVQA是一个用于评估视觉语言模型在仅使用RGB遥感图像进行3D地理空间推理能力的基准测试，包含11万个问题-答案对，涵盖16个任务类别和三个复杂度级别。现有VLMs在该任务上表现不佳，而领域特定微调能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D地理空间分析方法依赖昂贵的专业传感器（如LiDAR和多光谱传感器），限制了全球可访问性。现有方法难以整合多个3D线索、处理多样化查询并提供可解释的推理。需要开发仅使用RGB图像的3D地理空间推理方法。

Method: 创建Geo3DVQA基准测试，包含110k个精心策划的问题-答案对，涵盖16个任务类别和三个复杂度级别：单特征推理、多特征推理和应用级空间分析。使用RGB遥感图像，强调整合高程、天空视角因子和土地覆盖模式的现实场景。

Result: 评估10个最先进的VLMs显示RGB到3D推理的困难性：GPT-4o和Gemini-2.5-Flash分别仅达到28.6%和33.0%的准确率。领域特定微调的Qwen2.5-VL-7B达到49.6%准确率（提升24.8个百分点）。

Conclusion: Geo3DVQA揭示了当前VLMs在3D地理空间推理方面的局限性，同时展示了领域适应的有效性。该基准为可扩展、可访问和全面的3D地理空间分析引入了新的挑战前沿。

Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.

</details>


### [116] [ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation](https://arxiv.org/abs/2512.07328)
*Ziyang Mai,Yu-Wing Tai*

Main category: cs.CV

TL;DR: ContextAnyone是一个上下文感知的扩散框架，通过单张参考图像实现角色一致性的文本到视频生成，解决了现有方法在保持发型、服装、体型等上下文线索方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频生成方法在保持角色身份一致性方面存在局限，通常只关注面部特征而忽略了发型、服装、体型等重要的上下文线索，这影响了视频的视觉连贯性。

Method: 提出一个上下文感知的扩散框架，联合重建参考图像并生成新视频帧；使用Emphasize-Attention模块选择性强化参考感知特征；采用双引导损失结合扩散和参考重建目标；提出Gap-RoPE位置嵌入分离参考和视频token以稳定时序建模。

Result: 实验表明ContextAnyone在身份一致性和视觉质量方面优于现有的参考到视频方法，能够在不同动作和场景中生成连贯且保持上下文的角色视频。

Conclusion: ContextAnyone通过上下文感知的方法有效解决了角色一致性视频生成的问题，实现了从文本和单张参考图像生成高质量、角色一致性的视频内容。

Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.

</details>


### [117] [Generalized Referring Expression Segmentation on Aerial Photos](https://arxiv.org/abs/2512.07338)
*Luís Marnoto,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

TL;DR: Aerial-D是一个用于航空图像的大规模指代表达分割数据集，包含37,288张图像和1,522,523个指代表达，覆盖259,709个标注目标，涵盖21个类别。该数据集通过自动流水线构建，结合规则生成和LLM增强，并模拟历史成像条件。使用RSRefSeg架构训练模型，在当代基准测试中表现优异，同时能在历史航空照片的退化条件下保持准确性。


<details>
  <summary>Details</summary>
Motivation: 航空图像（如无人机航拍、历史航拍档案、高分辨率卫星图像等）的指代表达分割面临独特挑战：空间分辨率差异大、色彩使用不一致、目标可能只有几个像素、场景物体密度高且存在部分遮挡。需要专门的数据集来支持这一领域的研究。

Method: 1. 创建Aerial-D数据集：包含37,288张图像，1,522,523个指代表达，259,709个标注目标，覆盖21个类别。2. 采用全自动流水线构建：结合系统规则生成和大型语言模型（LLM）增强程序，丰富语言多样性和视觉细节关注。3. 使用过滤器模拟历史成像条件。4. 采用RSRefSeg架构，在Aerial-D和先前航空数据集上训练模型，实现文本驱动的统一实例和语义分割。

Result: 1. 联合训练在当代基准测试中取得竞争性性能。2. 在单色、棕褐色和颗粒状退化的档案航空照片条件下保持强准确性。3. 数据集、训练模型和完整软件管道已公开可用。

Conclusion: Aerial-D数据集为航空图像的指代表达分割提供了大规模资源，通过自动流水线构建和LLM增强提高了语言多样性。训练模型不仅在当代图像上表现良好，还能有效处理历史航空照片的退化条件，为现代和历史航空图像的文本驱动分割提供了统一解决方案。

Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .

</details>


### [118] [Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting](https://arxiv.org/abs/2512.07345)
*Shilong Jin,Haoran Duan,Litao Hua,Wentao Huang,Yuan Zhou*

Main category: cs.CV

TL;DR: TD-Attn是一个解决T2I扩散模型中先验视角偏差问题的框架，通过3D感知注意力引导和分层注意力调制来提升多视角一致性，可作为通用插件增强3D任务效果。


<details>
  <summary>Details</summary>
Motivation: 从文本到图像扩散模型蒸馏的3D任务（如生成或编辑）无需大量3D训练数据，但T2I模型存在先验视角偏差问题，导致不同视角间外观冲突，影响多视角一致性。

Method: 提出TD-Attn框架：1) 3D感知注意力引导模块构建视角一致的3D注意力高斯分布，补偿2D注意力图空间信息不足；2) 分层注意力调制模块使用语义引导树定位和调制对视角条件敏感的交叉注意力层，支持构建更一致的3D注意力高斯。

Result: 大量实验证明TD-Attn可作为通用插件，显著提升3D任务中的多视角一致性，同时支持可控和精确的3D编辑。

Conclusion: 通过数学分析揭示了T2I模型先验视角偏差的根源，提出的TD-Attn框架有效解决了多视角不一致问题，为3D任务提供了通用增强方案。

Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

</details>


### [119] [MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition](https://arxiv.org/abs/2512.07348)
*Xinyu Wei,Kangrui Cen,Hongyang Wei,Zhen Guo,Bairui Li,Zeqing Wang,Jinrui Zhang,Lei Zhang*

Main category: cs.CV

TL;DR: 该论文针对多图像组合（MICo）任务，构建了包含15万张图像的大规模数据集MICo-150K，开发了MICo-Bench评估基准，并提出了专门的评估指标Weighted-Ref-VIEScore。通过微调模型验证了数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 多图像组合任务面临高质量训练数据缺乏的挑战，现有方法难以生成一致且连贯的多参考图像合成结果，需要系统性的研究和资源支持。

Method: 将MICo任务分为7类代表性任务，利用强大模型合成大量平衡的合成图像，通过人工筛选和精炼构建MICo-150K数据集。还构建了分解与重组子集，将真实图像分解后重组。创建MICo-Bench评估基准和专门的评估指标。

Result: MICo-150K数据集有效提升了模型的MICo能力，基线模型Qwen-MICo在3图像组合任务上达到Qwen-Image-2509水平，且支持任意多图像输入。数据集、基准和基线模型为多图像组合研究提供了宝贵资源。

Conclusion: 该研究通过构建大规模数据集、评估基准和专用指标，系统性地解决了多图像组合任务的数据缺乏问题，为相关研究提供了重要基础资源，并验证了数据集对模型性能的有效提升。

Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.

</details>


### [120] [DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection](https://arxiv.org/abs/2512.07351)
*Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Sami Azam*

Main category: cs.CV

TL;DR: DeepAgent：一个多智能体协作框架，通过视觉和音频模态的互补分析来检测深度伪造视频，使用随机森林元分类器融合决策，在多个基准数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 合成媒体特别是深度伪造的日益普及给数字内容验证带来了挑战。现有方法通常将音频和视觉信息集成在单一模型中，容易受到模态不匹配、噪声和操纵的影响，需要更鲁棒的检测方法。

Method: 提出DeepAgent多智能体协作框架：Agent-1使用简化的AlexNet-based CNN检测深度伪造操作痕迹；Agent-2通过结合声学特征、Whisper音频转录和EasyOCR图像帧读取来检测视听不一致性；两个智能体的决策通过随机森林元分类器融合。

Result: 在Celeb-DF和FakeAVCeleb数据集上，Agent-1达到94.35%的测试准确率；在FakeAVCeleb数据集上，Agent-2达到93.69%准确率，元分类器达到81.56%准确率；在DeepFakeTIMIT上的跨数据集验证中，元分类器达到97.49%准确率。

Conclusion: 基于层次结构的融合通过减轻单个模态的弱点增强了鲁棒性，多智能体方法能有效处理深度伪造中的多种操纵类型，证明了多模态协作在深度伪造检测中的有效性。

Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.

</details>


### [121] [Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2512.07360)
*Qiming Huang,Hao Ai,Jianbo Jiao*

Main category: cs.CV

TL;DR: 提出一种结构感知的特征校正方法，通过构建区域邻接图来利用图像的低级特征，改善CLIP在细粒度视觉区域与文本关联时的性能，从而提升开放词汇语义分割的质量。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在大规模图像-文本对上进行预训练，主要关注全局语义对齐，但在将细粒度视觉区域与文本关联时表现不佳，导致局部区域预测噪声大且不一致。这是由于对比训练范式产生的分散偏差，仅使用CLIP特征难以缓解这一问题。

Method: 提出结构感知的特征校正方法：1）基于低级特征（如颜色和纹理）构建区域邻接图（RAG）来捕捉局部结构关系；2）利用该图来细化CLIP特征，增强局部区分能力；3）将实例特定的先验知识直接整合到特征校正过程中。

Result: 在多个开放词汇分割基准测试中，该方法有效抑制了分割噪声，提高了区域级一致性，并取得了强大的性能表现。

Conclusion: 通过结合从图像直接提取的实例特定先验知识，提出的结构感知特征校正方法能够有效改善CLIP在细粒度视觉-文本关联中的局限性，为开放词汇语义分割提供了更准确和一致的解决方案。

Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

</details>


### [122] [Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects](https://arxiv.org/abs/2512.07381)
*Shuohan Tao,Boyao Zhou,Hanzhang Tu,Yuwang Wang,Yebin Liu*

Main category: cs.CV

TL;DR: Tessellation GS：一种基于网格面的结构化2D高斯泼溅方法，用于从单相机重建动态场景，通过自适应面细分和重建基础模型先验，显著提升稀疏视图和动态场景的重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅（GS）在视角外推方面存在困难，由于其各向异性特性导致过拟合和泛化能力差，特别是在稀疏视图和动态场景重建中表现不佳。需要一种能够从单相机（连续移动或静态）重建动态场景的鲁棒方法。

Method: 提出Tessellation GS方法：1）将2D高斯约束在网格面的局部区域；2）通过网格面上的分层神经特征推断高斯属性；3）采用由细节感知损失函数驱动的自适应面细分策略指导高斯细分；4）利用重建基础模型的先验初始化高斯变形，实现从单静态相机重建一般动态物体的鲁棒重建。

Result: 该方法在表观和网格重建任务上优于先前SOTA方法：LPIPS降低29.1%，Chamfer距离降低49.2%。能够从单相机（连续移动或静态）成功重建动态场景，解决了优化方法先前难以处理的挑战。

Conclusion: Tessellation GS通过结构化2D高斯泼溅方法，结合网格面约束、自适应细分和基础模型先验，有效解决了传统3D GS在动态场景重建中的局限性，显著提升了重建质量和鲁棒性。

Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.

</details>


### [123] [LogicCBMs: Logic-Enhanced Concept-Based Learning](https://arxiv.org/abs/2512.07383)
*Deepika SN Vemuri,Gautham Bellamkonda,Aditya Pola,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 论文提出LogicCBM，在概念瓶颈模型基础上引入可微逻辑模块，通过命题逻辑连接概念，超越简单的线性组合，提高模型表达能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 概念瓶颈模型目前主要通过线性组合概念进行预测，但线性组合存在固有局限性。需要增强概念学习模型，使其能够超越简单的加权组合，利用逻辑操作来捕捉概念间关系并提高模型表达能力。

Method: 提出LogicCBM，引入精心设计的逻辑模块，通过可微逻辑操作连接从概念瓶颈模型学习到的概念。该模块允许使用各种逻辑操作生成最终预测，同时保持端到端可学习性。

Result: 在知名基准测试和合成数据集上的实证研究表明，这些模型具有更好的准确性，能够执行有效的干预，并且具有高度可解释性。

Conclusion: 通过命题逻辑增强概念瓶颈模型能够超越线性组合的局限性，提高模型表达能力，同时保持可解释性优势，在准确性和干预能力方面都有显著提升。

Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.

</details>


### [124] [How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline](https://arxiv.org/abs/2512.07385)
*Chunhui Zhang,Li Liu,Zhipeng Zhang,Yong Wang,Hao Wen,Xi Zhou,Shiming Ge,Yanfeng Wang*

Main category: cs.CV

TL;DR: 提出新的无人机反无人机视觉跟踪任务UAV-Anti-UAV，构建百万级数据集，并开发基于Mamba的基线方法MambaSTS，实现空间-时间-语义一体化学习。


<details>
  <summary>Details</summary>
Motivation: 当前反无人机研究主要关注固定地面摄像头捕获的RGB、红外或RGB-IR视频，缺乏从移动无人机平台跟踪目标无人机的关注。无人机反无人机任务面临双重动态干扰的挑战，需要新的解决方案。

Method: 提出MambaSTS方法，使用Mamba和Transformer分别学习全局语义和空间特征，利用状态空间模型的长序列建模能力，通过时间令牌传播机制建立视频级长期上下文。

Result: 构建了包含1,810个视频的百万级数据集，每个视频都有人工标注的边界框、语言提示和15个跟踪属性。对50种现代深度跟踪算法的实验评估表明，该领域仍有很大改进空间。

Conclusion: UAV-Anti-UAV是一个具有挑战性的新任务，提出的数据集和MambaSTS方法为该领域研究提供了基础，实验表明现有方法在该任务上仍有显著提升空间。

Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.

</details>


### [125] [Reconstructing Objects along Hand Interaction Timelines in Egocentric Video](https://arxiv.org/abs/2512.07394)
*Zhifan Zhu,Siddhant Bansal,Shashank Tripathi,Dima Damen*

Main category: cs.CV

TL;DR: 提出ROHIT任务，通过手交互时间线(HIT)重建物体姿态，利用约束优化传播(COP)框架提升重建效果


<details>
  <summary>Details</summary>
Motivation: 在视频中重建手交互物体的3D姿态面临挑战，特别是在缺乏3D真值的情况下。需要一种能够利用手与物体交互时序约束的方法来提升重建质量。

Method: 定义手交互时间线(HIT)，将物体状态分为：静态、接触、稳定抓握、释放、再静态。提出约束优化传播(COP)框架，在HIT上传播物体姿态约束，特别关注稳定抓握阶段。

Result: 在HOT3D数据集(1.2K稳定抓握片段)和EPIC-Kitchens数据集(2.4K片段，390个物体实例)上评估。COP将稳定抓握重建提升6.2-11.3%，HIT重建提升达24.5%。

Conclusion: ROHIT任务和COP框架能够有效利用手交互时序约束提升物体重建质量，特别是在缺乏3D真值的情况下，为日常交互视频中的物体重建提供了新方法。

Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.

</details>


### [126] [InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs](https://arxiv.org/abs/2512.07410)
*Bin Li,Ruichi Zhang,Han Liang,Jingyan Zhang,Juze Zhang,Xin Chen,Lan Xu,Jingyi Yu,Jingya Wang*

Main category: cs.CV

TL;DR: InterAgent：首个端到端的文本驱动物理多智能体人形控制框架，通过自回归扩散变换器和交互图外感受表示实现多智能体协调


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于单智能体场景，忽视了多智能体交互所需的物理合理相互作用，需要开发能够模拟人类社交行为中复杂协调的多智能体控制框架

Method: 1. 自回归扩散变换器配备多流块，解耦本体感受、外感受和动作以减轻跨模态干扰；2. 新颖的交互图外感受表示，显式捕捉细粒度关节到关节的空间依赖关系；3. 稀疏边基注意力机制，动态修剪冗余连接并强调关键的智能体间空间关系

Result: InterAgent在广泛实验中始终优于多个强基线，实现了最先进的性能，能够仅从文本提示生成连贯、物理合理且语义忠实的多智能体行为

Conclusion: InterAgent是首个文本驱动的物理多智能体人形控制框架，通过创新的架构设计成功实现了多智能体协调，为未来研究提供了重要基础

Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.

</details>


### [127] [Data-driven Exploration of Mobility Interaction Patterns](https://arxiv.org/abs/2512.07415)
*Gabriele Galatolo,Mirco Nanni*

Main category: cs.CV

TL;DR: 该论文提出了一种基于数据挖掘的方法，从实际移动数据中发现个体间的相互影响模式，而不是从预设的行为模型出发。


<details>
  <summary>Details</summary>
Motivation: 理解个体移动行为及其对外部世界的反应对于人群模拟和应急管理等应用至关重要。现有方法通常从预设的行为模型出发，但缺乏直接从数据中发现个体间相互影响模式的方法。

Method: 采用数据挖掘视角，从移动数据中搜索可能反映个体间相互作用的移动事件，并在此基础上寻找复杂、持久的事件模式和随时间演化的配置模式。

Result: 在汽车和行人两个真实案例研究中实例化了该方法，进行了全面的实验评估，包括性能、参数敏感性和样本结果解释。

Conclusion: 通过研究这些模式可以为个体间移动相互作用的机制提供新见解，有助于改进现有的模拟模型。

Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.

</details>


### [128] [When normalization hallucinates: unseen risks in AI-powered whole slide image processing](https://arxiv.org/abs/2512.07426)
*Karel Moens,Matthew B. Blaschko,Tinne Tuytelaars,Bart Diricx,Jonas De Vylder,Mustafa Yousif*

Main category: cs.CV

TL;DR: WSI归一化模型存在幻觉风险，在临床数据上表现不佳，作者提出新检测方法评估现有方法，揭示传统指标无法发现的严重问题


<details>
  <summary>Details</summary>
Motivation: 当前WSI归一化方法存在两个主要问题：1）输出趋向平均值可能掩盖诊断重要特征；2）更严重的是会产生幻觉内容（看似真实但实际不存在的伪影），这对下游分析构成严重威胁，且当前评估方法往往忽视这些问题

Method: 提出一种新颖的图像比较度量方法，专门用于自动检测归一化输出中的幻觉内容。使用该方法系统地评估了在真实临床数据上重新训练的多个知名归一化方法

Result: 发现当这些模型在真实临床数据上重新训练和评估时，幻觉出现频率令人担忧。传统指标无法捕捉到的显著不一致性和失败情况被揭示出来

Conclusion: 幻觉风险真实存在且被低估，需要更鲁棒、可解释的归一化技术和更严格的临床部署验证协议

Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.

</details>


### [129] [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469)
*Xiangpeng Yang,Ji Xie,Yiyuan Yang,Yan Huang,Min Xu,Qiang Wu*

Main category: cs.CV

TL;DR: VideoCoF提出了一种基于帧链推理的视频编辑方法，通过预测编辑区域潜在表示作为显式推理步骤，实现无需用户提供掩码的精确指令到区域对齐和细粒度视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法面临关键权衡：专家模型提供精确性但依赖任务特定的先验（如掩码），阻碍统一化；而统一的时序上下文学习模型无需掩码但缺乏显式空间线索，导致指令到区域映射不精确和定位能力弱。

Method: 提出VideoCoF框架，受思维链推理启发，强制视频扩散模型遵循"观察、推理、编辑"流程：首先预测推理标记（编辑区域潜在表示），然后生成目标视频标记。引入RoPE对齐策略，利用推理标记确保运动对齐并支持超出训练时长的长度外推。

Result: 仅使用5万个视频对的最小数据成本，在VideoCoF-Bench上实现了最先进的性能，验证了方法的效率和有效性。

Conclusion: VideoCoF通过显式推理步骤解决了视频编辑中精度与通用性的权衡问题，无需用户提供掩码即可实现精确的指令到区域对齐和细粒度编辑，同时支持运动对齐和长度外推。

Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.

</details>


### [130] [Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance](https://arxiv.org/abs/2512.07480)
*Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Zihan Zheng,Yuan Zhang,Yan Lu*

Main category: cs.CV

TL;DR: S2VC是一种基于单步扩散的视频编解码器，通过条件编码框架和高效的单步扩散生成器，在低比特率下实现高质量视频重建，同时大幅降低采样复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统和神经视频编解码器在低比特率下的感知质量提升仍然具有挑战性。现有方法要么因生成能力有限而产生伪影，要么依赖预训练扩散模型但采样复杂度高。需要一种既能提高感知质量又能降低计算成本的方法。

Method: 提出S2VC单步扩散视频编解码器：1）结合条件编码框架与高效单步扩散生成器；2）引入上下文语义指导，从缓冲特征中提取帧自适应语义，替代文本描述；3）在扩散U-Net中加入时间一致性指导，确保帧间时间连贯性。

Result: 实验表明S2VC在感知质量上达到最先进水平，相比之前的感知方法平均节省52.73%的比特率，证明了单步扩散在高效高质量视频压缩中的潜力。

Conclusion: S2VC通过单步扩散生成器和创新的语义与时间指导机制，成功解决了低比特率视频压缩中感知质量与计算效率的平衡问题，为高效高质量视频压缩提供了有前景的解决方案。

Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.

</details>


### [131] [MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer](https://arxiv.org/abs/2512.07500)
*Penghui Liu,Jiangshan Wang,Yutong Shen,Shanhui Mo,Chenyang Qi,Yue Ma*

Main category: cs.CV

TL;DR: MultiMotion是一个用于多对象视频运动迁移的统一框架，通过Mask-aware Attention Motion Flow（AMF）和RectPC采样器解决DiT架构中的运动纠缠问题，并建立了首个多对象运动迁移基准数据集。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiT）架构在多对象视频运动迁移中面临运动纠缠和缺乏对象级控制的固有挑战，需要一种能够明确解耦和控制多个对象运动特征的方法。

Method: 提出了MultiMotion框架，核心创新是Mask-aware Attention Motion Flow（AMF），利用SAM2掩码在DiT流程中显式解耦和控制多个对象的运动特征；同时引入RectPC，一种用于高效准确采样的高阶预测器-校正器求解器。

Result: MultiMotion实现了精确、语义对齐且时间一致的多对象运动迁移，保持了DiT的高质量和可扩展性；构建了首个专门用于DiT多对象运动迁移的基准数据集。

Conclusion: MultiMotion通过AMF和RectPC有效解决了多对象视频运动迁移中的运动纠缠和控制问题，为DiT架构在多对象场景中的应用提供了统一框架和评估基准。

Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.

</details>


### [132] [SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation](https://arxiv.org/abs/2512.07503)
*Yao Teng,Zhihuan Jiang,Han Shi,Xian Liu,Xuefei Ning,Guohao Dai,Yu Wang,Zhenguo Li,Xihui Liu*

Main category: cs.CV

TL;DR: SJD++是一种无需训练的概率并行解码算法，通过多令牌预测和草稿-验证机制，将自回归文本到图像生成的推理延迟减少2-3倍，步骤压缩2-7倍，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 大型自回归模型虽然能生成高质量高分辨率图像，但推理速度缓慢，因为需要数百到数千次顺序前向传递进行下一个令牌预测。需要加速自回归文本到图像生成过程。

Method: 提出Speculative Jacobi Decoding++ (SJD++)算法，结合Jacobi解码的迭代多令牌预测机制和推测采样的概率草稿-验证机制。关键创新是在每个验证阶段后重用高置信度草稿令牌，而不是重新采样所有令牌。

Result: 在多个代表性自回归文本到图像生成模型上进行实验，SJD++实现了2-3倍的推理延迟减少和2-7倍的步骤压缩，同时保持视觉质量，没有可观察到的退化。

Conclusion: SJD++是一种有效的训练免费加速方法，显著提升自回归图像生成效率，为实际应用提供了可行的解决方案。

Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.

</details>


### [133] [ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points](https://arxiv.org/abs/2512.07504)
*Ryota Okumura,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: ControlVP：通过用户引导修正生成图像中消失点不一致性的框架，提升几何一致性


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型（如Stable Diffusion）在视觉质量上表现出色，但经常存在几何不一致问题，特别是消失点不一致性，导致平行线在2D空间中的投影无法正确收敛，破坏了场景的结构真实感，尤其在建筑场景中更为明显。

Method: ControlVP是一个用户引导的框架，通过扩展预训练的扩散模型，引入基于建筑轮廓的结构指导，并添加几何约束来明确鼓励图像边缘与透视线索的对齐。

Result: 该方法增强了全局几何一致性，同时保持了与基线相当的视觉保真度，特别适用于需要精确空间结构的应用，如图像到3D重建。

Conclusion: ControlVP有效解决了生成图像中的消失点不一致问题，提升了空间真实感，为需要精确几何结构的应用提供了有价值的工具。

Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .

</details>


### [134] [MeshRipple: Structured Autoregressive Generation of Artist-Meshes](https://arxiv.org/abs/2512.07514)
*Junkai Lin,Hang Long,Huipeng Guo,Jielei Zhang,JiaYi Yang,Tianle Guo,Yang Yang,Jianwen Li,Wenxiao Zhang,Matthias Nießner,Wei Yang*

Main category: cs.CV

TL;DR: MeshRipple提出了一种新的网格生成方法，通过边界感知的BFS标记化和扩展预测策略，解决了现有自回归方法因内存限制导致的几何依赖断裂问题，能够生成具有高表面保真度和拓扑完整性的网格。


<details>
  <summary>Details</summary>
Motivation: 现有自回归网格生成器由于内存限制，需要将面序列化为片段进行训练和推理，这种不匹配破坏了长距离几何依赖关系，导致生成网格出现孔洞和碎片化组件。

Method: MeshRipple采用三个关键创新：1）边界感知的BFS标记化，使生成顺序与表面拓扑对齐；2）扩展预测策略，保持连贯、连接的表面增长；3）稀疏注意力全局内存，提供有效无界的感受野以解决长距离拓扑依赖。

Result: MeshRipple能够生成具有高表面保真度和拓扑完整性的网格，在性能上超越了近期强大的基线方法。

Conclusion: MeshRipple通过整合边界感知生成、扩展预测和全局内存机制，成功解决了自回归网格生成中的长距离依赖问题，实现了更高质量的网格生成。

Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.

</details>


### [135] [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](https://arxiv.org/abs/2512.07564)
*Kassoum Sanogo,Renzo Ardiccioni*

Main category: cs.CV

TL;DR: 提出无需训练的自校正框架，通过不确定性引导的视觉重注意机制减少视觉语言模型的幻觉生成，在多个基准测试上显著降低幻觉率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型经常生成看似合理但实际错误的幻觉内容，需要一种无需额外训练就能减少幻觉的方法。

Method: 提出训练免费的自校正框架，结合多维不确定性量化（令牌熵、注意力分散、语义一致性、声明置信度）和注意力引导的未探索区域裁剪，通过迭代精炼响应。

Result: 在POPE和MMHAL BENCH基准测试上，使用Qwen2.5-VL-7B模型，幻觉率相比基线降低9.8个百分点，对抗性分割上的对象存在准确率提高4.7个百分点。

Conclusion: 不确定性引导的视觉重注意机制能有效基于视觉证据进行校正，为可信赖的多模态系统提供了无需训练的自校正解决方案。

Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.

</details>


### [136] [All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs](https://arxiv.org/abs/2512.07580)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Longzhen Yang,Yihang Liu,Chengmei Yang,Ying Wen,Xianfeng Tang,Hui Liu,Yuyin Zhou,Lianghua He*

Main category: cs.CV

TL;DR: 研究发现视觉大语言模型中存在"信息地平线"现象，深层视觉token信息逐渐均匀化并最终消失，提出在深层使用随机剪枝来平衡性能与效率


<details>
  <summary>Details</summary>
Motivation: 现有训练无关的token剪枝方法在深层（如20层以上）表现不佳，甚至不如随机剪枝，研究者假设这是由于"token信息消失"现象导致的

Method: 提出通过测量移除token时模型输出概率的变化来量化token信息含量，分析视觉token在不同层的信息分布，发现"信息地平线"现象，并在深层采用随机剪枝策略

Result: 1. 发现视觉token信息随层加深逐渐均匀化并在中间层消失；2. 信息地平线位置因任务而异（OCR任务更深）；3. 与模型能力相关（更强模型使用更深视觉token）。结合随机剪枝的方法在Qwen-2.5-VL-7B上剪枝50%视觉token仍保持96.9%性能

Conclusion: 视觉token在深层变得冗余，随机剪枝在深层是有效策略，结合现有方法可达到最优效果，为VLLM效率优化提供了新视角

Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.

</details>


### [137] [LongCat-Image Technical Report](https://arxiv.org/abs/2512.07584)
*Meituan LongCat Team,Hanghang Ma,Haoxian Tan,Jiale Huang,Junqiang Wu,Jun-Yan He,Lishuai Gao,Songlin Xiao,Xiaoming Wei,Xiaoqi Ma,Xunliang Cai,Yayong Guan,Jie Hu*

Main category: cs.CV

TL;DR: LongCat-Image是一个开源的英中双语图像生成基础模型，在文本渲染、真实感、部署效率和开发者可访问性方面表现优异，采用紧凑的6B参数设计，并建立了完整的开源生态系统。


<details>
  <summary>Details</summary>
Motivation: 解决当前主流模型在多语言文本渲染、真实感、部署效率和开发者可访问性方面的核心挑战，特别是中文文本渲染的行业标准问题。

Method: 采用严格的数据策展策略，涵盖预训练、中期训练和SFT阶段，在RL阶段配合使用精心设计的奖励模型；采用紧凑的6B参数扩散模型架构，显著小于常见的20B+ MoE架构。

Result: 实现了新的SOTA性能，在文本渲染能力和真实感方面表现优异，特别是在中文字符渲染方面设定了新的行业标准；在图像编辑方面也达到SOTA结果，编辑一致性优于其他开源作品；部署效率高，VRAM使用少，推理速度快。

Conclusion: LongCat-Image通过创新的训练策略和紧凑架构，在多语言图像生成方面取得了突破性进展，并通过完整的开源生态系统为开发者和研究人员提供了强大支持，将推动视觉内容创作的前沿发展。

Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.

</details>


### [138] [Online Segment Any 3D Thing as Instance Tracking](https://arxiv.org/abs/2512.07599)
*Hanshi Wang,Zijian Cai,Jin Gao,Yiwei Zhang,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: AutoSeg3D将在线3D分割重构为实例跟踪问题，通过对象查询进行时空信息传播，在多个数据集上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的3D分割方法忽略了时间维度这一关键感知维度，而具身智能体需要动态环境感知能力。视角变化导致物体部分可见性，需要超越瞬时不完整视图的整体理解。

Method: 将在线3D分割重构为实例跟踪问题，利用对象查询进行时间信息传播：长期实例关联保持特征和身份一致性，短期实例更新丰富即时观测。引入空间一致性学习缓解VFMs的碎片化问题。

Result: 在ScanNet200上超越ESAM 2.8 AP，在ScanNet、SceneNN和3RScan数据集上均取得一致性能提升，建立新的SOTA。

Conclusion: 通过将3D分割重构为实例跟踪问题，利用稀疏对象查询进行时空信息交换和一致性学习，不仅增强空间理解，还避免了密集时间点云交互的计算负担，显著提升具身智能体的动态环境感知能力。

Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.

</details>


### [139] [Decomposition Sampling for Efficient Region Annotations in Active Learning](https://arxiv.org/abs/2512.07606)
*Jingna Qiu,Frauke Wilm,Mathias Öttl,Jonas Utz,Maja Schlereth,Moritz Schillinger,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: DECOMP是一种新的主动学习采样策略，专门针对密集预测任务，通过分解图像为类别特定组件并采样每个类别的区域，提高标注多样性和少数类性能


<details>
  <summary>Details</summary>
Motivation: 密集预测任务（特别是医学影像）的标注成本高且耗时，现有方法存在计算内存成本高、区域选择不相关、过度依赖不确定性采样等问题，需要更高效的主动学习策略

Method: 提出分解采样（DECOMP）：1）使用伪标签将图像分解为类别特定组件；2）从每个类别中采样区域；3）利用类别预测置信度指导采样过程，确保困难类别获得更多标注

Result: 在ROI分类、2D分割和3D分割任务中，DECOMP始终优于基线方法，能更好地采样少数类区域并提升这些困难类别的性能

Conclusion: DECOMP通过分解采样策略有效解决了密集预测任务中主动学习的挑战，提高了标注效率并改善了少数类性能，代码已开源

Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.

</details>


### [140] [MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation](https://arxiv.org/abs/2512.07628)
*Zhiqi Li,Wenhuan Li,Tengfei Wang,Zhenwei Wang,Junta Wu,Haoyuan Wang,Yunhan Yang,Zehuan Huang,Yang Li,Peidong Liu,Chunchao Guo*

Main category: cs.CV

TL;DR: MoCA是一个可组合的3D生成模型，通过重要性组件路由和未选组件压缩技术，解决了现有方法因全局注意力二次成本导致的扩展性问题，实现了高效、细粒度的3D资产创建。


<details>
  <summary>Details</summary>
Motivation: 组合性对于3D对象和场景生成至关重要，但现有的基于部分的3D生成方法由于全局注意力成本呈二次增长，在增加组件数量时扩展性差。

Method: MoCA采用两种关键设计：(1) 基于重要性的组件路由，选择top-k相关组件进行稀疏全局注意力；(2) 不重要组件压缩，在减少全局注意力计算复杂度的同时保留未选组件的上下文先验。

Result: 大量实验表明，MoCA在组合对象和场景生成任务上均优于基线方法，实现了可扩展组件数量的高效、细粒度3D资产创建。

Conclusion: MoCA通过创新的注意力机制设计，解决了组合式3D生成中的扩展性问题，为大规模组件的高效3D生成提供了有效解决方案。

Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA

</details>


### [141] [Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method](https://arxiv.org/abs/2512.07651)
*Yuanye Liu,Hanxiao Zhang,Nannan Shi,Yuxin Shi,Arif Mahmood,Murtaza Taj,Xiahai Zhuang*

Main category: cs.CV

TL;DR: LiQA数据集包含440名患者的多期相、多中心MRI扫描，用于肝分割和肝纤维化分期算法基准测试，在复杂真实世界条件下评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 肝纤维化是全球重大健康负担，需要准确分期以进行有效临床管理。现有算法在复杂真实世界条件下（如域偏移、模态缺失、空间错位）的鲁棒性需要评估。

Method: 建立LiQA数据集作为CARE 2024挑战赛基准，包含440名患者的多期相、多中心MRI扫描。最佳方法整合半监督学习框架与外部数据进行鲁棒分割，采用多视图共识方法与基于类激活图的规范化进行分期。

Result: 评估表明，利用多源数据和解剖约束显著增强了模型在临床环境中的鲁棒性，为肝纤维化分期提供了有效的基准测试框架。

Conclusion: LiQA数据集为肝分割和肝纤维化分期算法提供了全面的基准测试平台，展示了多源数据整合和解剖约束在提升临床模型鲁棒性方面的重要性。

Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.

</details>


### [142] [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](https://arxiv.org/abs/2512.07652)
*Hamad Almazrouei,Mariam Al Nasseri,Maha Alzaabi*

Main category: cs.CV

TL;DR: 提出AI驱动的自主水下航行器系统，集成YOLOv12 Nano实时检测、ResNet50特征提取、PCA降维、K-Means++聚类和GPT-4o Mini生成报告，用于海洋探索中的物体检测与分析。


<details>
  <summary>Details</summary>
Motivation: 传统海洋探索面临极端条件、能见度有限、成本高昂等挑战，导致大片海洋区域未被探索。需要自动化系统来降低人类潜水风险，提高任务效率，增强水下数据分析能力。

Method: 集成多技术框架：YOLOv12 Nano用于实时物体检测，ResNet50进行特征提取，PCA用于降维并保留98%方差，K-Means++聚类基于视觉特征分组海洋物体，GPT-4o Mini生成结构化报告和摘要。

Result: 在超过55,000张DeepFish和OzFish数据集图像上训练评估，系统检测性能达到mAP@0.5为0.512，精确率0.535，召回率0.438。PCA有效降维，K-Means成功聚类，LLM生成有洞察力的检测摘要。

Conclusion: 该集成方法显著降低人类潜水风险，提高任务效率，增强水下数据分析的速度和深度，为在挑战性海洋环境中进行更有效的科学研究与发现铺平道路。

Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.

</details>


### [143] [Optimization-Guided Diffusion for Interactive Scene Generation](https://arxiv.org/abs/2512.07661)
*Shiaho Li,Naisheng Ye,Tianyu Li,Kashyap Chitta,Tuo An,Peng Su,Boyang Wang,Haiou Liu,Chen Lv,Hongyang Li*

Main category: cs.CV

TL;DR: OMEGA是一个优化引导的无训练框架，通过约束优化在扩散采样中增强场景生成的结构一致性和交互感知，特别用于生成安全关键的对抗性驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 评估自动驾驶车辆需要真实多样的多智能体驾驶场景，但安全关键事件在现有数据集中罕见且代表性不足。现有数据驱动场景生成模型缺乏可控性或产生违反物理/社会约束的样本，限制了其实用性。

Method: 提出OMEGA框架：1) 在扩散模型的反向扩散步骤中通过约束优化重新锚定，引导生成物理合理和行为一致的轨迹；2) 将自我车辆与攻击者交互建模为分布空间的博弈论优化，近似纳什均衡以生成安全关键的对抗场景。

Result: 在nuPlan和Waymo数据集上，OMEGA将物理和行为有效场景比例从32.35%提升到72.27%（自由探索），从11%提升到80%（可控生成）。能生成5倍多的近碰撞帧（碰撞时间小于3秒）同时保持场景真实性。

Conclusion: OMEGA通过优化引导的扩散采样显著提高了驾驶场景生成的现实性、一致性和可控性，特别是对于安全关键对抗场景的生成，为自动驾驶评估提供了更有效的工具。

Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.

</details>


### [144] [EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset](https://arxiv.org/abs/2512.07668)
*Ronan John,Aditya Kesari,Vincenzo DiMatteo,Kristin Dana*

Main category: cs.CV

TL;DR: 提出了EgoCampus数据集和EgoCampusNet方法，用于预测户外校园环境中行人导航时的视觉注意力


<details>
  <summary>Details</summary>
Motivation: 现有自我中心数据集大多关注室内任务或缺乏眼动追踪数据，需要研究真实户外导航场景中的视觉注意力预测

Method: 使用Meta的Project Aria眼镜收集数据，包含眼动追踪、RGB相机、惯性传感器和GPS，开发了EgoCampusNet模型预测行人导航时的眼动注视点

Result: 创建了EgoCampus数据集，包含25条独特户外路径、超过6公里、80多名行人的眼动注释视频，为研究真实世界注意力提供了新资源

Conclusion: 该工作为研究真实世界视觉注意力和导航眼动预测模型提供了新的数据集和方法资源

Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .

</details>


### [145] [DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations](https://arxiv.org/abs/2512.07674)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: DIST-CLIP是一个用于MRI图像标准化的统一框架，通过解耦解剖内容和图像对比度，使用CLIP编码器提取对比度表示，并利用自适应风格转移模块实现灵活的图像标准化。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像分析中应用广泛，但临床泛化能力受限。主要障碍是数据异质性，特别是在MRI中，扫描仪硬件差异、采集协议多样性和序列参数变化导致显著的域偏移，掩盖了潜在的生物学信号。现有数据标准化方法存在不足：基于图像的方法需要目标图像，基于文本的方法依赖过于简化的标签或仅适用于有限变异的数据集，无法捕捉真实临床环境的异质性。

Method: 提出DIST-CLIP（基于CLIP指导的解耦风格转移）框架，明确解耦解剖内容和图像对比度。使用预训练的CLIP编码器提取对比度表示，通过新颖的自适应风格转移模块将这些对比度嵌入整合到解剖内容中。该框架灵活地使用目标图像或DICOM元数据进行指导。

Result: 在多样化的真实临床数据集上训练和评估DIST-CLIP，与最先进方法相比，在风格转换保真度和解剖结构保留方面都显示出显著改进。该框架为风格转移和MRI数据标准化提供了灵活的解决方案。

Conclusion: DIST-CLIP通过解耦解剖内容和图像对比度，结合CLIP编码器和自适应风格转移模块，为MRI数据标准化提供了一个统一且灵活的框架，能够有效处理真实临床环境中的数据异质性问题，显著提升了风格转换的保真度和解剖结构的保留能力。

Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.

</details>


### [146] [Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment](https://arxiv.org/abs/2512.07702)
*Sangha Park,Eunji Kim,Yeongtak Oh,Jooyoung Choi,Sungroh Yoon*

Main category: cs.CV

TL;DR: NPC提出了一种自动化负提示生成方法，通过识别和抑制不相关内容来改进文本-图像对齐，无需额外图像合成。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像生成取得了显著进展，但对于具有丰富组合结构或想象元素的提示，实现精确的文本-图像对齐仍然具有挑战性。

Method: 引入负提示图像校正（NPC）自动化流程，通过分析交叉注意力模式，使用验证器-描述器-提议器框架生成候选负提示，并通过显著文本空间评分进行排序。

Result: 在GenEval++和Imagine-Bench上，NPC表现优于强基线方法，在GenEval++上达到0.571 vs 0.371，在Imagine-Bench上获得最佳整体性能。

Conclusion: 通过指导模型不生成什么内容，NPC为扩散模型提供了原则性、全自动的文本-图像对齐增强方法。

Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.

</details>


### [147] [UnCageNet: Tracking and Pose Estimation of Caged Animal](https://arxiv.org/abs/2512.07712)
*Sayak Dutta,Harish Katti,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 提出三阶段预处理流程，通过笼子分割、修复和评估，解决动物追踪与姿态估计在笼状结构遮挡下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有动物追踪和姿态估计系统（如STEP、ViTPose）在处理带有笼状结构和系统性遮挡的图像视频时性能显著下降，需要解决这一局限性。

Method: 三阶段预处理流程：1) 使用Gabor增强的ResNet-UNet架构进行笼子分割，配备72个方向可调滤波器；2) 使用CRFill进行笼子修复，实现内容感知的遮挡区域重建；3) 在修复后的帧上进行姿态估计和追踪评估。

Result: 实验验证表明，通过该流程去除笼子遮挡后，姿态估计和追踪性能可达到与无遮挡环境相当的水平，关键点检测精度和轨迹一致性均有显著提升。

Conclusion: 提出的三阶段预处理流程能有效解决笼状结构遮挡问题，使动物追踪和姿态估计系统在复杂遮挡环境下仍能保持高性能。

Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.

</details>


### [148] [ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation](https://arxiv.org/abs/2512.07720)
*Fan Yang,Heyuan Li,Peihao Li,Weihao Yuan,Lingteng Qiu,Chaoyue Song,Cheng Chen,Yisheng He,Shifeng Zhang,Xiaoguang Han,Steven Hoi,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出一种结合3D重建模型和视频生成模型优势的新方法，从单张输入图像生成高质量的上半身3D虚拟形象，解决现有方法在纹理模糊、运动僵硬和结构不稳定等方面的问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D虚拟形象生成方法存在两个主要问题：基于大型重建模型的方法虽然快速且能产生稳定的身体结构，但常出现纹理模糊和运动僵硬；而基于生成视频模型的方法能合成逼真动态效果，但经常出现身体结构错误和身份漂移等不稳定行为。需要结合两者的优势来解决这些局限性。

Method: 提出一个新颖框架，使用3D重建模型提供稳健的结构和外观先验，然后引导实时自回归视频扩散模型进行渲染。这种方法结合了几何稳定性与生成能力，能够合成高频、逼真的细节和流畅动态效果。

Result: 实验表明，该方法显著减少了伪影，在视觉质量上相比领先方法有实质性改进，能够有效减少纹理模糊和运动僵硬，同时防止视频生成方法中常见的结构不一致问题。

Conclusion: 通过将3D重建的几何稳定性与视频模型的生成能力相结合，该方法能够生成具有逼真外观和动态、时间一致运动的高保真数字虚拟形象，为游戏和虚拟现实等实时应用提供了稳健高效的解决方案。

Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa

</details>


### [149] [SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination](https://arxiv.org/abs/2512.07730)
*Sangha Park,Seungryong Yoo,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: SAVE框架通过稀疏自编码器特征引导来减少多模态大语言模型中的物体幻觉问题，在多个基准测试中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然取得了显著进展，但仍然容易受到语言先验和视觉信息丢失导致的物体幻觉问题影响

Method: 提出SAVE框架，利用稀疏自编码器潜在特征引导模型。通过二元物体存在问答探针识别最能反映模型视觉信息处理的视觉理解特征，并沿着这些特征引导模型

Result: 在标准基准测试中优于最先进的无训练方法，CHAIR_S指标提升10个百分点，在POPE和MMHal-Bench上也有持续提升。多模型和多层评估证实了方法的鲁棒性和泛化能力

Conclusion: SAVE通过稀疏自编码器特征引导有效减少多模态大语言模型的物体幻觉，增强视觉基础理解，抑制不确定物体标记生成并增加对图像标记的关注

Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.

</details>


### [150] [SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery](https://arxiv.org/abs/2512.07733)
*Meng Cao,Xingyu Li,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: SpatialDreamer是一个强化学习框架，通过主动探索、视觉想象和证据推理的闭环过程，提升多模态大语言模型在复杂空间推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型在需要心理模拟的复杂空间推理任务上表现有限，主要因为它们依赖被动观察空间数据，缺乏主动的心理意象过程。

Method: 提出了SpatialDreamer框架，采用强化学习实现空间推理的闭环过程，包括主动探索、通过世界模型进行视觉想象和基于证据的推理。针对长序列推理任务中细粒度奖励监督不足的问题，提出了几何策略优化（GeoPO），引入树结构采样和具有几何一致性约束的步骤级奖励估计。

Result: 在多个具有挑战性的基准测试中，SpatialDreamer取得了高度竞争力的结果，表明在多模态大语言模型的人类式主动空间心理模拟方面取得了重要进展。

Conclusion: SpatialDreamer通过强化学习和主动心理模拟的方法，显著提升了多模态大语言模型在复杂空间推理任务中的能力，为人类式的空间心理模拟提供了重要进展。

Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.

</details>


### [151] [HLTCOE Evaluation Team at TREC 2025: VQA Track](https://arxiv.org/abs/2512.07738)
*Dengjia Zhang,Charles Weng,Katherine Guerrerio,Yi Lu,Kenton Murray,Alexander Martin,Reno Kriz,Benjamin Van Durme*

Main category: cs.CV

TL;DR: HLTCOE团队在TREC VQA的答案生成任务中提出了一个列表式学习框架，通过重排序候选答案来提高语义精度和排序一致性。


<details>
  <summary>Details</summary>
Motivation: 解决视频问答中答案生成的语义精度和排序一致性问题，特别是在需要时间推理和语义消歧的问题上。

Method: 使用基础多模态模型生成多个候选答案，然后通过基于掩码指针交叉熵损失与排序权重的模型进行重排序，该目标整合了指针候选选择、排序依赖权重和词汇限制下的掩码交叉熵。

Result: 实验显示在准确性和排序稳定性方面获得了一致的提升，特别是在需要时间推理和语义消歧的问题上效果显著。

Conclusion: 通过将生成建模与判别排序相结合，该方法能够产生连贯、细粒度的答案列表，实现了稳定且可解释的列表式优化。

Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.

</details>


### [152] [Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation](https://arxiv.org/abs/2512.07747)
*Shihao Zhao,Yitong Chen,Zeyinzi Jiang,Bojia Zi,Shaozhe Hao,Yu Liu,Chaojie Mao,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: Unison是一个低成本的多模态统一理解与生成模型，采用两阶段方案，仅需50万训练样本和50GPU小时，就能自动识别任务类型并提取参数，覆盖多种理解与生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态统一理解与生成方法存在两大问题：自回归方法需要大量计算资源，两阶段方法任务覆盖有限且生成质量差。两者都无法自动解析输入元信息，需要手动参数配置。

Method: 采用两阶段方案，连接预训练的理解和生成模型进行对齐微调。模型具备自动解析用户意图、确定目标任务类型、准确提取任务所需元信息的能力。

Result: 在仅50万训练样本和50GPU小时的极低成本下，模型能准确自动识别任务和提取参数，在多种理解与生成任务上取得优异性能。

Conclusion: Unison证明了在低成本设置下实现多模态统一理解与生成的可行性，通过自动任务解析和参数提取实现了无需人工干预的全自动化多模态任务处理。

Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.

</details>


### [153] [Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.07760)
*Menglin Wang,Xiaojin Gong,Jiachen Li,Genlin Ji*

Main category: cs.CV

TL;DR: 该论文提出了一种无监督可见光-红外行人重识别方法，通过模态感知Jaccard距离缓解模态差异带来的距离偏差，并采用"分割-对比"策略学习模态不变表示，在基准数据集上取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 无监督可见光-红外行人重识别面临跨模态关联估计的挑战，现有方法通常使用最优传输关联模态内聚类，容易传播局部聚类错误，且忽略了全局实例级关系。本文通过挖掘和关注可见光-红外模态偏差，从两个方面解决跨模态学习问题。

Method: 1. 提出模态感知Jaccard距离来缓解模态差异引起的距离偏差，通过全局聚类估计更可靠的跨模态关联；2. 设计"分割-对比"策略获取模态特定的全局原型，在全局关联指导下显式对齐这些原型，实现模态不变且ID可区分的表示学习。

Result: 该方法在基准VI-ReID数据集上获得了最先进的性能，显著优于现有方法，验证了其有效性。

Conclusion: 通过缓解模态偏差的全局关联和模态不变表示学习，本文提出的方法在无监督可见光-红外行人重识别任务中取得了显著改进，证明了关注模态偏差对跨模态学习的重要性。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.

</details>


### [154] [Distribution Matching Variational AutoEncoder](https://arxiv.org/abs/2512.07778)
*Sen Ye,Jianning Pei,Mengde Xu,Shuyang Gu,Chunyu Wang,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: DMVAE通过显式匹配编码器潜在分布与任意参考分布，超越传统VAE的高斯先验，系统研究哪种潜在分布更适合建模，发现SSL特征分布能在重建保真度和建模效率间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型（如VAE和基础模型对齐编码器）隐式约束潜在空间而不显式塑造其分布，不清楚哪种分布最适合建模。需要一种能显式对齐潜在分布与任意参考分布的方法。

Method: 提出Distribution-Matching VAE (DMVAE)，通过分布匹配约束显式对齐编码器的潜在分布与任意参考分布，超越传统VAE的高斯先验，可对齐自监督特征、扩散噪声或其他先验分布。

Result: 系统研究发现SSL特征分布能在重建保真度和建模效率间取得最佳平衡，在ImageNet上仅用64个训练周期达到gFID=3.2。表明选择合适的潜在分布结构（通过分布级对齐）是弥合易建模潜在空间与高保真图像合成之间差距的关键。

Conclusion: DMVAE通过显式分布匹配约束，能够系统研究不同潜在分布对建模的影响，发现SSL特征分布是最优选择。这表明分布级对齐比固定先验更重要，为视觉生成模型提供了新的设计思路。

Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.

</details>


### [155] [OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory](https://arxiv.org/abs/2512.07802)
*Zhaochong An,Menglin Jia,Haonan Qiu,Zijian Zhou,Xiaoke Huang,Zhiheng Liu,Weiming Ren,Kumara Kahatapitiya,Ding Liu,Sen He,Chenyang Zhang,Tao Xiang,Fanny Yang,Serge Belongie,Tian Xie*

Main category: cs.CV

TL;DR: OneStory提出了一种新的多镜头视频生成方法，通过全局跨镜头上下文建模实现连贯的长篇叙事视频生成，在文本和图像条件下都达到了最先进的叙事连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有多镜头视频生成方法难以有效建模长距离跨镜头上下文，因为它们依赖有限的时间窗口或单关键帧条件，导致在复杂叙事场景下性能下降。需要一种能够全局建模跨镜头上下文的方法来实现一致且可扩展的叙事生成。

Method: 将多镜头视频生成重新定义为"下一个镜头生成"任务，实现自回归镜头合成。引入两个关键模块：1) 帧选择模块，基于先前镜头的信息帧构建语义相关的全局记忆；2) 自适应条件模块，执行重要性引导的补丁化以生成紧凑的上下文进行直接条件化。还策划了高质量的多镜头数据集并设计了有效的训练策略。

Result: 在策划的60K数据集上从预训练的I2V模型微调后，OneStory在文本和图像条件设置下，在多样化和复杂场景中实现了最先进的叙事连贯性，能够生成可控且沉浸式的长篇视频叙事。

Conclusion: OneStory通过全局而紧凑的跨镜头上下文建模，实现了连贯且可扩展的叙事生成，为可控和沉浸式的长篇视频叙事提供了有效的解决方案。

Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.

</details>


### [156] [Multi-view Pyramid Transformer: Look Coarser to See Broader](https://arxiv.org/abs/2512.07806)
*Gyeongjin Kang,Seungkwon Yang,Seungtae Nam,Younggeun Lee,Jungwoo Kim,Eunbyung Park*

Main category: cs.CV

TL;DR: MVP是一种可扩展的多视角Transformer架构，能够从数十到数百张图像中单次前向传播重建大型3D场景，采用局部到全局的视角层次和精细到粗糙的表示层次，结合3D高斯泼溅实现高效高质量重建。


<details>
  <summary>Details</summary>
Motivation: 为了解决从大量图像中高效重建大型复杂3D场景的挑战，传统方法在处理数十到数百张图像时面临计算效率和可扩展性问题，需要一种能够同时处理多尺度信息并保持计算效率的架构。

Method: 提出多视角金字塔Transformer（MVP），基于两个核心设计原则：1）局部到全局的视角间层次结构，从局部视角逐步扩展到组视角再到完整场景；2）精细到粗糙的视角内层次结构，从详细空间表示逐步聚合为紧凑的信息密集token。该架构与3D高斯泼溅作为底层3D表示相结合。

Result: 在多样化数据集上验证表明，MVP在保持高效率和可扩展性的同时，实现了最先进的泛化重建质量，能够适应广泛的视角配置范围。

Conclusion: MVP通过双重层次结构设计，在计算效率和表示丰富性之间取得了平衡，为大规模3D场景重建提供了一种高效且可扩展的解决方案，展示了在复杂场景重建任务中的优越性能。

Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.

</details>


### [157] [Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes](https://arxiv.org/abs/2512.07807)
*Shai Krakovsky,Gal Fiebelman,Sagie Benaim,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 该论文提出了一种在3D高斯表示中嵌入语言场的新方法，通过引入极低维语义瓶颈特征和特征哈希编码器，解决了现有方法在语义特征对齐和效率方面的问题。


<details>
  <summary>Details</summary>
Motivation: 在3D表示中嵌入语言场可以实现空间环境的更丰富语义理解，将几何与描述性意义联系起来，从而支持更直观的人机交互（如自然语言查询和编辑场景）。然而，现有特征蒸馏方法在处理大规模互联网数据时面临语义特征错位以及内存和运行时效率低下的挑战。

Method: 1. 在底层3D高斯表示中引入极低维语义瓶颈特征；2. 通过渲染处理这些特征，并使用多分辨率、基于特征的哈希编码器；3. 引入衰减下采样器模块和多种正则化方法，解决地面真实2D特征的语义错位问题。

Result: 在HolyScenes数据集上的评估表明，该方法在性能和效率方面均超越了现有方法，显著提高了运行时效率和GPU内存使用效率。

Conclusion: 提出的方法有效解决了大规模场景中语言场嵌入的语义特征对齐和效率问题，为实现更丰富的空间语义理解和直观人机交互提供了可行的技术方案。

Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.

</details>


### [158] [WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling](https://arxiv.org/abs/2512.07821)
*Shaoheng Fang,Hanwen Jiang,Yunpeng Bai,Niloy J. Mitra,Qixing Huang*

Main category: cs.CV

TL;DR: WorldReel是一个原生时空一致的4D视频生成器，能够联合生成RGB帧和4D场景表示（点云图、相机轨迹、密集光流映射），实现动态场景和移动相机下的几何与外观一致性建模。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成器虽然实现了逼真的视觉效果，但在3D一致性方面存在根本性不足。为了解决视频生成中的时空不一致问题，特别是处理大范围非刚性运动和显著相机移动时，需要开发能够保持4D一致性的视频生成方法。

Method: WorldReel采用显式的4D场景表示，联合生成RGB帧和4D场景表示（点云图、相机轨迹、密集光流映射）。通过结合合成数据和真实数据进行训练：合成数据提供精确的4D监督（几何、运动和相机信息），真实视频提供视觉多样性和真实感。

Result: 实验表明WorldReel在动态场景和移动相机的视频生成中达到了新的最先进水平，在几何一致性、运动连贯性指标上表现优异，并减少了视角-时间伪影。该方法能够泛化到野外拍摄的视频，同时保持强大的几何保真度。

Conclusion: WorldReel将视频生成推向4D一致的世界建模，使智能体能够通过单一稳定的时空表示来渲染、交互和推理场景，为实现更一致的视频生成和世界建模提供了重要进展。

Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.

</details>


### [159] [OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing](https://arxiv.org/abs/2512.07826)
*Haoyang He,Jie Wang,Jiangning Zhang,Zhucun Xue,Xingyuan Bu,Qiangpeng Yang,Shilei Wen,Lei Xie*

Main category: cs.CV

TL;DR: OpenVE-3M是一个开源的大规模高质量指令式视频编辑数据集，包含空间对齐和非空间对齐两类编辑类型，通过精心设计的数据流水线生成，在规模、多样性、指令长度和质量上超越现有开源数据集。同时构建了OpenVE-Bench统一基准，并训练了OpenVE-Edit模型，在基准测试中超越了所有现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 指令式图像编辑数据集的质量和多样性不断提升，但大规模高质量的指令式视频编辑数据集仍然稀缺。为了解决这一差距，需要构建一个开源、大规模、高质量的指令式视频编辑数据集。

Method: 1. 构建OpenVE-3M数据集：包含空间对齐编辑（全局风格、背景变化、局部变化、局部移除、局部添加、字幕编辑）和非空间对齐编辑（相机多镜头编辑和创意编辑），通过精心设计的数据流水线生成并经过严格质量过滤。
2. 构建OpenVE-Bench基准：包含431个视频编辑对，涵盖多样化编辑任务，采用三个与人类判断高度一致的关键指标。
3. 训练OpenVE-Edit模型：基于OpenVE-3M数据集训练5B参数模型。

Result: 1. OpenVE-3M在规模、编辑类型多样性、指令长度和整体质量上超越了现有开源数据集。
2. OpenVE-Edit模型在OpenVE-Bench基准上取得了新的最先进性能，超越了包括14B基线在内的所有先前开源模型，展示了卓越的效率和效果。

Conclusion: OpenVE-3M填补了指令式视频编辑领域高质量数据集的空白，OpenVE-Bench提供了统一的评估基准，OpenVE-Edit模型证明了数据集的有效性，为视频编辑研究提供了重要资源。

Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.

</details>


### [160] [One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation](https://arxiv.org/abs/2512.07829)
*Yuan Gao,Chen Chen,Tianrong Chen,Jiatao Gu*

Main category: cs.CV

TL;DR: FAE（特征自编码器）是一个简单有效的框架，将预训练视觉表示适配到适合生成的低维潜在空间，仅需单个注意力层，同时保持重建和理解能力。


<details>
  <summary>Details</summary>
Motivation: 视觉生成模型通常在压缩潜在空间中运行以平衡训练效率和样本质量。现有方法难以将高质量预训练视觉表示适配到生成模型中，因为理解导向的特征与生成友好的潜在空间之间存在根本性不匹配。

Method: FAE通过耦合两个独立的深度解码器：一个训练用于重建原始特征空间，另一个将重建特征作为输入进行图像生成。该方法通用性强，可与多种自监督编码器（如DINO、SigLIP）配合，并集成到扩散模型和标准化流两种生成框架中。

Result: 在类别条件和文本到图像基准测试中，FAE表现优异。在ImageNet 256x256上，使用CFG的扩散模型达到接近SOTA的FID 1.29（800轮）和1.70（80轮）；不使用CFG时达到SOTA的FID 1.48（800轮）和2.08（80轮），展示了高质量和快速学习能力。

Conclusion: FAE提供了一种简单而有效的方法，将预训练视觉表示适配到生成友好的低维潜在空间，仅需最小架构修改，同时保持重建和理解能力，在多个基准测试中实现了优异的性能。

Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

</details>


### [161] [UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation](https://arxiv.org/abs/2512.07831)
*Jiehui Huang,Yuechen Zhang,Xu He,Yuan Gao,Zhi Cen,Bin Xia,Yan Zhou,Xin Tao,Pengfei Wan,Jiaya Jia*

Main category: cs.CV

TL;DR: UnityVideo是一个统一的多模态视频生成框架，通过联合学习多种模态（分割掩码、人体骨架、DensePose、光流、深度图）和训练范式，提升世界感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型受限于单模态条件，缺乏跨模态交互和模态多样性，限制了其对世界的全面理解能力。

Method: 提出两个核心组件：1) 动态加噪统一异构训练范式；2) 模态切换器和上下文学习器，通过模块化参数和上下文学习实现统一处理。构建了130万样本的大规模统一数据集。

Result: 联合优化加速了收敛，显著增强了零样本泛化能力。UnityVideo在视频质量、一致性和物理世界约束对齐方面表现优异。

Conclusion: UnityVideo通过统一多模态学习框架，解决了现有视频生成模型的世界感知局限性，实现了更全面、更一致的世界感知视频生成。

Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo

</details>


### [162] [Relational Visual Similarity](https://arxiv.org/abs/2512.07833)
*Thao Nguyen,Sicheng Mo,Krishna Kumar Singh,Yilin Wang,Jing Shi,Nicholas Kolkin,Eli Shechtman,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: 该论文提出了一种新的图像相似度度量方法，专注于捕捉人类感知的关系相似性而非表面属性相似性，通过匿名化标注数据集和微调视觉语言模型来实现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉相似度度量方法（如LPIPS、CLIP、DINO）仅关注感知属性相似性，无法捕捉人类能够感知的丰富关系相似性，这被认为是人类区别于其他物种的关键认知能力。

Method: 首先将关系图像相似性形式化为可测量问题；然后构建包含114k图像-匿名化标题的数据集，标题描述场景的关系逻辑而非表面内容；最后使用该数据集微调视觉语言模型来测量图像间的关系相似性。

Result: 开发出首个能够测量图像关系相似性的模型，该模型基于图像底层关系结构而非可见外观来连接图像，揭示了现有图像相似性模型在捕捉关系相似性方面的关键差距。

Conclusion: 关系相似性具有重要的现实应用价值，但现有图像相似性模型无法有效捕捉，该研究为视觉计算领域填补了这一关键空白，为基于关系结构的图像理解开辟了新方向。

Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.

</details>


### [163] [Voxify3D: Pixel Art Meets Volumetric Rendering](https://arxiv.org/abs/2512.07834)
*Yi-Chuan Huang,Jiewen Chan,Hao-Jen Chien,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Voxify3D是一个两阶段可微分框架，通过正交像素艺术监督、基于补丁的CLIP对齐和调色板约束的Gumbel-Softmax量化，实现从3D网格到体素艺术的自动化生成，解决了几何抽象、语义保持和离散颜色一致性的冲突需求。


<details>
  <summary>Details</summary>
Motivation: 体素艺术在游戏和数字媒体中广泛应用，但从3D网格自动生成面临挑战：现有方法要么过度简化几何，要么无法实现像素级精确、调色板约束的体素艺术美学。需要解决几何抽象、语义保持和离散颜色一致性之间的冲突需求。

Method: 提出Voxify3D两阶段可微分框架，包含三个核心组件：1) 正交像素艺术监督，消除透视畸变实现体素-像素精确对齐；2) 基于补丁的CLIP对齐，在不同离散化级别保持语义；3) 调色板约束的Gumbel-Softmax量化，在离散颜色空间实现可微分优化，支持可控调色板策略。

Result: 实验显示优越性能：CLIP-IQA得分37.12，用户偏好77.90%。支持多样化角色和可控抽象（2-8种颜色，20x-50x分辨率）。

Conclusion: Voxify3D通过协同整合正交监督、语义对齐和离散优化，成功解决了体素艺术生成中的基本挑战，实现了语义保持、像素艺术美学和端到端离散优化的统一。

Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [164] [Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity](https://arxiv.org/abs/2512.06106)
*Constanze Albrecht,Chayapatr Archiwaranguprok,Rachel Poonsiriwong,Awu Chen,Peggy Yin,Monchai Lertsutthiwong,Kavin Winson,Hal Hershfield,Pattie Maes,Pat Pataranutaporn*

Main category: cs.HC

TL;DR: 研究评估了AI生成未来自我的三种模态（文本、语音、头像）对心理影响的效果，发现所有个性化模态都能增强未来自我连续性、情感福祉和动机，头像模式带来最生动的体验，但模态间无显著差异。交互质量比形式更重要，Claude 4在心理效果上优于其他大语言模型。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在克隆声音、年龄进展面部渲染和自传叙事方面的进步，研究者想要探究不同模态的AI生成未来自我如何影响心理和情感效果，以及这些模态如何塑造当前决策和与未来的连接感。

Method: 采用随机对照研究（N=92），评估三种AI生成未来自我模态（文本、语音、头像）与中性对照条件。同时系统评估Claude 4与其他三个大语言模型（ChatGPT 3.5、Llama 4、Qwen 3）在心理和交互维度上的表现。

Result: 所有个性化模态都增强了未来自我连续性、情感福祉和动机，头像模式产生最大的生动性增益，但模态间无显著差异。交互质量指标（说服力、真实感、用户参与度）是心理和情感结果的强预测因子。内容分析显示文本强调职业规划，语音和头像促进个人反思。Claude 4在增强心理、情感和未来自我连续性结果方面优于其他模型。

Conclusion: AI生成未来自我能有效增强心理福祉和未来连接感，交互质量比具体形式更重要。Claude 4在提供高质量交互方面表现最佳，为未来自我干预的设计提供了重要指导。

Abstract: What if users could meet their future selves today? AI-generated future selves simulate meaningful encounters with a digital twin decades in the future. As AI systems advance, combining cloned voices, age-progressed facial rendering, and autobiographical narratives, a central question emerges: Does the modality of these future selves alter their psychological and affective impact? How might a text-based chatbot, a voice-only system, or a photorealistic avatar shape present-day decisions and our feeling of connection to the future? We report a randomized controlled study (N=92) evaluating three modalities of AI-generated future selves (text, voice, avatar) against a neutral control condition. We also report a systematic model evaluation between Claude 4 and three other Large Language Models (LLMs), assessing Claude 4 across psychological and interaction dimensions and establishing conversational AI quality as a critical determinant of intervention effectiveness. All personalized modalities strengthened Future Self-Continuity (FSC), emotional well-being, and motivation compared to control, with avatar producing the largest vividness gains, yet with no significant differences between formats. Interaction quality metrics, particularly persuasiveness, realism, and user engagement, emerged as robust predictors of psychological and affective outcomes, indicating that how compelling the interaction feels matters more than the form it takes. Content analysis found thematic patterns: text emphasized career planning, while voice and avatar facilitated personal reflection. Claude 4 outperformed ChatGPT 3.5, Llama 4, and Qwen 3 in enhancing psychological, affective, and FSC outcomes.

</details>


### [165] [CommentScope: A Comment-Embedded Assisted Reading System for a Long Text](https://arxiv.org/abs/2512.06408)
*Shuai Chen,Lei Han,Haoyu Wang,Zhaoman Zhong*

Main category: cs.HC

TL;DR: CommentScope系统通过将评论分类并与原文句子对齐，以视觉化方式嵌入阅读界面，有效提升长文本阅读体验。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上长文本普遍存在，读者面临信息过载和难以定位关键内容的问题。评论提供了理解、质疑和补充文本的外部视角，但其潜力因无序和结构混乱的呈现方式而受限。

Method: 提出CommentScope系统，包含两个核心模块：1）将评论分为五类并与相关句子对齐的"Rule+LLM"混合流水线；2）以内联或侧边注形式集成评论的呈现模块，辅以颜色、图表和高亮等视觉线索。

Result: 技术评估显示混合流水线在语义分类（准确率0.90）和位置对齐（准确率0.88）方面表现良好。用户研究（N=12）表明，句子末尾嵌入显著提高了评论发现准确性和阅读流畅度，同时降低了心理需求和感知努力。

Conclusion: CommentScope系统通过结构化嵌入评论，有效解决了长文本阅读中的信息过载问题，为读者提供了更好的阅读体验和评论发现能力。

Abstract: Long texts are ubiquitous on social platforms, yet readers often face information overload and struggle to locate key content. Comments provide valuable external perspectives for understanding, questioning, and complementing the text, but their potential is hindered by disorganized and unstructured presentation. Few studies have explored embedding comments directly into reading. As an exploratory step, we propose CommentScope, a system with two core modules: a pipeline that classifies comments into five types and aligns them with relevant sentences, and a presentation module that integrates comments inline or as side notes, supported by visual cues such as colors, charts, and highlights. Technical evaluation shows that the hybrid "Rule+LLM" pipeline achieved solid performance in semantic classification (accuracy=0.90) and position alignment (accuracy=0.88). A user study (N=12) further demonstrated that the sentence-end embedding significantly improved comment discovery accuracy and reading fluency while reducing mental demand and perceived effort.

</details>


### [166] ["Having Confidence in My Confidence Intervals": How Data Users Engage with Privacy-Protected Wikipedia Data](https://arxiv.org/abs/2512.06534)
*Harold Triedman,Jayshree Sarathy,Priyanka Nanayakkara,Rachel Cummings,Gabriel Kaptchuk,Sean Kross,Elissa M. Redmiles*

Main category: cs.HC

TL;DR: 研究探索数据用户对隐私保护噪声（如差分隐私和四舍五入）的感知与理解，通过文档设计和任务式情境调查发现用户能使用简单不确定性指标，但在复杂统计推断上存在困难，且对隐私-效用关系存在误解。


<details>
  <summary>Details</summary>
Motivation: 随着开放数据需求和隐私威胁增加，组织越来越多地采用差分隐私等隐私保护技术，但这些技术在实际数据用户中的接受度和理解程度尚未充分研究，需要了解用户如何感知和解释隐私保护噪声。

Method: 开发了展示两种维基百科页面浏览数据集噪声特性的文档（一种使用四舍五入启发式隐私，一种使用差分隐私），经过专家反馈后，通过任务式情境调查（n=15）探索数据用户对隐私保护噪声的感知、交互和解释。

Result: 参与者能够使用文档中的简单不确定性指标，但在计算多个噪声估计值的置信区间时遇到困难；与四舍五入数据相比，他们更擅长为差分隐私数据设计基于模拟的不确定性计算方法；令人惊讶的是，一些参与者错误地认为差分隐私更强的效用意味着更弱的隐私保护。

Conclusion: 基于研究发现，提出了文档和工具的设计建议，以更好地支持数据用户处理隐私噪声数据，需要改进用户教育和对隐私-效用关系的理解。

Abstract: In response to calls for open data and growing privacy threats, organizations are increasingly adopting privacy-preserving techniques such as differential privacy (DP) that inject statistical noise when generating published datasets. These techniques are designed to protect privacy of data subjects while enabling useful analyses, but their reception by data users is under-explored. We developed documentation that presents the noise characteristics of two Wikipedia pageview datasets: one using rounding (heuristic privacy) and another using DP (formal privacy). After incorporating expert feedback (n=5), we used these documents to conduct a task-based contextual inquiry (n=15) exploring how data users--largely unfamiliar with these methods--perceive, interact with, and interpret privacy-preserving noise during data analysis.
  Participants readily used simple uncertainty metrics from the documentation, but struggled when asked to compute confidence intervals across multiple noisy estimates. They were better able to devise simulation-based approaches for computing uncertainty with DP data compared to rounded data. Surprisingly, several participants incorrectly believed DP's stronger utility implied weaker privacy protections. Based on our findings, we offer design recommendations for documentation and tools to better support data users working with privacy-noised data.

</details>


### [167] [Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability](https://arxiv.org/abs/2512.06591)
*Joe Shymanski,Jacob Brue,Sandip Sen*

Main category: cs.HC

TL;DR: 该研究批评了当前可解释AI评估过度依赖主观用户满意度调查，发现主观满意度无法区分有意义的解释和无效的解释，建议未来评估应结合客观任务表现指标。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI评估主要依赖主观用户调查，可能无法准确衡量解释的实际有效性，需要探索更全面的评估方法。

Method: 通过社会安全福利申请年龄选择实验，比较三种条件：无解释、无效解释和有效解释，测量参与者的客观表现和主观满意度。

Result: 接收有效解释的参与者在客观心智模型测量上显著优于其他组，但用户对无效解释和有效解释的主观满意度评价相同。

Conclusion: 主观调查单独使用无法判断解释是否真正帮助用户建立有用的领域理解，未来评估应结合客观任务表现指标和主观评估。

Abstract: Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.

</details>


### [168] [Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age](https://arxiv.org/abs/2512.06616)
*Rasam Dorri,Rami Zwick*

Main category: cs.HC

TL;DR: 论文提出了"记忆权力不对称"概念，指出AI系统拥有远超人类的记忆能力，导致人际关系中出现新的权力失衡，威胁心理安全、宽恕和身份改变等人类关系基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI嵌入人际关系，其无限记忆能力与人类自然遗忘特性形成鲜明对比，这种不对称记忆能力可能导致结构性权力失衡，威胁传统人际关系赖以存在的心理安全、宽恕和身份改变等基础。

Method: 结合人类记忆、权力依赖理论、AI架构和消费者脆弱性研究，开发了包含四个维度（持久性、准确性、可访问性、整合性）的概念框架，以及记忆不对称转化为权力的四种机制（战略记忆部署、叙事控制、依赖不对称、脆弱性积累）。

Result: 提出了记忆权力不对称作为区别于信息不对称、隐私、监控和客户关系管理的独特概念，分析了其在个人、关系/公司和社会层面的下游后果，制定了边界条件命题。

Conclusion: 保护相互遗忘或至少对记忆的相互控制应成为AI时代的核心设计和政策目标，提出了六项设计原则（如设计遗忘、情境遏制、对称访问记录等）来恢复人机关系中更健康的记忆平衡。

Abstract: As artificial intelligence (AI) becomes embedded in personal and professional relationships, a new kind of power imbalance emerges from asymmetric memory capabilities. Human relationships have historically relied on mutual forgetting, the natural tendency for both parties to forget details over time, as a foundation for psychological safety, forgiveness, and identity change. By contrast, AI systems can record, store, and recombine interaction histories at scale, often indefinitely. We introduce Memory Power Asymmetry (MPA): a structural power imbalance that arises when one relationship partner (typically an AI-enabled firm) possesses a substantially superior capacity to record, retain, retrieve, and integrate the shared history of the relationship, and can selectively deploy that history in ways the other partner (the human) cannot. Drawing on research in human memory, power-dependence theory, AI architecture, and consumer vulnerability, we develop a conceptual framework with four dimensions of MPA (persistence, accuracy, accessibility, integration) and four mechanisms by which memory asymmetry is translated into power (strategic memory deployment, narrative control, dependence asymmetry, vulnerability accumulation). We theorize downstream consequences at individual, relational/firm, and societal levels, formulate boundary-conditioned propositions, and articulate six design principles for restoring a healthier balance of memory in human-AI relationships (e.g., forgetting by design, contextual containment, symmetric access to records). Our analysis positions MPA as a distinct construct relative to information asymmetry, privacy, surveillance, and customer relationship management, and argues that protecting mutual forgetting, or at least mutual control over memory, should become a central design and policy goal in the AI age.

</details>


### [169] [Exploring Teenagers' Trust in Al Chatbots: An Empirical Study of Chinese Middle-School Students](https://arxiv.org/abs/2512.06647)
*Siyu Qiu,Anqi Lin,Shiya Wang,Xingyu Lan*

Main category: cs.HC

TL;DR: 研究探讨青少年心理特征与对AI聊天机器人信任度的关系，发现心理韧性是信任的重要正向预测因子，年龄调节社交焦虑与信任的关系


<details>
  <summary>Details</summary>
Motivation: 现有关于人类对AI信任的研究主要关注成人群体，忽视了与AI技术互动日益频繁的青少年群体。基于青少年教育和心理学理论，本研究旨在探究青少年心理特征与对AI聊天机器人信任度的相关性

Method: 采用混合方法：在线问卷调查结合半结构化访谈。研究考察四个关键变量：AI素养、自我认同、社交焦虑和心理韧性

Result: 心理韧性是对AI信任的重要正向预测因子；年龄显著调节社交焦虑与信任之间的关系；访谈显示青少年普遍报告对AI信任度较高，倾向于高估自己的AI素养，且受社交媒体等外部因素影响

Conclusion: 青少年对AI的信任受到心理韧性的正向影响，年龄在社交焦虑与信任关系中起调节作用，青少年对AI的认知和信任受到多种心理特征和外部因素的影响

Abstract: Chatbots have become increasingly prevalent. A growing body of research focused on the issue of human trust in AI. However, most existing user studies are conducted primarily with adult groups, overlooking teenagers who are also engaging more frequently with AI technologies. Based on previous theories about teenage education and psychology, this study investigates the correlation between teenagers' psychological characteristics and their trust in AI chatbots, examining four key variables: AI literacy, ego identity, social anxiety, and psychological resilience. We adopted a mixed-methods approach, combining an online survey with semi-structured interviews. Our findings reveal that psychological resilience is a significant positive predictor of trust in AI, and that age significantly moderates the relationship between social anxiety and trust. The interviews further suggest that teenagers generally report relatively high levels of trust in AI, tend to overestimate their AI literacy, and are influenced by external factors such as social media.

</details>


### [170] [COIVis: Eye tracking-based Visual Exploration of Concept Learning in MOOC Videos](https://arxiv.org/abs/2512.06834)
*Zhiguang Zhou,Ruiqi Yu,Yuming Ma,Hao Ni,Guojun Li,Li Ye,Xiaoying Wang,Yize Li,Yong Wang*

Main category: cs.HC

TL;DR: COIVis是一个基于眼动追踪的可视化分析系统，用于分析MOOC视频学习过程中的概念级认知状态，帮助教师理解学习模式并提供个性化干预。


<details>
  <summary>Details</summary>
Motivation: MOOC虽然提供了高质量教学资源，但缺乏面对面互动，教师难以获取学习者实时反馈。传统的点击流或测验分数只能捕捉粗粒度学习结果，无法深入了解学习者时刻变化的认知状态。

Method: 从多模态视频内容中提取课程概念，将其与讲座的时间结构和屏幕空间对齐，定义兴趣概念(COIs)。将学习者注视轨迹转化为COI序列，基于眼动指标计算五个可解释的学习者状态特征：注意力、认知负荷、兴趣、偏好和同步性。提供叙事性多视图可视化，支持从群体概览到个体学习路径的探索。

Result: 通过两个案例研究和深入的用户反馈访谈评估，COIVis能够有效为教师提供关于学习者学习模式一致性和异常性的宝贵见解，支持及时个性化干预并优化教学设计。

Conclusion: COIVis系统通过眼动追踪和可视化分析，解决了MOOC环境中教师难以获取学习者实时认知状态反馈的问题，为个性化教学干预和教学设计优化提供了有效工具。

Abstract: Massive Open Online Courses (MOOCs) make high-quality instruction accessible. However, the lack of face-to-face interaction makes it difficult for instructors to obtain feedback on learners' performance and provide more effective instructional guidance. Traditional analytical approaches, such as clickstream logs or quiz scores, capture only coarse-grained learning outcomes and offer limited insight into learners' moment-to-moment cognitive states. In this study, we propose COIVis, an eye tracking-based visual analytics system that supports concept-level exploration of learning processes in MOOC videos. COIVis first extracts course concepts from multimodal video content and aligns them with the temporal structure and screen space of the lecture, defining Concepts of Interest (COIs), which anchor abstract concepts to specific spatiotemporal regions. Learners' gaze trajectories are transformed into COI sequences, and five interpretable learner-state features -- Attention, Cognitive Load, Interest, Preference, and Synchronicity -- are computed at the COI level based on eye tracking metrics. Building on these representations, COIVis provides a narrative, multi-view visualization enabling instructors to move from cohort-level overviews to individual learning paths, quickly locate problematic concepts, and compare diverse learning strategies. We evaluate COIVis through two case studies and in-depth user-feedback interviews. The results demonstrate that COIVis effectively provides instructors with valuable insights into the consistency and anomalies of learners' learning patterns, thereby supporting timely and personalized interventions for learners and optimizing instructional design.

</details>


### [171] [Robots with Attitudes: Influence of LLM-Driven Robot Personalities on Motivation and Performance](https://arxiv.org/abs/2512.06910)
*Dennis Becker,Kyra Ahrens,Connor Gäde,Erik Strahl,Stefan Wermter*

Main category: cs.HC

TL;DR: 研究探索大型语言模型在机器人个性建模中的应用，以及亲和与非亲和个性在合作任务中的影响。结果显示亲和型机器人更受欢迎，且亲和开放型机器人能提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型对机器人个性建模的影响，以及亲和与非亲和个性在合作任务中的作用，了解个性特征如何影响人机交互效果。

Method: 采用两部分研究设计：在线预研究用于个性验证，实验室主研究评估亲和与非亲和个性对喜爱度、动机和任务表现的影响。

Result: 亲和型机器人显著提高喜爱度；两种个性类型在内在动机上无显著差异；亲和且开放新经验的机器人能提升任务表现。

Conclusion: 大型语言模型可用于定制化机器人个性建模，精心选择的亲和型机器人个性能够积极影响人类感知，并在合作场景中带来更大成功。

Abstract: Large language models enable unscripted conversations while maintaining a consistent personality. One desirable personality trait in cooperative partners, known to improve task performance, is agreeableness. To explore the impact of large language models on personality modeling for robots, as well as the effect of agreeable and non-agreeable personalities in cooperative tasks, we conduct a two-part study. This includes an online pre-study for personality validation and a lab-based main study to evaluate the effects on likability, motivation, and task performance. The results demonstrate that the robot's agreeableness significantly enhances its likability. No significant difference in intrinsic motivation was observed between the two personality types. However, the findings suggest that a robot exhibiting agreeableness and openness to new experiences can enhance task performance. This study highlights the advantages of employing large language models for customized modeling of robot personalities and provides evidence that a carefully chosen agreeable robot personality can positively influence human perceptions and lead to greater success in cooperative scenarios.

</details>


### [172] [Human Agency and Creativity in AI-Assisted Learning Environments](https://arxiv.org/abs/2512.07117)
*Yun Dai*

Main category: cs.HC

TL;DR: 本章通过学生能动性视角探讨AI辅助学习环境中的人类创造力，分析了四种能动性理论视角及其与创造力的关系，并提出了AI能动参与的理论框架，强调从产品导向转向过程导向的创造力理解。


<details>
  <summary>Details</summary>
Motivation: 探索在生成式AI工具日益普及的教育环境中，如何理解学生能动性与创造力的关系，以及AI如何重塑学生在认知、社交和创造性过程中的角色。

Method: 首先分析四种能动性理论视角（工具性、努力性、动态涌现性、作者性），探讨每种视角下能动性与创造力的关系；然后提出AI能动参与的理论框架，将其置于生成式AI引入的认知、关系和伦理动态中；最后将框架与Mini-c创造力概念联系起来。

Result: 提出了一个理论框架，将能动性置于生成式AI工具引入的具体认知、关系和伦理动态中，支持从产品导向的创造力观转向过程导向的创造力观，强调能动参与和意义建构。

Conclusion: AI辅助学习环境中的创造力应被理解为能动参与和意义建构的过程，而非仅仅是产品导向的结果；未来研究应关注AI辅助学习中的创造性过程和表现。

Abstract: This chapter explores human creativity in AI-assisted learning environments through the lens of student agency. We begin by examining four theoretical perspectives on agency, including instrumental, effortful, dynamically emergent, and authorial agency, and analyze how each frames the relationship between agency and creativity. Under each theoretical perspective, we discuss how the integration of generative AI (GenAI) tools reshapes these dynamics by altering students' roles in cognitive, social, and creative processes. In the second part, we introduce a theoretical framework for AI agentic engagement, contextualizing agency within specific cognitive, relational, and ethical dynamics introduced by GenAI tools. This framework is linked to the concept of Mini-c creativity, emphasizing personal relevance and self-directed learning. Together, these perspectives support a shift from viewing creativity as product-oriented to understanding it as a process of agentive participation and meaning-making. We conclude with two directions for future research focused on the creative process and performance in AI-assisted learning.

</details>


### [173] [A Theoretical Framework of Student Agency in AI- Assisted Learning: A Grounded Theory Approach](https://arxiv.org/abs/2512.07143)
*Yun Dai,Sichen Lai*

Main category: cs.HC

TL;DR: 该研究探讨了高等教育学生在AI辅助学习环境中如何行使主体性，提出了一个基于实证数据的理论框架，将学生主体性描述为主动、有意、适应、反思和迭代的过程。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育中具有巨大潜力，但其价值取决于学生能否主动、负责和批判性地参与，这些品质与学生主体性密切相关。然而，学生主体性在教育话语中长期是一个复杂且模糊的概念，缺乏在AI辅助学习环境中澄清其独特性质和过程的实证研究。

Method: 采用质性研究方法，基于扎根理论方法构建理论框架。研究以主体性参与理论为指导，通过分析26名学生的真实体验数据，包括他们的生成式AI对话记录和捕捉其思维过程和决策的认知访谈。

Result: 研究发现学生主体性的四个关键方面：发起和（重新）引导、有意识的采纳、外部求助和反思学习。这些方面共同形成了一个经验性框架，将AI辅助学习中的学生主体性描述为主动、有意、适应、反思和迭代的过程。

Conclusion: 基于实证发现，研究讨论了理论意义和实践意义，为研究人员、教育工作者和政策制定者提供了关于如何在AI辅助学习环境中培养和支持学生主体性的见解。

Abstract: Generative AI(GenAI) is a kind of AI model capable of producing human-like content in various modalities, including text, image, audio, video, and computer programming. Although GenAI offers great potential for education, its value often depends on students' ability to engage with it actively, responsibly, and critically - qualities central to student agency. Nevertheless, student agency has long been a complex and ambiguous concept in educational discourses, with few empirical studies clarifying its distinct nature and process in AI-assisted learning environments. To address this gap, the qualitative study presented in this article examines how higher education students exercise agency in AI-assisted learning and proposes a theoretical framework using a grounded theory approach. Guided by agentic engagement theory, this article analyzes the authentic experiences of 26 students using data from their GenAI conversation records and cognitive interviews that capture their thought processes and decision-making. The findings identify four key aspects of student agency: initiating and (re)directing, mindful adoption, external help-seeking, and reflective learning. Together, these aspects form an empirically developed framework that characterizes student agency in AI-assisted learning as a proactive, intentional, adaptive, reflective, and iterative process. Based on the empirical findings, theoretical and practical implications are discussed for researchers, educators, and policymakers.

</details>


### [174] [Size Matters: The Impact of Avatar Size on User Experience in Healthcare Applications](https://arxiv.org/abs/2512.07357)
*Navid Ashrafi,Francesco Vona,Sina Hinzmann,Juliane Henning,Maurizio Vergari,Maximilian Warsinke,Catarina Pinto Moreira,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: 研究探索虚拟化身尺寸对医疗应用用户体验的影响，发现中等尺寸化身在吸引力和清晰度方面评分最高，社交存在感与刺激性和吸引力强相关，且存在性别偏好差异。


<details>
  <summary>Details</summary>
Motivation: 虽然虚拟化身在医疗应用中越来越普及，但社交距离和化身尺寸等关键因素尚未得到充分研究。本研究旨在探索用户在与不同尺寸虚拟化身交互时的体验和偏好。

Method: 研究让23名参与者与5种不同尺寸的虚拟化身交互（真人尺寸和4个较小尺寸，随机顺序呈现）。化身与AI聊天机器人完全集成，投影在参与者面前的墙上。参与者需评估每个化身后的系统可用性，并完成关于信任和社交存在感的问卷调查。

Result: 化身尺寸显著影响感知吸引力和清晰度，中等尺寸化身获得最高评分。社交存在感与刺激性和吸引力强相关，表明化身的视觉吸引力和交互性比物理尺寸更能影响用户参与度。此外，在UEQ+量表上观察到性别差异趋势：男性倾向于偏好真人尺寸，女性则稍偏好较小化身。

Conclusion: 化身设计和表现形式对于优化虚拟医疗环境的用户体验和信任至关重要。中等尺寸化身在吸引力和清晰度方面表现最佳，而社交存在感主要受视觉吸引力和交互性影响，而非物理尺寸。性别偏好差异也值得在设计中考虑。

Abstract: The usage of virtual avatars in healthcare applications has become widely popular; however, certain critical aspects, such as social distancing and avatar size, remain insufficiently explored. This research investigates user experience and preferences when interacting with a healthcare application utilizing virtual avatars displayed in different sizes. For our study, we had 23 participants interacting with five different avatars (a human-size avatar followed by four smaller avatars in a randomized order) varying in size, projected on a wall in front of them. The avatars were fully integrated with an artificial intelligence chatbot to make them conversational. Users were asked to rate the usability of the system after interacting with each avatar and complete a survey regarding trust and an additional questionnaire on social presence. The results of this study show that avatar size significantly influences the perceived attractiveness and perspicuity, with the medium-sized avatars receiving the highest ratings. Social presence correlated strongly with stimulation and attractiveness, suggesting that an avatar's visual appeal and interactivity influenced user engagement more than its physical size. Additionally, we observed a tendency for gender-specific differences on some of the UEQ+ scales, with male participants tending to prefer human-sized representations, while female participants slightly favored smaller avatars. These findings highlight the importance of avatar design and representation in optimizing user experience and trust in virtual healthcare environments.

</details>


### [175] [The Impact of Spatial Misalignment and Time Delay on Collaborative Presence in Augmented Reality](https://arxiv.org/abs/2512.07363)
*Michael Stern,Maurizio Vergari,Julia Schorlemmer,Francesco Vona,David Grieshammer,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: 研究探讨了协作AR中物体错位和时间延迟对用户体验和任务负荷的影响，发现空间错位显著增加工作负荷并降低用户体验质量，而时间延迟影响相对有限。


<details>
  <summary>Details</summary>
Motivation: 在协作AR环境中，精确的时间和空间对齐对于用户协调行动至关重要。系统延迟和物体错位会破坏沟通、降低任务效率并影响用户体验。先前研究主要关注个体AR交互，而这些不一致性对协作的影响尚未得到充分探索。

Method: 研究通过实验方法，招募32名参与者组成16对，在共享AR空间中完成空间放置任务。实验设置了6种条件，操纵物体对齐（完美对齐vs.随机错位）和时间延迟（0s、0.1s、0.4s）。错位通过在每条轴上随机偏移±20cm实现。参与者交替担任领导者（提供口头放置指令）和建造者（执行放置）角色。使用NASA-TLX评估任务负荷，UEQ评估用户体验质量。

Result: 结果显示，空间错位显著增加了感知工作负荷（NASA-TLX），并降低了用户体验问卷中的实用质量和吸引力评分。相比之下，时间延迟的影响较为有限。这表明在协作AR环境中，空间准确性比时间延迟对协作质量的影响更为关键。

Conclusion: 空间准确性在维持AR协作质量中起着关键作用。物体错位会显著增加用户的工作负荷并降低协作体验，而时间延迟的影响相对较小。这些发现强调了在协作AR系统中确保空间对齐的重要性，为AR系统设计提供了重要指导。

Abstract: Precise temporal and spatial alignment is critical in collaborative Augmented Reality (AR) where users rely on shared visual information to coordinate actions. System latency and object misalignment can disrupt communication, reduce task efficiency, and negatively impact the overall user experience. While previous research has primarily focused on individual AR interactions, the impact of these inconsistencies on collaboration remains underexplored. This article investigates how user experience and task load are affected by object misalignment and time delay in a shared AR space. To examine these factors, we conducted an experiment with 32 participants, organized into 16 pairs, who collaboratively completed a spatial placement task. Within each condition, both participants alternated roles, taking turns as the leader-providing verbal placement instructions-and the builder-executing the placement. Six conditions were tested, manipulating object alignment (perfectly aligned vs. randomly misaligned) and time delay (0s, 0.1s, 0.4s). The misalignment was applied randomly to each virtual object with a shift of +-20 cm on every axis to create a clear distinction in spatial perception. User experience and task load were assessed to evaluate how these factors influence collaboration and interaction in AR environments. Results showed that spatial misalignment significantly increased perceived workload (NASA-TLX) and lowered user ratings in Pragmatic quality and Attractiveness (UEQ), while time delay had a more limited effect. These findings highlight the critical role of spatial accuracy in maintaining collaboration quality in AR.

</details>


### [176] [Breaking Players' expectations: the Role of Non-player Characters' coherence and Consistency](https://arxiv.org/abs/2512.07388)
*Remi Poivet,Catherine Pelachaud,Malika Auvray*

Main category: cs.HC

TL;DR: 研究探讨了视频游戏中非玩家角色(NPC)的一致性与连贯性设计如何影响玩家对其智能性和可信度的评估，通过两个实验发现打破玩家初始期望会影响NPC评价


<details>
  <summary>Details</summary>
Motivation: NPC在塑造玩家游戏体验中起关键作用，但其一致性与连贯性设计如何影响玩家评价尚不明确。研究旨在填补这一知识空白，探索NPC设计对玩家期望和评估的影响。

Method: 在军事射击游戏背景下进行两个实验：实验一研究打破玩家初始期望对NPC评价的影响；实验二结合问卷、行为和生理测量，探究NPC一致性与连贯性对玩家期望和评估的影响。

Result: 打破玩家初始期望会影响对NPC的评估；一致且连贯的设计会强化玩家期望，而不连贯的设计则会挑战玩家期望。

Conclusion: NPC的一致性与连贯性设计显著影响玩家对其智能性和可信度的评价，这对游戏设计中NPC的优化具有重要指导意义。

Abstract: In video games, non-player characters (NPCs) play a pivotal role in shaping players' experiences. The design of these characters, encompassing their appearance and behaviors, can be manipulated in terms of coherence and consistency to maintain players' expectations or, on the contrary, to surprise them. The extent to which NPCs' coherence and consistency influence players' evaluation of them remains to be unveiled. To address this knowledge gap, two experiments were conducted in the context of a military shooter game. Players' evaluations of NPCs' perceived intelligence and believability were measured, as these two dimensions are fundamental to players' adoption of NPCs and subsequent commitment to them. The first experiment investigated the impact of disrupting players' initial expectations on their evaluations of NPCs. The second experiment focused on the influence of NPCs' coherence and consistency on both players' expectations and evaluation of NPCs, using a combination of questionnaires and behavioral and physiological measures. The results of our study show that disrupting players' initial expectations influences their assessment of NPCs, with coherent and consistent design reinforcing expectations and incoherent design challenging them.

</details>


### [177] [SemanticTours: A Conceptual Framework for Non-Linear, Knowledge Graph-Driven Data Tours](https://arxiv.org/abs/2512.07483)
*Daniel Fürst,Matthijs Jansen op de Haar,Mennatallah El-Assady,Daniel A Keim,Maximilian T. Fischer*

Main category: cs.HC

TL;DR: 本文提出SemanticTours，一种基于语义图模型的交互式导览系统，用于支持法律等知识密集型领域的分析推理，相比传统线性导览提供更灵活的图导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有交互式导览框架通常采用线性序列，不允许在刚性序列之外进行分支和假设细化，这在法律等知识密集型领域存在局限性。律师进行案件分析时需要迭代权衡不同法律规范并构建论证链，需要更灵活的非线性导航支持。

Method: 提出SemanticTours语义图模型，从基于序列转向基于图的导航。构建领域特定知识图，基于用户可定义的语义关系连接数据元素，这些关系支持定义导览的非线性图导航。应用于法律领域，设计了支持分析推理的可视化分析和交互概念，使用聚合图节点和语义镜头支持导航。

Result: 对六位法律领域专家进行评估，专家认为基于图的导览比序列导览更好地支持他们的分析推理。系统能够处理图导览的固有复杂性。

Conclusion: SemanticTours为法律和其他知识密集型领域支持分析推理的导览研究开辟了新机会，图导航比线性序列更适合复杂的分析推理任务。

Abstract: Interactive tours help users explore datasets and provide onboarding. They rely on a linear sequence of views, showing a curated set of relevant data selections and introduce user interfaces. Existing frameworks of tours, however, often do not allow for branching and refining hypotheses outside of a rigid sequence, which is important in knowledge-centric domains such as law. For example, lawyers performing analytical case analysis need to iteratively weigh up different legal norms and construct strings of arguments. To address this gap, we propose SemanticTours, a semantic, graph-based model of tours that shifts from a sequence-based towards a graph-based navigation. Our model constructs a domain-specific knowledge graph that connects data elements based on user-definable semantic relationships. These relationships enable non-linear graph navigation that defines tours. We apply SemanticTours to the domain of law and conceptualize a visual analytics design and interaction concept for analytical reasoning in legal case analysis. Our concept accounts for the inherent complexity of graph-based tours using aggregated graph nodes and supporting navigation with a semantic lens. During an evaluation with six domain experts from law, they suggest that graph-based tours better support their analytical reasoning than sequences. Our work opens research opportunities for such tours to support analytical reasoning in law and other knowledge-centric domains.

</details>


### [178] [A Retrospective on Ultrasound Mid-Air Haptics in HCI](https://arxiv.org/abs/2512.07613)
*Arthur Fleig*

Main category: cs.HC

TL;DR: 本文回顾了2013年UltraHaptics系统开创的超声空中触觉技术，追溯了该技术的研究发展轨迹，探讨了其在多感官交互、沉浸感和包容性方面的应用价值。


<details>
  <summary>Details</summary>
Motivation: 回顾超声空中触觉技术的开创性工作，追踪该技术在过去十年中的研究发展轨迹，反思其在人机交互领域的贡献和价值。

Method: 采用回顾性分析的方法，重新审视2013年的UltraHaptics系统，追溯该技术引发的后续研究路径，分析其在多感官交互、沉浸感和包容性方面的应用发展。

Result: 超声空中触觉技术已成为可行的人机交互模态，为后续研究奠定了技术和感知基础，促进了多感官交互、沉浸式体验和包容性设计的发展。

Conclusion: 超声空中触觉技术的研究展示了跨学科合作在推动新型交互技术发展中的价值，该技术为创造更丰富、包容的人机交互体验提供了重要支持。

Abstract: In 2013, the UltraHaptics system demonstrated that focused ultrasound could generate perceivable mid-air tactile sensations, building on earlier explorations of airborne ultrasound as a haptic medium. These contributions established ultrasound mid-air haptics (UMH) as a viable interaction modality and laid the technical and perceptual foundations for subsequent advances in Human-Computer Interaction (HCI). In this extended abstract, we revisit this formative work, trace the research and design trajectories it enabled, and reflect on how UMH has supported multisensory interaction, immersion, and inclusion. We also highlight how this line of research exemplifies the value of interdisciplinary collaboration to advance novel interactive technologies.

</details>


### [179] [Context-Adaptive Color Optimization for Web Accessibility: Balancing Perceptual Fidelity and Functional Requirements](https://arxiv.org/abs/2512.07623)
*Lalitha A R*

Main category: cs.HC

TL;DR: 该研究扩展了基于OKLCH的色彩可访问性优化方法，通过引入上下文自适应约束策略，在保持品牌色彩特性的同时，显著提高了色彩对比度优化的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统的严格算法（ΔE ≤ 5.0）虽然能保持品牌色彩保真度，但成功率仅为66-77%，无法满足更广泛的网页开发者需求，他们需要在无法满足严格感知约束时仍能获得可访问性解决方案。

Method: 提出两种模式：Mode~1采用递归优化，通过迭代中的小调整累积效果；Mode~2作为宽松回退模式处理极端情况。结合上下文感知约束松弛和绝对色调保持技术，在CM-Colors v0.5.0工具中实现。

Result: 在10,000个真实网页色彩对测试中，Mode~1在所有色彩对上达到93.68%成功率，在合理色彩对（对比度ρ>2.0）上达到100%成功率，比原方法提高27.23个百分点。Mode~2整体成功率达98.73%。中位感知变化保持为零，90百分位ΔE2000=15.55。

Conclusion: 上下文感知约束松弛结合绝对色调保持技术，能够在保持品牌色彩特性的同时实现实际可访问性合规，为开发者提供了可访问性与保真度权衡的明确控制，已在实际工具中得到部署应用。

Abstract: We extend our OKLCH-based accessibility optimization with context-adaptive constraint strategies that achieve near-universal success rates across diverse use cases. Our original strict algorithm reached 66-77% success by prioritizing minimal perceptual change ($ΔE \leq 5.0$), optimizing for enterprise contexts where brand fidelity is paramount. However, this one-size-fits-all approach fails to serve the broader ecosystem of web developers who need accessible solutions even when strict perceptual constraints cannot be satisfied. We introduce recursive optimization (Mode~1) that compounds small adjustments across iterations, achieving 93.68% success on all color pairs and 100% success on reasonable pairs (contrast ratio $ρ> 2.0$), representing a +27.23 percentage point improvement. A relaxed fallback mode (Mode~2) handles pathological edge cases, reaching 98.73% overall success. Evaluation on 10,000 realistic web color pairs demonstrates that context-aware constraint relaxation, combined with absolute hue preservation, enables practical accessibility compliance while maintaining brand color identity. The median perceptual change remains zero across all modes (most pairs already comply), while the 90th percentile reaches $ΔE_{2000} = 15.55$ in Mode~1 -- perceptually acceptable when hue invariance preserves the essential character of the original color. The approach is deployed in CM-Colors v0.5.0 (800+ monthly downloads), providing developers with explicit control over the accessibility-fidelity trade-off appropriate to their context.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [180] [Fast and Robust T1 Mapping Based on a 3D Dual-Echo UTE Sequence (PETALUTE) for SPION Biodistribution Assessment](https://arxiv.org/abs/2512.06237)
*Zhen Jiang,Stephen Sawiak,Alexandra Lipka,Xin Shen,Uzay Emir,Ali Özen,Mark Chiew,Justin Geise,Joseph Speth,Deng-Yuan Chang,Jessica Veenstra,Mitchell Gabalski,Luis Solorio,Gregory Tamer,Matthew Scarpelli*

Main category: physics.med-ph

TL;DR: 开发了基于PETALUTE序列的快速T1定量成像方法，用于超顺磁性氧化铁纳米颗粒的定量检测，相比传统方法具有更快的扫描速度、更广的浓度范围和更好的线性相关性。


<details>
  <summary>Details</summary>
Motivation: 超顺磁性氧化铁纳米颗粒（SPIONs）如ferumoxytol是有前景的诊疗一体化剂，可通过MRI检测。传统的弛豫时间映射方法存在磁化率伪影、长回波时间和扫描时间长等问题，限制了准确量化。需要开发快速、准确的定量成像方法。

Method: 开发了基于PETALUTE序列的快速T1定量成像协议，这是一种3D双回波超短回波MRI序列，采用玫瑰花形k空间轨迹和可变翻转角采集。使用含0-5000 ppm ferumoxytol的琼脂糖模型在7T MRI上进行扫描，并与供应商提供的RARE-VTR方法比较。通过两个翻转角（4度和20度）计算T1图，分析R1值与ferumoxytol浓度的相关性。在体实验中，对携带4T1乳腺肿瘤和侧腹肿瘤的小鼠在注射后24小时进行扫描。

Result: PETALUTE对所有浓度（除5000 ppm外）产生正对比，而RARE-VTR没有。PETALUTE显示R1与ferumoxytol浓度之间存在显著线性相关（R=0.975, p<0.01），而RARE-VTR相关性较差（R=0.672, p=0.144）。在体实验中，PETALUTE在4分19秒内完成高分辨率全腹部成像。注射ferumoxytol的小鼠侧腹肿瘤显示T1缩短，与铁摄取一致；在一个乳腺肿瘤中观察到T1值升高但T2*加权信号保留。

Conclusion: 基于PETALUTE的T1定量成像方法提供了快速、定量、正对比的ferumoxytol成像，相比传统RARE-VTR方法具有更大的空间覆盖范围和更宽的有效浓度范围，为SPIONs的定量检测提供了更好的工具。

Abstract: Superparamagnetic iron oxide nanoparticles (SPIONs) such as ferumoxytol are promising theranostic agents detectable with MRI. Relaxation time mapping offers reproducible, quantitative biomarkers of SPION distribution, but conventional methods suffer from susceptibility artifacts, long echo times, and extended scan durations, limiting accurate quantification. This study developed a fast, B1-corrected T1-mapping protocol using PETALUTE, a 3D dual-echo ultrashort-echo MRI sequence with a rosette k-space trajectory and variable flip-angle acquisition for quantitative ferumoxytol imaging. Agarose phantoms containing 0-5000 ppm ferumoxytol were scanned at 7T with PETALUTE and vendor-supplied RARE-VTR. PETALUTE T1 maps were derived from two flip angles (4 deg and 20 deg), and mean R1 values were correlated with ferumoxytol concentration. For in vivo feasibility, mice bearing 4T1 mammary and flank tumors were scanned 24 h post-injection (ferumoxytol: n=2, 40 mg/kg; control: n=1). Regions of interest in muscle and tumors were analyzed to compare T1 and R1 values obtained with both methods. PETALUTE produced positive contrast for all phantom concentrations except 5000 ppm, whereas RARE-VTR did not. PETALUTE demonstrated a significant linear correlation between R1 and ferumoxytol concentration (R=0.975, p<0.01), in contrast to RARE-VTR (R=0.672, p=0.144). In vivo, PETALUTE enabled high-resolution, whole-abdominal imaging in 4 min 19 s. Ferumoxytol-injected mice showed T1 shortening in flank tumors, consistent with iron uptake, and PETALUTE revealed elevated T1 value with preserved T2*-weighted signal in one mammary tumor. PETALUTE-based T1 mapping provides fast, quantitative, positive-contrast ferumoxytol imaging with greater spatial coverage and a wider usable concentration range than conventional RARE-VTR.

</details>


### [181] [Alterations of brain tissue structural complexity and disorder in Parkinson disease (PD): Fractal, multifractal, fractal transformation, and disorder strength analyses](https://arxiv.org/abs/2512.06326)
*Santanu Maity,Mousa Alrubayan,Mohammad Moshahid Khan,Prabhakar Pradhan*

Main category: physics.med-ph

TL;DR: 该研究使用分形、多重分形和分形功能分布方法分析帕金森病患者脑组织的结构复杂性变化，发现多种分布指标的显著偏差，为疾病定量分期和诊断提供新方法。


<details>
  <summary>Details</summary>
Motivation: 帕金森病存在进行性神经退行性变，但早期和细微的结构改变难以通过传统成像和分析方法检测。分形和多重分形框架能够量化复杂的生物结构，但其在帕金森病诊断中的应用尚未充分探索。

Method: 结合传统分形和多重分形分析，采用新开发的分形功能分布方法（将分布转换为高斯形式以增强量化），并应用逆参与比分析评估结构紊乱。这些方法共同分析帕金森病患者脑组织的结构复杂性。

Result: 发现帕金森病样本在多种分布指标上存在显著偏差；多重分形分析显示基于强度的测量存在阈值依赖性变化，与神经组织的稀疏性和异质性相关；逆参与比分析表明较大的像素尺寸与疾病进展中结构复杂性增加相关。

Conclusion: 这些互补分析构建了帕金森病相关组织破坏的多层次定量特征，为早期、客观评估疾病相关微结构变化奠定了基础，具有定量分期和诊断应用的潜力。

Abstract: Parkinson disease (PD) is marked by progressive neurodegeneration, yet early and subtle structural alterations in brain tissue remain difficult to detect with conventional imaging and analytical methods. Fractal and multifractal frameworks offer a principled way to quantify complex biological architecture, but their diagnostic utility in PD has been largely unexplored. In this study, we investigated the fractal and multifractal characteristics of human brain tissues to identify structural alterations associated with PD. Alongside conventional fractal and multifractal analysis, we employed a recently developed fractal functional distribution method that transforms distributions into a Gaussian form, thereby enhancing quantification. Using this combined approach, we found notable deviations across multiple distribution metrics in PD samples, offering potential for quantitative staging and diagnostic applications. The multifractal analysis revealed threshold-dependent variations in intensity-based measures, which are linked to the sparsity and heterogeneity of neural tissue and suggestive of potential biomarker value. Additionally, we applied inverse participation ratio (IPR) analysis to assess structural disorder, demonstrating that larger IPR pixel sizes correlate with increased structural complexity during disease progression. These complementary analyses outline a multi-layered quantitative profile of PD-related tissue disruption, offering a foundation for earlier, objective assessment of disease-associated microstructural change.

</details>


### [182] [Optimal experimental design with k-space data: application to inverse hemodynamics](https://arxiv.org/abs/2512.06712)
*Miriam Löcke,Ahmed Attia,Dariusz Ucínski,Cristóbal Bertoglio*

Main category: physics.med-ph

TL;DR: 本文提出了一个用于k空间MRI测量的最优实验设计框架，旨在寻找从k空间直接估计特定参数的最优采样掩码，相比传统采样模式能提高参数估计精度并减少方差，实现10倍采集时间加速。


<details>
  <summary>Details</summary>
Motivation: 获取高分辨率、高保真度的4D Flow MRI数据成本高且对患者负担大，因此需要利用高度欠采样数据来减少采集时间。现有采样掩码选择对逆问题解的性能和质量有显著影响，但如何为特定推断参数选择最佳采样模式仍不明确。

Method: 提出了一个用于k空间MRI测量的最优实验设计（OED）框架，将OED技术应用于传感器放置问题，寻找从k空间直接估计特定参数的最优采样掩码。

Result: OED优化的掩码在参数估计精度和方差方面持续优于传统采样模式，能够在保持精度的同时实现10倍的采集时间加速。

Conclusion: 这是首次将OED技术应用于k空间MRI测量场景，提出的框架能够有效优化采样掩码，显著提高参数估计性能并大幅减少数据采集时间。

Abstract: Subject-specific cardiovascular models rely on parameter estimation using measurements such as 4D Flow MRI data. However, acquiring high-resolution, high-fidelity functional flow data is costly and taxing for the patient. As a result, there is growing interest in using highly undersampled MRI data to reduce acquisition time and thus the cost, while maximizing the information gain from the data. Examples of such recent work include inverse problems to estimate boundary conditions of aortic blood flow from highly undersampled k-space data. The undersampled data is selected based on a predefined sampling mask which can significantly influences the performance and the quality of the solution of the inverse problem. While there are many established sampling patterns to collect undersampled data, it remains unclear how to select the best sampling pattern for a given set of inference parameters. In this paper we propose an Optimal Experimental Design (OED) framework for MRI measurements in k-space, aiming to find optimal masks for estimating specific parameters directly from k-space. As OED is typically applied to sensor placement problems in spatial locations, this is, to our knowledge, the first time the technique is used in this context. We demonstrate that the masks optimized by employing OED consistently outperform conventional sampling patterns in terms of parameter estimation accuracy and variance, facilitating a speed-up of 10x of the acquisition time while maintaining accuracy.

</details>


### [183] [Alterations of brain tissue structural complexity and disorder in Alzheimer's disease (AD): Fractal, multifractal, fractal transformation, and disorder strength analyses](https://arxiv.org/abs/2512.07061)
*Santanu Maity,Mousa Alrubayan,Mohammad Moshahid Khan,Prabhakar Pradhan*

Main category: physics.med-ph

TL;DR: 该研究开发了一个结合分形、多重分形分析和光定位技术的多参数框架，用于量化阿尔茨海默病患者脑组织的微观结构变化，为早期诊断提供新方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的特征是脑组织进行性微观结构恶化，但传统成像和组织病理学方法缺乏检测早期细微变化的敏感性。需要开发更敏感的量化和检测方法。

Method: 1. 结合分形和多重分形分析及其分布来量化脑组织结构变化；2. 引入创新的分形功能分布方法，将分形分布转换为高斯形式；3. 使用已建立的光定位技术——逆参与比分析来研究结构无序性和复杂性。

Result: 1. 分形分布参数在统计上易于解释，能够区分对照组织和病变组织；2. 发现分形和多重分形参数具有明显的阈值依赖性行为，反映了脑组织固有的稀疏性和异质性强度景观；3. IPR分析显示，随着像素尺寸增加，结构改变随疾病进展而加剧；4. 这些阈值敏感特征为定量阶段检测提供了框架，可作为早期病理转变的生物标志物。

Conclusion: 这些综合分析建立了一个稳健的多尺度定量框架，用于检测阿尔茨海默病的微观结构改变，为早期诊断和改进的病理评估提供了有前景的基础。

Abstract: Alzheimer's disease (AD) is characterized by progressive microstructural deterioration in brain tissue, yet conventional imaging and histopathology often lack the sensitivity needed to detect subtle early-stage changes. Here, we present a multiparametric framework combining fractal and multifractal analysis and their distributions to quantify structural alterations in human brain tissue affected by AD. Moreover, from the fractal and multifractal formalism, we introduced an innovative fractal functional distribution method, a novel technique that transforms fractal distribution into a Gaussian form. Statistically, these distribution parameters are easy to interpret and can distinguish between control and diseased tissues. Across samples, we identify pronounced threshold-dependent behavior of fractal and multifractal parameters, reflecting the intrinsic sparsity and heterogeneous intensity landscape of brain tissue. These threshold-sensitive signatures provide a framework for quantitative stage detection and may serve as biomarkers for early pathological transitions. In addition, we studied structural disorder and complexity using our established light localization technique, inverse participation ratio (IPR) analysis. IPR-based analysis demonstrates that increasing IPR pixel size highlights the elevation of structural alterations with disease progression. Together, these integrative analyses establish a robust, multi-scale quantitative framework for detecting microstructural alterations in AD, providing a promising foundation for early diagnosis and improved pathological assessment.

</details>


### [184] [Quantitative Characterization of Brain Tissue Alterations in Brain Cancer Using Fractal, Multifractal, and IPR Metrics](https://arxiv.org/abs/2512.07148)
*Mousa Alrubayan,Santanu Maity,Prabhakar Pradhan*

Main category: physics.med-ph

TL;DR: 该研究通过结合分形分析、分形函数变换、多重分形分析和逆参与比分析，开发了一个多参数框架来区分健康与癌变脑组织的微观结构差异，为脑癌早期检测提供了新的诊断方法。


<details>
  <summary>Details</summary>
Motivation: 准确表征脑组织微观结构对于脑癌的早期检测和诊断至关重要。现有方法可能无法充分捕捉组织结构的复杂性，因此需要开发更全面的分析框架来区分健康与癌变组织。

Method: 采用多参数框架结合分形分析、分形函数变换、多重分形分析和逆参与比分析。对明场显微镜图像应用盒计数方法估计分形维数及其对数形式和函数形式，分析组织结构的空间不规则性。

Result: 分形维数及其对数形式显示长尾分布可区分健康与癌变组织，而分形函数变换形式提供显著改善的区分能力。多重分形分析显示癌变样本的f(α) vs α曲线更宽，反映更高异质性。IPR分析显示癌变组织中纳米尺度质量密度变化增加，结构无序性更高。

Conclusion: 结合这些互补方法创建了一个强大的框架来测量组织复杂性，在改善脑癌检测的微观诊断方法方面具有巨大潜力，为早期诊断提供了新的定量分析工具。

Abstract: We studied the structural alterations between healthy and diseased brain tissues using a multiparametric framework combining fractal analysis, fractal functional transformation, multifractal analysis, and the Inverse Participation Ratio (IPR) analysis. Accurate characterization of brain tissue microstructure is crucial for early detection and diagnosis of cancer. By applying box-counting methods on brightfield microscopy images, we estimated the fractal dimension (Df) and its logarithmic (ln(Df)) and functional (ln(Dtf)) forms to highlight spatial irregularities in the tissue architecture. While Df and ln(Df) exhibited long-tailed distributions distinguishing healthy from cancer tissues, ln(Dtf) provided significantly improved differentiation by emphasizing local structural variations. Additionally, multifractal analysis revealed broader f(α) vs α curves in cancerous samples, reflecting higher heterogeneity. IPR analysis based on light localization further demonstrated increased nanoscale variations in mass density, reflecting higher structural disorder in cancer tissues. Combining these complementary approaches creates a robust framework for measuring tissue complexity and holds great potential to improve microscopic diagnostic methods for brain cancer detection.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [185] [POrTAL: Plan-Orchestrated Tree Assembly for Lookahead](https://arxiv.org/abs/2512.06002)
*Evan Conway,David Porfirio,David Chan,Mark Roberts,Laura M. Hiatt*

Main category: cs.RO

TL;DR: POrTAL是一种新的轻量级概率规划算法，结合了FF-Replan和POMCP的优势，在部分可观察环境中能快速找到比基线算法步骤更少的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在人类-机器人交互中，机器人通常只能部分观察世界，需要在不确定性下制定计划。现有的概率规划算法要么在机器人有限计算资源下效率低下，要么需要比预期更多的步骤来实现目标。

Method: 开发了Plan-Orchestrated Tree Assembly for Lookahead (POrTAL)算法，结合了FF-Replan和POMCP两种基线规划算法的优势，形成轻量级概率规划方法。

Result: 在一系列案例研究中，POrTAL能够快速找到解决方案，在步骤数量上优于基线算法。同时展示了POrTAL在不同时间约束下的性能表现。

Conclusion: POrTAL算法在部分可观察环境中提供了一种高效的轻量级概率规划解决方案，能够快速生成步骤更少的计划，适用于计算资源有限的机器人系统。

Abstract: Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.

</details>


### [186] [Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models](https://arxiv.org/abs/2512.06017)
*Laurence Liang*

Main category: cs.RO

TL;DR: 使用前沿视觉语言模型作为现成工具，从单张目标图像估计机器人手臂关节角度，评估了合成和真实世界图像数据上的性能基准


<details>
  <summary>Details</summary>
Motivation: 随着机器人手臂在工业和家庭应用中的普及，可靠的关节角度估计能提高安全性和性能保证，并可作为验证器进一步训练机器人策略

Method: 使用前沿视觉语言模型作为现成工具，从单张目标图像估计机器人手臂关节角度，在合成和真实世界图像数据对上评估性能

Result: 建立了当前前沿视觉语言模型在关节角度预测方面的性能基准，实证结果表明单独增加测试时间缩放或参数缩放并不能改善关节角度预测

Conclusion: 视觉语言模型可作为机器人手臂关节角度估计的有效工具，但需要更复杂的缩放策略来进一步提升性能

Abstract: Pose estimation of a robot arm from visual inputs is a challenging task. However, with the increasing adoption of robot arms for both industrial and residential use cases, reliable joint angle estimation can offer improved safety and performance guarantees, and also be used as a verifier to further train robot policies. This paper introduces using frontier vision-language models (VLMs) as an ``off-the-shelf" tool to estimate a robot arm's joint angles from a single target image. By evaluating frontier VLMs on both synthetic and real-world image-data pairs, this paper establishes a performance baseline attained by current FLMs. In addition, this paper presents empirical results suggesting that test time scaling or parameter scaling alone does not lead to improved joint angle predictions.

</details>


### [187] [Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction](https://arxiv.org/abs/2512.06038)
*Kelsey Fontenot,Anjali Gorti,Iva Goel,Tonio Buonassisi,Alexander E. Siemenn*

Main category: cs.RO

TL;DR: 开发了ASHE系统，通过机器人、双执行器分配器和深度学习计算机视觉实现自动化基板处理和交换，显著提高自驱动实验室中透明基板处理的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室在化学和材料发现中加速了实验流程，但基板处理和重新加载这一关键步骤常被忽视，特别是对于脆弱透明基板的自动化处理存在挑战。

Method: 开发了ASHE系统，结合机器人技术、双执行器分配器和深度学习驱动的计算机视觉，能够检测和纠正脆弱透明基板操作中的错误。

Result: 在130次独立试验中，首次放置准确率达到98.5%，仅发生两次基板错位，且成功检测到错误并自动纠正。

Conclusion: ASHE系统提高了自驱动实验室的自动化能力，通过更准确可靠的基板处理方法，进一步加速了新型化学和材料发现。

Abstract: Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.

</details>


### [188] [WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving](https://arxiv.org/abs/2512.06112)
*Yifang Xu,Jiahao Cui,Feipeng Cai,Zhihao Zhu,Hanlin Shang,Shan Luan,Mingwang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

TL;DR: WAM-Flow：一种基于离散流匹配的视觉-语言-动作模型，用于端到端自动驾驶轨迹规划，通过并行双向去噪实现可调节的计算精度权衡


<details>
  <summary>Details</summary>
Motivation: 传统自回归解码器在轨迹规划中存在计算效率低和顺序生成的限制，需要一种能够并行生成、支持粗到细优化且能平衡计算与精度的新方法

Method: 1. 使用度量对齐数值分词器通过三元边界学习保留标量几何信息；2. 几何感知流目标；3. 模拟器引导的GRPO对齐，集成安全性、自我进度和舒适度奖励；4. 多阶段适配将预训练自回归主干转换为非因果流模型；5. 通过持续多模态预训练增强道路场景能力

Result: 在NAVSIM v1基准测试中，1步推理达到89.1 PDMS，5步推理达到90.3 PDMS，优于自回归和基于扩散的VLA基线，展现了离散流匹配在端到端自动驾驶中的潜力

Conclusion: 离散流匹配为端到端自动驾驶提供了一个有前景的新范式，WAM-Flow通过并行双向去噪实现了计算效率与性能的平衡，为自动驾驶轨迹规划提供了更优的解决方案

Abstract: We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.

</details>


### [189] [GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers](https://arxiv.org/abs/2512.06147)
*Hochul Hwang,Soowan Yang,Jahir Sadik Monon,Nicholas A Giudice,Sunghoon Ivan Lee,Joydeep Biswas,Donghyun Kim*

Main category: cs.RO

TL;DR: GuideNav：基于视觉的教导-重复导航系统，模仿导盲犬训练方式，无需昂贵传感器即可实现公里级路径跟随


<details>
  <summary>Details</summary>
Motivation: 目前针对盲人和低视力人群的移动辅助系统研究虽多，但直接指导机器人导航设计的参考很少。为了填补这一空白，研究团队进行了全面的人类研究，包括访谈导盲犬使用者、白手杖使用者、导盲犬训练师等，并观察导盲犬辅助行走过程。

Method: 1. 先进行形成性研究：访谈26名导盲犬使用者、4名白手杖使用者、9名导盲犬训练师和1名定向行走训练师，观察15+小时的导盲犬辅助行走；2. 基于研究洞察开发GuideNav系统：这是一个仅使用视觉的教导-重复导航系统，模仿导盲犬训练方式；3. 系统构建拓扑路径表示，结合视觉地点识别与时间滤波，使用相对姿态估计器计算导航动作，无需依赖昂贵的LiDAR等传感器。

Result: 1. 在五个室外环境的实地测试中，GuideNav能够持续实现公里级路径跟随，即使在教导和重复运行之间存在明显场景变化的情况下仍保持可靠性；2. 用户研究（3名导盲犬使用者和1名导盲犬训练师）进一步确认了系统的可行性；3. 据研究团队所知，这是首次展示四足移动系统以类似导盲犬的方式检索路径。

Conclusion: GuideNav系统成功展示了仅使用视觉传感器即可实现可靠导航的可能性，为盲人和低视力人群的辅助系统开发提供了新的方向。该系统模仿导盲犬的训练和辅助方式，在复杂室外环境中表现出色，为未来人类中心的辅助系统设计提供了重要参考。

Abstract: While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.

</details>


### [190] [Real-Time Spatiotemporal Tubes for Dynamic Unsafe Sets](https://arxiv.org/abs/2512.06151)
*Ratnangshu Das,Siddhartha Upadhyay,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 提出了一种用于非线性纯反馈系统的实时时空管框架，可在规定时间内完成动态环境中的"到达-避障-停留"任务


<details>
  <summary>Details</summary>
Motivation: 解决非线性纯反馈系统在动态环境中实时完成复杂任务（到达目标、避开障碍、保持停留）的挑战，特别是在系统动力学未知的情况下

Method: 引入实时时空管框架，定义状态空间中随时间变化的球体，其中心和半径在线自适应调整；推导出闭式、无需近似的控制律，将系统输出约束在时空管内

Result: 提供了障碍物避让和按时任务完成的正式保证；通过移动机器人和飞行器的仿真和硬件实验验证了框架的有效性和可扩展性

Conclusion: 所提出的实时时空管框架能够有效处理非线性纯反馈系统在动态杂乱环境中的复杂任务，具有理论保证和实际应用价值

Abstract: This paper presents a real-time control framework for nonlinear pure-feedback systems with unknown dynamics to satisfy reach-avoid-stay tasks within a prescribed time in dynamic environments. To achieve this, we introduce a real-time spatiotemporal tube (STT) framework. An STT is defined as a time-varying ball in the state space whose center and radius adapt online using only real-time sensory input. A closed-form, approximation-free control law is then derived to constrain the system output within the STT, ensuring safety and task satisfaction. We provide formal guarantees for obstacle avoidance and on-time task completion. The effectiveness and scalability of the framework are demonstrated through simulations and hardware experiments on a mobile robot and an aerial vehicle, navigating in cluttered dynamic environments.

</details>


### [191] [REWW-ARM -- Remote Wire-Driven Mobile Robot: Design, Control, and Experimental Validation](https://arxiv.org/abs/2512.06192)
*Takahiro Hattori,Kento Kawaharazuka,Temma Suzuki,Keita Yoneda,Kei Okada*

Main category: cs.RO

TL;DR: 提出了一种新型"远程线缆驱动"系统，通过将电子设备与操作环境隔离，同时保留先进电子控制和驱动能力，扩展了机器人的工作范围。


<details>
  <summary>Details</summary>
Motivation: 电子设备对机器人至关重要，但限制了其可用环境。需要一种方法既能排除操作环境中的电子设备，又能保留先进的电子控制和驱动能力。

Method: 设计了"远程线缆驱动"系统，包括：1) 远程线缆传输机制(RWTM)；2) 无电子设备的远端移动机器人；3) 基于RWTM状态估计提供电子闭环控制的电机单元。开发了概念验证机器人"REWW-ARM"。

Result: 通过多个实验评估了REWW-ARM的机械和控制性能，展示了其在陆地和水下的移动、姿态控制和物体操作能力。

Conclusion: 远程线缆驱动系统有潜力应用于各种类型的机器人，从而扩展其操作范围。

Abstract: Electronic devices are essential for robots but limit their usable environments. To overcome this, methods excluding electronics from the operating environment while retaining advanced electronic control and actuation have been explored. These include the remote hydraulic drive of electronics-free mobile robots, which offer high reachability, and long wire-driven robot arms with motors consolidated at the base, which offer high environmental resistance. To combine the advantages of both, this study proposes a new system, "Remote Wire Drive." As a proof-of-concept, we designed and developed the Remote Wire-Driven robot "REWW-ARM", which consists of the following components: 1) a novel power transmission mechanism, the "Remote Wire Transmission Mechanism" (RWTM), the key technology of the Remote Wire Drive; 2) an electronics-free distal mobile robot driven by it; and 3) a motor-unit that generates power and provides electronic closed-loop control based on state estimation via the RWTM. In this study, we evaluated the mechanical and control performance of REWW-ARM through several experiments, demonstrating its capability for locomotion, posture control, and object manipulation both on land and underwater. This suggests the potential for applying the Remote Wire-Driven system to various types of robots, thereby expanding their operational range.

</details>


### [192] [Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation](https://arxiv.org/abs/2512.06198)
*Oussama Sifour,Soulaimane Berkane,Abdelhamid Tayebi*

Main category: cs.RO

TL;DR: 提出一种单距离辅助导航观测器，仅使用IMU、体坐标系矢量测量和固定锚点距离测量来重建刚体完整状态


<details>
  <summary>Details</summary>
Motivation: 开发轻量级自主导航解决方案，减少传感器需求，仅使用IMU、磁力计和单距离测量实现完整状态估计

Method: 首先构建扩展线性时变系统估计位置、速度和重力方向，然后结合体坐标系矢量测量重建SO(3)上的完整姿态，形成级联观测器架构

Result: 在均匀可观测条件下建立了级联设计的几乎全局渐近稳定性，仿真研究显示在三维轨迹上能准确估计位置、速度和姿态

Conclusion: 单距离辅助是一种轻量级且有效的自主导航模式，仅需最小传感器配置即可实现完整状态重建

Abstract: This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.

</details>


### [193] [Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots](https://arxiv.org/abs/2512.06207)
*Harshil Suthar,Dipankar Maity*

Main category: cs.RO

TL;DR: 多机器人团队在未知环境中协作：空中机器人负责探索建图并通过带宽有限信道向地面机器人传输部分地图信息，以帮助其导航到目标点。


<details>
  <summary>Details</summary>
Motivation: 在带宽受限的通信环境下，需要解决辅助机器人（空中机器人）在同时执行探索/建图任务时，应该传输什么信息、传输多少信息以及何时传输的问题，以优化整体系统性能。

Method: 提出一个框架：1) 基于信息价值(VoI)决定传输什么信息；2) 使用混合整数线性规划(MILP)确定传输多少信息；3) 基于效用分数的环境探索策略获取额外信息；4) 分析通信与运动之间的权衡。

Result: 该框架能够系统地在通信带宽限制下优化信息传输决策，平衡空中机器人的地图数据传输量与地面机器人的导航成本。

Conclusion: 通过信息价值评估、优化传输量和智能探索策略的集成方法，能够在带宽受限的多机器人系统中有效协调探索与通信，实现通信与运动成本的最优权衡。

Abstract: In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.

</details>


### [194] [Safe Model Predictive Diffusion with Shielding](https://arxiv.org/abs/2512.06261)
*Taekyung Kim,Keyvan Majd,Hideki Okamoto,Bardh Hoxha,Dimitra Panagou,Georgios Fainekos*

Main category: cs.RO

TL;DR: Safe MPD：一种无需训练、结合模型扩散框架与安全防护的轨迹规划方法，确保运动学可行性和安全性，避免后处理修正问题，在复杂规划任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为复杂机器人系统生成安全、运动学可行且最优的轨迹是机器人学中的核心挑战。现有方法常面临后处理修正导致的计算不可行性和可行性损失问题。

Method: 提出Safe Model Predictive Diffusion (Safe MPD)，一种无需训练的扩散规划器，将基于模型的扩散框架与安全防护机制相结合。通过在去噪过程中对所有样本强制执行可行性和安全性约束，避免后处理修正。

Result: 在具有挑战性的非凸规划问题（包括运动学和加速度控制的拖拉机-拖车系统）上进行验证。结果显示，该方法在成功率和安全性方面显著优于现有安全策略，同时实现亚秒级计算时间。

Conclusion: Safe MPD能够为复杂机器人系统生成既运动学可行又安全的轨迹，避免了传统后处理方法的局限性，在计算效率和性能方面表现出色。

Abstract: Generating safe, kinodynamically feasible, and optimal trajectories for complex robotic systems is a central challenge in robotics. This paper presents Safe Model Predictive Diffusion (Safe MPD), a training-free diffusion planner that unifies a model-based diffusion framework with a safety shield to generate trajectories that are both kinodynamically feasible and safe by construction. By enforcing feasibility and safety on all samples during the denoising process, our method avoids the common pitfalls of post-processing corrections, such as computational intractability and loss of feasibility. We validate our approach on challenging non-convex planning problems, including kinematic and acceleration-controlled tractor-trailer systems. The results show that it substantially outperforms existing safety strategies in success rate and safety, while achieving sub-second computation times.

</details>


### [195] [Leveraging Port-Hamiltonian Theory for Impedance Control Benchmarking](https://arxiv.org/abs/2512.06423)
*Leonardo F. Dos Santos,Elisa G. Vergamini,Cícero Zanette,Lucca Maitan,Thiago Boaventura*

Main category: cs.RO

TL;DR: 该研究提出了基于端口哈密顿系统的阻抗控制基准测试指标，包括因果一致的PH模型、可微的n自由度无源条件以及阻抗保真度度量，并在仿真中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏标准化的阻抗控制基准测试方法，需要开发能够评估阻抗控制器性能的量化指标，特别是在多自由度、时变参考和无需力/力矩传感器的情况下。

Method: 1. 为笛卡尔空间中的质量-弹簧-阻尼阻抗系统引入因果一致的端口哈密顿模型；2. 推导出可微的、不依赖力/力矩传感的n自由度无源条件，适用于时变参考；3. 基于自由运动中的阶跃响应功率定义阻抗保真度度量，捕捉动态解耦特性。

Result: 在Gazebo仿真中验证了所提指标，使用六自由度机械臂和四足机器人腿部进行测试。结果表明端口哈密顿框架适用于标准化的阻抗控制基准测试。

Conclusion: 提出的基于端口哈密顿系统的指标为阻抗控制提供了有效的基准测试框架，能够量化评估控制器性能，特别是在多自由度、时变参考和无需力传感的情况下。

Abstract: This work proposes PH-based metrics for benchmarking impedance control. A causality-consistent PH model is introduced for mass-spring-damper impedance in Cartesian space. Based on this model, a differentiable, force-torque sensing-independent, n-DoF passivity condition is derived, valid for time-varying references. An impedance fidelity metric is also defined from step-response power in free motion, capturing dynamic decoupling. The proposed metrics are validated in Gazebo simulations with a six-DoF manipulator and a quadruped leg. Results demonstrate the suitability of the PH framework for standardized impedance control benchmarking.

</details>


### [196] [Fault Tolerant Control of Mecanum Wheeled Mobile Robots](https://arxiv.org/abs/2512.06444)
*Xuehui Ma,Shiliang Zhang,Zhiyong Sun*

Main category: cs.RO

TL;DR: 提出一种用于麦克纳姆轮移动机器人的容错控制策略，能够同时处理完全执行器故障和部分故障，通过后验概率实时学习故障参数，确保机器人在不同故障情况下的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 麦克纳姆轮移动机器人容易受到执行器故障影响，现有容错控制方案主要针对完全故障（如电机失速），忽略了部分故障（如扭矩衰减），需要一种能同时处理两种故障类型的控制策略。

Method: 采用后验概率实时学习故障参数，通过聚合预定义故障对应的概率加权控制律来推导容错控制律，确保在不同故障发生水平下的控制鲁棒性。

Result: 仿真结果表明，该容错控制策略在多种场景下都能有效工作，验证了其在处理完全和部分执行器故障方面的有效性。

Conclusion: 提出的容错控制策略能够同时处理麦克纳姆轮移动机器人的完全和部分执行器故障，通过概率加权方法确保控制系统的鲁棒性和安全性，为实际应用提供了有效的故障处理方案。

Abstract: Mecanum wheeled mobile robots (MWMRs) are highly susceptible to actuator faults that degrade performance and risk mission failure. Current fault tolerant control (FTC) schemes for MWMRs target complete actuator failures like motor stall, ignoring partial faults e.g., in torque degradation. We propose an FTC strategy handling both fault types, where we adopt posterior probability to learn real-time fault parameters. We derive the FTC law by aggregating probability-weighed control laws corresponding to predefined faults. This ensures the robustness and safety of MWMR control despite varying levels of fault occurrence. Simulation results demonstrate the effectiveness of our FTC under diverse scenarios.

</details>


### [197] [Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains](https://arxiv.org/abs/2512.06486)
*Wanru Gong,Xinyi Zheng,Xiaopeng Yang,Xiaoqing Zhu*

Main category: cs.RO

TL;DR: 本文提出ECIM算法，通过结合熵控制和内在动机来减少四足机器人运动策略训练中的早熟收敛问题，在多种地形上获得更好的稳定性和能效表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO的深度强化学习算法在训练四足机器人运动策略时，虽然稳定且样本效率高，但经常出现早熟收敛问题，导致运动性能次优和任务表现下降。需要一种方法来减少这种早熟收敛。

Method: 提出熵控制内在动机（ECIM）算法，这是一种基于熵的强化学习方法，通过结合内在动机和自适应探索来减少早熟收敛。与PPO系列算法形成对比。

Result: 在Isaac Gym中六种地形类别（上坡、下坡、不平坦粗糙地形、上楼梯、下楼梯、平地）的实验表明：任务奖励提高4-12%，峰值身体俯仰振荡减少23-29%，关节加速度降低20-32%，关节扭矩消耗下降11-20%。

Conclusion: ECIM算法通过结合熵控制和内在动机控制，在不同地形上实现了更好的四足运动稳定性，同时降低了能量消耗，成为复杂机器人控制任务的实用选择。

Abstract: Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration.
  For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.

</details>


### [198] [Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments](https://arxiv.org/abs/2512.06517)
*Shifa Sulaiman,Akash Bachhar,Ming Shen,Simon Bøgh*

Main category: cs.RO

TL;DR: 提出了一种用于假肢手的视觉引导抓取算法，通过BVH视觉分割、RRT*轨迹规划和DLS逆运动学实现自适应抓取


<details>
  <summary>Details</summary>
Motivation: 假肢技术需要增强灵巧性和自主性，视觉方法能让假肢手在动态环境中更自然地与多样物体交互

Method: 集成感知、规划和控制的模块化管道：摄像头采集场景，BVH算法分割物体并定义边界框，RRT*算法生成候选轨迹计算抓取接触点，DLS逆运动学求解关节角度，独立确定每个手指抓取姿态

Result: 在仿真中验证了方法有效性，并在Linker Hand O7平台上进行了实验集成，支持实时自适应和非结构化环境操作

Conclusion: 提出的视觉引导抓取算法实现了假肢手的灵巧操作，模块化设计支持实时适应性和物体特定的抓取配置

Abstract: Recent advancements in prosthetic technology have increasingly focused on enhancing dexterity and autonomy through intelligent control systems. Vision-based approaches offer promising results for enabling prosthetic hands to interact more naturally with diverse objects in dynamic environments. Building on this foundation, the paper presents a vision-guided grasping algorithm for a prosthetic hand, integrating perception, planning, and control for dexterous manipulation. A camera mounted on the set up captures the scene, and a Bounding Volume Hierarchy (BVH)-based vision algorithm is employed to segment an object for grasping and define its bounding box. Grasp contact points are then computed by generating candidate trajectories using Rapidly-exploring Random Tree Star algorithm, and selecting fingertip end poses based on the minimum Euclidean distance between these trajectories and the objects point cloud. Each finger grasp pose is determined independently, enabling adaptive, object-specific configurations. Damped Least Square (DLS) based Inverse kinematics solver is used to compute the corresponding joint angles, which are subsequently transmitted to the finger actuators for execution. This modular pipeline enables per-finger grasp planning and supports real-time adaptability in unstructured environments. The proposed method is validated in simulation, and experimental integration on a Linker Hand O7 platform.

</details>


### [199] [Embodied Referring Expression Comprehension in Human-Robot Interaction](https://arxiv.org/abs/2512.06558)
*Md Mofijul Islam,Alexi Gladstone,Sujan Sarker,Ganesh Nanduru,Md Fahim,Keyan Du,Aman Chadha,Tariq Iqbal*

Main category: cs.RO

TL;DR: 论文提出了Refer360数据集和MuRes模块，用于改善机器人在人类环境中对具身指代表达的理解能力


<details>
  <summary>Details</summary>
Motivation: 随着机器人进入人类工作空间，需要理解具身人类指令以实现直观流畅的人机交互，但现有数据集存在视角偏差、单视角收集、非语言手势覆盖不足以及主要关注室内环境等问题

Method: 提出了Refer360数据集（大规模具身语言和非语言交互数据集）和MuRes模块（多模态引导残差模块），MuRes作为信息瓶颈提取模态特定信号并增强预训练表示

Result: 在四个HRI数据集（包括Refer360）上的实验表明，当前多模态模型无法全面捕捉具身交互，但使用MuRes增强后性能持续提升

Conclusion: Refer360成为有价值的基准数据集，引导残差学习有潜力推进机器人在人类环境中对具身指代表达的理解

Abstract: As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.

</details>


### [200] [Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input](https://arxiv.org/abs/2512.06571)
*Zifan Xu,Myoungkyu Seo,Dongmyeong Lee,Hao Fu,Jiaheng Hu,Jiaxun Cui,Yuqian Jiang,Zhihan Wang,Anastasiia Brund,Joydeep Biswas,Peter Stone*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的系统，使仿人机器人能够在感知不确定的情况下执行鲁棒的连续踢球动作，适应不同的球门配置。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人踢球技能学习面临挑战：需要快速摆腿、单脚支撑的姿势稳定性，以及在噪声感知输入和外部扰动（如对手）下的鲁棒性。现有方法难以在感知不确定的情况下实现鲁棒的连续踢球。

Method: 采用四阶段师生训练框架：1) 长距离追球（教师）；2) 定向踢球（教师）；3) 教师策略蒸馏（学生）；4) 学生适应与精炼（学生）。关键设计包括定制奖励函数、真实噪声建模和在线约束强化学习用于适应与精炼。

Result: 在仿真和真实机器人上的广泛评估显示，系统在不同球门配置下具有强大的踢球准确性和进球成功率。消融研究进一步证明了约束强化学习、噪声建模和适应阶段的必要性。

Conclusion: 该工作提出了一个在感知不完美情况下学习鲁棒连续仿人踢球的系统，为仿人全身控制的视觉运动技能学习建立了基准任务。

Abstract: Learning fast and robust ball-kicking skills is a critical capability for humanoid soccer robots, yet it remains a challenging problem due to the need for rapid leg swings, postural stability on a single support foot, and robustness under noisy sensory input and external perturbations (e.g., opponents). This paper presents a reinforcement learning (RL)-based system that enables humanoid robots to execute robust continual ball-kicking with adaptability to different ball-goal configurations. The system extends a typical teacher-student training framework -- in which a "teacher" policy is trained with ground truth state information and the "student" learns to mimic it with noisy, imperfect sensing -- by including four training stages: (1) long-distance ball chasing (teacher); (2) directional kicking (teacher); (3) teacher policy distillation (student); and (4) student adaptation and refinement (student). Key design elements -- including tailored reward functions, realistic noise modeling, and online constrained RL for adaptation and refinement -- are critical for closing the sim-to-real gap and sustaining performance under perceptual uncertainty. Extensive evaluations in both simulation and on a real robot demonstrate strong kicking accuracy and goal-scoring success across diverse ball-goal configurations. Ablation studies further highlight the necessity of the constrained RL, noise modeling, and the adaptation stage. This work presents a system for learning robust continual humanoid ball-kicking under imperfect perception, establishing a benchmark task for visuomotor skill learning in humanoid whole-body control.

</details>


### [201] [A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance](https://arxiv.org/abs/2512.06608)
*Xinyu Zhou,Songhao Piao,Chao Gao,Liguo Chen*

Main category: cs.RO

TL;DR: 该论文提出了一个统一框架，用于公平评估人群导航方法，并引入强调轨迹曲率优化的新型奖励塑造策略，显著提升了轨迹质量和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前人群导航研究存在两个主要问题：1）评估指标相对优先级分析不足，导致对具有不同目标的方法评估不公平；2）轨迹连续性指标（特别是需要C²平滑度的指标）很少被纳入。现有DRL方法通常优先考虑效率和近端舒适度，而忽视轨迹优化或仅通过简单、未经验证的平滑度奖励来处理。

Method: 论文提出：1）一个统一框架，通过检查多个优化目标的优先级和联合评估，实现导航方法的公平透明评估；2）一种新颖的奖励塑造策略，明确强调轨迹曲率优化，显著提升轨迹质量和多尺度场景下的适应性。

Result: 通过广泛的2D和3D实验，证明所提方法相比最先进方法实现了更优越的性能，轨迹质量和适应性在多尺度场景中得到显著增强。

Conclusion: 有效的轨迹优化对于确保自然性、增强舒适度和最大化导航系统能效至关重要。所提出的框架和奖励策略填补了当前研究的空白，为人群导航方法提供了更公平的评估标准和更好的轨迹优化方案。

Abstract: Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.

</details>


### [202] [Robust Optimization-based Autonomous Dynamic Soaring with a Fixed-Wing UAV](https://arxiv.org/abs/2512.06610)
*Marvin Harms,Jaeyoung Lim,David Rohr,Friedrich Rockenbauer,Nicholas Lawrance,Roland Siegwart*

Main category: cs.RO

TL;DR: 提出了一种用于固定翼无人机自主动态翱翔的框架，利用风场显式表示和鲁棒路径跟踪控制，在仿真和真实飞行中验证了其在风切变条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 动态翱翔是一种利用风切变层能量的飞行技术，可以实现无需内部能源的无限飞行。研究旨在开发一个能够实现固定翼无人机自主动态翱翔的鲁棒框架。

Method: 提出一个包含风场显式表示、经典无人机引导控制方法的框架。通过构建点对点鲁棒参考路径来处理风场估计误差，并开发了鲁棒路径跟踪控制器。

Result: 在仿真中展示了框架在不同风况、估计误差和干扰下的鲁棒动态翱翔性能。关键组件（包括能量预测和路径跟踪鲁棒性）在真实飞行中得到验证，确保了仿真与现实的差距很小。

Conclusion: 研究结果表明，所提出的框架能够在风切变条件下实现自主动态翱翔飞行，为无人机利用风能进行长期飞行提供了可行的技术方案。

Abstract: Dynamic soaring is a flying technique to exploit the energy available in wind shear layers, enabling potentially unlimited flight without the need for internal energy sources. We propose a framework for autonomous dynamic soaring with a fixed-wing unmanned aerial vehicle (UAV). The framework makes use of an explicit representation of the wind field and a classical approach for guidance and control of the UAV. Robustness to wind field estimation error is achieved by constructing point-wise robust reference paths for dynamic soaring and the development of a robust path following controller for the fixed-wing UAV. The framework is evaluated in dynamic soaring scenarios in simulation and real flight tests. In simulation, we demonstrate robust dynamic soaring flight subject to varied wind conditions, estimation errors and disturbances. Critical components of the framework, including energy predictions and path-following robustness, are further validated in real flights to assure small sim-to-real gap. Together, our results strongly indicate the ability of the proposed framework to achieve autonomous dynamic soaring flight in wind shear.

</details>


### [203] [MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment](https://arxiv.org/abs/2512.06628)
*Ruicheng Zhang,Mingyang Zhang,Jun Zhou,Zhangrui Guo,Xiaofan Liu,Zunnan Xu,Zhizhou Zhong,Puxin Yan,Haocheng Luo,Xiu Li*

Main category: cs.RO

TL;DR: MIND-V是一个分层框架，通过语义推理、行为语义桥接和视频生成三个核心组件，结合强化学习后训练，生成物理合理且逻辑连贯的长时程机器人操作视频。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作视频生成模型只能合成简单动作的短片段，且依赖手动定义轨迹，缺乏多样化的长时程操作数据。需要开发能够生成物理合理、逻辑连贯的长时程操作视频的方法。

Method: MIND-V采用分层框架：1) 语义推理中心(SRH)使用预训练视觉语言模型进行任务规划；2) 行为语义桥接(BSB)将抽象指令转换为领域不变表示；3) 运动视频生成器(MVG)进行条件视频渲染。采用分阶段视觉未来展开测试时优化策略，并通过GRPO强化学习后训练，使用基于V-JEPA世界模型的物理预见一致性奖励来确保物理合理性。

Result: MIND-V在长时程机器人操作视频生成方面达到了最先进的性能，建立了可扩展且可控的具身数据合成范式。

Conclusion: MIND-V通过分层框架结合认知科学原理，成功解决了长时程机器人操作视频生成的挑战，为具身模仿学习提供了高质量的数据合成方法。

Abstract: Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.

</details>


### [204] [Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving](https://arxiv.org/abs/2512.06664)
*Wei-Bin Kou,Guangxu Zhu,Jingreng Lei,Chen Zhang,Yik-Chung Wu,Jianping Wang*

Main category: cs.RO

TL;DR: 本文提出MoE-RAM，一种基于大模型的统计增强解耦路由聚合机制，用于提升自动驾驶场景下的语义分割性能，通过统计检索匹配和自适应权重调整来优化专家选择和融合。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景复杂多样，单一深度学习模型难以覆盖所有可能条件（如不同天气、交通密度、道路类型）。大模型驱动的混合专家范式虽有潜力，但面临路由不精确和聚合效率低的问题。

Method: 提出MoE-RAM机制：1）通过统计检索机制增强专家路由，将大模型提取的潜在特征与缓存的专家原型特征匹配；2）通过测量专家即时特征与大模型潜在特征的统计距离，自适应重加权专家输出进行融合。

Result: 在自动驾驶数据集上的大量实验表明，MoE-RAM相比其他MoE基线和传统单模型方法具有优越性，显著提升了预测性能。

Conclusion: MoE-RAM通过统计增强的路由和聚合机制，有效解决了混合专家系统中的专家选择和融合问题，提升了自动驾驶语义分割任务的性能，为复杂场景下的模型适应提供了有效解决方案。

Abstract: Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.

</details>


### [205] [FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving](https://arxiv.org/abs/2512.06676)
*Wei-Bin Kou,Guangxu Zhu,Bingyang Cheng,Chen Zhang,Yik-Chung Wu,Jianping Wang*

Main category: cs.RO

TL;DR: FedDSR通过引入中间层监督和正则化，解决联邦学习中非IID数据导致的泛化差和收敛慢问题，在自动驾驶语义分割任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在自动驾驶中面临非独立同分布数据导致的模型泛化能力差和收敛速度慢的挑战，需要新的方法来提升联邦学习在异构驾驶环境下的性能。

Method: 提出FedDSR框架，包含三个核心策略：(1)基于架构无关标准选择多个中间层；(2)计算互信息和负熵作为中间损失和正则项，与输出层损失结合形成统一优化目标；(3)在中央服务器聚合基于上述规则训练的车辆模型。

Result: 在语义分割任务上，FedDSR相比其他联邦学习基线方法，mIoU提升最高达8.93%，训练轮次减少28.57%，显著提升模型泛化能力和收敛速度。

Conclusion: FedDSR通过中间层监督和正则化有效解决了联邦自动驾驶中的非IID数据问题，提升了模型性能并加速收敛，适合实际联邦自动驾驶生态系统部署。

Abstract: Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.

</details>


### [206] [Model-Less Feedback Control of Space-based Continuum Manipulators using Backbone Tension Optimization](https://arxiv.org/abs/2512.06754)
*Shrreya Rajneesh,Nikita Pavle,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: cs.RO

TL;DR: 提出了一种无需模型的连续体机械臂控制框架，通过经验初始化雅可比矩阵并在线优化，避免了传统模型依赖方法的不准确性和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 连续体机械臂在受限环境中具有优势，但其无限维变形、未建模内部摩擦和配置相关刚度导致基于模型的方法不可靠，雅可比预测不准确，存在人工奇点和不稳定驱动行为。

Method: 采用完全无模型控制框架，使用经验初始化雅可比矩阵并通过差分凸更新在线优化；通过实时二次规划计算驱动器增量，同时避免肌腱松弛和几何限制；引入骨架张力优化项来调节轴向载荷并抑制协同激活压缩。

Result: 在圆形、五边形和正方形轨迹上验证，展示了平滑收敛、稳定张力演变和亚毫米级稳态精度，无需任何模型校准或参数识别。

Conclusion: 该控制器为受限环境中的连续体操作提供了可扩展的模型依赖替代方案，能够实现高精度稳定控制。

Abstract: Continuum manipulators offer intrinsic dexterity and safe geometric compliance for navigation within confined and obstacle-rich environments. However, their infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness fundamentally limit the reliability of model-based kinematic formulations, resulting in inaccurate Jacobian predictions, artificial singularities, and unstable actuation behavior. Motivated by these limitations, this work presents a complete model-less control framework that bypasses kinematic modeling by using an empirically initialized Jacobian refined online through differential convex updates. Tip motion is generated via a real-time quadratic program that computes actuator increments while enforcing tendon slack avoidance and geometric limits. A backbone tension optimization term is introduced in this paper to regulate axial loading and suppress co-activation compression. The framework is validated across circular, pentagonal, and square trajectories, demonstrating smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy without any model calibration or parameter identification. These results establish the proposed controller as a scalable alternative to model-dependent continuum manipulation in a constrained environment.

</details>


### [207] [db-LaCAM: Fast and Scalable Multi-Robot Kinodynamic Motion Planning with Discontinuity-Bounded Search and Lightweight MAPF](https://arxiv.org/abs/2512.06796)
*Akmaral Moldagalieva,Keisuke Okumura,Amanda Prorok,Wolfgang Hönig*

Main category: cs.RO

TL;DR: db-LaCAM结合多智能体路径规划的可扩展性与动力学规划的动态感知能力，通过预计算运动基元处理机器人动力学，支持50个机器人场景，运行速度提升10倍


<details>
  <summary>Details</summary>
Motivation: 现有多机器人动力学运动规划器由于计算负担重，难以处理超过几个机器人的场景，限制了可扩展性并导致规划时间过长

Method: 提出discontinuity-Bounded LaCAM (db-LaCAM)规划器，利用预计算的尊重机器人动力学的运动基元生成视界长度的运动序列，允许用户定义连续运动之间的不连续性

Result: db-LaCAM可扩展到50个机器人的场景，运行速度比最先进规划器快10倍，同时保持相当的解决方案质量；在2D和3D环境中验证了单轮车和3D双积分器等动力学模型

Conclusion: 该方法成功结合了MAPF算法的可扩展性与动力学规划器的动态感知能力，在飞行机器人和带拖车汽车机器人等物理实验中验证了轨迹的安全执行

Abstract: State-of-the-art multi-robot kinodynamic motion planners struggle to handle more than a few robots due to high computational burden, which limits their scalability and results in slow planning time.
  In this work, we combine the scalability and speed of modern multi-agent path finding (MAPF) algorithms with the dynamic-awareness of kinodynamic planners to address these limitations.
  To this end, we propose discontinuity-Bounded LaCAM (db-LaCAM), a planner that utilizes a precomputed set of motion primitives that respect robot dynamics to generate horizon-length motion sequences, while allowing a user-defined discontinuity between successive motions.
  The planner db-LaCAM is resolution-complete with respect to motion primitives and supports arbitrary robot dynamics.
  Extensive experiments demonstrate that db-LaCAM scales efficiently to scenarios with up to 50 robots, achieving up to ten times faster runtime compared to state-of-the-art planners, while maintaining comparable solution quality.
  The approach is validated in both 2D and 3D environments with dynamics such as the unicycle and 3D double integrator.
  We demonstrate the safe execution of trajectories planned with db-LaCAM in two distinct physical experiments involving teams of flying robots and car-with-trailer robots.

</details>


### [208] [Dynamic Visual SLAM using a General 3D Prior](https://arxiv.org/abs/2512.06868)
*Xingguang Zhong,Liren Jin,Marija Popović,Jens Behley,Cyrill Stachniss*

Main category: cs.RO

TL;DR: 提出一种新颖的单目视觉SLAM系统，能够在动态场景中鲁棒地估计相机位姿，通过结合几何补丁在线束调整和前馈重建模型的互补优势。


<details>
  <summary>Details</summary>
Motivation: 在动态自然环境中，场景动态会严重降低相机位姿估计精度，而可靠的增量式相机位姿估计和3D重建对于机器人、交互式可视化和增强现实等应用至关重要。

Method: 提出一种新颖的单目视觉SLAM系统，结合几何补丁在线束调整和前馈重建模型的互补优势。使用前馈重建模型精确过滤动态区域，并利用其深度预测增强基于补丁的视觉SLAM鲁棒性。通过将深度预测与束调整估计的补丁对齐，鲁棒处理前馈重建模型批量应用固有的尺度模糊问题。

Result: 系统能够在动态场景中鲁棒地估计相机位姿，通过深度预测与补丁对齐有效处理尺度模糊问题，提高了动态环境下的SLAM性能。

Conclusion: 该方法通过结合几何束调整和前馈重建模型，实现了在动态自然环境中鲁棒的相机位姿估计，为机器人、交互式可视化和增强现实等应用提供了可靠的解决方案。

Abstract: Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.

</details>


### [209] [From Zero to High-Speed Racing: An Autonomous Racing Stack](https://arxiv.org/abs/2512.06892)
*Hassan Jardali,Durgakant Pushp,Youwei Yu,Mahmoud Ali,Ihab S. Mohamed,Alejandro Murillo-Gonzalez,Paul D. Coen,Md. Al-Masrur Khan,Reddy Charan Pulivendula,Saeoul Park,Lingchuan Zhou,Lantao Liu*

Main category: cs.RO

TL;DR: 本文介绍了用于Indy自动驾驶挑战赛的自动驾驶赛车堆栈（ARS）的三代迭代，在椭圆和公路赛道上实现了最高260公里/小时的速度，并发布了高速多传感器数据集。


<details>
  <summary>Details</summary>
Motivation: 高速头对头自动驾驶赛车面临重大技术和后勤挑战，包括精确定位、快速感知、动态规划和实时控制，同时受到赛道访问限制和昂贵硬件成本的制约。

Method: 开发了自动驾驶赛车堆栈（ARS）的三代迭代（ARS1、ARS2、ARS3），采用模块化架构，在椭圆和公路赛道上进行验证，并收集多传感器数据集。

Result: ARS在椭圆和公路赛道上实现了最高260公里/小时的速度，通过性能评估对比了不同赛道环境下的控制、感知和估计系统表现。

Conclusion: 研究揭示了真实世界高速全尺寸自动驾驶赛车面临的独特挑战和见解，并发布了有价值的高速多传感器数据集供社区使用。

Abstract: High-speed, head-to-head autonomous racing presents substantial technical and logistical challenges, including precise localization, rapid perception, dynamic planning, and real-time control-compounded by limited track access and costly hardware. This paper introduces the Autonomous Race Stack (ARS), developed by the IU Luddy Autonomous Racing team for the Indy Autonomous Challenge (IAC). We present three iterations of our ARS, each validated on different tracks and achieving speeds up to 260 km/h. Our contributions include: (i) the modular architecture and evolution of the ARS across ARS1, ARS2, and ARS3; (ii) a detailed performance evaluation that contrasts control, perception, and estimation across oval and road-course environments; and (iii) the release of a high-speed, multi-sensor dataset collected from oval and road-course tracks. Our findings highlight the unique challenges and insights from real-world high-speed full-scale autonomous racing.

</details>


### [210] [Control of Powered Ankle-Foot Prostheses on Compliant Terrain: A Quantitative Approach to Stability Enhancement](https://arxiv.org/abs/2512.06896)
*Chrysostomos Karakasis,Camryn Scully,Robert Salati,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 该研究验证了一种基于导纳的控制策略，通过动态调整动力假肢的准刚度来增强在柔性地面上的步态稳定性，相比刚性地面控制器显著改善了步态稳定性。


<details>
  <summary>Details</summary>
Motivation: 下肢截肢者在柔性地面上行走面临更高的跌倒风险，而现有的动力踝足假肢控制策略主要针对刚性地面优化，缺乏对柔性或顺应性地面的适应性控制方法。

Method: 提出基于导纳的控制策略，动态调整动力假肢的准刚度；通过3名健康受试者在两种柔性地面（刚度63和25 kN/m）上进行实验；使用相位图和两种步行稳定性指标评估控制器性能。

Result: 相比为刚性地面设计的标准相位变量控制器，所提出的导纳控制器在所有柔性条件下都一致地改善了步态稳定性，直接降低了跌倒风险。

Conclusion: 自适应、稳定性感知的假肢控制策略具有减少真实环境中跌倒风险的潜力，能够提升康复机器人中人-假肢交互的鲁棒性。

Abstract: Walking on compliant terrain presents a substantial challenge for individuals with lower-limb amputation, further elevating their already high risk of falling. While powered ankle-foot prostheses have demonstrated adaptability across speeds and rigid terrains, control strategies optimized for soft or compliant surfaces remain underexplored. This work experimentally validates an admittance-based control strategy that dynamically adjusts the quasi-stiffness of powered prostheses to enhance gait stability on compliant ground. Human subject experiments were conducted with three healthy individuals walking on two bilaterally compliant surfaces with ground stiffness values of 63 and 25 kN/m, representative of real-world soft environments. Controller performance was quantified using phase portraits and two walking stability metrics, offering a direct assessment of fall risk. Compared to a standard phase-variable controller developed for rigid terrain, the proposed admittance controller consistently improved gait stability across all compliant conditions. These results demonstrate the potential of adaptive, stability-aware prosthesis control to reduce fall risk in real-world environments and advance the robustness of human-prosthesis interaction in rehabilitation robotics.

</details>


### [211] [Ground Compliance Improves Retention of Visual Feedback-Based Propulsion Training for Gait Rehabilitation](https://arxiv.org/abs/2512.06897)
*Bradley Hobbs,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 研究比较了在视觉反馈步态训练中加入地面顺应性是否比单独使用视觉反馈更能有效增加推进力，发现结合两种方法能更有效地学习和维持推进力提升。


<details>
  <summary>Details</summary>
Motivation: 探索在视觉反馈步态训练中加入地面顺应性是否比单独使用视觉反馈更能有效增加推进力，这对针对中风后推进力缺陷的长期康复治疗有重要意义。

Method: 10名健康参与者在定制的分带跑步机上行走，所有参与者都接受地面反作用力的实时视觉反馈。一组还体验了地面顺应性的变化，而对照组仅接受视觉反馈。

Result: 推进地面反作用力（POF）的成功增加并能在干预后持续，特别是在体验地面顺应性的组中。该组还表现出肌肉活动和关节运动学的持久后效，表明对增加推进的自然策略有更稳健的学习。

Conclusion: 结合地面顺应性与视觉反馈能增强推进力的学习，支持在针对推进力缺陷（如中风后）的长期康复中使用顺应性地面的潜力，展示了视觉和本体感觉系统在步态适应中的协调作用。

Abstract: This study investigates whether adding ground compliance to visual feedback (VF) gait training is more effective at increasing push-off force (POF) compared to using VF alone, with implications for gait rehabilitation. Ten healthy participants walked on a custom split-belt treadmill. All participants received real-time visual feedback of their ground reaction forces. One group also experienced changes in ground compliance, while a control group received only visual feedback. Intentional increases in propulsive ground reaction forces (POF) were successfully achieved and sustained post-intervention, especially in the group that experienced ground compliance. This group also demonstrated lasting after-effects in muscle activity and joint kinematics, indicating a more robust learning of natural strategies to increase propulsion. This work demonstrates how visual and proprioceptive systems coordinate during gait adaptation. It uniquely shows that combining ground compliance with visual feedback enhances the learning of propulsive forces, supporting the potential use of compliant terrain in long-term rehabilitation targeting propulsion deficits, such as those following a stroke.

</details>


### [212] [Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields](https://arxiv.org/abs/2512.06912)
*Rushiraj Gadhvi,Sandeep Manjanna*

Main category: cs.RO

TL;DR: 基于强化学习的自主水面航行器在涡流场中的节能导航方法，相比现有技术节能30-50%


<details>
  <summary>Details</summary>
Motivation: 模仿khalasi（传统航海者）利用洋流高效航行的直觉，解决自主水面航行器在严格能量预算下执行长期任务时的节能导航挑战，特别是在部分可观测的涡流场中传统路径规划方法效果不佳的问题

Method: 提出基于Soft Actor Critic的端到端强化学习框架，仅使用局部速度测量学习流感知导航策略

Result: 在多样化和动态丰富的场景中评估，该方法显示出显著的节能效果，并能鲁棒地泛化到未见过的流场条件，导航路径相比现有技术节能30-50%

Conclusion: 该方法为海洋环境中长期自主航行提供了有前景的路径，实现了类似传统航海者利用洋流的高效导航能力

Abstract: For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.

</details>


### [213] [Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs](https://arxiv.org/abs/2512.06935)
*Nicolò Botteghi,Owen Brook,Urban Fasel,Federico Califano*

Main category: cs.RO

TL;DR: 提出一种基于稀疏字典学习的数值方法，用于设计IDA-PBC控制器，无需精确求解匹配PDEs，使IDA-PBC能应用于超越稳定的复杂任务


<details>
  <summary>Details</summary>
Motivation: 传统IDA-PBC方法需要解析求解复杂的偏微分方程（匹配条件），这在实际应用中极具挑战性，限制了该方法在复杂物理系统和超越稳定任务中的应用

Method: 将IDA-PBC问题转化为神经常微分方程学习问题，采用稀疏字典学习参数化期望闭环系统为非线性状态依赖函数的稀疏线性组合，通过求解多目标优化问题优化控制器参数

Result: 该方法使IDA-PBC能够应用于超越稳定的复杂任务（如发现周期性振荡行为），并能推导出包含残差项的闭环系统闭式表达式

Conclusion: 提出的数值方法克服了传统IDA-PBC需要解析求解匹配PDEs的限制，扩展了该方法在复杂物理系统和任务中的应用范围

Abstract: Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks.
  In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms

</details>


### [214] [Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge](https://arxiv.org/abs/2512.06951)
*Ilia Larchenko,Gleb Zarin,Akash Karnatak*

Main category: cs.RO

TL;DR: 本文提出了一种在BEHAVIOR 2025挑战赛中获胜的视觉-动作策略，该策略在50个多样化长时域家庭任务中取得了26%的q-score成绩，通过相关噪声流匹配、可学习混合层注意力等创新技术提升了性能。


<details>
  <summary>Details</summary>
Motivation: BEHAVIOR挑战赛是一个大规模基准测试，包含50个多样化的长时域家庭任务，需要双手操作、导航和上下文感知决策。现有方法在处理这些复杂任务时面临效率和平滑性挑战，需要更有效的训练和推理方法。

Method: 基于Pi0.5架构，引入了相关噪声流匹配技术来提升训练效率并实现相关感知的图像修复以获得平滑动作序列；应用可学习混合层注意力和系统2阶段跟踪来解决歧义；训练采用多样本流匹配减少方差，推理时使用动作压缩和挑战特定的校正规则。

Result: 在BEHAVIOR 2025挑战赛中获得第一名，在公共和私有排行榜上的50个任务中均实现了26%的q-score。

Conclusion: 提出的相关噪声流匹配、可学习混合层注意力等技术有效提升了视觉-动作策略在复杂家庭任务中的性能，在BEHAVIOR挑战赛中取得了最佳结果，证明了这些创新方法的有效性。

Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.

</details>


### [215] [VideoVLA: Video Generators Can Be Generalizable Robot Manipulators](https://arxiv.org/abs/2512.06963)
*Yichao Shen,Fangyun Wei,Zhiying Du,Yaobo Liang,Yan Lu,Jiaolong Yang,Nanning Zheng,Baining Guo*

Main category: cs.RO

TL;DR: VideoVLA将大型视频生成模型转化为机器人操作器，通过联合建模视频、语言和动作，预测动作序列和未来视觉结果，实现机器人操作的泛化能力


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在泛化到新任务、新物体和新环境方面能力有限，需要探索新的范式来提升机器人操作的泛化能力

Method: 基于多模态扩散变换器，利用预训练的视频生成模型进行联合视觉和动作预测，给定语言指令和图像，预测动作序列和未来视觉结果

Result: 高质量的未来视觉想象与可靠的动作预测和任务成功相关，VideoVLA展现出强大的泛化能力，包括模仿其他具身智能的技能和处理新物体

Conclusion: 同时预测动作及其视觉后果的双重预测策略代表了机器人学习范式的转变，解锁了操作系统的泛化能力，强调了视觉想象在操作中的重要性

Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.

</details>


### [216] [Parametric Design of a Cable-Driven Coaxial Spherical Parallel Mechanism for Ultrasound Scans](https://arxiv.org/abs/2512.06995)
*Maryam Seraj,Mohammad Hossein Kamrava,Carlo Tiseo*

Main category: cs.RO

TL;DR: 本文提出了一种电缆驱动同轴球面并联机构(CDC-SPM)的设计方法和运动学分析，用于医疗遥操作中的高保真触觉接口，通过减少末端执行器质量来提高动态响应性能。


<details>
  <summary>Details</summary>
Motivation: 医疗遥操作中的触觉接口需要在工作空间、灵活性、刚度、惯性和带宽之间取得平衡，特别是在需要纯旋转运动的应用中。现有系统在末端执行器质量较大时会导致惯性负载增加，影响动态响应性能。

Method: 采用电缆驱动同轴球面并联机构(CDC-SPM)设计，通过平行和同轴驱动实现解耦的旋转自由度，具有各向同性的力和扭矩传递特性。该设计减少了机器人臂末端执行器的质量，从而降低惯性负载。

Result: 仿真和分析表明，CDC-SPM提供了准确、响应迅速且安全的运动特性，适用于高精度触觉应用。该机构在力和扭矩传递方面表现出各向同性特性。

Conclusion: CDC-SPM机制在医疗遥操作任务中具有重要应用潜力，特别是在超声成像等需要精确直观操作的场景中，能够提供高保真的触觉反馈。

Abstract: Haptic interfaces play a critical role in medical teleoperation by enabling surgeons to interact with remote environments through realistic force and motion feedback. Achieving high fidelity in such systems requires balancing performance trade-off among workspace, dexterity, stiffness, inertia, and bandwidth, particularly in applications demanding pure rotational motion. This paper presents the design methodology and kinematic analysis of a Cable-Driven Coaxial Spherical Parallel Mechanism (CDC-SPM) developed to address these challenges. The proposed cable-driven interface design allows for reducing the mass placed at the robot arm end-effector, thereby minimizing inertial loads, enhancing stiffness, and improving dynamic responsiveness. Through parallel and coaxial actuation, the mechanism achieves decoupled rotational degrees of freedom with isotropic force and torque transmission. Simulation and analysis demonstrate that the CDC-SPM provides accurate, responsive, and safe motion characteristics suitable for high-precision haptic applications. These results highlight the mechanism's potential for medical teleoperation tasks such as ultrasound imaging, where precise and intuitive manipulation is essential.

</details>


### [217] [A Flexible Funnel-Shaped Robotic Hand with an Integrated Single-Sheet Valve for Milligram-Scale Powder Handling](https://arxiv.org/abs/2512.07091)
*Tomoya Takahashi,Yusaku Nakajima,Cristian Camilo Beltran-Hernandez,Yuki Kuroda,Kazutoshi Tanaka,Masashi Hamaya,Kanta Ono,Yoshitaka Ushiku*

Main category: cs.RO

TL;DR: 提出一种新型漏斗形柔性机械手，通过可控阀门和基于粉末流动模型的反馈控制系统，实现毫克级粉末的精确称量，误差在2mg以内（80%试验），最大误差约20mg。


<details>
  <summary>Details</summary>
Motivation: 实验室自动化在加速固态材料发现方面具有潜力，但毫克级粉末处理的全自动化仍面临重大挑战，主要由于粉末的复杂流动动力学和实验室任务的多样性。

Method: 开发一种漏斗形柔性机械手，保留先前工作的柔软性和锥形设计，在锥顶加入可控阀门以实现精确的毫克级粉末增量分配。系统通过基于粉末流动模型和在线参数识别的反馈控制系统与外部天平集成。

Result: 使用玻璃珠、谷氨酸钠和二氧化钛进行实验评估，80%的试验误差在2mg以内，最大误差约20mg（目标范围20mg-3g）。与直接PID控制相比，基于模型的控制显著提高了精度和收敛速度。

Conclusion: 该系统展示了高效灵活的粉末称量潜力，具有向更大数量级扩展的能力，适用于广泛的实验室自动化任务。

Abstract: Laboratory Automation (LA) has the potential to accelerate solid-state materials discovery by enabling continuous robotic operation without human intervention. While robotic systems have been developed for tasks such as powder grinding and X-ray diffraction (XRD) analysis, fully automating powder handling at the milligram scale remains a significant challenge due to the complex flow dynamics of powders and the diversity of laboratory tasks. To address this challenge, this study proposes a novel, funnel-shaped, flexible robotic hand that preserves the softness and conical sheet designs in prior work while incorporating a controllable valve at the cone apex to enable precise, incremental dispensing of milligram-scale powder quantities. The hand is integrated with an external balance through a feedback control system based on a model of powder flow and online parameter identification. Experimental evaluations with glass beads, monosodium glutamate, and titanium dioxide demonstrated that 80% of the trials achieved an error within 2 mg, and the maximum error observed was approximately 20 mg across a target range of 20 mg to 3 g. In addition, by incorporating flow prediction models commonly used for hoppers and performing online parameter identification, the system is able to adapt to variations in powder dynamics. Compared to direct PID control, the proposed model-based control significantly improved both accuracy and convergence speed. These results highlight the potential of the proposed system to enable efficient and flexible powder weighing, with scalability toward larger quantities and applicability to a broad range of laboratory automation tasks.

</details>


### [218] [Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots](https://arxiv.org/abs/2512.07114)
*Jue Wang,Mingsong Jiang,Luis A. Ramirez,Bilige Yang,Mujun Zhang,Esteban Figueroa,Wenzhong Yan,Rebecca Kramer-Bottiglio*

Main category: cs.RO

TL;DR: 提出了一种替代柔顺性建模方法，通过在刚体模拟器中引入代表软材料变形的间接变量，实现了在刚体模拟中学习软体变形机器人的控制策略，并成功转移到真实硬件。


<details>
  <summary>Details</summary>
Motivation: 自适应形态发生机器人需要适应不同任务和环境条件，但软体组件带来了模拟和控制挑战。软体模拟器精度和计算效率有限，而刚体模拟器无法捕捉软材料动力学。

Method: 采用替代柔顺性建模方法：在刚体模拟器中引入代表软材料变形的间接变量（有效肢体长度和肢体质心变化），结合强化学习和这些间接变量的广泛随机化。

Result: 在刚体模拟中实现了可靠策略学习，策略直接转移到硬件，在平坦硬质基底上表现出高保真度，在流变复杂地形上转移保真度较低但鲁棒。学习到的闭环步态展现出前所未有的地面机动性，运输成本比开环基准降低一个数量级。

Conclusion: 替代柔顺性建模方法有效解决了软体变形机器人的模拟和控制挑战，实现了从模拟到真实的高性能转移，并在多种自然地形上展示了稳定的多步态运动能力。

Abstract: Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.

</details>


### [219] [Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07130)
*Zebin Xing,Yupeng Zheng,Qichao Zhang,Zhixing Ding,Pengxuan Yang,Songen Gu,Zhongpu Xia,Dongbin Zhao*

Main category: cs.RO

TL;DR: Mimir是一个新颖的分层双系统框架，通过目标点不确定性估计和多重速率引导机制，在端到端自动驾驶中实现了更强的鲁棒性和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法受限于不准确的高层引导信号和复杂引导模块的计算开销，需要一种既能提高鲁棒性又能加速推理的解决方案。

Method: 提出Mimir框架：1) 使用拉普拉斯分布估计目标点不确定性以增强鲁棒性；2) 引入多重速率引导机制，提前预测扩展目标点以加速推理。

Result: 在Navhard和Navtest基准测试中，Mimir在驾驶评分EPDMS上超越先前最佳方法20%，同时高层模块推理速度提升1.6倍且不损失准确性。

Conclusion: Mimir通过不确定性估计和多重速率引导机制，有效解决了端到端自动驾驶中引导信号不准确和计算开销大的问题，实现了鲁棒性和效率的平衡。

Abstract: End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving

</details>


### [220] [Time-Varying Formation Tracking Control of Wheeled Mobile Robots With Region Constraint: A Generalized Udwadia-Kalaba Framework](https://arxiv.org/abs/2512.07137)
*Kang Yijie,Hao Yuqing,Wang Qingyun,Chen Guanrong*

Main category: cs.RO

TL;DR: 基于广义Udwadia-Kalaba框架研究了具有区域约束的轮式移动机器人时变编队跟踪控制，考虑了有向加权通信拓扑，通过约束方程和微分同胚变换设计控制器，确保机器人安全。


<details>
  <summary>Details</summary>
Motivation: 现有时变编队跟踪控制研究未考虑区域约束，无法保证机器人的安全性。本文旨在解决具有区域约束的轮式移动机器人时变编队跟踪控制问题。

Method: 采用广义Udwadia-Kalaba框架，将时变编队跟踪控制目标重新表述为约束方程，通过微分同胚变换处理区域约束，在有向加权通信拓扑下设计控制器。

Result: 设计了具有区域约束的时变编队跟踪控制器，通过数值仿真验证了所提控制策略的有效性。

Conclusion: 提出的基于广义Udwadia-Kalaba框架的控制方法能够有效实现具有区域约束的轮式移动机器人时变编队跟踪控制，确保机器人安全。

Abstract: In this paper, the time-varying formation tracking control of wheeled mobile robots with region constraint is investigated from a generalized Udwadia-Kalaba framework. The communication topology is directed, weighted and has a spanning tree with the leader being the root. By reformulating the time-varying formation tracking control objective as a constrained equation and transforming the region constraint by a diffeomorphism, the time-varying formation tracking controller with the region constraint is designed under the generalized Udwadia-Kalaba framework. Compared with the existing works on time-varying formation tracking control, the region constraint is takeninto account in this paper, which ensures the safety of the robots.Finally, some numerical simulations are presented to illustrate the effectiveness of the proposed control strategy.

</details>


### [221] [Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality](https://arxiv.org/abs/2512.07221)
*Zichao Shu,Shitao Bei,Lijun Li,Zetao Chen*

Main category: cs.RO

TL;DR: 提出一种连续时间最大似然估计器，通过整合IMU数据补偿运动捕捉系统抖动，实现更精确的SLAM基准测试


<details>
  <summary>Details</summary>
Motivation: 随着XR沉浸感标准提高，SLAM基准测试要求更严格。传统基于运动捕捉系统的地面真值存在时空校准误差和固有抖动问题，限制了旋转误差和帧间抖动等关键指标的准确评估

Method: 提出连续时间最大似然估计器，整合辅助IMU数据补偿MoCap抖动；采用可变时间同步方法和基于螺旋同余约束的位姿残差，实现多传感器与待测设备的精确时空校准

Result: 实验结果表明该方法优于现有方法，达到了对最先进XR设备SLAM算法进行全面基准测试所需的精度，并验证了多个领先XR设备和开源SLAM算法的实用性

Conclusion: 该方法解决了SLAM基准测试中地面真值精度不足的问题，为XR应用中的SLAM算法评估提供了更准确的基准测试工具，代码已开源

Abstract: Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.

</details>


### [222] [Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots](https://arxiv.org/abs/2512.07303)
*Gianpietro Battocletti,Dimitris Boskos,Bart De Schutter*

Main category: cs.RO

TL;DR: 本文提出了一种基于拓扑模型的系绳机器人路径规划方法，通过建立配置空间与工作空间通用覆盖空间的联系，构建单纯复形模型，相比传统方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有系绳机器人路径规划方法通常依赖离散的配置空间表示，无法同时捕捉系绳的拓扑信息和机器人的连续位置，需要更有效的模型来解决这一问题。

Method: 首先建立系绳机器人配置空间与工作空间通用覆盖空间的联系，然后基于此开发算法计算配置空间的单纯复形模型，该模型是连续的且能捕捉拓扑信息。

Result: 所提出的模型计算时间仅为构建传统同伦增强图所需时间的一小部分，是连续模型，能够支持多种路径规划算法解决系绳机器人的路径规划任务。

Conclusion: 通过构建拓扑模型，本文方法显著改进了现有配置空间表示方法的性能，为系绳机器人路径规划提供了更高效、连续的解决方案。

Abstract: Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.

</details>


### [223] [Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection](https://arxiv.org/abs/2512.07316)
*Gianpietro Battocletti,Dimitris Boskos,Bart De Schutter*

Main category: cs.RO

TL;DR: 本文提出了一种用于无人水面艇（USV）对接的协同控制方法，采用集中式模型预测控制（MPC）使两艘USV共同协作在约定位置完成对接，相比传统单艇主动对接方法更快速高效。


<details>
  <summary>Details</summary>
Motivation: 现有USV自主对接方法通常将一艘USV视为静止目标，另一艘负责主动对接。这种单边主动方式在效率上存在局限，特别是在多USV协同作业场景中。本文旨在开发一种更高效的协同对接方法，让两艘USV共同协作完成对接任务。

Method: 采用集中式模型预测控制（MPC）方法解决控制问题。该方法基于模型预测，能够生成可行的轨迹并保证约束满足。通过MPC预测模型，可以预测并抵消外部扰动（如水流）对USV的影响，实现扰动抑制。

Result: 仿真结果表明，与现有方法相比，所提出的协同对接方法能够实现更快、更高效的对接。MPC方法能够有效处理外部扰动，特别是在几乎静止的扰动（如水流）情况下表现优异。

Conclusion: 提出的集中式MPC协同对接方法为USV-USV对接提供了更优解决方案，通过两艇协作和模型预测控制，实现了更快速、高效的对接性能，并能有效应对外部扰动。

Abstract: Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.

</details>


### [224] [Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin](https://arxiv.org/abs/2512.07359)
*Bin Zhao,Yiwen Lu,Haohua Zhu,Xiao Li,Sheng Yi*

Main category: cs.RO

TL;DR: 提出一个从光学动作捕捉到多刚体手部模型的完整流程，通过个性化MANO模型转换为URDF表示，解决SO(3)关节旋转到运动学约束关节的投影问题，实现实时物理仿真。


<details>
  <summary>Details</summary>
Motivation: 数字孪生应用中的人手仿真需要平衡解剖学保真度和计算效率，现有模型难以同时满足真实外观和实时物理仿真的需求。

Method: 从特定人手的光学动作捕捉开始，构建个性化MANO模型并转换为URDF表示；针对单自由度关节推导闭式解，针对双自由度关节引入BCH校正迭代方法处理旋转非交换性。

Result: 数字孪生实验中，强化学习策略控制多刚体手部重放捕捉的人类演示；定量评估显示亚厘米级重建误差，并在多样化操作任务中成功执行抓取。

Conclusion: 提出的完整流程能够构建既保持真实外观又支持实时物理仿真的多刚体手部模型，为数字孪生应用提供有效解决方案。

Abstract: Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.

</details>


### [225] [ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning](https://arxiv.org/abs/2512.07371)
*Byungju Kim,Jinu Pahk,Chungwoo Lee,Jaejoon Kim,Jangha Lee,Theo Taeyeong Kim,Kyuhwan Shim,Jun Ki Lee,Byoung-Tak Zhang*

Main category: cs.RO

TL;DR: ESPADA框架通过语义感知的演示分割实现机器人操作加速，在保持成功率的同时达到约2倍速度提升


<details>
  <summary>Details</summary>
Motivation: 基于行为克隆的视觉运动策略虽然能实现精确操作，但继承了人类演示的缓慢节奏，限制了实际部署。现有加速方法主要依赖统计或启发式线索，忽略了任务语义，无法适应多样化的操作场景。

Method: ESPADA框架使用VLM-LLM管道结合3D夹爪-物体关系对演示进行语义分割，仅在非关键段进行激进下采样，同时保留精度关键阶段。通过动态时间规整在仅含动态特征的数据上传播分段标签，实现从单个标注片段到完整数据集的扩展。

Result: 在仿真和真实世界实验中，与ACT和DP基线相比，ESPADA实现了约2倍的速度提升，同时保持成功率，缩小了人类演示与高效机器人控制之间的差距。

Conclusion: ESPADA提供了一种语义和空间感知的框架，能够在不需额外数据、架构修改或重新训练的情况下，通过智能演示分割实现机器人操作的显著加速。

Abstract: Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.

</details>


### [226] [Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction](https://arxiv.org/abs/2512.07464)
*Haolin Song,Hongbo Zhu,Tao Yu,Yan Liu,Mingqi Yuan,Wengang Zhou,Hua Chen,Houqiang Li*

Main category: cs.RO

TL;DR: 提出一种感知式人形机器人运动框架，通过深度相机实时重建地形高度图，结合统一强化学习策略实现复杂地形下的稳健运动控制


<details>
  <summary>Details</summary>
Motivation: 全尺寸人形机器人在复杂地形（如长楼梯）上实现可靠运动仍然具有挑战性，有限的感知能力、模糊的地形线索以及步态时序调整不足容易导致单步失误引发失衡

Method: 1. 使用向下深度相机实时观察脚部支撑区域，通过紧凑U-Net重建密集的自我中心高度图；2. 将感知高度图与本体感觉观察输入统一强化学习策略，生成关节指令和全局步态相位信号；3. 采用单阶段连续师生训练方案进行高效策略学习和知识迁移

Result: 在31自由度、1.65米高的人形机器人上进行实验，在仿真和真实环境中均展示了稳健的运动能力，包括前后上下楼梯以及跨越46厘米间隙

Conclusion: 将地形感知、步态调节和全身控制融合到单一强化学习策略中的框架，能够有效提升人形机器人在复杂地形下的运动稳健性

Abstract: For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/

</details>


### [227] [From Real-World Traffic Data to Relevant Critical Scenarios](https://arxiv.org/abs/2512.07482)
*Florian Lüttner,Nicole Neis,Daniel Stadler,Robin Moss,Mirjam Fehling-Kaschek,Matthias Pfriem,Alexander Stolz,Jens Ziehn*

Main category: cs.RO

TL;DR: 论文提出了一种高速公路换道场景的安全相关场景识别方法，通过真实数据采集、关键性度量评估和合成场景生成，提高自动驾驶系统验证效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆和高级驾驶辅助系统需要在广泛相关场景中可靠运行，但由于自由度众多，识别完整的安全相关场景具有挑战性。随着功能复杂性增加，"未知不安全"场景数量增多，需要从简单场景（如高速公路）开始提前识别相关场景。

Method: 1) 采集和处理高速公路真实交通数据；2) 在轨迹数据上应用关键性度量评估场景；3) 将计算出的度量与具体换道场景和数据采集条件关联；4) 基于记录场景生成合成场景以应对"未知不安全"场景。

Result: 开发了一个处理链，能够识别安全相关场景，开发数据驱动方法提取这些场景，并通过采样在高速公路上生成合成关键场景。该方法在AVEAS项目中实施。

Conclusion: 通过分析高速公路换道场景，建立了一个从数据采集到场景识别和合成的完整框架，有助于提高自动驾驶系统验证效率，并为更复杂环境（如城市交通）的场景识别奠定基础。

Abstract: The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly "unknown unsafe" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of "unknown unsafe" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.

</details>


### [228] [VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform](https://arxiv.org/abs/2512.07507)
*Yiming Cui,Shiyu Fang,Jiarui Zhang,Yan Huang,Chengkai Xu,Bing Zhu,Hao Zhang,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: VP-AutoTest是一个虚拟物理融合测试平台，通过集成多种交通元素、支持单/多车交互测试、采用对抗测试和平行推演，结合多维评估框架和AI专家系统，为自动驾驶提供高效可信的测试解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶测试方法（虚拟仿真、封闭场地、公共道路）存在车辆状态不真实、测试能力有限、成本高等问题，而现有的虚拟物理融合测试又面临元素类型有限、测试范围窄、评估指标固定等挑战。

Method: 提出VP-AutoTest平台，集成10+种虚拟和物理元素（车辆、行人、路侧设施等），支持单车辆交互和多车辆协同测试，采用对抗测试和平行推演加速故障发现，通过OBU和Redis通信实现V2V/V2I协同，结合多维评估框架和AI专家系统进行性能评估和缺陷诊断。

Result: 平台能够复现实世界交通参与者的多样性，加速算法极限探索和故障检测，实现全级别协同自动驾驶的无缝通信，并通过与真实实验对比进行可信度自评估，确保测试的保真度和效率。

Conclusion: VP-AutoTest通过创新的虚拟物理融合测试方法，解决了传统测试的局限性，为自动驾驶系统提供了更全面、高效、可信的测试解决方案，推动了自动驾驶测试技术的发展。

Abstract: The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.

</details>


### [229] [See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations](https://arxiv.org/abs/2512.07582)
*Guangyan Chen,Meiling Wang,Qi Shao,Zichen Zhou,Weixin Mao,Te Cui,Minzhao Zhu,Yinan Deng,Luojie Yang,Zhanqi Zhang,Yi Yang,Hua Chen,Yufeng Yue*

Main category: cs.RO

TL;DR: ViVLA是一种通用机器人操作策略，能够通过单次专家演示视频在测试时高效学习新任务，在未见任务上实现超过30%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在超出训练分布的任务上泛化能力有限，而人类仅通过观察一次演示就能学习新技能。受此启发，研究旨在开发能够从单次专家演示视频中学习新技能的通用机器人操作策略。

Method: 提出ViVLA策略，联合处理专家演示视频和机器人视觉观测，预测演示动作序列和后续机器人动作，从专家行为中提取精细操作知识并转移到机器人。开发可扩展的专家-智能体配对数据生成管道，从易获取的人类视频合成配对轨迹，结合公开数据集，共生成892,911个训练样本。

Result: ViVLA能够仅通过单次专家演示视频在测试时学习新操作技能。在未见LIBERO任务上实现超过30%的改进，跨具身视频保持35%以上的增益。真实世界实验显示从人类视频中有效学习，在未见任务上获得超过38%的改进。

Conclusion: ViVLA通过从单次专家演示视频中学习，显著提升了机器人操作策略的泛化能力，实现了更接近人类学习效率的机器人技能获取方式。

Abstract: Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.

</details>


### [230] [Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots](https://arxiv.org/abs/2512.07673)
*Matthias Heyrman,Chenhao Li,Victor Klemm,Dongho Kang,Stelian Coros,Marco Hutter*

Main category: cs.RO

TL;DR: MDME是一种统一结构化与非结构化特征的运动表示方法，通过小波编码和概率嵌入实现实时机器人模仿，无需重定向或任务特定调优


<details>
  <summary>Details</summary>
Motivation: 现有运动控制器往往忽略运动中的固有模式，而之前的表示学习方法未能同时捕捉人类和动物运动中的结构化周期性模式和非规则变化

Method: 提出多域运动嵌入(MDME)，使用基于小波的编码器和并行概率嵌入来统一结构化与非结构化特征的嵌入，从最小输入集生成丰富的参考运动表示

Result: 在无重定向的实时运动模仿任务中，MDME在重建保真度和对未见运动的泛化能力方面优于先前方法，能够在人形和四足机器人平台上准确复现复杂轨迹

Conclusion: MDME作为一种通用且结构感知的基础表示，支持可扩展的实时机器人模仿，通过零样本部署即可复现新颖运动风格，无需任务特定调优或在线重定向

Abstract: Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.

</details>


### [231] [AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation](https://arxiv.org/abs/2512.07680)
*P. A. Wigner,L. Romanello,A. Hammad,P. H. Nguyen,T. Lan,S. F. Armanini,B. B. Kocer,M. Kovac*

Main category: cs.RO

TL;DR: 该论文提出了一种可在空中部署的爬行机器人，用于在树冠中进行自适应运动和操作。系统结合了柔性微刺履带、双履带旋转夹持器和弹性尾部，能够在不同曲率和倾斜度的树枝上实现安全附着和稳定穿越。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够有效在树冠环境中进行移动和操作的机器人平台，填补空中机器人与地面生态机器人之间的技术空白，为环境采样和冠层内传感提供低功耗、鲁棒的解决方案。

Method: 采用柔性微刺履带系统提供抓附能力，结合双履带旋转夹持器实现稳定抓握，配备弹性尾部增强平衡。系统通过无人机-绳索部署系统进行空中部署，能够在不同倾斜度和曲率的树枝上自适应移动。

Result: 实验显示系统能够在90度身体滚动和倾斜下可靠抓握，在倾斜度达67.5度的树枝上有效攀爬，水平树枝上最大速度达0.55体长/秒。柔性履带允许10度偏航转向，功耗测量显示无量纲运输成本比典型空中机器人悬停功耗低一个数量级。

Conclusion: 该空中可部署爬行机器人系统为环境采样和冠层内传感提供了鲁棒、低功耗的平台，成功弥合了空中与表面生态机器人之间的技术差距，展示了在复杂树冠环境中自适应运动和操作的可行性。

Abstract: This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination.
  Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces.
  Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.

</details>


### [232] [Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks](https://arxiv.org/abs/2512.07697)
*Aileen Liao,Dong-Ki Kim,Max Olan Smith,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: DA-DP是一种延迟感知扩散策略框架，通过在训练和推理中显式考虑推理延迟来改善机器人策略学习，相比零延迟方法在各种任务、机器人和延迟条件下表现更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 机器人感知和选择动作时，世界持续变化，推理延迟造成观察状态与执行状态之间存在数十到数百毫秒的差距，现有方法通常忽略这种延迟，导致性能下降。

Method: 提出延迟感知扩散策略(DA-DP)框架，将零延迟轨迹校正为延迟补偿对应轨迹，并通过延迟条件增强策略，该框架与架构无关，可推广到扩散策略之外。

Result: 在各种任务、机器人和延迟条件下进行实证验证，发现DA-DP相比延迟不感知方法对延迟更具鲁棒性，成功率高。

Conclusion: DA-DP为延迟感知模仿学习提供通用模式，鼓励评估协议报告性能作为测量延迟的函数，而不仅仅是任务难度。

Abstract: As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.

</details>


### [233] [Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next](https://arxiv.org/abs/2512.07765)
*Gustavo A. Cardona,Shubham S. Kumbhar,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 本文综述了物理人-人形机器人交互的三大支柱：人形建模与控制、人类意图估计和计算人类模型，分析了各领域现状、挑战与局限，提出了跨支柱整合路径和基于交互模态与机器人参与度的统一分类法。


<details>
  <summary>Details</summary>
Motivation: 物理人-人形机器人交互在非结构化、以人为中心的环境中部署机器人具有重要意义。当前研究虽在各领域取得进展，但跨支柱整合有限，阻碍了稳健、可扩展和自适应的交互。本文旨在通过系统综述和整合框架，推动该领域发展。

Method: 通过三个核心支柱（人形建模与控制、人类意图估计、计算人类模型）综述代表性方法，识别开放挑战和局限。提出跨支柱整合路径，并引入基于交互模态（直接/间接）和机器人参与度（辅助/合作/协作）的统一分类法。

Result: 识别了各支柱的关键挑战：需要处理不确定人类动力学的全身控制策略、有限传感下的实时意图推断、考虑人类物理状态变化的建模技术。虽然各领域进展显著，但跨支柱整合仍然有限。

Conclusion: 提出了跨支柱整合的具体路径和统一分类框架，为未来研究提供路线图，旨在实现稳健、安全、直观的物理交互，使人类系统能在多样化的现实环境中有效理解、预测和与人类伙伴协作。

Abstract: Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.

</details>


### [234] [OptMap: Geometric Map Distillation via Submodular Maximization](https://arxiv.org/abs/2512.07775)
*David Thorne,Nathan Chan,Christa S. Robison,Philip R. Osteen,Brett T. Lopez*

Main category: cs.RO

TL;DR: OptMap是一个几何地图蒸馏算法，通过子模优化实现实时、应用特定的地图生成，解决了LiDAR数据选择中NP难的组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 自主机器人需要不同尺度的几何地图来支持各种感知和决策算法，但LiDAR传感器生成的大量几何数据需要进行信息丰富、尺寸受限的地图选择，这是一个NP难的组合优化问题。

Method: 提出了OptMap算法，利用具有递减回报特性的子模函数最大化，采用多项式时间算法获得近似最优解。设计了新颖的子模奖励函数来量化信息性、减少输入集大小并最小化顺序收集数据集中的偏差。还提出了动态重排序的流式子模算法来改进解的质量并处理输入顺序偏差。

Result: 在开源和自定义数据集上进行了测试，特别关注长时间建图会话，展示了OptMap的最小计算需求。提供了ROS1和ROS2开源软件包，可与任何LiDAR SLAM算法配合使用。

Conclusion: OptMap通过理论和算法创新实现了实时、应用特定的地图生成，解决了LiDAR数据选择中的计算挑战，为自主机器人系统提供了高效的地图蒸馏解决方案。

Abstract: Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.

</details>


### [235] [Inchworm-Inspired Soft Robot with Groove-Guided Locomotion](https://arxiv.org/abs/2512.07813)
*Hari Prakash Thanabalan,Lars Bengtsson,Ugo Lafont,Giovanni Volpe*

Main category: cs.RO

TL;DR: 研究人员开发了一种受尺蠖启发的软体机器人，通过图案化基底被动控制运动方向，使用单个卷曲介电弹性体驱动器，无需复杂控制系统即可实现精确方向控制。


<details>
  <summary>Details</summary>
Motivation: 传统软体机器人需要多个执行器来实现方向控制，这增加了机械复杂性、控制系统难度和能耗。需要一种更简单、更节能的方向控制方法。

Method: 设计受尺蠖启发的软体机器人，采用单个卷曲介电弹性体驱动器，通过3D打印基底上的沟槽图案被动引导机器人的对齐和轨迹，通过改变沟槽角度来控制运动方向。

Result: 系统实验表明，改变沟槽角度能够精确控制机器人的运动方向，无需复杂的驱动策略，显著降低了能耗并简化了机器人设计。

Conclusion: 这种沟槽引导方法减少了能量消耗，简化了机器人设计，扩展了仿生软体机器人在搜索救援、管道检查和行星探索等领域的应用潜力。

Abstract: Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.

</details>


### [236] [Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation](https://arxiv.org/abs/2512.07819)
*Shubham S. Kumbhar,Abhijeet M. Kulkarni,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 提出了一种人形机器人与人类伙伴协作搬运的控制框架，支持平移和旋转运动，包含高层规划器、低层控制器和刚度调节机制，在Digit人形平台上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够让人形机器人与人类伙伴进行协作搬运的控制框架，支持平移和旋转运动，这是协作搬运场景中的基本运动需求。

Method: 框架包含三个组件：1）高层规划器引入交互线性倒立摆模型，结合导纳模型和MPC规划生成动态可行的步态计划；2）基于QP的全身控制器执行计划，考虑人形-物体耦合动力学；3）刚度调节机制调节机器人与物体交互，确保收敛到期望的相对配置。

Result: 在Digit人形平台上进行了真实世界实验验证，提出了量化协作效率的指标，展示了平移、转向和半圆形轨迹等协作行为，证明了刚度调节在协作任务中的重要性。

Conclusion: 该控制框架有效支持人形机器人与人类伙伴的协作搬运任务，提出的效率指标揭示了柔顺性在协作中的关键作用，为高层和低层控制提供了有价值的轨迹特性洞察。

Abstract: We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [237] [Stronger is not better: Better Augmentations in Contrastive Learning for Medical Image Segmentation](https://arxiv.org/abs/2512.05992)
*Azeez Idris,Abdurahman Ali Mohammed,Samuel Fanijo*

Main category: eess.IV

TL;DR: 该研究评估了自监督对比学习中强数据增强对医学图像语义分割的影响，发现现有增强方法并不总是提升性能，并探索了更有效的增强策略。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习在多个下游任务中表现出色，但其中的强数据增强组件对医学图像语义分割的效果尚不明确，需要系统评估。

Method: 通过实验评估现有强数据增强方法（多种增强技术组合）对医学图像语义分割的影响，并探索其他可能提升性能的增强策略。

Result: 研究发现现有数据增强方法并不总是能提升医学图像语义分割性能，但通过实验发现了其他能够改善性能的增强技术。

Conclusion: 针对医学图像语义分割任务，需要重新审视和设计适合的自监督对比学习数据增强策略，现有通用增强方法可能不适用。

Abstract: Self-supervised contrastive learning is among the recent representation learning methods that have shown performance gains in several downstream tasks including semantic segmentation. This paper evaluates strong data augmentation, one of the most important components for self-supervised contrastive learning's improved performance. Strong data augmentation involves applying the composition of multiple augmentation techniques on images. Surprisingly, we find that the existing data augmentations do not always improve performance for semantic segmentation for medical images. We experiment with other augmentations that provide improved performance.

</details>


### [238] [Physics-Guided Diffusion Priors for Multi-Slice Reconstruction in Scientific Imaging](https://arxiv.org/abs/2512.06977)
*Laurentius Valdy,Richard D. Paul,Alessio Quercia,Zhuo Cao,Xuan Zhao,Hanno Scharr,Arya Bangun*

Main category: eess.IV

TL;DR: 提出了一种结合分区扩散先验与物理约束的框架，用于从有限测量数据中进行准确的多切片重建，显著降低GPU内存使用同时保持高质量重建


<details>
  <summary>Details</summary>
Motivation: 医学和科学成像中需要从有限测量数据快速获取准确的多切片重建，但面临病态问题和计算内存需求高的挑战

Method: 集成分区扩散先验与物理约束的框架，通过分区策略减少GPU内存使用，结合物理模型约束提升重建质量

Result: 在MRI和4D-STEM两种模态上均优于纯物理方法和完整多切片重建基线，同时提升分布内准确性和对分布外数据的泛化能力

Conclusion: 提出的框架有效解决了多切片重建中的内存和计算挑战，在保持高质量重建的同时实现了内存效率提升和良好的泛化性能

Abstract: Accurate multi-slice reconstruction from limited measurement data is crucial to speed up the acquisition process in medical and scientific imaging. However, it remains challenging due to the ill-posed nature of the problem and the high computational and memory demands. We propose a framework that addresses these challenges by integrating partitioned diffusion priors with physics-based constraints. By doing so, we substantially reduce memory usage per GPU while preserving high reconstruction quality, outperforming both physics-only and full multi-slice reconstruction baselines for different modalities, namely Magnetic Resonance Imaging (MRI) and four-dimensional Scanning Transmission Electron Microscopy (4D-STEM). Additionally, we show that the proposed method improves in-distribution accuracy as well as strong generalization to out-of-distribution datasets.

</details>


### [239] [Affine Subspace Models and Clustering for Patch-Based Image Denoising](https://arxiv.org/abs/2512.07259)
*Tharindu Wickremasinghe,Marco F. Duarte*

Main category: eess.IV

TL;DR: 该论文研究使用仿射子空间模型替代线性子空间进行图像块聚类，以更好地匹配图像块的非负特性，并提出了基于仿射子空间聚类的最小二乘投影去噪算法。


<details>
  <summary>Details</summary>
Motivation: 传统的图像块处理方法使用线性子空间模型，但图像块是非负的，在向量空间中并不围绕原点分布，因此线性子空间模型不能很好地匹配图像块的实际几何结构。

Method: 提出使用仿射子空间模型进行图像块聚类，并开发了基于仿射子空间聚类的最小二乘投影去噪算法。论文还回顾了多种解决仿射子空间聚类问题的算法方法。

Result: 实验结果显示，仿射子空间聚类在聚类和去噪性能方面都有显著提升，验证了仿射模型相比线性模型能更好地匹配图像块的几何特性。

Conclusion: 仿射子空间模型比线性子空间模型更适合图像块聚类，能更准确地描述图像块在向量空间中的分布，从而在聚类和去噪任务中取得更好的性能。

Abstract: Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.

</details>


### [240] [From sparse recovery to plug-and-play priors, understanding trade-offs for stable recovery with generalized projected gradient descent](https://arxiv.org/abs/2512.07397)
*Ali Joundi,Yann Traonmilin,Jean-François Aujol*

Main category: eess.IV

TL;DR: 本文扩展了广义投影梯度下降（GPGD）框架，增强了其对模型和投影误差的鲁棒性，提出了结构化噪声适应方法和深度投影先验的正则化策略，在稀疏恢复和图像逆问题中验证了可识别性与稳定性的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏恢复方法和基于深度学习的投影先验方法在噪声和模型误差下的鲁棒性不足，需要统一的框架来平衡可识别性与稳定性。

Method: 扩展GPGD框架的收敛性分析，提出广义反投影策略适应结构化噪声，引入归一化幂等正则化学习深度投影先验。

Result: 理论证明了GPGD对模型和投影误差的鲁棒性，数值实验在稀疏恢复和图像逆问题中展示了改进的稳定性和噪声适应性。

Conclusion: GPGD框架为稀疏恢复和逆问题提供了统一的鲁棒解决方案，通过适当的正则化和噪声适应策略可以有效平衡可识别性与稳定性。

Abstract: We consider the problem of recovering an unknown low-dimensional vector from noisy, underdetermined observations. We focus on the Generalized Projected Gradient Descent (GPGD) framework, which unifies traditional sparse recovery methods and modern approaches using learned deep projective priors. We extend previous convergence results to robustness to model and projection errors. We use these theoretical results to explore ways to better control stability and robustness constants. To reduce recovery errors due to measurement noise, we consider generalized back-projection strategies to adapt GPGD to structured noise, such as sparse outliers. To improve the stability of GPGD, we propose a normalized idempotent regularization for the learning of deep projective priors. We provide numerical experiments in the context of sparse recovery and image inverse problems, highlighting the trade-offs between identifiability and stability that can be achieved with such methods.

</details>
