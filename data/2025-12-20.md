<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 19]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict是一个几何感知的视觉-语言-动作框架，通过预测性运动学和几何先验增强连续动作策略，提升3D空间推理能力


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在机器人操作中表现出良好的泛化能力，但主要是反应式的且以2D为中心，在需要精确3D推理的任务中可靠性不足

Method: 引入轨迹级模块编码运动历史并预测多步3D关键点轨迹，以及预测性3D高斯几何模块预测工作空间几何，通过深度渲染作为训练监督，推理时仅需轻量查询令牌

Result: 在RoboCasa Human-50、LIBERO和真实世界操作任务中，GeoPredict持续优于强VLA基线，特别是在几何密集和空间要求高的场景中

Conclusion: GeoPredict通过整合预测性几何先验，显著提升了VLA模型在需要精确3D推理的机器人操作任务中的性能

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [2] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: OpenTouch是首个野外环境下的自我中心视角全手触觉数据集，包含5.1小时同步的视频-触觉-姿态数据，以及2900个带详细文本标注的剪辑片段，用于推进多模态感知研究。


<details>
  <summary>Details</summary>
Motivation: 人类手是与物理世界交互的主要界面，但自我中心视角感知很少能准确知道何时、何地以及以多大力度发生接触。现有的可穿戴触觉传感器稀缺，且没有野外数据集能将第一人称视频与全手触摸对齐。

Method: 提出了OpenTouch数据集，包含同步的视频-触觉-姿态数据，并基于此建立了检索和分类基准测试，探究触觉如何支撑感知和行动。

Result: 触觉信号为抓握理解提供了紧凑而强大的线索，增强了跨模态对齐，并且可以从野外视频查询中可靠地检索出来。

Conclusion: 通过发布这个带标注的视觉-触觉-姿态数据集和基准测试，旨在推进多模态自我中心感知、具身学习以及接触丰富的机器人操作研究。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [3] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 提出EgoMAN数据集和模型，用于解决现有3D手部轨迹预测中运动与语义监督解耦、推理与动作弱关联的问题


<details>
  <summary>Details</summary>
Motivation: 现有3D手部轨迹预测研究存在两个主要局限：1) 数据集将运动与语义监督解耦；2) 模型对推理和动作的关联较弱。需要同时解决语义推理和运动生成的问题

Method: 1) 创建EgoMAN数据集：大规模第一人称数据集，包含21.9万条6DoF轨迹和300万结构化QA对，支持语义、空间和运动推理；2) 提出EgoMAN模型：推理到运动框架，通过轨迹标记接口连接视觉语言推理和运动生成，采用渐进式训练对齐推理与运动动态

Result: 方法能够生成准确且阶段感知的轨迹，在真实场景中具有良好的泛化能力

Conclusion: 通过结合大规模语义丰富数据集和推理到运动框架，成功解决了3D手部轨迹预测中语义推理与运动生成的耦合问题，实现了准确且可泛化的阶段感知轨迹预测

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [4] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 该研究评估了YOLOv8不同变体在车牌识别和字符识别任务上的性能，提出了一种优化的识别流水线，在保持计算效率的同时实现了高精度。


<details>
  <summary>Details</summary>
Motivation: 在交通管理和车辆监控领域，高效的车牌检测与识别至关重要。尽管已有多种方法，但在多样化环境中保持实时准确性仍然具有挑战性。本研究旨在推进智能交通系统的发展。

Method: 使用两个不同的数据集进行训练和评估，测试YOLOv8变体在车牌识别和字符识别任务上的性能。提出了一种基于x轴位置的自定义字符排序方法，并设计了优化的识别流水线。

Result: YOLOv8 Nano在车牌识别任务上达到0.964的精确度和0.918的mAP50；YOLOv8 Small在字符识别任务上达到0.92的精确度和0.91的mAP50。提出的优化流水线在保持计算效率的同时确保了高准确性。

Conclusion: 研究提出的YOLOv8 Nano用于车牌识别、YOLOv8 Small用于字符识别的优化配置，为未来在边缘设备上的实际部署建立了坚实基础，是迈向更智能、更高效城市基础设施的重要一步。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [5] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 提出了一种紧凑的图像到文本架构，仅使用单个正面X光图像生成胸部X光报告，通过解剖注意力机制提升临床相关区域的生成质量，在资源有限的情况下实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的放射学报告生成系统（如MAIRA-2和MedPaLM-M）依赖大规模多模态训练、临床元数据和多个成像视图，资源密集且难以普及。需要开发更紧凑、仅依赖单张图像的方法，使放射学报告生成技术能够在资源有限的环境中应用。

Method: 结合冻结的DINOv3 Vision Transformer编码器和GPT-2解码器，通过分层解剖注意力机制增强。该机制利用肺部和心脏分割掩码，通过分层高斯平滑引导注意力到临床相关区域，不增加可训练参数。模型仅使用单个正面胸部X光图像生成报告发现部分。

Result: 在MIMIC-CXR数据集上评估，CheXpert指标显示：5种关键病理的Macro-F1提升168%（0.083→0.238），Micro-F1提升146%（0.137→0.337）；14种观察结果的性能提升86%（0.170→0.316）。结构连贯性方面，RadGraph F1提升9.7%。模型虽小且仅依赖图像，但解剖引导显著改善了空间定位和临床相关区域的连贯性。

Conclusion: 解码器级别的解剖引导能够改善空间定位，增强临床相关区域的连贯性。紧凑的图像到文本架构在资源有限的情况下也能实现显著的放射学报告生成性能提升，为更广泛的应用提供了可能性。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [6] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 研究发现GenEval基准测试存在严重的基准漂移问题，已偏离人类判断，因此提出了改进的GenEval 2基准和Soft-TIFA评估方法


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型评估面临挑战：需要法官模型评分，测试提示需对当前模型具有挑战性但对法官不难。这些约束可能导致基准随时间漂移，静态基准法官无法跟上新模型能力

Method: 1) 展示GenEval基准存在显著漂移问题；2) 通过大规模人类研究验证；3) 提出新基准GenEval 2，改进原始视觉概念覆盖和组合性；4) 提出Soft-TIFA评估方法，结合视觉原语判断

Result: GenEval基准已严重偏离人类判断，绝对误差高达17.7%，表明该基准已饱和。GenEval 2对当前模型更具挑战性，Soft-TIFA评估方法与人类判断更一致且不易漂移

Conclusion: 基准漂移是T2I评估的重要问题，需要持续审计和改进。GenEval 2提供了更强的基准，但避免漂移仍需努力，强调自动化模型评估基准需要持续维护

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [7] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: RePlan是一个用于复杂指令图像编辑的框架，通过区域对齐规划和扩散编辑实现精确的多区域并行编辑，无需迭代修复


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑模型在处理指令-视觉复杂性（IV-Complexity）时表现不佳，即复杂指令遇到杂乱或模糊场景时难以精确编辑

Method: 提出RePlan框架，包含视觉语言规划器和扩散编辑器：规划器通过逐步推理分解指令并显式定位到目标区域；编辑器使用无需训练的注意力区域注入机制进行编辑，支持精确的并行多区域编辑

Result: 在IV-Complex设置下，RePlan始终优于使用更大数据集训练的基线模型，显著提升了区域精度和整体保真度

Conclusion: RePlan通过规划-执行框架有效解决了复杂指令下的图像编辑问题，在区域精度和编辑保真度方面表现出色，为指令-视觉复杂场景下的图像编辑提供了有效解决方案

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [8] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: ReMeDI-SAM3是一种无需训练的增强型视频分割方法，针对内窥镜手术场景改进了SAM3框架，通过智能内存管理、插值扩展和特征重识别，显著提升了手术器械分割的准确性和遮挡恢复能力。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频中手术器械的准确分割对计算机辅助干预至关重要，但面临遮挡、快速运动、镜面伪影和器械重复进入等挑战。现有SAM3框架在手术场景中表现受限，存在内存更新不区分、容量固定和遮挡后身份恢复弱等问题。

Method: 提出ReMeDI-SAM3，包含三个核心组件：1) 相关性感知内存过滤，配备专用遮挡感知内存存储遮挡前帧；2) 分段插值方案扩展有效内存容量；3) 基于特征的重识别模块，结合时间投票实现可靠的遮挡后身份消歧。

Result: 在EndoVis17和EndoVis18数据集上的零样本评估显示，相比原始SAM3分别获得约7%和16%的绝对mcIoU提升，甚至超越了之前需要训练的方法。

Conclusion: ReMeDI-SAM3通过改进的内存管理和重识别机制，有效解决了手术器械分割中的遮挡和身份恢复问题，在无需训练的情况下显著提升了分割性能，为计算机辅助手术提供了更可靠的工具。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [9] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: M-PhyGs：从视频中估计多材料复杂自然物体（如花朵）的材料组成和物理参数的新方法


<details>
  <summary>Details</summary>
Motivation: 现有方法假设物体是单一材料、预先学习动力学或简单拓扑结构，但真实物体（如花朵）通常具有复杂的材料组成和几何形状，需要新的方法来估计其物理材料参数

Method: 提出Multi-material Physical Gaussians (M-PhyGs)，通过新引入的级联3D和2D损失以及时间小批量处理，从短视频中联合分割物体为相似材料并恢复其连续介质力学参数，同时考虑重力影响

Result: 在Phlowers数据集（人与花朵交互的新数据集）上的实验结果表明M-PhyGs及其组件在估计多材料物理参数方面的准确性和有效性

Conclusion: M-PhyGs能够从自然环境中拍摄的短视频中准确估计复杂多材料自然物体的材料组成和物理参数，为物理感知的计算机视觉提供了新方法

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [10] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait是一种端到端的视频扩散变换器，能够合成保持身份一致性的无限长度肖像视频，同时实现高达6倍的推理速度加速。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的长肖像动画加速方法难以确保身份一致性，需要一种既能保持身份特征又能加速推理的解决方案。

Method: 1. 使用现成的提取器计算身份无关的面部表情特征；2. 引入归一化面部表情块，通过均值和方差归一化对齐面部特征与扩散潜在空间；3. 采用动态滑动窗口方案，在重叠区域进行加权混合；4. 基于特定时间步的潜在变化率和扩散层间的导数幅度比，使用当前时间步的高阶潜在导数直接预测未来时间步的潜在表示。

Result: 在基准测试中，FlashPortrait在质量和数量上都表现出有效性，能够合成身份保持的无限长度视频，同时实现高达6倍的推理速度加速。

Conclusion: FlashPortrait通过创新的归一化面部特征对齐、动态滑动窗口和高阶导数预测方法，成功解决了长肖像动画中的身份一致性和推理速度问题。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [11] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: VIVA是一个基于指令的视频编辑框架，通过VLM引导编码和奖励优化解决现有方法在复杂真实世界指令上的泛化问题


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频编辑方法通常在简单编辑操作的配对数据上训练，这限制了它们对多样复杂真实世界指令的泛化能力

Method: 1) 引入VLM基础指导器，将文本指令、视频首帧和可选参考图像编码为视觉基础指令表示；2) 提出Edit-GRPO后训练阶段，将组相对策略优化应用于视频编辑领域；3) 设计数据构建流程生成多样高质量的基础编辑操作配对数据

Result: VIVA在指令遵循、泛化能力和编辑质量方面优于现有最先进方法

Conclusion: VIVA通过VLM引导编码和奖励优化框架，有效解决了基于指令的视频编辑在复杂真实场景中的泛化问题

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [12] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 论文提出了SceneDiff Benchmark（首个带物体实例标注的多视角变化检测基准）和SceneDiff方法（无需训练的多视角物体变化检测方法），在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何检测同一场景在不同时间捕获的图像或视频中物体的添加、移除或移动。这种变化检测对于机器人整理、施工进度和安全监控等应用很重要，主要挑战是不同视角可能导致物体错误地显示为已变化。

Method: 提出SceneDiff方法：1）在3D中对齐捕获的场景；2）提取物体区域；3）比较空间和语义区域特征来检测变化。该方法利用预训练的3D、分割和图像编码模型，无需额外训练。

Result: 在多视角和双视角基准测试中，该方法大幅优于现有方法（相对AP分别提升94%和37.4%）。创建了包含350个多样化视频对和数千个变化物体的SceneDiff Benchmark。

Conclusion: 提出的SceneDiff Benchmark填补了多视角变化检测基准的空白，而SceneDiff方法通过3D对齐和特征比较有效解决了视角变化带来的挑战，在变化检测任务上取得了显著改进。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [13] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SFTok是一种新型离散图像分词器，通过多步迭代机制实现精确重建，在64个token的高压缩率下达到最先进的图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前离散分词器在图像重建质量上仍落后于连续分词器，限制了其在多模态系统中的采用。需要解决训练-推理不一致性问题，提升离散分词器的重建性能。

Method: 提出SFTok离散分词器，采用多步迭代重建机制，结合自强制引导视觉重建和去偏拟合训练策略，解决多步过程中的训练-推理不一致问题。

Result: 在ImageNet上达到rFID=1.21的最先进重建质量，在类别到图像生成任务中取得gFID=2.29的优异性能，仅使用每张图像64个token的高压缩率。

Conclusion: SFTok通过创新的训练策略和重建机制，显著提升了离散分词器的图像重建质量，为高分辨率图像生成提供了高效的离散表示方案。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [14] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出UniStereo数据集和StereoPilot模型，解决单目到立体视频转换中的深度模糊和格式不一致问题，实现高效高质量转换


<details>
  <summary>Details</summary>
Motivation: 立体显示设备快速增长，但高质量立体视频制作成本高、复杂度大，现有单目到立体转换方法存在误差传播、深度模糊和格式不一致等问题

Method: 提出UniStereo统一数据集支持两种立体格式；设计StereoPilot前馈模型，无需显式深度图或迭代扩散采样，包含可学习域切换器和循环一致性损失

Result: StereoPilot在视觉保真度和计算效率上显著优于现有最先进方法，能无缝适应不同立体格式并提升一致性

Conclusion: 通过统一数据集和高效前馈模型，解决了立体视频转换中的关键挑战，为高质量立体内容生成提供了有效解决方案

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [15] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: AdaTooler-V是一个多模态大语言模型，通过自适应工具使用机制，仅在视觉问题真正需要工具时才调用视觉工具，避免不必要的工具调用，从而减少推理开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型存在盲目工具使用的问题，即使在不必要的情况下也会调用视觉工具，这显著增加了推理开销并降低了模型性能。为了解决这个问题，需要开发能够自适应判断何时需要工具使用的模型。

Method: 提出了AdaTooler-V模型，采用AT-GRPO强化学习算法，根据每个样本的工具效益分数自适应调整奖励尺度，鼓励模型仅在工具能真正带来改进时才调用工具。同时构建了两个训练数据集：AdaTooler-V-CoT-100k用于SFT冷启动，AdaTooler-V-300k用于带可验证奖励的强化学习，涵盖单图像、多图像和视频数据。

Result: 在12个基准测试上的实验表明，AdaTooler-V在多样化的视觉推理任务中表现出强大的推理能力，优于现有方法。特别是AdaTooler-V-7B在高分辨率基准V*上达到了89.8%的准确率，超过了商业专有模型GPT-4o和Gemini 1.5 Pro。

Conclusion: AdaTooler-V通过自适应工具使用机制有效解决了现有模型盲目调用视觉工具的问题，在减少推理开销的同时提升了性能，在多个基准测试上取得了优异表现，甚至超越了商业专有模型。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [16] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V是一个简单有效的基于指令的视频编辑框架，通过数据构建、架构简化和控制统一三个方面的创新，实现了高质量的指令驱动视频编辑。


<details>
  <summary>Details</summary>
Motivation: 图像编辑技术发展迅速，但视频编辑仍面临一致性、控制和泛化性等挑战。作者旨在探索数据、架构和控制的设计空间，开发一个简单有效的指令驱动视频编辑框架。

Method: 1. 数据方面：利用现有专家模型构建多样化视频对；通过单帧监督和共享仿射运动伪对将图像编辑对提升为视频；挖掘密集标注的视频片段；添加过渡监督。2. 模型方面：发现预训练文本到视频模型已具备编辑能力，采用简单的序列拼接条件和轻量级LoRA微调。3. 控制方面：通过单一掩码机制统一时空控制，支持可选参考图像。

Result: EasyV2V在视频编辑任务上取得了最先进的结果，超越了同期和商业系统。支持多种输入格式：视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本。

Conclusion: EasyV2V通过系统性地探索数据、架构和控制的设计空间，提供了一个简单而强大的视频编辑框架，在保持编辑一致性和控制灵活性的同时，实现了高质量的指令驱动视频编辑。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [17] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: AuditDM是一个自动化的多模态大语言模型审计框架，通过强化学习训练审计员生成挑战性问题和反事实图像，主动发现模型失败模式，并利用这些发现改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态大语言模型评估方法缺乏可解释性，且往往无法充分揭示不同模型之间的显著能力差距。需要一种更主动、可解释的方法来发现模型弱点并改进性能。

Method: 通过强化学习微调一个MLLM作为审计员，使其生成能够最大化目标模型之间分歧的挑战性问题和反事实图像。训练完成后，审计员能够发现多样化的、可解释的失败案例，这些案例无需标注即可用于模型修正。

Result: 在Gemma-3和PaliGemma-2等先进模型上，AuditDM发现了超过20种不同的失败类型。基于这些发现进行微调后，所有模型在16个基准测试中均获得一致提升，甚至使一个3B参数的模型超越了其28B参数的对应版本。

Conclusion: 随着数据扩展达到收益递减阶段，有针对性的模型审计为模型诊断和改进提供了有效途径。AuditDM框架能够主动发现模型弱点并利用这些发现提升模型性能。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [18] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出Generative Refocusing方法，通过DeblurNet恢复全焦图像和BokehNet生成可控散景，采用半监督训练结合合成数据和真实散景图像，实现单图像重对焦、文本引导调整和自定义光圈形状。


<details>
  <summary>Details</summary>
Motivation: 景深控制在摄影中很重要，但获得完美对焦通常需要多次尝试或特殊设备。单图像重对焦仍然困难，涉及恢复清晰内容和创建真实散景。现有方法需要全焦输入、依赖模拟器合成数据且光圈控制有限。

Method: 提出Generative Refocusing两阶段方法：1) DeblurNet从各种输入恢复全焦图像；2) BokehNet生成可控散景。主要创新是半监督训练，结合合成配对数据和未配对的真实散景图像，利用EXIF元数据捕捉真实光学特性。

Result: 实验表明在散焦去模糊、散景合成和重对焦基准测试中取得最佳性能。Generative Refocusing还支持文本引导调整和自定义光圈形状。

Conclusion: 提出的Generative Refocusing方法通过创新的半监督训练策略，有效解决了单图像重对焦问题，不仅性能优越，还提供了灵活的控制功能。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [19] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个多模态可提示世界事件框架，结合文本、轨迹和参考图像，实现用户导向的丰富模拟，支持多智能体交互、物体进出、参考引导外观和反直觉事件生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：纯文本方法表达能力有限，现有轨迹控制的图像到视频方法缺乏语义意图。需要一种能够结合运动、时序、可见性、语义意图和视觉基础的多模态方法，生成连贯可控的世界事件。

Method: 采用多模态方法，结合轨迹（编码运动、时序和可见性）、自然语言（语义意图）和参考图像（物体身份的视觉基础），支持生成包含多智能体交互、物体进出、参考引导外观和反直觉事件的连贯可控视频。

Result: 生成的视频不仅具有时间连贯性，还展现出涌现一致性，能够在物体暂时消失时保持物体身份和场景一致性。框架支持表达丰富的世界事件生成。

Conclusion: WorldCanvas通过支持表达性世界事件生成，将世界模型从被动预测器推进为交互式、用户可塑造的模拟器，实现了更丰富、更可控的世界模拟能力。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>
