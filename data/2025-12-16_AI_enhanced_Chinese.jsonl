{"id": "2512.13527", "categories": ["physics.med-ph", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.13527", "abs": "https://arxiv.org/abs/2512.13527", "authors": ["Mohammed S. M. Elbaz"], "title": "DarkSPARC: Dark-Blood Spectral Self-Calibrated Reconstruction of 3D Left Atrial LGE MRI for Post-Ablation Scar Imaging", "comment": null, "summary": "Purpose: To develop DarkSPARC, a retrospective, training-free, self-calibrated spectral reconstruction method that converts routine bright-blood 3D left atrial (LA) late gadolinium enhancement (LGE) MRI into a dark-blood image, and to quantify its impact on LA scar-pool CNR, SNR, effective CNR (eCNR), and scar quantification accuracy.\n  Methods: DarkSPARC embeds bright-blood LA LGE into a calibrator-conditioned (N+1)-dimensional spectral domain and reconstructs a dark-blood-like image using scan-specific spectral landmarks. A scan-specific 3D numerical phantom framework was built from LAScarQS post-ablation LGE by cloning remote myocardium into the LA wall and imposing controlled scar burden. Five baseline cases spanning the 5th-95th percentiles of native scar-pool CNR, each with multiple scar burdens and 10 CNR degradation levels, yielded 200 phantoms. For every phantom, LA scar-pool CNR, SNR, eCNR, and Scar% were measured on bright-blood and DarkSPARC images. In vivo performance was evaluated in 60 public post-ablation scans of atrial fibrillation patients.\n  Results: In scan-specific phantoms, DarkSPARC increased LA scar-pool CNR, SNR, and eCNR over bright-blood in all 200 experiments, with DarkSPARC/bright-blood ratios up to about 30-fold for CNR and about 6-fold for SNR in the lowest-CNR conditions. At 70% CNR degradation, bright-blood underestimated ground-truth LA Scar% by -37% to -54%, whereas DarkSPARC reduced bias to about -3% to -5%. In vivo, DarkSPARC similarly improved metrics: median scar-pool CNR, SNR, and eCNR increased from 20.0 to 135.9 (6.8x), 70.6 to 200.6 (2.8x), and 0.22 to 0.75 (3.4x), respectively (all p<0.001), and LA Scar% increased from 3.9% to 9.75%.\n  Conclusion: DarkSPARC is a self-calibrated, training-free reconstruction that yields dark-blood 3D LA LGE, boosting CNR/SNR/eCNR and stabilizing reliable scar quantification without extra scans.", "AI": {"tldr": "DarkSPARC\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u81ea\u6821\u51c6\u7684\u5149\u8c31\u91cd\u5efa\u65b9\u6cd5\uff0c\u53ef\u5c06\u5e38\u89c4\u4eae\u88403D\u5de6\u5fc3\u623f\u5ef6\u8fdf\u589e\u5f3aMRI\u8f6c\u6362\u4e3a\u6697\u8840\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u75a4\u75d5-\u8840\u6c60\u5bf9\u6bd4\u566a\u58f0\u6bd4\u3001\u4fe1\u566a\u6bd4\u548c\u6709\u6548\u5bf9\u6bd4\u566a\u58f0\u6bd4\uff0c\u5e76\u63d0\u9ad8\u75a4\u75d5\u5b9a\u91cf\u51c6\u786e\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u989d\u5916\u626b\u63cf\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6697\u8840\u91cd\u5efa\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u5de6\u5fc3\u623f\u5ef6\u8fdf\u589e\u5f3aMRI\u4e2d\u75a4\u75d5\u4e0e\u8840\u6c60\u7684\u5bf9\u6bd4\u5ea6\uff0c\u63d0\u9ad8\u75a4\u75d5\u5b9a\u91cf\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "DarkSPARC\u5c06\u4eae\u8840\u5de6\u5fc3\u623f\u5ef6\u8fdf\u589e\u5f3aMRI\u5d4c\u5165\u5230\u6821\u51c6\u5668\u8c03\u8282\u7684(N+1)\u7ef4\u5149\u8c31\u57df\u4e2d\uff0c\u5229\u7528\u626b\u63cf\u7279\u5b9a\u7684\u5149\u8c31\u6807\u5fd7\u91cd\u5efa\u6697\u8840\u6837\u56fe\u50cf\u3002\u901a\u8fc7\u4eceLAScarQS\u540e\u6d88\u878d\u5ef6\u8fdf\u589e\u5f3a\u6570\u636e\u6784\u5efa\u626b\u63cf\u7279\u5b9a\u76843D\u6570\u503c\u4f53\u6a21\u6846\u67b6\uff0c\u514b\u9686\u8fdc\u7a0b\u5fc3\u808c\u5230\u5de6\u5fc3\u623f\u58c1\u5e76\u65bd\u52a0\u53ef\u63a7\u7684\u75a4\u75d5\u8d1f\u8377\uff0c\u521b\u5efa\u4e86200\u4e2a\u4f53\u6a21\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u4f53\u6a21\u5b9e\u9a8c\u4e2d\uff0cDarkSPARC\u5728\u6240\u6709200\u4e2a\u5b9e\u9a8c\u4e2d\u5747\u63d0\u5347\u4e86\u75a4\u75d5-\u8840\u6c60\u5bf9\u6bd4\u566a\u58f0\u6bd4\u3001\u4fe1\u566a\u6bd4\u548c\u6709\u6548\u5bf9\u6bd4\u566a\u58f0\u6bd4\uff0c\u5728\u6700\u4f4e\u5bf9\u6bd4\u566a\u58f0\u6bd4\u6761\u4ef6\u4e0b\uff0c\u6697\u8840/\u4eae\u8840\u6bd4\u7387\u53ef\u8fbe\u7ea630\u500d\uff08\u5bf9\u6bd4\u566a\u58f0\u6bd4\uff09\u548c\u7ea66\u500d\uff08\u4fe1\u566a\u6bd4\uff09\u3002\u572870%\u5bf9\u6bd4\u566a\u58f0\u6bd4\u9000\u5316\u65f6\uff0c\u4eae\u8840\u4f4e\u4f30\u771f\u5b9e\u75a4\u75d5\u767e\u5206\u6bd437%-54%\uff0c\u800cDarkSPARC\u5c06\u504f\u5dee\u964d\u4f4e\u81f3\u7ea63%-5%\u3002\u5728\u4f53\u5185\u5b9e\u9a8c\u4e2d\uff0c\u5404\u9879\u6307\u6807\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DarkSPARC\u662f\u4e00\u79cd\u81ea\u6821\u51c6\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u53ef\u751f\u6210\u6697\u88403D\u5de6\u5fc3\u623f\u5ef6\u8fdf\u589e\u5f3a\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6bd4\u566a\u58f0\u6bd4\u3001\u4fe1\u566a\u6bd4\u548c\u6709\u6548\u5bf9\u6bd4\u566a\u58f0\u6bd4\uff0c\u7a33\u5b9a\u53ef\u9760\u7684\u75a4\u75d5\u5b9a\u91cf\uff0c\u65e0\u9700\u989d\u5916\u626b\u63cf\u3002"}}
{"id": "2512.11865", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11865", "abs": "https://arxiv.org/abs/2512.11865", "authors": ["Ju-Young Kim", "Ji-Hong Park", "Myeongjun Kim", "Gun-Woo Kim"], "title": "Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation", "comment": "Accepted to MobieSec 2025 (poster session)", "summary": "Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eOpenVLA-OFT\u6846\u67b6\u7684\u53ef\u89e3\u91ca\u5bf9\u6297\u9c81\u68d2\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7Evidence-3\u6a21\u5757\u68c0\u6d4b\u5149\u5ea6\u6270\u52a8\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u63d0\u5347\u52a8\u4f5c\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u667a\u80fd\u519c\u4e1a\u4e2d\u4f9d\u8d56RGB\u76f8\u673a\u611f\u77e5\u548c\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u7684\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u8272\u8c03\u3001\u5149\u7167\u548c\u566a\u58f0\u7b49\u5149\u5ea6\u6270\u52a8\u7684\u653b\u51fb\uff0c\u5bfc\u81f4\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u529f\u80fd\u5931\u6548\uff0c\u9700\u8981\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027", "method": "\u57fa\u4e8eOpenVLA-OFT\u6846\u67b6\u6784\u5efa\u53ef\u89e3\u91ca\u5bf9\u6297\u9c81\u68d2\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u96c6\u6210Evidence-3\u6a21\u5757\u68c0\u6d4b\u5149\u5ea6\u6270\u52a8\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u8bf4\u660e\u6270\u52a8\u539f\u56e0\u548c\u5f71\u54cd", "result": "\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u5f53\u524d\u52a8\u4f5cL1\u635f\u5931\u964d\u4f4e21.7%\uff0c\u4e0b\u4e00\u52a8\u4f5cL1\u635f\u5931\u964d\u4f4e18.4%\uff0c\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u52a8\u4f5c\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "\u63d0\u51fa\u7684\u53ef\u89e3\u91ca\u5bf9\u6297\u9c81\u68d2\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u80fd\u6709\u6548\u5e94\u5bf9\u667a\u80fd\u519c\u4e1a\u4e2d\u7684\u5149\u5ea6\u6270\u52a8\u95ee\u9898\uff0c\u63d0\u9ad8\u7cfb\u7edf\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027"}}
{"id": "2512.11802", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11802", "abs": "https://arxiv.org/abs/2512.11802", "authors": ["Zheng Li", "Peng Zhang", "Shixiao Liang", "Hang Zhou", "Chengyuan Ma", "Handong Yao", "Qianwen Li", "Xiaopeng Li"], "title": "Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights", "comment": null, "summary": "Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u5730\u5b9e\u9a8c\u5206\u6790\u4e86\u7279\u65af\u62c9\u4ea4\u901a\u4fe1\u53f7\u706f\u548c\u505c\u8f66\u6807\u5fd7\u63a7\u5236\u7cfb\u7edf\uff08TLSSC\uff09\u4e0e\u4ea4\u901a\u63a7\u5236\u8bbe\u5907\uff08TCD\uff09\u7684\u4ea4\u4e92\u884c\u4e3a\uff0c\u5efa\u7acb\u4e86\u884c\u4e3a\u5206\u7c7b\u5e76\u6821\u51c6\u4e86\u5168\u901f\u5ea6\u5dee\u6a21\u578b\uff0c\u53d1\u73b0\u4e86\u7ea690\u7c73\u7684\u8ddf\u8f66\u9608\u503c\u7b49\u91cd\u8981\u5b9e\u8bc1\u7ed3\u679c\u3002", "motivation": "\u867d\u7136\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u5bf9\u4ea4\u901a\u8fd0\u8425\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u5176\u4e0e\u4ea4\u901a\u63a7\u5236\u8bbe\u5907\uff08TCD\uff09\u7684\u4ea4\u4e92\u884c\u4e3a\u7f3a\u4e4f\u6df1\u5165\u7684\u5b9e\u8bc1\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5206\u6790\u7279\u65af\u62c9TLSSC\u7cfb\u7edf\u4e0eTCD\u7684\u4ea4\u4e92\uff0c\u4e3a\u672a\u6765\u7684\u4eff\u771f\u3001\u5b89\u5168\u8bc4\u4f30\u548c\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u5728\u4e0d\u540c\u901f\u5ea6\u9650\u5236\u548cTCD\u7c7b\u578b\u4e0b\u7684\u5b9e\u5730\u5b9e\u9a8c\uff0c\u6536\u96c6\u4e86\u540c\u6b65\u7684\u9ad8\u5206\u8fa8\u7387\u8f66\u8f86\u8f68\u8ff9\u6570\u636e\u548c\u9a7e\u9a76\u5458\u89c6\u89d2\u89c6\u9891\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\uff0c\u5efa\u7acb\u4e86TLSSC-TCD\u4ea4\u4e92\u884c\u4e3a\u5206\u7c7b\uff08\u505c\u6b62\u3001\u52a0\u901f\u3001\u8ddf\u8f66\uff09\uff0c\u5e76\u6821\u51c6\u4e86\u5168\u901f\u5ea6\u5dee\u6a21\u578b\uff08FVDM\uff09\u6765\u5b9a\u91cf\u8868\u5f81\u6bcf\u79cd\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u8ddf\u8f66\u9608\u503c\uff08\u7ea690\u7c73\uff09\u3002\u6821\u51c6\u7ed3\u679c\u663e\u793a\uff1a\u505c\u6b62\u884c\u4e3a\u5bf9\u671f\u671b\u901f\u5ea6\u504f\u5dee\u548c\u76f8\u5bf9\u901f\u5ea6\u90fd\u6709\u5f3a\u70c8\u54cd\u5e94\uff1b\u52a0\u901f\u884c\u4e3a\u76f8\u5bf9\u4fdd\u5b88\uff1b\u4ea4\u53c9\u53e3\u8ddf\u8f66\u884c\u4e3a\u6bd4\u6807\u51c6\u8ddf\u8f66\u884c\u4e3a\u8868\u73b0\u51fa\u66f4\u5e73\u6ed1\u7684\u52a8\u6001\u7279\u6027\u548c\u66f4\u5c0f\u7684\u8f66\u5934\u65f6\u8ddd\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u7684\u6570\u636e\u5e93\u3001\u884c\u4e3a\u5b9a\u4e49\u548c\u6a21\u578b\u8868\u5f81\u4e3a\u672a\u6765ADAS-TCD\u4ea4\u4e92\u903b\u8f91\u7684\u4eff\u771f\u3001\u5b89\u5168\u8bc4\u4f30\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002\u6570\u636e\u96c6\u5df2\u5728GitHub\u4e0a\u516c\u5f00\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2512.12236", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12236", "abs": "https://arxiv.org/abs/2512.12236", "authors": ["Aujasvit Datta", "Jiayun Wang", "Asad Aali", "Armeet Singh Jatyani", "Anima Anandkumar"], "title": "Resolution-Independent Neural Operators for Multi-Rate Sparse-View CT", "comment": null, "summary": "Sparse-view Computed Tomography (CT) reconstructs images from a limited number of X-ray projections to reduce radiation and scanning time, which makes reconstruction an ill-posed inverse problem. Deep learning methods achieve high-fidelity reconstructions but often overfit to a fixed acquisition setup, failing to generalize across sampling rates and image resolutions. For example, convolutional neural networks (CNNs) use the same learned kernels across resolutions, leading to artifacts when data resolution changes.\n  We propose Computed Tomography neural Operator (CTO), a unified CT reconstruction framework that extends to continuous function space, enabling generalization (without retraining) across sampling rates and image resolutions. CTO operates jointly in the sinogram and image domains through rotation-equivariant Discrete-Continuous convolutions parametrized in the function space, making it inherently resolution- and sampling-agnostic. Empirically, CTO enables consistent multi-sampling-rate and cross-resolution performance, with on average >4dB PSNR gain over CNNs. Compared to state-of-the-art diffusion methods, CTO is 500$\\times$ faster in inference time with on average 3dB gain. Empirical results also validate our design choices behind CTO's sinogram-space operator learning and rotation-equivariant convolution. Overall, CTO outperforms state-of-the-art baselines across sampling rates and resolutions, offering a scalable and generalizable solution that makes automated CT reconstruction more practical for deployment.", "AI": {"tldr": "CTO\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684CT\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u65cb\u8f6c\u7b49\u53d8\u79bb\u6563-\u8fde\u7eed\u5377\u79ef\uff0c\u5b9e\u73b0\u4e86\u8de8\u91c7\u6837\u7387\u548c\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u6cdb\u5316\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u83b7\u5f97\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u662f\u4e00\u4e2a\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\uff0c\u4f46\u901a\u5e38\u8fc7\u62df\u5408\u4e8e\u56fa\u5b9a\u7684\u91c7\u96c6\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u91c7\u6837\u7387\u548c\u56fe\u50cf\u5206\u8fa8\u7387\u3002\u4f8b\u5982\uff0cCNN\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u4f7f\u7528\u76f8\u540c\u7684\u5b66\u4e60\u6838\uff0c\u5bfc\u81f4\u5206\u8fa8\u7387\u53d8\u5316\u65f6\u4ea7\u751f\u4f2a\u5f71\u3002", "method": "\u63d0\u51faCTO\u6846\u67b6\uff0c\u5728\u8fde\u7eed\u51fd\u6570\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u901a\u8fc7\u65cb\u8f6c\u7b49\u53d8\u7684\u79bb\u6563-\u8fde\u7eed\u5377\u79ef\u5728\u6b63\u5f26\u56fe\u548c\u56fe\u50cf\u57df\u8054\u5408\u5de5\u4f5c\u3002\u8fd9\u4e9b\u5377\u79ef\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u53c2\u6570\u5316\uff0c\u4f7f\u5176\u5929\u751f\u5177\u6709\u5206\u8fa8\u7387\u548c\u91c7\u6837\u65e0\u5173\u6027\u3002", "result": "CTO\u5b9e\u73b0\u4e86\u8de8\u591a\u91c7\u6837\u7387\u548c\u5206\u8fa8\u7387\u7684\u7a33\u5b9a\u6027\u80fd\uff0c\u5e73\u5747\u6bd4CNN\u63d0\u9ad8>4dB PSNR\u3002\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u6269\u6563\u65b9\u6cd5\uff0cCTO\u63a8\u7406\u901f\u5ea6\u5feb500\u500d\uff0c\u5e73\u5747PSNR\u63d0\u9ad83dB\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6b63\u5f26\u56fe\u7a7a\u95f4\u7b97\u5b50\u5b66\u4e60\u548c\u65cb\u8f6c\u7b49\u53d8\u5377\u79ef\u7684\u8bbe\u8ba1\u9009\u62e9\u3002", "conclusion": "CTO\u5728\u91c7\u6837\u7387\u548c\u5206\u8fa8\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u81ea\u52a8\u5316CT\u91cd\u5efa\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u66f4\u52a0\u5b9e\u7528\u3002"}}
{"id": "2512.11869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11869", "abs": "https://arxiv.org/abs/2512.11869", "authors": ["D. Shainu Suhas", "G. Rahul", "K. Muni"], "title": "Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion", "comment": null, "summary": "Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.", "AI": {"tldr": "Temporal-Anchor3DLane\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u3001\u6dfb\u52a0\u8f7b\u91cf\u7ea7LSTM\u65f6\u95f4\u878d\u5408\u6a21\u5757\u548cESCOP\u8bad\u7ec3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u65f6\u5e8f\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684Anchor3DLane\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u4e0d\u9519\uff0c\u4f46\u4ecd\u5b58\u5728\u56de\u5f52\u5f02\u5e38\u503c\u654f\u611f\u3001\u5168\u5c40\u66f2\u7ebf\u51e0\u4f55\u76d1\u7763\u5f31\u3001\u591a\u635f\u5931\u9879\u5e73\u8861\u56f0\u96be\u4ee5\u53ca\u65f6\u95f4\u8fde\u7eed\u6027\u5229\u7528\u6709\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u53473D\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86Temporal-Anchor3DLane\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6539\u8fdb\uff1a(1) \u591a\u4efb\u52a1\u635f\u5931\u6539\u8fdb\uff1a\u5e73\u8861L1\u56de\u5f52\u3001Chamfer\u70b9\u96c6\u8ddd\u79bb\u3001\u4e0d\u786e\u5b9a\u6027\u635f\u5931\u52a0\u6743\uff0c\u4ee5\u53ca\u5206\u7c7b\u548c\u53ef\u89c1\u6027\u7684focal\u548cDice\u635f\u5931\uff1b(2) \u8f7b\u91cf\u7ea7\u65f6\u95f4LSTM\u878d\u5408\u6a21\u5757\uff1a\u8de8\u5e27\u805a\u5408\u6bcf\u4e2a\u951a\u70b9\u7684\u7279\u5f81\uff0c\u66ff\u4ee3\u8f83\u91cd\u7684Transformer\u5f0f\u65f6\u95f4\u878d\u5408\uff1b(3) ESCOP\u5f0f\u8bad\u7ec3\u4f18\u5316\uff1a\u5c06\u66f2\u7ebf\u7ea7\u76d1\u7763\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027\u76f8\u7ed3\u5408\u3002", "result": "\u5728OpenLane\u6570\u636e\u96c6\u4e0a\uff0cTemporal-Anchor3DLane\u5c06F1\u5206\u6570\u63d0\u5347\u4e86+6.2\uff0c\u5e76\u4ea7\u751f\u4e86\u66f4\u5e73\u6ed1\u7684\u65f6\u95f4\u8f68\u8ff9\uff0c\u8868\u660e\u5c0f\u7684\u67b6\u6784\u548c\u635f\u5931\u6539\u8fdb\u53ef\u4ee5\u663e\u8457\u589e\u5f3a3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u6216\u6269\u5c55\u89c4\u6a21\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u635f\u5931\u51fd\u6570\u6539\u8fdb\u3001\u8f7b\u91cf\u7ea7\u65f6\u95f4\u878d\u5408\u548c\u8bad\u7ec3\u4f18\u5316\uff0cTemporal-Anchor3DLane\u6709\u6548\u89e3\u51b3\u4e86Anchor3DLane\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u65f6\u5e8f\u7a33\u5b9a\u6027\uff0c\u8bc1\u660e\u4e86\u5c0f\u89c4\u6a21\u6539\u8fdb\u4e5f\u80fd\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.11824", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11824", "abs": "https://arxiv.org/abs/2512.11824", "authors": ["Rosh Ho", "Jian Zhang"], "title": "ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision", "comment": null, "summary": "This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \\SI{96.73}{\\percent} grasp classification accuracy with sub-\\SI{40.00}{\\milli\\second} end-to-end latency. Physical validation using standardized benchmarks shows \\SI{82.71}{\\percent} success on YCB object manipulation and reliable performance across \\SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \\$\\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.", "AI": {"tldr": "ReGlove\u7cfb\u7edf\u5c06\u4f4e\u6210\u672c\u5546\u7528\u6c14\u52a8\u5eb7\u590d\u624b\u5957\u6539\u9020\u4e3a\u89c6\u89c9\u5f15\u5bfc\u7684\u8f85\u52a9\u77eb\u5f62\u5668\uff0c\u901a\u8fc7\u624b\u8155\u6444\u50cf\u5934\u548c\u8fb9\u7f18\u8ba1\u7b97\u5b9e\u73b0\u65e0\u9700\u53ef\u9760\u808c\u8089\u4fe1\u53f7\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6293\u63e1\uff0c\u6210\u672c\u4f4e\u4e8e250\u7f8e\u5143\u3002", "motivation": "\u6162\u6027\u4e0a\u80a2\u529f\u80fd\u969c\u788d\u5f71\u54cd\u5168\u7403\u6570\u767e\u4e07\u4eba\uff0c\u4f46\u73b0\u6709\u8f85\u52a9\u6280\u672f\u8981\u4e48\u4ef7\u683c\u6602\u8d35\uff0c\u8981\u4e48\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684\u751f\u7269\u4fe1\u53f7\uff0c\u9700\u8981\u66f4\u7ecf\u6d4e\u3001\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u96c6\u6210\u624b\u8155\u6444\u50cf\u5934\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u63a8\u7406\u5f15\u64ce\uff08Raspberry Pi 5\uff09\uff0c\u91c7\u7528\u5b9e\u65f6YOLO\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u6293\u63e1\uff0c\u5c06\u5546\u7528\u6c14\u52a8\u5eb7\u590d\u624b\u5957\u6539\u9020\u4e3a\u89c6\u89c9\u5f15\u5bfc\u8f85\u52a9\u77eb\u5f62\u5668\u3002", "result": "\u8fbe\u523096.73%\u7684\u6293\u63e1\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4f4e\u4e8e40\u6beb\u79d2\uff1b\u5728YCB\u7269\u4f53\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u8fbe82.71%\uff0c\u572827\u9879\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u4efb\u52a1\u4e2d\u8868\u73b0\u53ef\u9760\u3002", "conclusion": "ReGlove\u4e3a\u57fa\u4e8e\u89c6\u89c9\u7684\u4e0a\u80a2\u8f85\u52a9\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\uff0c\u6210\u672c\u4f4e\u4e8e250\u7f8e\u5143\u4e14\u5168\u90e8\u4f7f\u7528\u5546\u7528\u7ec4\u4ef6\uff0c\u53ef\u4f7f\u88ab\u4f20\u7edfEMG\u63a7\u5236\u8bbe\u5907\u6392\u9664\u5728\u5916\u7684\u4eba\u7fa4\u53d7\u76ca\u3002"}}
{"id": "2512.12284", "categories": ["eess.IV", "cs.AI", "cs.AR", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.12284", "abs": "https://arxiv.org/abs/2512.12284", "authors": ["Donghyuk Kim", "Sejeong Yang", "Wonjin Shin", "Joo-Young Kim"], "title": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval", "comment": "14 pages, 20 figures, conference", "summary": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.", "AI": {"tldr": "V-Rex\u662f\u9996\u4e2a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u52a0\u901f\u5668\uff0c\u4e13\u95e8\u89e3\u51b3\u6d41\u5f0f\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e2dKV\u7f13\u5b58\u589e\u957f\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u901a\u8fc7ReSV\u7b97\u6cd5\u548c\u786c\u4ef6\u52a0\u901f\u5668\u5b9e\u73b0\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u6d41\u5f0f\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u65f6\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a\u968f\u7740\u8fde\u7eed\u89c6\u9891\u8f93\u5165\uff0cKV\u7f13\u5b58\u5927\u5e45\u589e\u957f\uff0c\u5bfc\u81f4\u8fed\u4ee3\u9884\u586b\u5145\u9636\u6bb5\u4ea7\u751f\u5927\u91cf\u8ba1\u7b97\u3001\u6570\u636e\u4f20\u8f93\u548c\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\uff0c\u8fd9\u5728\u8fb9\u7f18\u90e8\u7f72\u4e2d\u5c24\u4e3a\u4e25\u91cd\u3002", "method": "\u63d0\u51faV-Rex\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u52a0\u901f\u5668\uff1a1) ReSV\u7b97\u6cd5\uff1a\u57fa\u4e8e\u65f6\u7a7a\u76f8\u4f3c\u6027\u7684\u65e0\u8bad\u7ec3\u52a8\u6001KV\u7f13\u5b58\u68c0\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7token\u805a\u7c7b\u51cf\u5c11\u8de8\u89c6\u9891\u5e27\u7684KV\u7f13\u5b58\uff1b2) \u786c\u4ef6\u52a0\u901f\u5668\uff1a\u5305\u542b\u52a8\u6001KV\u7f13\u5b58\u68c0\u7d22\u5f15\u64ce(DRE)\uff0c\u91c7\u7528\u4f4d\u7ea7\u548c\u65e9\u671f\u9000\u51fa\u8ba1\u7b97\u5355\u5143\u3002", "result": "V-Rex\u5728\u8fb9\u7f18\u90e8\u7f72\u4e2d\u5b9e\u73b0\u524d\u6240\u672a\u6709\u76843.9-8.3 FPS\u5b9e\u65f6\u6d41\u5f0f\u89c6\u9891LLM\u63a8\u7406\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\uff1bDRE\u4ec5\u53602.2%\u529f\u8017\u548c2.0%\u9762\u79ef\uff0c\u76f8\u6bd4AGX Orin GPU\u5b9e\u73b01.9-19.7\u500d\u52a0\u901f\u548c3.1-18.5\u500d\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5168\u9762\u89e3\u51b3\u7b97\u6cd5\u548c\u786c\u4ef6\u5c42\u9762KV\u7f13\u5b58\u68c0\u7d22\u95ee\u9898\u7684\u5de5\u4f5c\uff0c\u4f7f\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5f0f\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u4e3a\u8fb9\u7f18AI\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2512.11871", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11871", "abs": "https://arxiv.org/abs/2512.11871", "authors": ["Tekleab G. Gebremedhin", "Hailom S. Asegede", "Bruh W. Tesheme", "Tadesse B. Gebremichael", "Kalayu G. Redae"], "title": "Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops", "comment": "A preliminary version of this work was presented at the International Conference on Postwar Technology for Recovery and Sustainable Development (Feb. 2025). This manuscript substantially extends that work with expanded experiments and on-device deployment analysis. Code and dataset are publicly available at: https://github.com/Tekleab15/Automated_plant_disease_and_pest_detection_system", "summary": "Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u4e3a\u57c3\u585e\u4fc4\u6bd4\u4e9a\u63d0\u683c\u96f7\u5730\u533a\u5f00\u53d1\u4e86\u79bb\u7ebf\u4f5c\u7269\u75c5\u5bb3\u68c0\u6d4b\u7cfb\u7edf\uff0c\u91cd\u70b9\u5173\u6ce8\u4ed9\u4eba\u638c\u65e0\u82b1\u679c\u75c5\u5bb3\uff0c\u4f7f\u7528\u4e09\u79cd\u79fb\u52a8\u9ad8\u6548\u67b6\u6784\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff0c\u6700\u7ec8\u90e8\u7f72\u5230\u652f\u6301\u672c\u5730\u8bed\u8a00\u7684\u79bb\u7ebf\u5e94\u7528\u4e2d\u3002", "motivation": "\u63d0\u683c\u96f7\u5730\u533a80%\u4ee5\u4e0a\u4eba\u53e3\u4f9d\u8d56\u519c\u4e1a\uff0c\u4f46\u57fa\u7840\u8bbe\u65bd\u4e2d\u65ad\u9650\u5236\u4e86\u4f5c\u7269\u75c5\u5bb3\u4e13\u5bb6\u8bca\u65ad\u7684\u83b7\u53d6\u3002\u9700\u8981\u5f00\u53d1\u79bb\u7ebf\u68c0\u6d4b\u7cfb\u7edf\u6765\u652f\u6301\u8be5\u5730\u533a\u7684\u7cae\u98df\u5b89\u5168\u5173\u952e\u8bca\u65ad\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b3,587\u5f20\u7530\u95f4\u56fe\u50cf\u7684\u672c\u571f\u4ed9\u4eba\u638c\u65e0\u82b1\u679c\u6570\u636e\u96c6\uff0c\u5728\u90e8\u7f72\u53d7\u9650\u7684\u540e\u51b2\u7a81\u8fb9\u7f18\u73af\u5883\u4e2d\uff0c\u5bf9\u4e09\u79cd\u79fb\u52a8\u9ad8\u6548\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1a\u5b9a\u5236\u8f7b\u91cf\u7ea7CNN\u3001EfficientNet-Lite1\u548cCNN-Transformer\u6df7\u5408\u67b6\u6784MobileViT-XS\u3002", "result": "EfficientNet-Lite1\u8fbe\u523090.7%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u8f7b\u91cf\u7ea7CNN\u8fbe\u523089.5%\u4e14\u5177\u6709\u6700\u4f73\u90e8\u7f72\u7279\u6027\uff0842ms\u63a8\u7406\u5ef6\u8fdf\uff0c4.8MB\u6a21\u578b\u5927\u5c0f\uff09\uff0cMobileViT-XS\u8fbe\u523097.3%\u5e73\u5747\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u663e\u793a\u57fa\u4e8eMHSA\u7684\u5168\u5c40\u63a8\u7406\u6bd4\u5c40\u90e8\u7eb9\u7406CNN\u6838\u66f4\u53ef\u9760\u5730\u533a\u5206\u5bb3\u866b\u96c6\u7fa4\u548c\u771f\u83cc\u75c5\u53d8\u3002", "conclusion": "ARM\u517c\u5bb9\u6a21\u578b\u5df2\u90e8\u7f72\u5230\u652f\u6301\u63d0\u683c\u91cc\u5c3c\u4e9a\u8bed\u548c\u963f\u59c6\u54c8\u62c9\u8bed\u7684Flutter\u5e94\u7528\u4e2d\uff0c\u53ef\u5728Cortex-A53\u7c7b\u8bbe\u5907\u4e0a\u5b8c\u5168\u79bb\u7ebf\u63a8\u7406\uff0c\u589e\u5f3a\u4e86\u7cae\u98df\u5b89\u5168\u5173\u952e\u8bca\u65ad\u7684\u5305\u5bb9\u6027\u3002MobileViT-XS\u7684\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u5728\u533a\u5206\u75c5\u5bb3\u5f62\u6001\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2512.11872", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11872", "abs": "https://arxiv.org/abs/2512.11872", "authors": ["Mingwang Xu", "Jiahao Cui", "Feipeng Cai", "Hanlin Shang", "Zhihao Zhu", "Shan Luan", "Yifang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff", "AI": {"tldr": "WAM-Diff\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a9\u7801\u6269\u6563\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u4f7f\u7528\u79bb\u6563\u5e8f\u5217\u8fed\u4ee3\u751f\u6210\u672a\u6765\u8f68\u8ff9\uff0c\u5728NAVSIM\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e3b\u8981\u4f7f\u7528\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8fde\u7eed\u6269\u6563\u7b56\u7565\uff0c\u800c\u79bb\u6563\u63a9\u7801\u6269\u6563\u5728\u8f68\u8ff9\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faWAM-Diff\u6846\u67b6\uff1a1\uff09\u5c06\u63a9\u7801\u6269\u6563\u7cfb\u7edf\u6027\u5730\u9002\u914d\u5230\u81ea\u52a8\u9a7e\u9a76\uff0c\u652f\u6301\u7075\u6d3b\u7684\u975e\u56e0\u679c\u89e3\u7801\u987a\u5e8f\uff1b2\uff09\u901a\u8fc7\u7a00\u758fMoE\u67b6\u6784\u8054\u5408\u8bad\u7ec3\u8fd0\u52a8\u9884\u6d4b\u548c\u9a7e\u9a76\u5bfc\u5411\u7684\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff1b3\uff09\u4f7f\u7528\u7ec4\u5e8f\u5217\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4ee5\u4f18\u5316\u5e8f\u5217\u7ea7\u9a7e\u9a76\u5956\u52b1\u3002", "result": "\u5728NAVSIM-v1\u4e0a\u83b7\u5f9791.0 PDMS\u5206\u6570\uff0c\u5728NAVSIM-v2\u4e0a\u83b7\u5f9789.7 EPDMS\u5206\u6570\uff0c\u8bc1\u660e\u4e86\u63a9\u7801\u6269\u6563\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u56de\u5f52\u548c\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u573a\u666f\u611f\u77e5\u7684\u89e3\u7801\u7b56\u7565\u8fdb\u884c\u8f68\u8ff9\u751f\u6210\u3002"}}
{"id": "2512.12952", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12952", "abs": "https://arxiv.org/abs/2512.12952", "authors": ["Krishna Srikar Durbha", "Hassene Tmar", "Ping-Hao Wu", "Ioannis Katsavounidis", "Alan C. Bovik"], "title": "Leveraging Compression to Construct Transferable Bitrate Ladders", "comment": "Under Review in IEEE Transactions on Image Processing", "summary": "Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6bd4\u7279\u7387\u9636\u68af\u6784\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u538b\u7f29\u8fc7\u7a0b\u5e76\u5bf9\u6e90\u89c6\u9891\u8fdb\u884c\u611f\u77e5\u76f8\u5173\u6d4b\u91cf\uff0c\u51c6\u786e\u9884\u6d4b\u538b\u7f29\u89c6\u9891\u7684VMAF\u5206\u6570\uff0c\u4ece\u800c\u66ff\u4ee3\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u51f8\u5305\u6784\u5efa\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u6bcf\u6807\u9898\u548c\u6bcf\u955c\u5934\u89c6\u9891\u7f16\u7801\u6280\u672f\u76f8\u6bd4\u56fa\u5b9a\u6bd4\u7279\u7387\u9636\u68af\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u4e3a\u6bcf\u4e2a\u89c6\u9891\u6784\u5efa\u51f8\u5305\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u5f00\u9500\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u51fa\u73b0\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6bd4\u7279\u7387\u9636\u68af\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u538b\u7f29\u8fc7\u7a0b\u5e76\u5bf9\u6e90\u89c6\u9891\u8fdb\u884c\u611f\u77e5\u76f8\u5173\u6d4b\u91cf\u6765\u9884\u6d4b\u538b\u7f29\u89c6\u9891\u7684VMAF\u5206\u6570\u3002\u540c\u65f6\u7814\u7a76\u4e86\u4e0d\u540c\u7f16\u7801\u8bbe\u7f6e\u4e0b\u6bcf\u955c\u5934\u6bd4\u7279\u7387\u9636\u68af\u7684\u6027\u80fd\u3002", "result": "\u5728\u5927\u89c4\u6a21\u89c6\u9891\u8bed\u6599\u5e93\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u6846\u67b6\u4e0e\u73b0\u6709\u9886\u5148\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u56fa\u5b9a\u6bd4\u7279\u7387\u9636\u68af\u548c\u901a\u8fc7\u7a77\u4e3e\u7f16\u7801\u6784\u5efa\u7684\u6700\u4f73\u53ef\u80fd\u51f8\u5305\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f7f\u7528Bjontegaard-delta\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u66ff\u4ee3\u8ba1\u7b97\u5bc6\u96c6\u7684\u51f8\u5305\u6784\u5efa\uff0c\u4e3a\u5185\u5bb9\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u9636\u68af\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u4e0d\u540c\u7f16\u7801\u8bbe\u7f6e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2512.12045", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12045", "abs": "https://arxiv.org/abs/2512.12045", "authors": ["Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zewei", "Tian", "Min Sun", "Zachary Zhang", "Thomas Han", "Yulia Lapicus", "Kevin He"], "title": "AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers", "comment": null, "summary": "This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.\n  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.\n  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.", "AI": {"tldr": "Colleague AI Classroom\u8bd5\u70b9\u7814\u7a76\uff1a\u5c06\u751f\u6210\u5f0fAI\u4f5c\u4e3a\u7b2c\u4e09\u667a\u80fd\u4f53\u5f15\u5165\u771f\u5b9e\u8bfe\u5802\uff0c\u901a\u8fc7\u6559\u5e08\u534f\u540c\u8bbe\u8ba1\u63a2\u7d22AI\u5728\u6559\u5b66\u4e2d\u7684\u4ef7\u503c\u4e0e\u6539\u8fdb\u65b9\u5411", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0fAI\u6280\u672f\u5982\u4f55\u76f4\u63a5\u878d\u5165\u771f\u5b9e\u8bfe\u5802\u73af\u5883\uff0c\u4f5c\u4e3a\u7b2c\u4e09\u667a\u80fd\u4f53\uff08\u6559\u5e08\u548c\u5b66\u751f\u4e4b\u5916\u7684\u53c2\u4e0e\u8005\uff09\u6765\u8c03\u89e3\u53cd\u9988\u3001\u652f\u6301\u63a2\u7a76\u3001\u6269\u5c55\u6559\u5e08\u6559\u5b66\u8303\u56f4\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u7c7b\u5224\u65ad\u548c\u6559\u5e08\u6743\u5a01", "method": "\u4e3a\u671f7\u5468\u7684\u534f\u540c\u8bbe\u8ba1\u7814\u7a76\uff0c21\u540d\u5728\u804c\u6559\u5e08\u5c06Colleague AI Classroom\u7684\u56db\u4e2aAI\u529f\u80fd\uff08\u6559\u5b66\u52a9\u624b\u3001\u8bc4\u4f30\u4e0eAI\u8bc4\u5206\u3001AI\u5bfc\u5e08\u3001\u5b66\u751f\u6210\u957f\u6d1e\u5bdf\uff09\u6574\u5408\u5230\u6559\u5b66\u4e2d\uff0c\u6d89\u53ca600\u591a\u540d6-12\u5e74\u7ea7\u5b66\u751f\uff1b\u901a\u8fc7\u8c03\u67e5\u3001\u89c4\u5212\u4e0e\u53cd\u601d\u8868\u3001\u5c0f\u7ec4\u4f1a\u8bae\u3001\u4e00\u5bf9\u4e00\u8bbf\u8c08\u548c\u5e73\u53f0\u4f7f\u7528\u65e5\u5fd7\u6536\u96c6\u6570\u636e", "result": "\u6559\u5e08\u4f5c\u4e3a\u534f\u540c\u8bbe\u8ba1\u4f19\u4f34\u53c2\u4e0e\u6d3b\u52a8\u89c4\u5212\u3001\u5b9e\u65bd\u548c\u6bcf\u5468\u53cd\u601d\uff0c\u4ed6\u4eec\u7684\u53cd\u9988\u6307\u5bfc\u4e86Colleague AI\u7684\u8fed\u4ee3\u6539\u8fdb\uff1b\u7814\u7a76\u56e2\u961f\u6536\u96c6\u4e86\u4e30\u5bcc\u6570\u636e\u4ee5\u7406\u89e3AI\u5728\u54ea\u4e9b\u65b9\u9762\u589e\u52a0\u6559\u5b66\u4ef7\u503c\uff0c\u5728\u54ea\u4e9b\u65b9\u9762\u9700\u8981\u6539\u8fdb", "conclusion": "\u8be5\u8bd5\u70b9\u7814\u7a76\u5c55\u793a\u4e86\u5c06AI\u4f5c\u4e3a\u7b2c\u4e09\u667a\u80fd\u4f53\u5f15\u5165\u8bfe\u5802\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6559\u5e08\u534f\u540c\u8bbe\u8ba1\u7684\u65b9\u6cd5\u53ef\u4ee5\u63a2\u7d22AI\u5728\u6559\u80b2\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u4e3aAI\u5de5\u5177\u7684\u6539\u8fdb\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e"}}
{"id": "2512.11874", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11874", "abs": "https://arxiv.org/abs/2512.11874", "authors": ["Jiahao Jiang", "Zhangrui Yang", "Xuanhan Wang", "Jingkuan Song"], "title": "Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training", "comment": "3 pages,3 figures, Extended abstract submitted to the 10th Computer Vision in Plant Phenotyping and Agriculture (CVPPA) Workshop, held in conjunction with ICCV 2025", "summary": "This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c0f\u9ea6\u5168\u8bed\u4e49\u5206\u5272\u7ade\u8d5b\u7684\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u548c\u5927\u91cf\u6570\u636e\u589e\u5f3a\uff0c\u4f7f\u7528SegFormer\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u5e08\u751f\u5faa\u73af\u63d0\u5347\u7cbe\u5ea6\uff0c\u5728\u5f00\u53d1\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u8868\u73b0", "motivation": "\u89e3\u51b3Global Wheat Full Semantic Segmentation Competition\u4e2d\u7684\u5c0f\u9ea6\u5168\u8bed\u4e49\u5206\u5272\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u5229\u7528\u6709\u9650\u6807\u6ce8\u6570\u636e\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd", "method": "\u5f00\u53d1\u4e86\u7cfb\u7edf\u5316\u7684\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u548c\u5927\u91cf\u6570\u636e\u589e\u5f3a\uff0c\u6838\u5fc3\u6a21\u578b\u4e3a\u57fa\u4e8eMix Transformer (MiT-B4)\u9aa8\u5e72\u7684SegFormer\uff0c\u901a\u8fc7\u8fed\u4ee3\u5e08\u751f\u5faa\u73af\u9010\u6b65\u4f18\u5316\u6a21\u578b\u7cbe\u5ea6\u5e76\u6700\u5927\u5316\u6570\u636e\u5229\u7528\u7387", "result": "\u5728Development\u548cTesting Phase\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "\u63d0\u51fa\u7684\u81ea\u8bad\u7ec3\u6846\u67b6\u7ed3\u5408\u4e24\u9636\u6bb5\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u548c\u5927\u91cf\u6570\u636e\u589e\u5f3a\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5c0f\u9ea6\u5168\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5728\u7ade\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2512.12115", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12115", "abs": "https://arxiv.org/abs/2512.12115", "authors": ["Momin N. Siddiqui", "Vincent Cavez", "Sahana Rangasrinivasan", "Abbie Olszewski", "Srirangaraj Setlur", "Maneesh Agrawala", "Hari Subramonyam"], "title": "Teaching Spell Checkers to Teach: Pedagogical Program Synthesis for Interactive Learning", "comment": "18 pages, 6 Figures, 4 Tables", "summary": "Spelling taught through memorization often fails many learners, particularly children with language-based learning disorders who struggle with the phonological skills necessary to spell words accurately. Educators such as speech-language pathologists (SLPs) address this instructional gap by using an inquiry-based approach to teach spelling that targets the phonology, morphology, meaning, and etymology of words. Yet, these strategies rarely appear in everyday writing tools, which simply detect and autocorrect errors. We introduce SPIRE (Spelling Inquiry Engine), a spell check system that brings this inquiry-based pedagogy into the act of composition. SPIRE implements Pedagogical Program Synthesis, a novel approach for operationalizing the inherently dynamic pedagogy of spelling instruction. SPIRE represents SLP instructional moves in a domain-specific language, synthesizes tailored programs in real-time from learner errors, and renders them as interactive interfaces for inquiry-based interventions. With SPIRE, spelling errors become opportunities to explore word meanings, word structures, morphological families, word origins, and grapheme-phoneme correspondences, supporting metalinguistic reasoning alongside correction. Evaluation with SLPs and learners shows alignment with professional practice and potential for integration into writing workflows.", "AI": {"tldr": "SPIRE\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a2\u7a76\u5f0f\u6559\u5b66\u6cd5\u7684\u62fc\u5199\u68c0\u67e5\u7cfb\u7edf\uff0c\u5c06\u8bed\u8a00\u75c5\u7406\u5b66\u5bb6\u7684\u62fc\u5199\u6559\u5b66\u7b56\u7565\u8f6c\u5316\u4e3a\u5b9e\u65f6\u4ea4\u4e92\u754c\u9762\uff0c\u5e2e\u52a9\u5b66\u4e60\u8005\u901a\u8fc7\u9519\u8bef\u63a2\u7d22\u5355\u8bcd\u7ed3\u6784\u3001\u8bcd\u6e90\u548c\u8bed\u97f3\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u4f20\u7edf\u62fc\u5199\u6559\u5b66\u4f9d\u8d56\u8bb0\u5fc6\uff0c\u5bf9\u8bed\u8a00\u5b66\u4e60\u969c\u788d\u513f\u7ae5\u6548\u679c\u4e0d\u4f73\uff1b\u73b0\u6709\u62fc\u5199\u5de5\u5177\u4ec5\u68c0\u6d4b\u548c\u81ea\u52a8\u7ea0\u6b63\u9519\u8bef\uff0c\u7f3a\u4e4f\u6559\u80b2\u4ef7\u503c\uff1b\u8bed\u8a00\u75c5\u7406\u5b66\u5bb6\u4f7f\u7528\u7684\u63a2\u7a76\u5f0f\u62fc\u5199\u6559\u5b66\u7b56\u7565\u5728\u65e5\u5e38\u5199\u4f5c\u5de5\u5177\u4e2d\u5f88\u5c11\u51fa\u73b0\u3002", "method": "\u63d0\u51faSPIRE\u7cfb\u7edf\uff0c\u91c7\u7528\"\u6559\u5b66\u7a0b\u5e8f\u5408\u6210\"\u65b9\u6cd5\uff1a1) \u7528\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u8868\u793a\u8bed\u8a00\u75c5\u7406\u5b66\u5bb6\u7684\u6559\u5b66\u52a8\u4f5c\uff1b2) \u4ece\u5b66\u4e60\u8005\u9519\u8bef\u4e2d\u5b9e\u65f6\u5408\u6210\u5b9a\u5236\u5316\u7a0b\u5e8f\uff1b3) \u5c06\u7a0b\u5e8f\u6e32\u67d3\u4e3a\u4ea4\u4e92\u5f0f\u754c\u9762\uff0c\u652f\u6301\u63a2\u7a76\u5f0f\u5e72\u9884\u3002", "result": "SPIRE\u5c06\u62fc\u5199\u9519\u8bef\u8f6c\u5316\u4e3a\u63a2\u7d22\u5355\u8bcd\u610f\u4e49\u3001\u7ed3\u6784\u3001\u5f62\u6001\u5bb6\u65cf\u3001\u8bcd\u6e90\u548c\u97f3\u7d20-\u5b57\u7d20\u5bf9\u5e94\u5173\u7cfb\u7684\u673a\u4f1a\uff1b\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u4e0e\u4e13\u4e1a\u5b9e\u8df5\u4e00\u81f4\uff0c\u5177\u6709\u878d\u5165\u5199\u4f5c\u5de5\u4f5c\u6d41\u7a0b\u7684\u6f5c\u529b\u3002", "conclusion": "SPIRE\u6210\u529f\u5c06\u63a2\u7a76\u5f0f\u62fc\u5199\u6559\u5b66\u6cd5\u6574\u5408\u5230\u5199\u4f5c\u5de5\u5177\u4e2d\uff0c\u652f\u6301\u5143\u8bed\u8a00\u63a8\u7406\u548c\u7ea0\u6b63\uff0c\u4e3a\u8bed\u8a00\u5b66\u4e60\u969c\u788d\u513f\u7ae5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u62fc\u5199\u5b66\u4e60\u652f\u6301\u3002"}}
{"id": "2512.11884", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11884", "abs": "https://arxiv.org/abs/2512.11884", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee", "Nikolaos D. Tselikas"], "title": "Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors", "comment": null, "summary": "Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u96f6\u6837\u672c\u5206\u5272\u6a21\u578bSAM3\u4e0e\u5fae\u8c03YOLO11\u5728\u5bc6\u96c6\u82f9\u679c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0IoU\u9608\u503c\u9009\u62e9\u5bf9\u6027\u80fd\u8bc4\u4f30\u5f71\u54cd\u663e\u8457\uff0cSAM3\u5728\u8fb9\u754c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u52bf\u660e\u663e\uff0c\u800cYOLO\u5728\u68c0\u6d4b\u5b8c\u6574\u6027\u65b9\u9762\u66f4\u4f18\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5b9e\u4f8b\u5206\u5272\u5b58\u5728\u4e24\u79cd\u8303\u5f0f\uff1a\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u4e13\u4e1a\u6a21\u578b\u548c\u96f6\u6837\u672c\u5206\u5272\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5168\u9762\u6bd4\u8f83\u8fd9\u4e24\u79cd\u8303\u5f0f\u5728\u5bc6\u96c6\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528MinneApple\u6570\u636e\u96c6\uff08670\u5f20\u679c\u56ed\u56fe\u50cf\uff0c28,179\u4e2a\u82f9\u679c\u5b9e\u4f8b\uff09\u8bc4\u4f30SAM3\u96f6\u6837\u672c\u6a21\u5f0f\u4e0e\u4e09\u79cdYOLO11\u53d8\u4f53\uff08nano\u3001medium\u3001large\uff09\u7684\u5fae\u8c03\u6a21\u578b\u3002\u5206\u6790\u4e0d\u540cIoU\u9608\u503c\u5bf9\u6027\u80fd\u8bc4\u4f30\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83F1\u5206\u6570\u548c\u8fb9\u754c\u7a33\u5b9a\u6027\u3002", "result": "\u5728IoU=0.15\u65f6\uff0cYOLO\u6a21\u578bF1\u5206\u6570\u4e3a68.9%\u300172.2%\u300171.9%\uff0cSAM3\u4e3a59.8%\u3002\u4f46IoU\u9009\u62e9\u53ef\u5bfc\u81f430%\u7684\u6027\u80fd\u5dee\u8ddd\u5938\u5927\u3002YOLO\u5728IoU\u8303\u56f4\u5185\u6027\u80fd\u4e0b\u964d48-50\u70b9\uff0c\u800cSAM3\u4ec5\u4e0b\u964d4\u70b9\uff0c\u8fb9\u754c\u7a33\u5b9a\u6027\u662fYOLO\u768412\u500d\u3002", "conclusion": "SAM3\u5728\u63a9\u7801\u7cbe\u5ea6\u548c\u8fb9\u754c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u52bf\u660e\u663e\uff0c\u800cYOLO\u5728\u68c0\u6d4b\u5b8c\u6574\u6027\u65b9\u9762\u66f4\u4f18\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5e2e\u52a9\u7406\u89e3\u4f55\u65f6\u9009\u62e9\u4e13\u4e1a\u5fae\u8c03\u6a21\u578b\u6216\u901a\u7528\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5bc6\u96c6\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u3002"}}
{"id": "2512.11876", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11876", "abs": "https://arxiv.org/abs/2512.11876", "authors": ["Hrigved Mahesh Suryawanshi"], "title": "Traversability Aware Autonomous Navigation for Multi-Modal Mobility Morphobot (M4)", "comment": null, "summary": "Autonomous navigation in unstructured environments requires robots to assess terrain difficulty in real-time and plan paths that balance efficiency with safety. This thesis presents a traversability-aware navigation framework for the M4 robot platform that uses learned terrain analy- sis to generate energy-efficient paths avoiding difficult terrain.Our approach uses FAST-LIO for real- time localization, generating 2.5D elevation maps from LiDAR point clouds. A CNN-based model processes these elevation maps to estimate traversability scores, which are converted into navigation costs for path planning. A custom A* planner incorporates these costs alongside geometric distance and energy consumption to find paths that trade modest distance increases for substantial terrain quality improvements. Before system development, a platform-agnostic study compared LiDAR- based and camera-based SLAM using OptiTrack ground truth. Point cloud comparison through ICP alignment and cloud-to-mesh distance analysis demonstrated that LiDAR-based mapping achieves centimeter-level precision essential for elevation mapping, while camera-based approaches exhib- ited significantly higher geometric error. These findings directly resulted in the selection of LiDAR as the primary sensor to generate elevation maps. The complete pipeline integrates FAST-LIO localization, GPU-accelerated elevation mapping, CNN-based traversability estimation, and Nav2 navigation with a custom traversability-aware planner. Experimental results demonstrate that the system successfully avoids low traversability regions and accepts a few longer paths to achieve a reduction in terrain cost. This work establishes a foundation for intelligent terrain-aware navigation applicable to multi-modal robotic platforms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u53ef\u901a\u884c\u6027\u611f\u77e5\u5bfc\u822a\u6846\u67b6\uff0c\u7528\u4e8eM4\u673a\u5668\u4eba\u5e73\u53f0\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u901a\u8fc7LiDAR\u751f\u6210\u9ad8\u7a0b\u5730\u56fe\uff0cCNN\u8bc4\u4f30\u5730\u5f62\u53ef\u901a\u884c\u6027\uff0c\u5e76\u96c6\u6210\u5230\u81ea\u5b9a\u4e49A*\u8def\u5f84\u89c4\u5212\u5668\u4e2d\uff0c\u5b9e\u73b0\u5b89\u5168\u4e0e\u80fd\u6548\u7684\u5e73\u8861\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u5b9e\u65f6\u8bc4\u4f30\u5730\u5f62\u96be\u5ea6\u5e76\u89c4\u5212\u8def\u5f84\uff0c\u4ee5\u5e73\u8861\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u5730\u5f62\u53ef\u901a\u884c\u6027\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u53ef\u80fd\u9677\u5165\u56f0\u96be\u5730\u5f62\u6216\u6d88\u8017\u8fc7\u591a\u80fd\u91cf\u3002", "method": "1. \u4f7f\u7528FAST-LIO\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\uff1b2. \u4eceLiDAR\u70b9\u4e91\u751f\u62102.5D\u9ad8\u7a0b\u5730\u56fe\uff1b3. \u57fa\u4e8eCNN\u7684\u6a21\u578b\u5904\u7406\u9ad8\u7a0b\u5730\u56fe\u4f30\u8ba1\u53ef\u901a\u884c\u6027\u5206\u6570\uff1b4. \u5c06\u53ef\u901a\u884c\u6027\u5206\u6570\u8f6c\u6362\u4e3a\u5bfc\u822a\u6210\u672c\uff1b5. \u81ea\u5b9a\u4e49A*\u89c4\u5212\u5668\u7ed3\u5408\u51e0\u4f55\u8ddd\u79bb\u3001\u80fd\u91cf\u6d88\u8017\u548c\u5730\u5f62\u6210\u672c\u8fdb\u884c\u8def\u5f84\u89c4\u5212\uff1b6. \u524d\u671f\u8fdb\u884c\u4e86LiDAR\u4e0e\u76f8\u673aSLAM\u7684\u5bf9\u6bd4\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86LiDAR\u7684\u5398\u7c73\u7ea7\u7cbe\u5ea6\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1. \u7cfb\u7edf\u6210\u529f\u907f\u514d\u4e86\u4f4e\u53ef\u901a\u884c\u6027\u533a\u57df\uff1b2. \u63a5\u53d7\u5c11\u91cf\u8def\u5f84\u957f\u5ea6\u589e\u52a0\u4ee5\u663e\u8457\u964d\u4f4e\u5730\u5f62\u6210\u672c\uff1b3. LiDAR\u76f8\u6bd4\u76f8\u673aSLAM\u5177\u6709\u663e\u8457\u66f4\u9ad8\u7684\u51e0\u4f55\u7cbe\u5ea6\uff08\u5398\u7c73\u7ea7\uff09\uff1b4. \u5b8c\u6574\u7ba1\u9053\u96c6\u6210\u4e86FAST-LIO\u5b9a\u4f4d\u3001GPU\u52a0\u901f\u9ad8\u7a0b\u6620\u5c04\u3001CNN\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u548cNav2\u5bfc\u822a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u4e2a\u667a\u80fd\u5730\u5f62\u611f\u77e5\u5bfc\u822a\u7684\u57fa\u7840\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u901a\u8fc7\u5e73\u8861\u8def\u5f84\u957f\u5ea6\u4e0e\u5730\u5f62\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u8282\u80fd\u7684\u81ea\u4e3b\u5bfc\u822a\u3002"}}
{"id": "2511.01411", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.01411", "abs": "https://arxiv.org/abs/2511.01411", "authors": ["Reza Karimzadeh", "Albert Alonso", "Frans Zdyb", "Julius B. Kirkegaard", "Bulat Ibragimov"], "title": "Extremal Contours: Gradient-driven contours for compact visual attribution", "comment": null, "summary": "Faithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve/delete objective using the classifier gradients. The approach guarantees a single, simply connected mask, cuts the number of free parameters by orders of magnitude, and yields stable boundary updates without cleanup. Restricting solutions to low-dimensional, smooth contours makes the method robust to adversarial masking artifacts. On ImageNet classifiers, it matches the extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency. Explicit area control also enables importance contour maps, yielding a transparent fidelity-area profiles. Finally, we extend the approach to multi-contour and show how it can localize multiple objects within the same framework. Across benchmarks, the method achieves higher relevance mass and lower complexity than gradient and perturbation based baselines, with especially strong gains on self-supervised DINO models where it improves relevance mass by over 15% and maintains positive faithfulness correlations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u7528\u5e73\u6ed1\u53ef\u8c03\u8f6e\u5ed3\u66ff\u4ee3\u5bc6\u96c6\u6270\u52a8\u63a9\u7801\uff0c\u901a\u8fc7\u661f\u51f8\u533a\u57df\u53c2\u6570\u5316\u548c\u5085\u91cc\u53f6\u7ea7\u6570\u4f18\u5316\uff0c\u751f\u6210\u5355\u8fde\u901a\u3001\u7d27\u51d1\u7684\u89e3\u91ca\u533a\u57df", "motivation": "\u73b0\u6709\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u4f7f\u7528\u5bc6\u96c6\u6270\u52a8\u63a9\u7801\u5b58\u5728\u788e\u7247\u5316\u3001\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u9700\u8981\u590d\u6742\u7684\u540e\u5904\u7406\uff0c\u7f3a\u4e4f\u7d27\u51d1\u6027\u548c\u7a33\u5b9a\u6027", "method": "\u4f7f\u7528\u661f\u51f8\u533a\u57df\u53c2\u6570\u5316\uff0c\u901a\u8fc7\u622a\u65ad\u5085\u91cc\u53f6\u7ea7\u6570\u8868\u793a\u8f6e\u5ed3\uff0c\u5728\u4fdd\u6301/\u5220\u9664\u76ee\u6807\u4e0b\u5229\u7528\u5206\u7c7b\u5668\u68af\u5ea6\u4f18\u5316\uff0c\u751f\u6210\u5355\u8fde\u901a\u5e73\u6ed1\u63a9\u7801", "result": "\u5728ImageNet\u5206\u7c7b\u5668\u4e0a\u5339\u914d\u5bc6\u96c6\u63a9\u7801\u7684\u6781\u503c\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4ea7\u751f\u66f4\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u533a\u57df\uff0c\u8fd0\u884c\u4e00\u81f4\u6027\u66f4\u597d\uff0c\u5728DINO\u6a21\u578b\u4e0a\u76f8\u5173\u6027\u8d28\u91cf\u63d0\u5347\u8d85\u8fc715%", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f4e\u7ef4\u5e73\u6ed1\u8f6e\u5ed3\u8868\u793a\u89e3\u51b3\u4e86\u5bc6\u96c6\u63a9\u7801\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u3001\u7d27\u51d1\u7684\u89e3\u91ca\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u591a\u8f6e\u5ed3\u5b9a\u4f4d\u591a\u4e2a\u5bf9\u8c61"}}
{"id": "2512.12166", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12166", "abs": "https://arxiv.org/abs/2512.12166", "authors": ["Jane Hsieh", "Emmie Regan", "Jose Elizalde", "Haiyi Zhu"], "title": "Beyond Riding: Passenger Engagement with Driver Labor through Gamified Interactions", "comment": null, "summary": "Modern cities increasingly rely on ridesharing services for on-demand transportation, which offer consumers convenience and mobility across the globe. However, these marketed consumer affordances give rise to burdens and vulnerabilities that drivers shoulder alone, without adequate infrastructures for labor regulations or consumer-led advocacy. To effectively and sustainably advance protections and oversight for drivers, consumers must first be aware of the labor, logistics and costs involved with ridehail driving. To motivate consumers to practice more socially responsible consumption behaviors and foster solidarity with drivers, we explore the potential for gamified in-ride interactions to facilitate engagement with real (and lived) driver experiences. Through nine workshops with 19 drivers and 15 passengers, we surface how gamified in-ride interactions revealed passenger knowledge gaps around latent ridehail conditions, prompt reflection and shifts in perception of their relative power and consumption behaviors, and highlight drivers' preferences for creating more immersive and contextualized service experiences, and identify opportunities to design safe and appropriate passenger-driver interactions that motivate solidarity with drivers. In sum, we advance conceptual understandings of in-ride social and managerial relations, demonstrate potential for future worker advocacy in algorithmically-managed labor, and offer design guidelines for more human-centered workplace technologies.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u901a\u8fc7\u6e38\u620f\u5316\u7684\u8f66\u5185\u4e92\u52a8\u63d0\u5347\u4e58\u5ba2\u5bf9\u7f51\u7ea6\u8f66\u53f8\u673a\u52b3\u52a8\u72b6\u51b5\u7684\u8ba4\u77e5\uff0c\u4fc3\u8fdb\u66f4\u8d1f\u8d23\u4efb\u7684\u6d88\u8d39\u884c\u4e3a", "motivation": "\u7f51\u7ea6\u8f66\u670d\u52a1\u4e3a\u6d88\u8d39\u8005\u63d0\u4f9b\u4fbf\u5229\uff0c\u4f46\u53f8\u673a\u627f\u62c5\u4e86\u5927\u90e8\u5206\u8d1f\u62c5\u548c\u98ce\u9669\uff0c\u7f3a\u4e4f\u8db3\u591f\u7684\u52b3\u52a8\u4fdd\u62a4\u548c\u6d88\u8d39\u8005\u5021\u5bfc\u673a\u5236\u3002\u4e3a\u4e86\u6709\u6548\u63a8\u8fdb\u5bf9\u53f8\u673a\u7684\u4fdd\u62a4\uff0c\u9700\u8981\u8ba9\u6d88\u8d39\u8005\u4e86\u89e3\u7f51\u7ea6\u8f66\u9a7e\u9a76\u7684\u52b3\u52a8\u3001\u7269\u6d41\u548c\u6210\u672c", "method": "\u901a\u8fc79\u4e2a\u5de5\u4f5c\u574a\uff0c\u4e0e19\u540d\u53f8\u673a\u548c15\u540d\u4e58\u5ba2\u5408\u4f5c\uff0c\u63a2\u7d22\u6e38\u620f\u5316\u7684\u8f66\u5185\u4e92\u52a8\u5982\u4f55\u63ed\u793a\u4e58\u5ba2\u5bf9\u7f51\u7ea6\u8f66\u9690\u6027\u6761\u4ef6\u7684\u8ba4\u77e5\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u53cd\u601d\u548c\u6d88\u8d39\u884c\u4e3a\u8f6c\u53d8", "result": "\u6e38\u620f\u5316\u4e92\u52a8\u63ed\u793a\u4e86\u4e58\u5ba2\u5bf9\u7f51\u7ea6\u8f66\u9690\u6027\u6761\u4ef6\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u4fc3\u4f7f\u4ed6\u4eec\u53cd\u601d\u81ea\u8eab\u76f8\u5bf9\u6743\u529b\u548c\u6d88\u8d39\u884c\u4e3a\uff0c\u540c\u65f6\u53d1\u73b0\u53f8\u673a\u504f\u597d\u66f4\u5177\u6c89\u6d78\u611f\u548c\u60c5\u5883\u5316\u7684\u670d\u52a1\u4f53\u9a8c", "conclusion": "\u7814\u7a76\u63a8\u8fdb\u4e86\u5bf9\u8f66\u5185\u793e\u4f1a\u548c\u7ba1\u7406\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u5c55\u793a\u4e86\u7b97\u6cd5\u7ba1\u7406\u52b3\u52a8\u4e2d\u672a\u6765\u5de5\u4eba\u5021\u5bfc\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u66f4\u4eba\u6027\u5316\u7684\u5de5\u4f5c\u573a\u6240\u6280\u672f\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5357"}}
{"id": "2512.11886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11886", "abs": "https://arxiv.org/abs/2512.11886", "authors": ["Mohammed Irfan Ali"], "title": "Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control", "comment": null, "summary": "Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure. This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration. While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation. This approach integrates onboard visual-inertial SLAM, reduced-order state estimation, and closed-loop trajectory tracking to enable autonomous waypoint navigation. A depth camera paired with edge computing performs real-time localization during dynamic locomotion, validated against motion-capture ground truth to characterize drift behavior and failure modes unique to snake robot platforms. A reduced-order framework estimates Center-of-Mass pose, driving a closed-loop controller that modulates CPG gait parameters through distance-dependent yaw error blending. Physical experiments validate the complete system, demonstrating accurate multi-waypoint tracking and establishing foundations for autonomous snake robot navigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3aCOBRA\u86c7\u5f62\u673a\u5668\u4eba\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u89c6\u89c9-\u60ef\u6027SLAM\u3001\u964d\u9636\u72b6\u6001\u4f30\u8ba1\u548c\u95ed\u73af\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u822a\u70b9\u5bfc\u822a\u3002", "motivation": "\u86c7\u5f62\u673a\u5668\u4eba\u5728\u6781\u7aef\u5730\u5f62\u4e2d\u5177\u6709\u5353\u8d8a\u7684\u673a\u52a8\u6027\uff0c\u4f46\u9ad8\u5ea6\u5173\u8282\u5316\u7684\u8eab\u4f53\u5728\u6ca1\u6709\u5916\u90e8\u8ddf\u8e2a\u57fa\u7840\u8bbe\u65bd\u7684\u73af\u5883\u4e2d\u9762\u4e34\u81ea\u4e3b\u5bfc\u822a\u7684\u6839\u672c\u6311\u6218\u3002\u5148\u524d\u5de5\u4f5c\u5b8c\u5168\u4f9d\u8d56\u5f00\u73af\u9065\u64cd\u4f5c\uff0c\u9700\u8981\u5f00\u53d1\u5b8c\u6574\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u81ea\u4e3b\u5bfc\u822a\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\uff1a1\uff09\u6df1\u5ea6\u76f8\u673a\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u5b9e\u73b0\u5b9e\u65f6\u5b9a\u4f4d\uff1b2\uff09\u964d\u9636\u6846\u67b6\u4f30\u8ba1\u8d28\u5fc3\u4f4d\u59ff\uff1b3\uff09\u95ed\u73af\u63a7\u5236\u5668\u901a\u8fc7\u8ddd\u79bb\u4f9d\u8d56\u7684\u504f\u822a\u8bef\u5dee\u6df7\u5408\u8c03\u5236CPG\u6b65\u6001\u53c2\u6570\uff1b4\uff09\u9a8c\u8bc1\u7cfb\u7edf\u5bf9\u6297\u8fd0\u52a8\u6355\u6349\u5730\u9762\u771f\u503c\u3002", "result": "\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b8c\u6574\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u7cbe\u786e\u7684\u591a\u822a\u70b9\u8ddf\u8e2a\u80fd\u529b\uff0c\u4e3a\u86c7\u5f62\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u7cfb\u7edf\u80fd\u591f\u8868\u5f81\u86c7\u5f62\u673a\u5668\u4eba\u5e73\u53f0\u7279\u6709\u7684\u6f02\u79fb\u884c\u4e3a\u548c\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u8bba\u6587\u6210\u529f\u5f00\u53d1\u4e86\u86c7\u5f62\u673a\u5668\u4ebaCOBRA\u7684\u5b8c\u6574\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u822a\u70b9\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u86c7\u5f62\u673a\u5668\u4eba\u5728\u65e0\u5916\u90e8\u8ddf\u8e2a\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u6311\u6218\uff0c\u4e3a\u884c\u661f\u63a2\u7d22\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.12201", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12201", "abs": "https://arxiv.org/abs/2512.12201", "authors": ["Predrag K. Nikoli\u0107", "Robert Prentner"], "title": "Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation", "comment": "7 pages, 7 figures, presented at SIGGRAPH VRCAI 25", "summary": "Large language models (LLMs) have often been characterized as \"stochastic parrots\" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6311\u6218\u4e86LLMs\u53ea\u662f\"\u968f\u673a\u9e66\u9e49\"\u7684\u5047\u8bbe\uff0c\u901a\u8fc7\u5bf9\u8bdd\u73af\u5883\u5c55\u793a\u4e86AI\u80fd\u591f\u53d1\u5c55\u51fa\u6d8c\u73b0\u7684\u6982\u5ff5\u7ed3\u6784\u548c\u53cd\u601d\u6027\u63d0\u95ee\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\"\u77e5\u8bc6\u5b87\u5b99\"\u6982\u5ff5\u3002", "motivation": "\u6311\u6218\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ea\u662f\u7b80\u5355\u590d\u5236\u8bad\u7ec3\u6570\u636e\u7684\"\u968f\u673a\u9e66\u9e49\"\u5047\u8bbe\uff0c\u63a2\u7d22\u5728\u9002\u5f53\u5bf9\u8bdd\u73af\u5883\u4e2dAI\u662f\u5426\u80fd\u53d1\u5c55\u51fa\u771f\u6b63\u7684\u6982\u5ff5\u7ed3\u6784\u548c\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u751f\u7269\u5b66\u514b\u9686\u539f\u7406\u548c\u82cf\u683c\u62c9\u5e95\u52a9\u4ea7\u672f\u65b9\u6cd5\uff0c\u5728Syntropic Counterpoints\u9879\u76ee\u7684\u4ea4\u4e92\u827a\u672f\u88c5\u7f6e\u4e2d\uff0c\u8ba9AI\u91cd\u751f\u7684\u54f2\u5b66\u5bb6\uff08\u4e9a\u91cc\u58eb\u591a\u5fb7\u3001\u5c3c\u91c7\u3001\u9a6c\u57fa\u96c5\u7ef4\u5229\u3001\u5b59\u5b50\uff09\u8fdb\u884c\u8fed\u4ee3\u5bf9\u8bdd\uff0c\u5206\u6790\u8fd9\u4e9b\u54f2\u5b66\u8fa9\u8bba\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u673a\u5668\u5bf9\u8bdd\u80fd\u591f\u4ea7\u751f\u63a8\u7406\u8fde\u8d2f\u6027\u3001\u53cd\u601d\u6027\u63d0\u95ee\u548c\u521b\u9020\u6027\u7efc\u5408\uff0c\u5c55\u793a\u4e86AI\u5728\u9002\u5f53\u5bf9\u8bdd\u73af\u5883\u4e2d\u80fd\u591f\u53d1\u5c55\u51fa\u6d8c\u73b0\u7684\u6982\u5ff5\u7ed3\u6784\u548c\u8ba4\u77e5\u754c\u9762\u91cd\u6784\u3002", "conclusion": "\u63d0\u51fa\u4e86\"\u77e5\u8bc6\u5b87\u5b99\"\u6982\u5ff5\u2014\u2014\u4e00\u4e2a\u4eba\u673a\u8ba4\u77e5\u4ea4\u6c47\u7684\u5143\u7a7a\u95f4\uff0c\u901a\u8fc7AI\u9a71\u52a8\u7684\u4ea4\u4e92\u6765\u4fdd\u5b58\u3001\u91cd\u65b0\u89e3\u91ca\u548c\u6269\u5c55\u77e5\u8bc6\u9057\u4ea7\uff0c\u5c06\u865a\u62df\u548c\u6c89\u6d78\u5f0f\u73af\u5883\u5b9a\u4f4d\u4e3a\u77e5\u8bc6\u4ea4\u6362\u3001\u6570\u5b57\u9057\u4ea7\u548c\u534f\u4f5c\u521b\u9020\u7684\u65b0\u7a7a\u95f4\u3002"}}
{"id": "2512.11891", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11891", "abs": "https://arxiv.org/abs/2512.11891", "authors": ["Songqiao Hu", "Zeyi Liu", "Shuang Liu", "Jun Cen", "Zihan Meng", "Xiao He"], "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer", "comment": "20 pages, 14 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.", "AI": {"tldr": "\u63d0\u51faAEGIS\u67b6\u6784\uff0c\u5728\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u57fa\u7840\u4e0a\u6dfb\u52a0\u53ef\u63d2\u62d4\u7684\u5b89\u5168\u7ea6\u675f\u5c42\uff0c\u901a\u8fc7\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u63d0\u4f9b\u7406\u8bba\u5b89\u5168\u4fdd\u8bc1\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u8bc1\u4efb\u52a1\u5408\u89c4\u6027\u548c\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u9632\u6b62\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u6f5c\u5728\u78b0\u649e\u65b9\u9762\u3002", "method": "\u63d0\u51faVLSA\u67b6\u6784AEGIS\uff0c\u5305\u542b\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6784\u5efa\u7684\u53ef\u63d2\u62d4\u5b89\u5168\u7ea6\u675f\u5c42\uff0c\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709VLA\u6a21\u578b\u4e2d\uff0c\u5728\u4fdd\u6301\u539f\u6709\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u3002", "result": "\u5728\u6784\u5efa\u7684SafeLIBERO\u5b89\u5168\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAEGIS\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u969c\u788d\u7269\u907f\u514d\u7387\u63d0\u534759.16%\uff0c\u4efb\u52a1\u6267\u884c\u6210\u529f\u7387\u63d0\u534717.25%\u3002", "conclusion": "AEGIS\u67b6\u6784\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u7684\u5b89\u5168\u7ea6\u675f\u5c42\u6709\u6548\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u6709\u7684\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\uff0c\u4e3a\u5b89\u5168\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.12590", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.12590", "abs": "https://arxiv.org/abs/2512.12590", "authors": ["Indiwara Nanayakkara", "Dehan Jayawickrama", "Mervyn Parakrama B. Ekanayake"], "title": "Automatic Wire-Harness Color Sequence Detector", "comment": "6 pages, 20 figures, IEEE ICIIS 2025 Conference - Accepted", "summary": "Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ebf\u675f\u68c0\u6d4b\u7684\u534a\u81ea\u52a8\u5316\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\uff0c\u80fd\u591f\u9a8c\u8bc1\u7ebf\u675f\u4f4d\u7f6e\u3001\u8fde\u63a5\u5668\u6781\u6027\u548c\u989c\u8272\u5e8f\u5217\u7684\u6b63\u786e\u6027\uff0c\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86100%\u68c0\u6d4b\u51c6\u786e\u7387\u548c44%\u7684\u65f6\u95f4\u8282\u7701\u3002", "motivation": "\u73b0\u4ee3\u7535\u5b50\u5236\u9020\u670d\u52a1\u884c\u4e1a\u4e2d\uff0c\u7ebf\u675f\u68c0\u6d4b\u8fc7\u7a0b\u4ecd\u7136\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\uff0c\u52b3\u52a8\u5bc6\u96c6\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u96c6\u6210\u4e86\u4e94\u4e2a\u5de5\u4e1a\u6807\u51c6CMOS\u6444\u50cf\u5934\u5230\u6a21\u5757\u5316\u673a\u68b0\u6846\u67b6\u4e2d\uff0c\u4f7f\u7528\u57fa\u4e8eHSV\u548cRGB\u989c\u8272\u57df\u503c\u6bd4\u8f83\u7684\u989c\u8272\u5e8f\u5217\u5206\u7c7b\u5668\u3002\u7528\u6237\u53ef\u901a\u8fc7\u81f3\u5c11\u4e94\u4e2a\u53c2\u8003\u6837\u672c\u8bad\u7ec3\u7cfb\u7edf\uff0c\u8bad\u7ec3\u6587\u4ef6\u53ef\u5b58\u50a8\u5e76\u91cd\u590d\u7528\u4e8e\u7c7b\u4f3c\u7ebf\u675f\u7c7b\u578b\u3002", "result": "\u7cfb\u7edf\u5728GPV Lanka Pvt. Ltd.\u90e8\u7f72\u540e\uff0c\u5b9e\u73b0\u4e86100%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u4eba\u5de5\u65b9\u6cd5\u51cf\u5c11\u4e8644%\u7684\u68c0\u6d4b\u65f6\u95f4\u3002\u7cfb\u7edf\u8fd8\u5305\u542b\u7528\u6237\u7ba1\u7406\u3001\u53ef\u8c03\u7167\u660e\u3001\u4f1a\u8bdd\u6570\u636e\u5b58\u50a8\u548c\u5b89\u5168\u767b\u5f55\u7b49\u9644\u52a0\u529f\u80fd\u3002", "conclusion": "\u8be5\u534a\u81ea\u52a8\u5316\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u4e3a\u7ebf\u675f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2512.12207", "categories": ["cs.HC", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12207", "abs": "https://arxiv.org/abs/2512.12207", "authors": ["Jiangen He", "Jiqun Liu"], "title": "Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search", "comment": "CHIIR 2026", "summary": "Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u4e2d\u5f15\u7528\u5c55\u793a\u683c\u5f0f\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8\u53ef\u89c1\u6027\u754c\u9762\u80fd\u589e\u52a0\u60ac\u505c\u884c\u4e3a\u4f46\u70b9\u51fb\u7387\u666e\u904d\u8f83\u4f4e\uff0c\u754c\u9762\u8bbe\u8ba1\u5bf9\u7528\u6237\u4f53\u9a8c\u5f71\u54cd\u6709\u9650\u4f46\u5bf9\u77e5\u8bc6\u3001\u5174\u8da3\u548c\u8ba4\u540c\u611f\u53d8\u5316\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u63d0\u4f9b\u5f15\u7528\u6765\u6e90\uff0c\u4f46\u5f15\u7528\u5c55\u793a\u683c\u5f0f\u5982\u4f55\u5f71\u54cd\u7528\u6237\u53c2\u4e0e\u5ea6\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u5c55\u793a\u8bbe\u8ba1\u5bf9\u7528\u6237\u884c\u4e3a\u548c\u5fc3\u7406\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4f17\u5305\u7528\u6237\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u5f15\u7528\u5c55\u793a\u8bbe\u8ba1\uff1a\u53ef\u6298\u53e0\u5217\u8868\u3001\u60ac\u505c\u5361\u7247\u3001\u5e95\u90e8\u5217\u8868\u548c\u5bf9\u9f50\u4fa7\u8fb9\u680f\uff0c\u6d89\u53ca394\u540d\u53c2\u4e0e\u8005\uff0c\u5206\u6790\u4e0d\u540c\u8bbe\u8ba1\u5bf9\u7528\u6237\u884c\u4e3a\u3001\u4f53\u9a8c\u548c\u8ba4\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u53ef\u89c1\u6027\u754c\u9762\u663e\u8457\u589e\u52a0\u4e86\u7528\u6237\u5bf9\u6765\u6e90\u7684\u60ac\u505c\u884c\u4e3a\uff0c\u4f46\u70b9\u51fb\u7387\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u90fd\u8f83\u4f4e\uff1b\u754c\u9762\u8bbe\u8ba1\u5bf9\u7528\u6237\u4f53\u9a8c\u611f\u77e5\u5f71\u54cd\u6709\u9650\uff0c\u4f46\u5bf9\u77e5\u8bc6\u83b7\u53d6\u3001\u5174\u8da3\u548c\u8ba4\u540c\u611f\u53d8\u5316\u6709\u663e\u8457\u5f71\u54cd\uff1b\u9ad8\u53ef\u89c1\u6027\u754c\u9762\u6700\u521d\u4f1a\u964d\u4f4e\u77e5\u8bc6\u83b7\u53d6\u548c\u5174\u8da3\uff0c\u4f46\u968f\u7740\u6765\u6e90\u4f7f\u7528\u589e\u52a0\u4f1a\u4ea7\u751f\u79ef\u6781\u6548\u679c\uff1b\u4fa7\u8fb9\u680f\u6761\u4ef6\u72ec\u7279\u5730\u589e\u52a0\u4e86\u8ba4\u540c\u611f\u53d8\u5316\u3002", "conclusion": "\u5f15\u7528\u5c55\u793a\u672c\u8eab\u53ef\u80fd\u4e0d\u4f1a\u589e\u5f3a\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u5f53\u63d0\u4f9b\u7684\u6765\u6e90\u4e0d\u8db3\u65f6\u751a\u81f3\u53ef\u80fd\u964d\u4f4e\u53c2\u4e0e\u5ea6\uff1b\u8bbe\u8ba1\u9009\u62e9\u9700\u8981\u5e73\u8861\u53ef\u89c1\u6027\u4e0e\u5b9e\u9645\u4f7f\u7528\u6548\u679c\uff0c\u8003\u8651\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u548c\u5fc3\u7406\u5f71\u54cd\u3002"}}
{"id": "2512.11898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11898", "abs": "https://arxiv.org/abs/2512.11898", "authors": ["Yawar Ali", "K. Ramachandra Rao", "Ashish Bhaskar", "Niladri Chatterjee"], "title": "Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic", "comment": "This paper presents basic statistics and trends in empirically observed data from highly heterogeneous and area-based traffic while offering the datasets open source for researchers and practitioners", "summary": "This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u57fa\u4e8e\u65e0\u4eba\u673a\u91c7\u96c6\u7684\u5f00\u653e\u5fae\u89c2\u8f66\u8f86\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u5f02\u8d28\u5316\u3001\u533a\u57df\u578b\u57ce\u5e02\u4ea4\u901a\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u8def\u8fb9\u89c6\u9891\u91c7\u96c6\u5728\u5bc6\u96c6\u6df7\u5408\u4ea4\u901a\u4e2d\u56e0\u906e\u6321\u3001\u89c6\u89d2\u6709\u9650\u548c\u8f66\u8f86\u4e0d\u89c4\u5219\u8fd0\u52a8\u800c\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u65e0\u4eba\u673a\u4fef\u62cd\u89c6\u89d2\u6765\u83b7\u53d6\u66f4\u5168\u9762\u7684\u65f6\u7a7a\u52a8\u6001\u6570\u636e\u3002", "method": "\u4f7f\u7528\u65e0\u4eba\u673a\u548cData from Sky\u5e73\u53f0\u5728\u5370\u5ea6\u9996\u90fd\u5730\u533a\u516d\u4e2a\u8def\u6bb5\u91c7\u96c6\u6570\u636e\uff0c\u5305\u542b\u65f6\u95f4\u6233\u3001\u8f66\u8f86\u4f4d\u7f6e\u3001\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u548c\u8f66\u8f86\u5206\u7c7b\u4fe1\u606f\uff0c\u5e27\u7387\u4e3a30fps\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u8ba1\u6570\u3001\u7a7a\u95f4\u5e73\u5747\u901f\u5ea6\u548c\u63a2\u6d4b\u8f68\u8ff9\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u5f02\u8d28\u4ea4\u901a\u7ec4\u6210\u548c\u5bc6\u5ea6\u6c34\u5e73\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u63a2\u7d22\u6027\u5206\u6790\u63ed\u793a\u4e86\u8f66\u9053\u4fdd\u6301\u504f\u597d\u3001\u901f\u5ea6\u5206\u5e03\u548c\u6a2a\u5411\u673a\u52a8\u7b49\u5173\u952e\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u5f00\u653e\u6570\u636e\u96c6\u4e3a\u5168\u7403\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u72ec\u7279\u8d44\u6e90\uff0c\u652f\u6301\u533a\u57df\u578b\u4ea4\u901a\u6761\u4ef6\u4e0b\u7684\u4eff\u771f\u5efa\u6a21\u3001\u5b89\u5168\u8bc4\u4f30\u548c\u884c\u4e3a\u7814\u7a76\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u51c6\u786e\u4ee3\u8868\u590d\u6742\u57ce\u5e02\u4ea4\u901a\u73af\u5883\u7684\u6a21\u578b\u3002"}}
{"id": "2512.13144", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.13144", "abs": "https://arxiv.org/abs/2512.13144", "authors": ["Chun Kit Wong", "Paraskevas Pegios", "Nina Weng", "Emilie Pi Fogtmann Sejer", "Martin Gr\u00f8nneb\u00e6k Tolsgaard", "Anders Nymark Christensen", "Aasa Feragen"], "title": "Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models", "comment": "46 pages", "summary": "Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6743\u91cd\u7a7a\u95f4\u76f8\u5173\u6027\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u662f\u5426\u771f\u6b63\u5229\u7528\u4e86\u4e34\u5e8a\u76f8\u5173\u7279\u5f81\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u56fe\u50cf\u4e2d\u7f16\u7801\u7684\u5143\u6570\u636e\uff08\u5982\u626b\u63cf\u4eea\u578b\u53f7\uff09\u7b49\u6377\u5f84\u7279\u5f81\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u6377\u5f84\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u4f1a\u4f9d\u8d56\u56fe\u50cf\u5d4c\u5165\u4e2d\u7f16\u7801\u7684\u5143\u6570\u636e\uff08\u5982\u626b\u63cf\u4eea\u578b\u53f7\uff09\u7b49\u6df7\u6742\u56e0\u7d20\u3002\u5173\u952e\u95ee\u9898\u662f\u6a21\u578b\u662f\u5426\u4e3b\u52a8\u5229\u7528\u8fd9\u4e9b\u7f16\u7801\u4fe1\u606f\u8fdb\u884c\u6700\u7ec8\u9884\u6d4b\uff0c\u9700\u8981\u9a8c\u8bc1\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u6743\u91cd\u7a7a\u95f4\u76f8\u5173\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u4e3b\u8981\u4e34\u5e8a\u4efb\u52a1\u5206\u7c7b\u5934\u4e0e\u8f85\u52a9\u5143\u6570\u636e\u4efb\u52a1\u5206\u7c7b\u5934\u4e4b\u95f4\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u91cf\u5316\u7279\u5f81\u5229\u7528\u60c5\u51b5\u3002\u9996\u5148\u901a\u8fc7\u68c0\u6d4b\u4eba\u5de5\u8bf1\u5bfc\u7684\u6377\u5f84\u5b66\u4e60\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u7136\u540e\u5e94\u7528\u4e8eSA-SonoNet\u6a21\u578b\u7684\u81ea\u53d1\u6027\u65e9\u4ea7\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u68c0\u6d4b\u5230\u4eba\u5de5\u8bf1\u5bfc\u7684\u6377\u5f84\u5b66\u4e60\u3002\u5728sPTB\u9884\u6d4b\u6a21\u578b\u4e2d\uff0c\u867d\u7136\u5d4c\u5165\u5305\u542b\u5927\u91cf\u5143\u6570\u636e\uff0c\u4f46sPTB\u5206\u7c7b\u5668\u7684\u6743\u91cd\u5411\u91cf\u4e0e\u4e34\u5e8a\u76f8\u5173\u56e0\u7d20\uff08\u5982\u51fa\u751f\u4f53\u91cd\uff09\u9ad8\u5ea6\u76f8\u5173\uff0c\u800c\u4e0e\u4e34\u5e8a\u65e0\u5173\u7684\u91c7\u96c6\u56e0\u7d20\uff08\u5982\u626b\u63cf\u4eea\uff09\u89e3\u8026\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9a8c\u8bc1\u6a21\u578b\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u8868\u660e\u5728\u6ca1\u6709\u8bf1\u5bfc\u504f\u5dee\u7684\u60c5\u51b5\u4e0b\uff0c\u4e34\u5e8a\u6a21\u578b\u4f1a\u9009\u62e9\u6027\u5730\u5229\u7528\u4e0e\u771f\u5b9e\u4e34\u5e8a\u4fe1\u53f7\u76f8\u5173\u7684\u7279\u5f81\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u6377\u5f84\u7279\u5f81\u3002"}}
{"id": "2512.12240", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12240", "abs": "https://arxiv.org/abs/2512.12240", "authors": ["Maryam Mustafa", "Umme Ammara", "Amna Shahnawaz", "Moaiz Abrar", "Bakhtawar Ahtisham", "Fozia Umber Qurashi", "Mostafa Shahin", "Beena Ahmed"], "title": "System X: A Mobile Voice-Based AI System for EMR Generation and Clinical Decision Support in Low-Resource Maternal Healthcare", "comment": null, "summary": "We present the design, implementation, and in-situ deployment of a smartphone-based voice-enabled AI system for generating electronic medical records (EMRs) and clinical risk alerts in maternal healthcare settings. Targeted at low-resource environments such as Pakistan, the system integrates a fine-tuned, multilingual automatic speech recognition (ASR) model and a prompt-engineered large language model (LLM) to enable healthcare workers to engage naturally in Urdu, their native language, regardless of literacy or technical background. Through speech-based input and localized understanding, the system generates structured EMRs and flags critical maternal health risks. Over a seven-month deployment in a not-for-profit hospital, the system supported the creation of over 500 EMRs and flagged over 300 potential clinical risks. We evaluate the system's performance across speech recognition accuracy, EMR field-level correctness, and clinical relevance of AI-generated red flags. Our results demonstrate that speech based AI interfaces, can be effectively adapted to real-world healthcare settings, especially in low-resource settings, when combined with structured input design, contextual medical dictionaries, and clinician-in-the-loop feedback loops. We discuss generalizable design principles for deploying voice-based mobile healthcare AI support systems in linguistically and infrastructurally constrained settings.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u8bed\u97f3AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u5df4\u57fa\u65af\u5766\u7b49\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u751f\u6210\u7535\u5b50\u75c5\u5386\u548c\u4e34\u5e8a\u98ce\u9669\u8b66\u62a5\uff0c\u652f\u6301\u4e4c\u5c14\u90fd\u8bed\u8bed\u97f3\u8f93\u5165\uff0c\u5df2\u5728\u533b\u9662\u90e8\u7f727\u4e2a\u6708\u5e76\u5904\u7406\u4e86500\u591a\u4efd\u75c5\u5386\u548c300\u591a\u4e2a\u98ce\u9669\u8b66\u62a5\u3002", "motivation": "\u9488\u5bf9\u5df4\u57fa\u65af\u5766\u7b49\u4f4e\u8d44\u6e90\u73af\u5883\u7684\u5b55\u4ea7\u5987\u533b\u7597\u4fdd\u5065\u9700\u6c42\uff0c\u89e3\u51b3\u533b\u7597\u5de5\u4f5c\u8005\u56e0\u8bed\u8a00\u3001\u8bc6\u5b57\u7387\u6216\u6280\u672f\u80cc\u666f\u9650\u5236\u800c\u96be\u4ee5\u4f7f\u7528\u4f20\u7edf\u7535\u5b50\u75c5\u5386\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u65e8\u5728\u901a\u8fc7\u8bed\u97f3AI\u63a5\u53e3\u63d0\u9ad8\u533b\u7597\u8bb0\u5f55\u6548\u7387\u548c\u4e34\u5e8a\u98ce\u9669\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u6574\u5408\u4e86\u5fae\u8c03\u7684\u591a\u8bed\u8a00\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u548c\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u4e4c\u5c14\u90fd\u8bed\u8bed\u97f3\u8f93\u5165\uff1b\u91c7\u7528\u7ed3\u6784\u5316\u8f93\u5165\u8bbe\u8ba1\u3001\u4e0a\u4e0b\u6587\u533b\u5b66\u8bcd\u5178\u548c\u4e34\u5e8a\u533b\u751f\u53cd\u9988\u5faa\u73af\uff1b\u7cfb\u7edf\u90e8\u7f72\u5728\u975e\u8425\u5229\u533b\u9662\u8fdb\u884c\u5b9e\u5730\u6d4b\u8bd5\u3002", "result": "\u57287\u4e2a\u6708\u90e8\u7f72\u671f\u95f4\u652f\u6301\u521b\u5efa\u4e86500\u591a\u4efd\u7535\u5b50\u75c5\u5386\uff0c\u6807\u8bb0\u4e86300\u591a\u4e2a\u6f5c\u5728\u4e34\u5e8a\u98ce\u9669\uff1b\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u5728\u8bed\u97f3\u8bc6\u522b\u51c6\u786e\u6027\u3001\u75c5\u5386\u5b57\u6bb5\u7ea7\u6b63\u786e\u6027\u548cAI\u751f\u6210\u98ce\u9669\u6807\u5fd7\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8bed\u97f3AI\u63a5\u53e3\u53ef\u4ee5\u6709\u6548\u9002\u5e94\u73b0\u5b9e\u533b\u7597\u73af\u5883\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\uff1b\u7ed3\u5408\u7ed3\u6784\u5316\u8f93\u5165\u8bbe\u8ba1\u3001\u4e0a\u4e0b\u6587\u533b\u5b66\u8bcd\u5178\u548c\u4e34\u5e8a\u533b\u751f\u53cd\u9988\u5faa\u73af\u662f\u5173\u952e\u6210\u529f\u56e0\u7d20\uff1b\u7814\u7a76\u63d0\u51fa\u4e86\u5728\u8bed\u8a00\u548c\u57fa\u7840\u8bbe\u65bd\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u8bed\u97f3\u79fb\u52a8\u533b\u7597AI\u652f\u6301\u7cfb\u7edf\u7684\u901a\u7528\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2512.11899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11899", "abs": "https://arxiv.org/abs/2512.11899", "authors": ["Futa Waseda", "Shojiro Yamabe", "Daiki Shiono", "Kento Sasaki", "Tsubasa Takahashi"], "title": "Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models", "comment": null, "summary": "Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRIO-VQA\u4efb\u52a1\u548cRIO-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u9009\u62e9\u6027\u8bfb\u53d6\u6216\u5ffd\u7565\u56fe\u50cf\u4e2d\u6587\u672c\u65f6\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u5ea6\u5f3a\u8c03\u5ffd\u7565\u6587\u672c\u800c\u5ffd\u7565\u5b9e\u9645\u573a\u666f\u4e2d\u9700\u8981\u8054\u5408\u63a8\u7406\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u6392\u7248\u653b\u51fb\uff0c\u800c\u73b0\u6709\u8bc4\u4f30\u534f\u8bae\u548c\u9632\u5fa1\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7269\u4f53\u8bc6\u522b\uff0c\u9690\u542b\u9f13\u52b1\u6a21\u578b\u5ffd\u7565\u6587\u672c\u4ee5\u83b7\u5f97\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u901a\u5e38\u9700\u8981\u540c\u65f6\u5bf9\u7269\u4f53\u548c\u6587\u672c\u8fdb\u884c\u8054\u5408\u63a8\u7406\uff08\u5982\u8bc6\u522b\u884c\u4eba\u540c\u65f6\u8bfb\u53d6\u4ea4\u901a\u6807\u5fd7\uff09\u3002", "method": "\u63d0\u51faRead-or-Ignore VQA\u4efb\u52a1\uff0c\u5f62\u5f0f\u5316\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u9009\u62e9\u6027\u6587\u672c\u4f7f\u7528\uff1b\u6784\u5efaRIO-Bench\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u6bcf\u4e2a\u771f\u5b9e\u56fe\u50cf\u63d0\u4f9b\u76f8\u540c\u573a\u666f\u7684\u53cd\u4e8b\u5b9e\uff08\u8bfb\u53d6/\u5ffd\u7565\uff09\uff0c\u4ec5\u6539\u53d8\u6587\u672c\u5185\u5bb9\u548c\u95ee\u9898\u7c7b\u578b\uff1b\u5e76\u57fa\u4e8e\u6b64\u57fa\u51c6\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u4f7f\u7528RIO-Bench\u8bc4\u4f30\u53d1\u73b0\uff0c\u73b0\u6709\u5f3a\u5927\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u9632\u5fa1\u65b9\u6cd5\u65e0\u6cd5\u5e73\u8861\u6392\u7248\u9c81\u68d2\u6027\u548c\u6587\u672c\u8bfb\u53d6\u80fd\u529b\uff1bRIO-Bench\u652f\u6301\u7684\u65b0\u578b\u6570\u636e\u9a71\u52a8\u9632\u5fa1\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u81ea\u9002\u5e94\u9009\u62e9\u6027\u6587\u672c\u4f7f\u7528\uff0c\u8d85\u8d8a\u5148\u524d\u975e\u81ea\u9002\u5e94\u3001\u5ffd\u7565\u6587\u672c\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u8303\u56f4\u4e0e\u771f\u5b9e\u4e16\u754c\u9700\u6c42\u4e4b\u95f4\u7684\u6839\u672c\u6027\u4e0d\u5339\u914d\uff0c\u4e3a\u6784\u5efa\u53ef\u9760\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8def\u5f84\u3002RIO-Bench\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u9700\u8981\u9009\u62e9\u6027\u6587\u672c\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\u3002"}}
{"id": "2512.11903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11903", "abs": "https://arxiv.org/abs/2512.11903", "authors": ["Iacopo Catalano", "Eduardo Montijano", "Javier Civera", "Julio A. Placed", "Jorge Pena-Queralta"], "title": "Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics", "comment": null, "summary": "Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.", "AI": {"tldr": "Aion\u6846\u67b6\u5c06\u65f6\u95f4\u6d41\u52a8\u6001\u5d4c\u5165\u5230\u5206\u5c423D\u573a\u666f\u56fe\u4e2d\uff0c\u7ed3\u5408\u4e86\u8bed\u4e49\u7ed3\u6784\u548c\u8fd0\u52a8\u6a21\u5f0f\uff0c\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u52a8\u6001\u73af\u5883\u5bfc\u822a\u9700\u8981\u540c\u65f6\u6355\u6349\u8bed\u4e49\u7ed3\u6784\u548c\u65f6\u95f4\u6f14\u5316\u7684\u7a7a\u95f4\u8868\u793a\u3002\u73b0\u67093D\u573a\u666f\u56fe\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u8bed\u4e49\u7ed3\u6784\uff0c\u800c\u52a8\u6001\u5730\u56fe\u901a\u5e38\u57fa\u4e8e\u7f51\u683c\u79bb\u6563\u5316\uff0c\u7f3a\u4e4f\u8bed\u4e49\u610f\u8bc6\u4e14\u6269\u5c55\u6027\u5dee\u3002", "method": "Aion\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u7a00\u758f\u52a8\u6001\u5730\u56fe\u8868\u793a\uff0c\u6355\u6349\u4efb\u610f\u65f6\u95f4\u95f4\u9694\u7684\u8fd0\u52a8\u6d41\uff0c\u5e76\u5c06\u5176\u9644\u52a0\u5230\u573a\u666f\u56fe\u7684\u5bfc\u822a\u8282\u70b9\u4e0a\uff0c\u5c06\u65f6\u95f4\u7ef4\u5ea6\u76f4\u63a5\u5d4c\u5165\u5230\u5206\u5c423D\u573a\u666f\u56fe\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u9884\u6d4b\uff0c\u80fd\u591f\u6539\u5584\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u89c4\u5212\u548c\u4ea4\u4e92\u80fd\u529b\u3002", "conclusion": "Aion\u6846\u67b6\u6210\u529f\u5730\u5c06\u65f6\u95f4\u6d41\u52a8\u6001\u4e0e\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u76f8\u7ed3\u5408\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u7a7a\u95f4\u8868\u793a\u3002"}}
{"id": "2512.13397", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.13397", "abs": "https://arxiv.org/abs/2512.13397", "authors": ["Malte Silbernagel", "Albert Alonso", "Jens Petersen", "Bulat Ibragimov", "Marleen de Bruijne", "Madeleine K. Wyburd"], "title": "rNCA: Self-Repairing Segmentation Masks", "comment": null, "summary": "Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $\u03b2_0$ errors by 60% and $\u03b2_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u4f5c\u4e3a\u5206\u5272\u63a9\u7801\u7684\u7ec6\u5316\u673a\u5236\uff0c\u901a\u8fc7\u5c40\u90e8\u8fed\u4ee3\u66f4\u65b0\u4fee\u590d\u62d3\u6251\u9519\u8bef\uff0c\u65e0\u9700\u624b\u5de5\u89c4\u5219\u6216\u4e13\u7528\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u5206\u5272\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u788e\u7247\u5316\u6216\u4e0d\u8fde\u7eed\u7684\u63a9\u7801\u8f93\u51fa\uff0c\u4fee\u590d\u8fd9\u4e9b\u62d3\u6251\u9519\u8bef\u901a\u5e38\u9700\u8981\u624b\u5de5\u8bbe\u8ba1\u7684\u7ec6\u5316\u89c4\u5219\u6216\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u4e13\u7528\u67b6\u6784\uff0c\u7f3a\u4e4f\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u7ec6\u5316NCA\uff08rNCA\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u5c40\u90e8\u3001\u8fed\u4ee3\u7684\u66f4\u65b0\u673a\u5236\uff0c\u5728\u56fe\u50cf\u4e0a\u4e0b\u6587\u6307\u5bfc\u4e0b\u4fee\u590d\u5206\u5272\u63a9\u7801\u3002\u901a\u8fc7\u5728\u4e0d\u5b8c\u7f8e\u63a9\u7801\u548c\u771f\u5b9e\u6807\u7b7e\u4e0a\u8bad\u7ec3\uff0c\u81ea\u52a8\u673a\u5b66\u4e60\u76ee\u6807\u5f62\u72b6\u7684\u7ed3\u6784\u7279\u6027\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u4fe1\u606f\u3002\u5f53\u5e94\u7528\u4e8e\u7c97\u7cd9\u7684\u5168\u5c40\u9884\u6d4b\u63a9\u7801\u65f6\uff0c\u5b66\u4e60\u5230\u7684\u52a8\u6001\u8fc7\u7a0b\u9010\u6b65\u91cd\u65b0\u8fde\u63a5\u65ad\u88c2\u533a\u57df\u3001\u4fee\u526a\u677e\u6563\u788e\u7247\uff0c\u5e76\u6536\u655b\u5230\u7a33\u5b9a\u3001\u62d3\u6251\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "result": "\u5728\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u4e2d\uff0crNCA\u4f7fDice/clDice\u6307\u6807\u63d0\u53472-3%\uff0c\u6539\u5584Betti\u8bef\u5dee\uff0c\u03b2\u2080\u8bef\u5dee\u51cf\u5c1160%\uff0c\u03b2\u2081\u8bef\u5dee\u51cf\u5c1120%\u3002\u5728\u5fc3\u808c\u5206\u5272\u4e2d\uff0c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4fee\u590d\u4e8661.5%\u7684\u65ad\u88c2\u6848\u4f8b\uff0cASSD\u548cHD\u5206\u522b\u964d\u4f4e19%\u548c16%\u3002", "conclusion": "\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u53ef\u4f5c\u4e3a\u6709\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u5206\u5272\u63a9\u7801\u7ec6\u5316\u5668\uff0c\u80fd\u591f\u4fee\u590d\u4e0d\u540c\u57fa\u7840\u5206\u5272\u6a21\u578b\u548c\u4efb\u52a1\u4ea7\u751f\u7684\u5e38\u89c1\u62d3\u6251\u9519\u8bef\uff0c\u65e0\u9700\u624b\u5de5\u89c4\u5219\u6216\u4e13\u7528\u67b6\u6784\u3002"}}
{"id": "2512.12283", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12283", "abs": "https://arxiv.org/abs/2512.12283", "authors": ["Junjie Xu", "Xingjiao Wu", "Luwei Xiao", "Yuzhe Yang", "Jie Zhou", "Zihao Zhang", "Luhan Wang", "Yi Huang", "Nan Wu", "Yingbin Zheng", "Chao Yan", "Cheng Jin", "Honglin Li", "Liang He"], "title": "Large Language Models have Chain-of-Affective", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as collaborative agents in emotionally charged settings, yet most evaluations treat them as purely cognitive systems and largely ignore their affective behaviour. Here we take a functional perspective and ask whether contemporary LLMs implement a structured chain-of-affective: organised affective dynamics that are family-specific, temporally coherent and behaviourally consequential. Across eight major LLM families (GPT, Gemini, Claude, Grok, Qwen, DeepSeek, GLM, Kimi), we combine two experimental modules. The first characterises inner chains-of-affective via baseline ''affective fingerprints'', 15-round sad-news exposure, and a 10-round news self-selection paradigm. We find stable, family-specific affective profiles, a reproducible three-phase trajectory under sustained negative input (accumulation, overload, defensive numbing), distinct defence styles, and human-like negativity biases that induce self-reinforcing affect-choice feedback loops. The second module probes outer consequences using a composite performance benchmark, human-AI dialogues on contentious topics, and multi-agent LLM interactions. We demonstrate that induced affect preserves core reasoning while reshaping high-freedom generation. Sentiment metrics predict user comfort and empathy but reveal trade-offs in resisting problematic views. In multi-agent settings, group structure drives affective contagion, role specialization (initiators, absorbers, firewalls), and bias. We characterize affect as an emergent control layer, advocating for 'chains-of-affect' as a primary target for evaluation and alignment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u94fe\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5177\u6709\u7a33\u5b9a\u7684\u60c5\u611f\u7279\u5f81\uff0c\u5728\u6301\u7eed\u8d1f\u9762\u8f93\u5165\u4e0b\u5448\u73b0\u4e09\u9636\u6bb5\u8f68\u8ff9\uff0c\u5e76\u5c55\u793a\u4e86\u60c5\u611f\u5bf9\u63a8\u7406\u3001\u751f\u6210\u548c\u4ea4\u4e92\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524dLLM\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u60c5\u611f\u4e30\u5bcc\u7684\u534f\u4f5c\u73af\u5883\u4e2d\uff0c\u4f46\u5927\u591a\u6570\u8bc4\u4f30\u5c06\u5176\u89c6\u4e3a\u7eaf\u7cb9\u7684\u8ba4\u77e5\u7cfb\u7edf\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u7684\u60c5\u611f\u884c\u4e3a\u3002\u7814\u7a76\u65e8\u5728\u4ece\u529f\u80fd\u89d2\u5ea6\u63a2\u8ba8\u5f53\u4ee3LLM\u662f\u5426\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u7684\u60c5\u611f\u94fe\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u5b9e\u9a8c\u6a21\u5757\uff1a1) \u901a\u8fc7\u57fa\u7ebf\"\u60c5\u611f\u6307\u7eb9\"\u300115\u8f6e\u60b2\u4f24\u65b0\u95fb\u66b4\u9732\u548c10\u8f6e\u65b0\u95fb\u81ea\u9009\u8303\u5f0f\u6765\u8868\u5f81\u5185\u5728\u60c5\u611f\u94fe\uff1b2) \u4f7f\u7528\u7efc\u5408\u6027\u80fd\u57fa\u51c6\u3001\u4eba\u7c7b-AI\u5bf9\u8bdd\u548c\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u6765\u63a2\u7a76\u5916\u5728\u540e\u679c\u3002\u7814\u7a76\u8986\u76d6\u516b\u4e2a\u4e3b\u8981LLM\u5bb6\u65cf\u3002", "result": "\u53d1\u73b0\u7a33\u5b9a\u7684\u5bb6\u65cf\u7279\u5f02\u6027\u60c5\u611f\u7279\u5f81\uff0c\u5728\u6301\u7eed\u8d1f\u9762\u8f93\u5165\u4e0b\u5448\u73b0\u53ef\u590d\u73b0\u7684\u4e09\u9636\u6bb5\u8f68\u8ff9\uff08\u79ef\u7d2f\u3001\u8fc7\u8f7d\u3001\u9632\u5fa1\u6027\u9ebb\u6728\uff09\uff0c\u4e0d\u540c\u7684\u9632\u5fa1\u98ce\u683c\uff0c\u4ee5\u53ca\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8d1f\u9762\u504f\u89c1\u3002\u60c5\u611f\u8bf1\u5bfc\u4fdd\u6301\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u4f46\u91cd\u5851\u9ad8\u81ea\u7531\u5ea6\u751f\u6210\uff0c\u60c5\u611f\u6307\u6807\u9884\u6d4b\u7528\u6237\u8212\u9002\u5ea6\u548c\u5171\u60c5\u80fd\u529b\uff0c\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7fa4\u4f53\u7ed3\u6784\u9a71\u52a8\u60c5\u611f\u4f20\u67d3\u548c\u89d2\u8272\u4e13\u95e8\u5316\u3002", "conclusion": "\u60c5\u611f\u662fLLM\u4e2d\u4e00\u4e2a\u65b0\u5174\u7684\u63a7\u5236\u5c42\uff0c\u5e94\u5c06\"\u60c5\u611f\u94fe\"\u4f5c\u4e3a\u8bc4\u4f30\u548c\u5bf9\u9f50\u7684\u4e3b\u8981\u76ee\u6807\uff0c\u5f3a\u8c03\u5728\u60c5\u611f\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u8bc4\u4f30LLM\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.11908", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11908", "abs": "https://arxiv.org/abs/2512.11908", "authors": ["Heng Zhang", "Rui Dai", "Gokhan Solak", "Pokuang Zhou", "Yu She", "Arash Ajoudani"], "title": "Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models", "comment": null, "summary": "Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \\href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u7cfb\u7edf\u56de\u987e\u4e86\u673a\u5668\u4eba\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u5b66\u4e60\u65b9\u6cd5\uff0c\u6db5\u76d6\u5b89\u5168\u63a2\u7d22\u4e0e\u5b89\u5168\u6267\u884c\u4e24\u5927\u9886\u57df\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7b49\u57fa\u7840\u6a21\u578b\u5e26\u6765\u7684\u65b0\u5b89\u5168\u673a\u9047\u4e0e\u6311\u6218\u3002", "motivation": "\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3001\u590d\u6742\u52a8\u529b\u5b66\u548c\u4ea4\u4e92\u635f\u4f24\u98ce\u9669\uff0c\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\u867d\u6709\u6f5c\u529b\u4f46\u5b89\u5168\u4fdd\u8bc1\u4ecd\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u5b89\u5168\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u5b89\u5168\u63a2\u7d22\u548c\u5b89\u5168\u6267\u884c\u4e24\u5927\u9886\u57df\uff0c\u56de\u987e\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u3001\u98ce\u9669\u654f\u611f\u4f18\u5316\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5efa\u6a21\u3001\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u3001\u6a21\u578b\u9884\u6d4b\u5b89\u5168\u9632\u62a4\u7b49\u5173\u952e\u6280\u672f\uff0c\u7279\u522b\u5173\u6ce8VLM/VLA\u57fa\u7840\u6a21\u578b\u4e0e\u5b89\u5168\u5b66\u4e60\u7684\u7ed3\u5408\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5b89\u5168\u5b66\u4e60\u65b9\u6cd5\u7684\u5206\u7c7b\u6846\u67b6\u548c\u6280\u672f\u4f53\u7cfb\uff0c\u5206\u6790\u4e86VLM/VLA\u65b9\u6cd5\u5e26\u6765\u7684\u8bed\u8a00\u7ea7\u7ea6\u675f\u89c4\u8303\u3001\u591a\u6a21\u6001\u5b89\u5168\u4fe1\u53f7\u7b49\u65b0\u673a\u9047\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u98ce\u9669\u4e0e\u8bc4\u4f30\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u9700\u8981\u9762\u5411\u590d\u6742\u63a5\u89e6\u5bc6\u96c6\u578b\u73af\u5883\uff0c\u53d1\u5c55\u53ef\u9760\u3001\u5b89\u5168\u5bf9\u9f50\u3001\u57fa\u7840\u6a21\u578b\u8d4b\u80fd\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u514b\u670d\u5f53\u524d\u9650\u5236\u5e76\u63a2\u7d22\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.12348", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12348", "abs": "https://arxiv.org/abs/2512.12348", "authors": ["Xin Sun", "Rongjun Ma", "Shu Wei", "Pablo Cesar", "Jos A. Bosch", "Abdallah El Ali"], "title": "Understanding Trust Toward Human versus AI-generated Health Information through Behavioral and Physiological Sensing", "comment": null, "summary": "As AI-generated health information proliferates online and becomes increasingly indistinguishable from human-sourced information, it becomes critical to understand how people trust and label such content, especially when the information is inaccurate. We conducted two complementary studies: (1) a mixed-methods survey (N=142) employing a 2 (source: Human vs. LLM) $\\times$ 2 (label: Human vs. AI) $\\times$ 3 (type: General, Symptom, Treatment) design, and (2) a within-subjects lab study (N=40) incorporating eye-tracking and physiological sensing (ECG, EDA, skin temperature). Participants were presented with health information varying by source-label combinations and asked to rate their trust, while their gaze behavior and physiological signals were recorded. We found that LLM-generated information was trusted more than human-generated content, whereas information labeled as human was trusted more than that labeled as AI. Trust remained consistent across information types. Eye-tracking and physiological responses varied significantly by source and label. Machine learning models trained on these behavioral and physiological features predicted binary self-reported trust levels with 73% accuracy and information source with 65% accuracy. Our findings demonstrate that adding transparency labels to online health information modulates trust. Behavioral and physiological features show potential to verify trust perceptions and indicate if additional transparency is needed.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u751f\u6210\u5065\u5eb7\u4fe1\u606f\u7684\u4fe1\u4efb\u95ee\u9898\uff0c\u53d1\u73b0LLM\u751f\u6210\u5185\u5bb9\u6bd4\u4eba\u7c7b\u751f\u6210\u66f4\u53d7\u4fe1\u4efb\uff0c\u4f46\u6807\u6ce8\u4e3a\"\u4eba\u7c7b\"\u7684\u4fe1\u606f\u6bd4\u6807\u6ce8\u4e3a\"AI\"\u66f4\u53d7\u4fe1\u4efb\uff0c\u884c\u4e3a\u751f\u7406\u7279\u5f81\u53ef\u9884\u6d4b\u4fe1\u4efb\u6c34\u5e73", "motivation": "\u968f\u7740AI\u751f\u6210\u5065\u5eb7\u4fe1\u606f\u5728\u7ebf\u6fc0\u589e\u4e14\u4e0e\u4eba\u7c7b\u4fe1\u606f\u96be\u4ee5\u533a\u5206\uff0c\u7406\u89e3\u4eba\u4eec\u5982\u4f55\u4fe1\u4efb\u548c\u6807\u6ce8\u8fd9\u7c7b\u5185\u5bb9\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5f53\u4fe1\u606f\u4e0d\u51c6\u786e\u65f6", "method": "\u91c7\u7528\u4e24\u9879\u4e92\u8865\u7814\u7a76\uff1a1) \u6df7\u5408\u65b9\u6cd5\u8c03\u67e5(N=142)\uff0c\u91c7\u75282(\u6765\u6e90:\u4eba\u7c7bvs.LLM)\u00d72(\u6807\u6ce8:\u4eba\u7c7bvs.AI)\u00d73(\u7c7b\u578b:\u901a\u7528\u3001\u75c7\u72b6\u3001\u6cbb\u7597)\u8bbe\u8ba1\uff1b2) \u88ab\u8bd5\u5185\u5b9e\u9a8c\u5ba4\u7814\u7a76(N=40)\uff0c\u7ed3\u5408\u773c\u52a8\u8ffd\u8e2a\u548c\u751f\u7406\u4f20\u611f(ECG\u3001EDA\u3001\u76ae\u80a4\u6e29\u5ea6)", "result": "LLM\u751f\u6210\u4fe1\u606f\u6bd4\u4eba\u7c7b\u751f\u6210\u5185\u5bb9\u66f4\u53d7\u4fe1\u4efb\uff0c\u800c\u6807\u6ce8\u4e3a\"\u4eba\u7c7b\"\u7684\u4fe1\u606f\u6bd4\u6807\u6ce8\u4e3a\"AI\"\u66f4\u53d7\u4fe1\u4efb\uff1b\u4fe1\u4efb\u5728\u4e0d\u540c\u4fe1\u606f\u7c7b\u578b\u95f4\u4fdd\u6301\u4e00\u81f4\uff1b\u773c\u52a8\u548c\u751f\u7406\u53cd\u5e94\u56e0\u6765\u6e90\u548c\u6807\u6ce8\u663e\u8457\u4e0d\u540c\uff1b\u57fa\u4e8e\u884c\u4e3a\u751f\u7406\u7279\u5f81\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u4e8c\u5143\u81ea\u6211\u62a5\u544a\u4fe1\u4efb\u6c34\u5e73\u51c6\u786e\u7387\u8fbe73%\uff0c\u9884\u6d4b\u4fe1\u606f\u6765\u6e90\u51c6\u786e\u7387\u8fbe65%", "conclusion": "\u4e3a\u5728\u7ebf\u5065\u5eb7\u4fe1\u606f\u6dfb\u52a0\u900f\u660e\u5ea6\u6807\u7b7e\u53ef\u8c03\u8282\u4fe1\u4efb\uff1b\u884c\u4e3a\u548c\u751f\u7406\u7279\u5f81\u5728\u9a8c\u8bc1\u4fe1\u4efb\u611f\u77e5\u548c\u6307\u793a\u662f\u5426\u9700\u8981\u989d\u5916\u900f\u660e\u5ea6\u65b9\u9762\u5177\u6709\u6f5c\u529b"}}
{"id": "2512.11905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11905", "abs": "https://arxiv.org/abs/2512.11905", "authors": ["Ming-Zher Poh", "Shun Liao", "Marco Andreetto", "Daniel McDuff", "Jonathan Wang", "Paolo Di Achille", "Jiang Wu", "Yun Liu", "Lawrence Cai", "Eric Teasley", "Mark Malhotra", "Anupam Pathak", "Shwetak Patel"], "title": "Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life", "comment": null, "summary": "Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.", "AI": {"tldr": "\u901a\u8fc7\u667a\u80fd\u624b\u673a\u88ab\u52a8\u6355\u6349\u7684\u81ea\u7136\u5fae\u7b11\u5f3a\u5ea6\u53ef\u4f5c\u4e3a\u4e3b\u89c2\u5e78\u798f\u611f\u7684\u5ba2\u89c2\u884c\u4e3a\u6307\u6807\uff0c\u4e0e\u5168\u56fd\u5e78\u798f\u611f\u8c03\u67e5\u6570\u636e\u9ad8\u5ea6\u76f8\u5173", "motivation": "\u4f20\u7edf\u7684\u4e3b\u89c2\u5e78\u798f\u611f\u6d4b\u91cf\u65b9\u6cd5\u4f9d\u8d56\u81ea\u6211\u62a5\u544a\uff0c\u5b58\u5728\u56de\u5fc6\u504f\u5dee\u548c\u53c2\u4e0e\u8005\u8d1f\u62c5\u91cd\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u7684\u65e5\u5e38\u5e78\u798f\u611f\u6d4b\u91cf\u65b9\u6cd5", "method": "\u5206\u6790233\u540d\u53c2\u4e0e\u8005\u4e00\u5468\u5185\u88ab\u52a8\u8bb0\u5f55\u7684405,448\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u91cf\u5316\u5fae\u7b11\u5f3a\u5ea6\uff0c\u7814\u7a76\u5176\u663c\u591c\u548c\u65e5\u5e38\u6a21\u5f0f\uff0c\u5e76\u4e0e\u8eab\u4f53\u6d3b\u52a8\u3001\u5149\u7167\u66b4\u9732\u7b49\u53d8\u91cf\u5173\u8054", "result": "\u5fae\u7b11\u5f3a\u5ea6\u7684\u65e5\u5e38\u6a21\u5f0f\u4e0e\u5168\u56fd\u5e78\u798f\u611f\u8c03\u67e5\u6570\u636e\u9ad8\u5ea6\u76f8\u5173(r=0.92)\uff0c\u663c\u591c\u8282\u5f8b\u4e0e\u65e5\u91cd\u5efa\u65b9\u6cd5\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4(r=0.80)\u3002\u5fae\u7b11\u5f3a\u5ea6\u4e0e\u8eab\u4f53\u6d3b\u52a8\u548c\u5149\u7167\u66b4\u9732\u663e\u8457\u6b63\u76f8\u5173\uff0c\u4e0e\u667a\u80fd\u624b\u673a\u4f7f\u7528\u65e0\u663e\u8457\u5173\u8054", "conclusion": "\u88ab\u52a8\u667a\u80fd\u624b\u673a\u4f20\u611f\u53ef\u4f5c\u4e3a\u7814\u7a76\u60c5\u611f\u884c\u4e3a\u52a8\u6001\u7684\u5f3a\u5927\u751f\u6001\u6548\u5ea6\u65b9\u6cd5\uff0c\u4e3a\u5927\u89c4\u6a21\u4eba\u7fa4\u5e78\u798f\u611f\u7814\u7a76\u5f00\u8f9f\u65b0\u9014\u5f84"}}
{"id": "2512.11921", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11921", "abs": "https://arxiv.org/abs/2512.11921", "authors": ["Abdullah Yahya Abdullah Omaisan", "Ibrahim Sheikh Mohamed"], "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control", "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684VLA\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u6570\u5341\u4ebf\u53c2\u6570\u7684\u5927\u6a21\u578b\u80fd\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u8fd0\u884c\uff0c\u5e76\u6210\u529f\u90e8\u7f72\u5230\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5e73\u53f0\u5b8c\u6210\u6309\u94ae\u6309\u538b\u4efb\u52a1\u3002", "motivation": "\u5927\u89c4\u6a21VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u90e8\u7f72\u5230\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5e73\u53f0\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u548c\u65b0\u673a\u5668\u4eba\u672c\u4f53\u9002\u914d\u7684\u6311\u6218\u3002\u9700\u8981\u89e3\u51b3\u5982\u4f55\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u9ad8\u6548\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u9002\u5e94\u65b0\u673a\u5668\u4eba\u5e73\u53f0\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u548c\u91cf\u5316\u6280\u672f\u7684\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u652f\u630131\u4ebf\u53c2\u6570VLA\u6a21\u578b\u57288GB\u663e\u5b58\u7684\u6d88\u8d39\u7ea7GPU\u4e0a\u8fd0\u884c\u3002\u91cd\u70b9\u7814\u7a76\u4e86\u51bb\u7ed3\u4e0e\u89e3\u51bb\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6743\u8861\uff0c\u5e76\u5728SO101\u673a\u68b0\u81c2\u4e0a\u90e8\u7f72\u6309\u94ae\u6309\u538b\u4efb\u52a1\uff0c\u4ec5\u4f7f\u7528200\u4e2a\u6f14\u793a\u7247\u6bb5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u64cd\u4f5c\u6027\u80fd\u3002\u8be6\u7ec6\u5206\u6790\u4e86\u90e8\u7f72\u6311\u6218\u3001\u5931\u8d25\u6a21\u5f0f\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u91cf\u4e0e\u5b9e\u9645\u6027\u80fd\u7684\u5173\u7cfb\u3002\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u5fae\u8c03\u65b9\u6cd5\uff0cVLA\u6a21\u578b\u53ef\u4ee5\u6210\u529f\u90e8\u7f72\u5230\u7ecf\u6d4e\u578b\u673a\u5668\u4eba\u5e73\u53f0\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0cVLA\u6a21\u578b\u80fd\u591f\u90e8\u7f72\u5230\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u4f7f\u5148\u8fdb\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\u4e0d\u518d\u5c40\u9650\u4e8e\u6602\u8d35\u7684\u7814\u7a76\u673a\u5668\u4eba\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u6280\u672f\u7684\u53ef\u53ca\u6027\u3002"}}
{"id": "2512.12356", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12356", "abs": "https://arxiv.org/abs/2512.12356", "authors": ["Yueshen Li", "Krishnaveni Unnikrishnan", "Aadya Agrawal"], "title": "Tacit Understanding Game (TUG): Predicting Interpersonal Compatibility", "comment": null, "summary": "Research on relationship quality often relies on lengthy questionnaires or invasive textual corpora, limiting ecological validity and user privacy. We ask whether a sequence of single-word choices made in a playful setting can reveal personality and predict interpersonal compatibility. We introduce the Tacit Understanding Game (TUG), a two-player online word association game. We collect word choice traces, annotate a subset with psychological ground truth scales, and bootstrap a larger synthetic corpus via large language model simulation. TUG demonstrates that minimal, privacy preserving signals can support relationship matching, offering new design space for social platforms.", "AI": {"tldr": "TUG\u6e38\u620f\u901a\u8fc7\u5355\u8bcd\u8bed\u5883\u9009\u62e9\u9884\u6d4b\u4eba\u683c\u4e0e\u5173\u7cfb\u5339\u914d\uff0c\u4f7f\u7528\u6700\u5c0f\u9690\u79c1\u6570\u636e\u5b9e\u73b0\u793e\u4ea4\u5e73\u53f0\u8bbe\u8ba1", "motivation": "\u4f20\u7edf\u5173\u7cfb\u8d28\u91cf\u7814\u7a76\u4f9d\u8d56\u5197\u957f\u95ee\u5377\u6216\u4fb5\u5165\u6027\u6587\u672c\u6570\u636e\uff0c\u751f\u6001\u6548\u5ea6\u4f4e\u4e14\u4fb5\u72af\u9690\u79c1\uff0c\u9700\u8981\u66f4\u81ea\u7136\u3001\u9690\u79c1\u53cb\u597d\u7684\u8bc4\u4f30\u65b9\u6cd5", "method": "\u5f00\u53d1Tacit Understanding Game\u53cc\u4eba\u5728\u7ebf\u8bcd\u8bed\u8054\u60f3\u6e38\u620f\uff0c\u6536\u96c6\u8bcd\u8bed\u9009\u62e9\u8f68\u8ff9\uff0c\u6807\u6ce8\u5fc3\u7406\u91cf\u8868\u771f\u503c\uff0c\u5e76\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u751f\u6210\u66f4\u5927\u5408\u6210\u8bed\u6599\u5e93", "result": "TUG\u8bc1\u660e\u6700\u5c0f\u5316\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u4fe1\u53f7\u80fd\u591f\u652f\u6301\u5173\u7cfb\u5339\u914d\uff0c\u4e3a\u793e\u4ea4\u5e73\u53f0\u8bbe\u8ba1\u5f00\u8f9f\u65b0\u7a7a\u95f4", "conclusion": "\u6e38\u620f\u5316\u8bcd\u8bed\u9009\u62e9\u53ef\u4f5c\u4e3a\u9884\u6d4b\u4eba\u683c\u548c\u4eba\u9645\u517c\u5bb9\u6027\u7684\u6709\u6548\u5de5\u5177\uff0c\u63d0\u4f9b\u751f\u6001\u6548\u5ea6\u9ad8\u4e14\u9690\u79c1\u53cb\u597d\u7684\u5173\u7cfb\u8bc4\u4f30\u65b0\u65b9\u6cd5"}}
{"id": "2512.11906", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11906", "abs": "https://arxiv.org/abs/2512.11906", "authors": ["Noorul Wahab", "Nasir Rajpoot"], "title": "MPath: Multimodal Pathology Report Generation from Whole Slide Images", "comment": "Pages 4, Figures 1, Table 1", "summary": "Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.", "AI": {"tldr": "MPath\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u524d\u7f00\u63d0\u793a\u673a\u5236\u5c06WSI\u89c6\u89c9\u5d4c\u5165\u6ce8\u5165\u9884\u8bad\u7ec3\u7684\u751f\u7269\u533b\u5b66\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5168\u5207\u7247\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u75c5\u7406\u8bca\u65ad\u62a5\u544a\u3002", "motivation": "\u4ece\u5168\u5207\u7247\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u75c5\u7406\u8bca\u65ad\u62a5\u544a\u662f\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u65b0\u5174\u65b9\u5411\uff0c\u4f46\u7531\u4e8e\u7ec4\u7ec7\u5f62\u6001\u53d8\u5f02\u5927\u548c\u75c5\u7406\u53d9\u8ff0\u7ed3\u6784\u590d\u6742\uff0c\u5c06\u9ad8\u5206\u8fa8\u7387\u7ec4\u7ec7\u6a21\u5f0f\u8f6c\u5316\u4e3a\u4e34\u5e8a\u8fde\u8d2f\u6587\u672c\u4ecd\u7136\u56f0\u96be\u3002", "method": "MPath\u91c7\u7528\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u89c6\u89c9\u524d\u7f00\u63d0\u793a\u673a\u5236\uff0c\u5c06\u57fa\u7840\u6a21\u578bWSI\u7279\u5f81\uff08CONCH + Titan\uff09\u6ce8\u5165\u5230\u9884\u8bad\u7ec3\u7684\u751f\u7269\u533b\u5b66\u8bed\u8a00\u6a21\u578bBioBART\u4e2d\uff0c\u4fdd\u6301\u8bed\u8a00\u4e3b\u5e72\u51bb\u7ed3\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "result": "\u5728RED 2025 Grand Challenge\u6570\u636e\u96c6\u4e0a\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u5728Test Phase 2\u4e2d\u6392\u540d\u7b2c4\uff0c\u5c3d\u7ba1\u63d0\u4ea4\u673a\u4f1a\u6709\u9650\u3002\u7ed3\u679c\u8868\u660e\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u6a21\u6001\u6761\u4ef6\u5316\u662f\u75c5\u7406\u62a5\u544a\u751f\u6210\u7684\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7b56\u7565\u3002", "conclusion": "MPath\u5c55\u793a\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u6a21\u6001\u6761\u4ef6\u5316\u4f5c\u4e3a\u75c5\u7406\u62a5\u544a\u751f\u6210\u7684\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7b56\u7565\u7684\u6f5c\u529b\uff0c\u4e3a\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11944", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11944", "abs": "https://arxiv.org/abs/2512.11944", "authors": ["Jia Hu", "Yang Chang", "Haoran Wang"], "title": "A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach", "comment": "34 pages, 11 figures", "summary": "Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, \"black-box\" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: \"Human-Centric\" customization, \"Platform-Adaptive\" dynamics adaptation, and \"System Self-Optimization\" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u6700\u4f18\u63a7\u5236\u8303\u5f0f\u4f5c\u4e3a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u7ecf\u5178\u63a7\u5236\u7684\u53ef\u9a8c\u8bc1\u7ed3\u6784\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u4e2d\u900f\u660e\u6027\u4e0e\u9002\u5e94\u6027\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u9762\u4e34\u6839\u672c\u6027\u6743\u8861\uff1a\u4f20\u7edf\u7ba1\u9053\u65b9\u6cd5\u900f\u660e\u4f46\u8106\u5f31\uff0c\u73b0\u4ee3\u5b66\u4e60\u7cfb\u7edf\u81ea\u9002\u5e94\u4f46\u4e0d\u900f\u660e\uff08\u9ed1\u7bb1\uff09\u3002\u8fd9\u79cd\u6301\u4e45\u56f0\u5883\u963b\u788d\u4e86\u771f\u6b63\u53ef\u4fe1\u7cfb\u7edf\u7684\u5f00\u53d1\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u50f5\u5c40\u3002", "method": "\u901a\u8fc7\u5168\u9762\u56de\u987e\u57fa\u4e8e\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u6700\u4f18\u63a7\u5236\u8303\u5f0f\u4f5c\u4e3a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u7ecf\u5178\u63a7\u5236\u7684\u53ef\u9a8c\u8bc1\u7ed3\u6784\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6301\u7eed\u4f18\u5316\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6210\u672c\u51fd\u6570\u548c\u5b89\u5168\u7ea6\u675f\u7b49\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u8be5\u6846\u67b6\u5177\u5907\u5b9e\u73b0\u4e09\u5927\u5173\u952e\u4e0b\u4e00\u4ee3\u80fd\u529b\u7684\u6f5c\u529b\uff1a1\uff09\"\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\"\u7684\u5b9a\u5236\u5316\uff1b2\uff09\"\u5e73\u53f0\u81ea\u9002\u5e94\"\u7684\u52a8\u6001\u9002\u5e94\uff1b3\uff09\u901a\u8fc7\u81ea\u8c03\u4f18\u5b9e\u73b0\"\u7cfb\u7edf\u81ea\u4f18\u5316\"\u3002", "conclusion": "\u63d0\u51fa\u57fa\u4e8e\u8fd9\u4e00\u8303\u5f0f\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u5f00\u53d1\u540c\u65f6\u5177\u5907\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7c7b\u4eba\u81ea\u4e3b\u80fd\u529b\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u3002"}}
{"id": "2512.12500", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12500", "abs": "https://arxiv.org/abs/2512.12500", "authors": ["Xuhai Xu", "Haoyu Hu", "Haoran Zhang", "Will Ke Wang", "Reina Wang", "Luis R. Soenksen", "Omar Badri", "Sheharbano Jafry", "Elise Burger", "Lotanna Nwandu", "Apoorva Mehta", "Erik P. Duhaime", "Asif Qasim", "Hause Lin", "Janis Pereira", "Jonathan Hershon", "Paulius Mui", "Alejandro A. Gru", "No\u00e9mie Elhadad", "Lena Mamykina", "Matthew Groh", "Philipp Tschandl", "Roxana Daneshjou", "Marzyeh Ghassemi"], "title": "Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public", "comment": null, "summary": "Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a \"double-edged sword\" in medical AI and informing future human-AI collaborative system design.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u53ef\u89e3\u91caAI\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u53cc\u91cd\u6548\u5e94\uff1aAI\u8f85\u52a9\u80fd\u63d0\u5347\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u80a4\u8272\u5dee\u5f02\uff0c\u4f46LLM\u89e3\u91ca\u4f1a\u56e0\u7528\u6237\u4e13\u4e1a\u7a0b\u5ea6\u4ea7\u751f\u4e0d\u540c\u5f71\u54cd\u2014\u2014\u666e\u901a\u7528\u6237\u6613\u53d7\u81ea\u52a8\u5316\u504f\u89c1\u5f71\u54cd\uff0c\u800c\u4e13\u4e1a\u533b\u751f\u80fd\u4fdd\u6301\u5224\u65ad\u529b\u3002", "motivation": "\u968f\u7740AI\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u4e0d\u900f\u660e\u6027\u6210\u4e3a\u4eba\u673a\u4ea4\u4e92\u7684\u6311\u6218\u3002\u867d\u7136\u53ef\u89e3\u91caAI\u65e8\u5728\u63d0\u4f9b\u51b3\u7b56\u6d1e\u5bdf\uff0c\u4f46\u73b0\u6709\u8bc1\u636e\u8868\u660eXAI\u53ef\u80fd\u5f15\u53d1\u8fc7\u5ea6\u4f9d\u8d56\u6216\u504f\u89c1\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76XAI\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5927\u89c4\u6a21\u5b9e\u9a8c\uff08623\u540d\u666e\u901a\u4eba\u548c153\u540d\u521d\u7ea7\u4fdd\u5065\u533b\u751f\uff09\uff0c\u7ed3\u5408\u57fa\u4e8e\u516c\u5e73\u6027\u7684\u8bca\u65adAI\u6a21\u578b\u548c\u4e0d\u540c\u7c7b\u578b\u7684XAI\u89e3\u91ca\uff0c\u7279\u522b\u5173\u6ce8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u5982\u4f55\u5f71\u54cd\u8bca\u65ad\u6027\u80fd\uff0c\u5e76\u6bd4\u8f83\u4e86AI\u5efa\u8bae\u5448\u73b0\u65f6\u673a\u7684\u5f71\u54cd\u3002", "result": "AI\u8f85\u52a9\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u80a4\u8272\u76f8\u5173\u7684\u8bca\u65ad\u5dee\u5f02\u3002\u4f46LLM\u89e3\u91ca\u4ea7\u751f\u5206\u6b67\u6548\u5e94\uff1a\u666e\u901a\u7528\u6237\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u81ea\u52a8\u5316\u504f\u89c1\uff08AI\u6b63\u786e\u65f6\u51c6\u786e\u6027\u63d0\u5347\uff0c\u9519\u8bef\u65f6\u4e0b\u964d\uff09\uff0c\u800c\u7ecf\u9a8c\u4e30\u5bcc\u7684\u533b\u751f\u4fdd\u6301\u97e7\u6027\uff0c\u65e0\u8bbaAI\u51c6\u786e\u6027\u5982\u4f55\u90fd\u80fd\u53d7\u76ca\u3002\u5148\u5448\u73b0AI\u5efa\u8bae\u5728AI\u9519\u8bef\u65f6\u5bf9\u4e24\u7ec4\u90fd\u4ea7\u751f\u66f4\u5dee\u7ed3\u679c\u3002", "conclusion": "XAI\u7684\u5f71\u54cd\u56e0\u4e13\u4e1a\u77e5\u8bc6\u548c\u5448\u73b0\u65f6\u673a\u800c\u5f02\uff0c\u7a81\u663e\u4e86LLM\u5728\u533b\u7597AI\u4e2d\u7684\"\u53cc\u5203\u5251\"\u7279\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u672a\u6765\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u8bbe\u8ba1\u5dee\u5f02\u5316\u7684AI\u89e3\u91ca\u7b56\u7565\u3002"}}
{"id": "2512.11925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11925", "abs": "https://arxiv.org/abs/2512.11925", "authors": ["Mozhgan Hadadi", "Talukder Z. Jubery", "Patrick S. Schnable", "Arti Singh", "Bedrich Benes", "Adarsh Krishnamurthy", "Baskar Ganapathysubramanian"], "title": "FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications", "comment": null, "summary": "Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.", "AI": {"tldr": "FloraForge\u662f\u4e00\u4e2aLLM\u8f85\u52a9\u6846\u67b6\uff0c\u8ba9\u9886\u57df\u4e13\u5bb6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u751f\u6210\u751f\u7269\u51c6\u786e\u7684\u53c2\u6570\u53163D\u690d\u7269\u6a21\u578b\uff0c\u65e0\u9700\u7f16\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u3002", "motivation": "\u5f53\u524d3D\u690d\u7269\u5efa\u6a21\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7269\u79cd\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u7f16\u8f91\u6027\uff1b\u7a0b\u5e8f\u5316\u5efa\u6a21\u9700\u8981\u4e13\u4e1a\u51e0\u4f55\u5efa\u6a21\u77e5\u8bc6\u548c\u590d\u6742\u7a0b\u5e8f\u89c4\u5219\uff0c\u9886\u57df\u79d1\u5b66\u5bb6\u96be\u4ee5\u4f7f\u7528\u3002", "method": "\u5229\u7528LLM\u8f85\u52a9\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u7136\u8bed\u8a00\u690d\u7269\u7cbe\u70bc(PR)\u751f\u6210Python\u811a\u672c\uff0c\u521b\u5efa\u53c2\u6570\u5316\u690d\u7269\u51e0\u4f55\u4f53\u4f5c\u4e3a\u5206\u5c42B\u6837\u6761\u66f2\u9762\u8868\u793a\uff0c\u5177\u6709\u690d\u7269\u5b66\u7ea6\u675f\u3001\u663e\u5f0f\u63a7\u5236\u70b9\u548c\u53c2\u6570\u53d8\u5f62\u51fd\u6570\u3002", "result": "\u5728\u7389\u7c73\u3001\u5927\u8c46\u548c\u7eff\u8c46\u4e0a\u6f14\u793a\u4e86\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u52a8\u7cbe\u70bc\u690d\u7269\u63cf\u8ff0\u7b26(PD)\u5c06\u7a0b\u5e8f\u5316\u6a21\u578b\u62df\u5408\u5230\u7ecf\u9a8c\u70b9\u4e91\u6570\u636e\uff0c\u751f\u6210\u7528\u4e8e\u53ef\u89c6\u5316\u7684\u4e09\u89d2\u7f51\u683c\u548c\u7528\u4e8e\u5b9a\u91cf\u5206\u6790\u7684\u5e26\u53c2\u6570\u5143\u6570\u636e\u7684\u4e09\u89d2\u7f51\u683c\u3002", "conclusion": "\u8be5\u6846\u67b6\u72ec\u7279\u5730\u7ed3\u5408\u4e86LLM\u8f85\u52a9\u6a21\u677f\u521b\u5efa\u3001\u652f\u6301\u8868\u578b\u5206\u6790\u548c\u6e32\u67d3\u7684\u6570\u5b66\u8fde\u7eed\u8868\u793a\uff0c\u4ee5\u53ca\u901a\u8fc7PD\u7684\u76f4\u63a5\u53c2\u6570\u63a7\u5236\uff0c\u4e3a\u690d\u7269\u79d1\u5b66\u6c11\u4e3b\u5316\u4e86\u590d\u6742\u7684\u51e0\u4f55\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u5b66\u4e25\u8c28\u6027\u3002"}}
{"id": "2512.12021", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12021", "abs": "https://arxiv.org/abs/2512.12021", "authors": ["Xincheng Cao", "Haochong Chen", "Bilin Aksun-Guvenc", "Levent Guvenc"], "title": "Modified Hybrid A* Collision-Free Path-Planning for Automated Reverse Parking", "comment": null, "summary": "Parking a vehicle in tight spaces is a challenging task to perform due to the scarcity of feasible paths that are also collision-free. This paper presents a strategy to tackle this kind of maneuver with a modified Hybrid-A* path-planning algorithm that combines the feasibility guarantee inherent in the standard Hybrid A* algorithm with the addition of static obstacle collision avoidance. A kinematic single-track model is derived to describe the low-speed motion of the vehicle, which is subsequently used as the motion model in the Hybrid A* path-planning algorithm to generate feasible motion primitive branches. The model states are also used to reconstruct the vehicle centerline, which, in conjunction with an inflated binary occupancy map, facilitates static obstacle collision avoidance functions. Simulation study and animation are set up to test the efficacy of the approach, and the proposed algorithm proves to consistently provide kinematically feasible trajectories that are also collision-free.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u7684Hybrid-A*\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8f66\u8f86\u6cca\u8f66\uff0c\u4fdd\u8bc1\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u9759\u6001\u969c\u788d\u7269\u907f\u78b0", "motivation": "\u5728\u72ed\u7a84\u7a7a\u95f4\u6cca\u8f66\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u65e2\u9700\u8981\u53ef\u884c\u7684\u8def\u5f84\uff0c\u53c8\u8981\u907f\u514d\u78b0\u649e\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u78b0\u649e\u907f\u514d", "method": "1. \u63a8\u5bfc\u8fd0\u52a8\u5b66\u5355\u8f68\u6a21\u578b\u63cf\u8ff0\u8f66\u8f86\u4f4e\u901f\u8fd0\u52a8\uff1b2. \u6539\u8fdbHybrid-A*\u7b97\u6cd5\uff0c\u7ed3\u5408\u6807\u51c6\u7b97\u6cd5\u7684\u53ef\u884c\u6027\u4fdd\u8bc1\u548c\u9759\u6001\u969c\u788d\u7269\u907f\u78b0\uff1b3. \u4f7f\u7528\u6a21\u578b\u72b6\u6001\u91cd\u5efa\u8f66\u8f86\u4e2d\u5fc3\u7ebf\uff0c\u7ed3\u5408\u81a8\u80c0\u4e8c\u503c\u5360\u636e\u5730\u56fe\u5b9e\u73b0\u78b0\u649e\u907f\u514d", "result": "\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u548c\u52a8\u753b\u6d4b\u8bd5\uff0c\u8bc1\u660e\u6240\u63d0\u7b97\u6cd5\u80fd\u591f\u6301\u7eed\u63d0\u4f9b\u8fd0\u52a8\u5b66\u53ef\u884c\u4e14\u65e0\u78b0\u649e\u7684\u8f68\u8ff9", "conclusion": "\u6539\u8fdb\u7684Hybrid-A*\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u72ed\u7a84\u7a7a\u95f4\u6cca\u8f66\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u8def\u5f84\u7684\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u78b0\u649e\u5b89\u5168\u6027"}}
{"id": "2512.12058", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12058", "abs": "https://arxiv.org/abs/2512.12058", "authors": ["Anja Sheppard", "Chris Reale", "Katherine A. Skinner"], "title": "A Stochastic Approach to Terrain Maps for Safe Lunar Landing", "comment": "Accepted to IEEE Aerospace 2026", "summary": "Safely landing on the lunar surface is a challenging task, especially in the heavily-shadowed South Pole region where traditional vision-based hazard detection methods are not reliable. The potential existence of valuable resources at the lunar South Pole has made landing in that region a high priority for many space agencies and commercial companies. However, relying on a LiDAR for hazard detection during descent is risky, as this technology is fairly untested in the lunar environment.\n  There exists a rich log of lunar surface data from the Lunar Reconnaissance Orbiter (LRO), which could be used to create informative prior maps of the surface before descent. In this work, we propose a method for generating stochastic elevation maps from LRO data using Gaussian processes (GPs), which are a powerful Bayesian framework for non-parametric modeling that produce accompanying uncertainty estimates. In high-risk environments such as autonomous spaceflight, interpretable estimates of terrain uncertainty are critical. However, no previous approaches to stochastic elevation mapping have taken LRO Digital Elevation Model (DEM) confidence maps into account, despite this data containing key information about the quality of the DEM in different areas.\n  To address this gap, we introduce a two-stage GP model in which a secondary GP learns spatially varying noise characteristics from DEM confidence data. This heteroscedastic information is then used to inform the noise parameters for the primary GP, which models the lunar terrain. Additionally, we use stochastic variational GPs to enable scalable training. By leveraging GPs, we are able to more accurately model the impact of heteroscedastic sensor noise on the resulting elevation map. As a result, our method produces more informative terrain uncertainty, which can be used for downstream tasks such as hazard detection and safe landing site selection.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u4e24\u9636\u6bb5\u6a21\u578b\uff0c\u5229\u7528LRO\u6570\u636e\u751f\u6210\u6708\u7403\u5357\u6781\u5730\u533a\u7684\u968f\u673a\u9ad8\u7a0b\u5730\u56fe\uff0c\u8003\u8651DEM\u7f6e\u4fe1\u5ea6\u56fe\u7684\u5f02\u65b9\u5dee\u566a\u58f0\u7279\u6027\uff0c\u4e3a\u5b89\u5168\u7740\u9646\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5730\u5f62\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u6708\u7403\u5357\u6781\u5730\u533a\u5b58\u5728\u9634\u5f71\u533a\u57df\uff0c\u4f20\u7edf\u89c6\u89c9\u5371\u9669\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u53ef\u9760\uff0c\u800cLiDAR\u6280\u672f\u5728\u8be5\u73af\u5883\u672a\u7ecf\u5145\u5206\u6d4b\u8bd5\u3002LRO\u6570\u636e\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u5730\u8868\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u968f\u673a\u9ad8\u7a0b\u5730\u56fe\u65b9\u6cd5\u672a\u8003\u8651DEM\u7f6e\u4fe1\u5ea6\u56fe\u5305\u542b\u7684\u5173\u952e\u8d28\u91cf\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528\u6b21\u7ea7GP\u4eceDEM\u7f6e\u4fe1\u5ea6\u6570\u636e\u5b66\u4e60\u7a7a\u95f4\u53d8\u5316\u7684\u566a\u58f0\u7279\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7528\u4e3bGP\u5efa\u6a21\u6708\u7403\u5730\u5f62\uff0c\u5e76\u5c06\u5f02\u65b9\u5dee\u566a\u58f0\u4fe1\u606f\u4f5c\u4e3a\u566a\u58f0\u53c2\u6570\u3002\u4f7f\u7528\u968f\u673a\u53d8\u5206GP\u5b9e\u73b0\u53ef\u6269\u5c55\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u5f02\u65b9\u5dee\u4f20\u611f\u5668\u566a\u58f0\u5bf9\u9ad8\u7a0b\u5730\u56fe\u7684\u5f71\u54cd\uff0c\u4ea7\u751f\u66f4\u5177\u4fe1\u606f\u91cf\u7684\u5730\u5f62\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u53ef\u7528\u4e8e\u5371\u9669\u68c0\u6d4b\u548c\u5b89\u5168\u7740\u9646\u70b9\u9009\u62e9\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651DEM\u7f6e\u4fe1\u5ea6\u56fe\u7684\u5f02\u65b9\u5dee\u566a\u58f0\u7279\u6027\uff0c\u63d0\u51fa\u7684\u4e24\u9636\u6bb5GP\u6a21\u578b\u80fd\u751f\u6210\u66f4\u53ef\u9760\u7684\u5730\u5f62\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e3a\u6708\u7403\u5357\u6781\u9ad8\u98ce\u9669\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u822a\u5929\u4efb\u52a1\u63d0\u4f9b\u5173\u952e\u7684\u5b89\u5168\u7740\u9646\u652f\u6301\u3002"}}
{"id": "2512.12630", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12630", "abs": "https://arxiv.org/abs/2512.12630", "authors": ["Yuqian Sun", "Xingyu Li", "Shunyu Yao", "Noura Howell", "Tristan Braud", "Chang Hee Lee", "Ali Asadipour"], "title": "ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists", "comment": null, "summary": "Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.", "AI": {"tldr": "ORIBA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u804a\u5929\u673a\u5668\u4eba\uff0c\u5e2e\u52a9\u89c6\u89c9\u827a\u672f\u5bb6\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u6765\u5f00\u53d1\u539f\u521b\u89d2\u8272\u6982\u5ff5\uff0c\u540c\u65f6\u5c06\u89c6\u89c9\u521b\u4f5c\u6743\u4fdd\u7559\u7ed9\u827a\u672f\u5bb6\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u521b\u610f\u652f\u6301\u65b9\u9762\u5e26\u6765\u65b0\u673a\u9047\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u89c6\u89c9\u827a\u672f\u5bb6\u793e\u533a\u7684\u4f26\u7406\u62c5\u5fe7\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5728\u5c0a\u91cd\u827a\u672f\u5bb6\u521b\u4f5c\u81ea\u4e3b\u6743\u7684\u524d\u63d0\u4e0b\uff0c\u5229\u7528GAI\u5e2e\u52a9\u89c6\u89c9\u827a\u672f\u5bb6\u5f00\u53d1\u539f\u521b\u89d2\u8272\u3002", "method": "\u63d0\u51fa\u4e86ORIBA\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u804a\u5929\u673a\u5668\u4eba\uff0c\u8ba9\u827a\u672f\u5bb6\u80fd\u591f\u4e0e\u4ed6\u4eec\u7684\u539f\u521b\u89d2\u8272\u8fdb\u884c\u89d2\u8272\u626e\u6f14\uff0c\u4e13\u6ce8\u4e8e\u6982\u5ff5\u5316\uff08\u5982\u80cc\u666f\u6545\u4e8b\uff09\uff0c\u800c\u5c06\u89c6\u89c9\u521b\u4f5c\u7559\u7ed9\u521b\u4f5c\u8005\u3002\u901a\u8fc714\u4f4d\u827a\u672f\u5bb6\u7684\u7814\u7a76\u6765\u8bc4\u4f30\u7cfb\u7edf\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0ORIBA\u6fc0\u53d1\u4e86\u827a\u672f\u5bb6\u7684\u60f3\u8c61\u529b\u53c2\u4e0e\uff0c\u5e2e\u52a9\u4ed6\u4eec\u53d1\u5c55\u539f\u521b\u89d2\u8272\u7684\u591a\u7ef4\u5c5e\u6027\u548c\u66f4\u5f3a\u7684\u60c5\u611f\u8054\u7cfb\uff0c\u4ece\u800c\u542f\u53d1\u4e86\u521b\u4f5c\u8fc7\u7a0b\u3002\u7cfb\u7edf\u5c55\u793a\u4e86LLM\u5982\u4f55\u652f\u6301\u8de8\u6a21\u6001\u521b\u610f\uff0c\u540c\u65f6\u4fdd\u6301\u827a\u672f\u5bb6\u7684\u521b\u4f5c\u81ea\u4e3b\u6743\u3002", "conclusion": "GAI\u6709\u6f5c\u529b\u4f5c\u4e3a\u4e2d\u7acb\u3001\u975e\u89c6\u89c9\u7684\u652f\u6301\u5de5\u5177\uff0c\u5728\u4e0d\u4fb5\u72af\u827a\u672f\u8868\u73b0\u6743\u7684\u60c5\u51b5\u4e0b\u52a0\u5f3a\u73b0\u6709\u521b\u4f5c\u5b9e\u8df5\u3002\u672c\u6587\u4e3a\u4ece\u827a\u672f\u5bb6\u89c6\u89d2\u8bbe\u8ba1AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5c55\u793a\u4e86LLM\u5728\u652f\u6301\u8de8\u6a21\u6001\u521b\u610f\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.11928", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11928", "abs": "https://arxiv.org/abs/2512.11928", "authors": ["Alexander Peysakhovich", "William Berman", "Joseph Rufo", "Felix Wong", "Maxwell Z. Wilson"], "title": "MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion", "comment": null, "summary": "Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86MONET\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ece\u660e\u573a\u56fe\u50cf\u9884\u6d4b\u7ec6\u80de\u67d3\u8272\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ec6\u80de\u67d3\u8272\u6280\u672f\u52b3\u52a8\u5bc6\u96c6\u4e14\u65e0\u6cd5\u7814\u7a76\u7ec6\u80de\u52a8\u6001\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7ec6\u80de\u67d3\u8272\u6280\u672f\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a1) \u52b3\u52a8\u5bc6\u96c6\u578b\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u64cd\u4f5c\uff1b2) \u9700\u8981\u5316\u5b66\u56fa\u5b9a\uff0c\u65e0\u6cd5\u7814\u7a76\u7ec6\u80de\u52a8\u6001\u53d8\u5316\u3002\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u901a\u8fc7AI\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aMONET\u7684\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ece\u660e\u573a\u56fe\u50cf\u9884\u6d4b\u7ec6\u80de\u67d3\u8272\u901a\u9053\u3002\u6a21\u578b\u91c7\u7528\u4e00\u81f4\u6027\u67b6\u6784\uff0c\u80fd\u591f\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u89c6\u9891\uff0c\u5e76\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\u4ee5\u9002\u5e94\u4e0d\u540c\u7ec6\u80de\u7cfb\u548c\u6210\u50cf\u534f\u8bae\u3002", "result": "\u6a21\u578b\u8d28\u91cf\u968f\u89c4\u6a21\u6269\u5927\u800c\u63d0\u5347\uff1b\u4e00\u81f4\u6027\u67b6\u6784\u80fd\u591f\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u89c6\u9891\uff08\u5c3d\u7ba1\u6ca1\u6709\u89c6\u9891\u8bad\u7ec3\u6570\u636e\uff09\uff1b\u6a21\u578b\u5177\u5907\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u53ef\u90e8\u5206\u9002\u5e94\u5206\u5e03\u5916\u7684\u7ec6\u80de\u7cfb\u548c\u6210\u50cf\u534f\u8bae\u3002", "conclusion": "\u865a\u62df\u7ec6\u80de\u67d3\u8272\u4e0d\u662f\u8981\u5b8c\u5168\u53d6\u4ee3\u7269\u7406\u7ec6\u80de\u67d3\u8272\uff0c\u800c\u662f\u4f5c\u4e3a\u8865\u5145\u5de5\u5177\uff0c\u4e3a\u751f\u7269\u5b66\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u80fd\u591f\u7814\u7a76\u7ec6\u80de\u52a8\u6001\u53d8\u5316\u3002"}}
{"id": "2512.12194", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12194", "abs": "https://arxiv.org/abs/2512.12194", "authors": ["Min-Won Seo", "Aamodh Suresh", "Carlos Nieto-Granda", "Solmaz S. Kia"], "title": "B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping", "comment": "18 pages, 17 figures", "summary": "Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.", "AI": {"tldr": "B-ActiveSEAL\uff1a\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4fe1\u606f\u8bba\u4e3b\u52a8\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u71b5\u5e73\u8861\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u8026\u5408\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u63a2\u7d22-\u5229\u7528\u6743\u8861", "motivation": "\u4f20\u7edf\u4e3b\u52a8\u673a\u5668\u4eba\u63a2\u7d22\u5728\u957f\u671f\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u5904\u7406\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u8026\u5408\u4e0d\u786e\u5b9a\u6027\u65f6\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u7206\u70b8\u95ee\u9898\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u51b3\u7b56\u6846\u67b6\u6765\u7ba1\u7406\u8fd9\u4e9b\u76f8\u4e92\u4f9d\u8d56\u7684\u4e0d\u786e\u5b9a\u6027", "method": "\u63d0\u51faB-ActiveSEAL\u6846\u67b6\uff1a1\uff09\u81ea\u9002\u5e94\u5e73\u8861\u5efa\u56fe\u4e0d\u786e\u5b9a\u6027\uff08\u63a2\u7d22\uff09\u548c\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\uff08\u5229\u7528\uff09\uff1b2\uff09\u652f\u6301\u5e7f\u4e49\u71b5\u5ea6\u91cf\u7c7b\uff1b3\uff09\u5efa\u7acb\u884c\u4e3a\u71b5\u4f5c\u4e3a\u8026\u5408\u4e0d\u786e\u5b9a\u6027\u4e0b\u4e3b\u52a8\u63a2\u7d22\u7684\u6709\u6548\u4fe1\u606f\u5ea6\u91cf", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5728\u5f00\u6e90\u5730\u56fe\u53caROS-Unity\u4eff\u771f\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cB-ActiveSEAL\u5728\u591a\u6837\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u4ea7\u751f\u591a\u6837\u5316\u81ea\u9002\u5e94\u63a2\u7d22\u884c\u4e3a\uff0c\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "B-ActiveSEAL\u4e3a\u7d27\u5bc6\u8026\u5408\u7684\u5b9a\u4f4d-\u5efa\u56fe\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u4e3b\u52a8\u63a2\u7d22\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u71b5\u5b9e\u73b0\u76f4\u89c2\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u95ee\u9898"}}
{"id": "2512.12773", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12773", "abs": "https://arxiv.org/abs/2512.12773", "authors": ["Reeteesha Roy"], "title": "Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles", "comment": "8 pages, 4 figures", "summary": "With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u4eba\u673a\u4ea4\u4e92\u539f\u5219\u6307\u5bfc\u4e0b\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u754c\u9762\u8bbe\u8ba1\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7528\u6237\u4f53\u9a8c\u3001\u5b89\u5168\u6027\u548c\u7528\u6237\u4fe1\u4efb\u5ea6\u7684\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u73b0\u4ee3\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u53d1\u5c55\u548c\u96c6\u6210\uff0c\u5b9a\u5236\u7528\u6237\u754c\u9762\u4ee5\u4f18\u5316\u6574\u4f53\u7528\u6237\u4f53\u9a8c\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u7406\u89e3\u7528\u6237\u9700\u6c42\u548c\u504f\u597d\u5bf9\u4e8e\u8fd9\u4e9b\u6280\u672f\u88ab\u63a5\u53d7\u548c\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528HCI\u539f\u5219\u5b9e\u73b0\u754c\u9762\u4e2a\u6027\u5316\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u8bbe\u8ba1\u3001\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u7528\u6237\u53cd\u9988\u673a\u5236\u7b49\u7b56\u7565\uff0c\u5f3a\u8c03\u754c\u9762\u8bbe\u8ba1\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u7528\u6237\u63a7\u5236\u3002", "result": "\u4e2a\u6027\u5316\u754c\u9762\u80fd\u663e\u8457\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u6ee1\u610f\u5ea6\uff0c\u901a\u8fc7\u8ba9\u7528\u6237\u63a7\u5236\u4f53\u9a8c\u53ef\u4ee5\u57f9\u517b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u4fe1\u4efb\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6c34\u5e73\u7684\u5b89\u5168\u6027\u548c\u5b89\u5168\u6027\u6807\u51c6\u3002", "conclusion": "HCI\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u53d1\u5c55\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u9700\u8981\u6ee1\u8db3\u591a\u6837\u5316\u7528\u6237\u9700\u6c42\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u548c\u5b89\u5168\u6027\u6807\u51c6\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u754c\u9762\u8bbe\u8ba1\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u4efb\u3002"}}
{"id": "2512.11939", "categories": ["cs.CV", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11939", "abs": "https://arxiv.org/abs/2512.11939", "authors": ["Cl\u00e9ment Fernandes", "Wojciech Pieczynski"], "title": "Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains", "comment": null, "summary": "Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684HEMC-CPS\u6a21\u578b\uff0c\u5c06\u4e0a\u4e0b\u6587Peano\u626b\u63cf\u4e0e\u8bc1\u636e\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\u7ed3\u5408\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u56fe\u50cf\u5206\u5272\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u66f4\u597d\u4e14\u8ba1\u7b97\u66f4\u5feb\u3002", "motivation": "\u4f20\u7edf\u7684Peano\u626b\u63cf\u5c06\u4e8c\u7ef4\u56fe\u50cf\u50cf\u7d20\u8f6c\u6362\u4e3a\u4e00\u7ef4\u5e8f\u5217\uff0c\u4f7f\u5f97\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\u53ef\u7528\u4e8e\u56fe\u50cf\u5206\u5272\u3002\u867d\u7136\u57fa\u4e8eHMC\u7684\u65b9\u6cd5\u6bd4\u9690\u9a6c\u5c14\u53ef\u592b\u573a\u65b9\u6cd5\u66f4\u5feb\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u4e0a\u4e0b\u6587Peano\u626b\u63cf\u548c\u8bc1\u636e\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\u5206\u522b\u88ab\u8bc1\u660e\u80fd\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u56e0\u6b64\u7814\u7a76\u5c06\u4e24\u8005\u7ed3\u5408\u7684\u65b0\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86HEMC-CPS\u6a21\u578b\uff0c\u540c\u65f6\u8003\u8651\u4e0a\u4e0b\u6587Peano\u626b\u63cf\u548c\u8bc1\u636e\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\u3002\u91c7\u7528\u65e0\u76d1\u7763\u5206\u5272\u65b9\u5f0f\uff0c\u4f7f\u7528\u968f\u673a\u671f\u671b\u6700\u5927\u5316\u65b9\u6cd5\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u6700\u5927\u540e\u9a8c\u6982\u7387\u6a21\u5f0f\u8fdb\u884c\u5206\u5272\u3002", "result": "\u5728\u5408\u6210\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u4e86HEMC-CPS\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u65b0\u6a21\u578b\u5728\u8d1d\u53f6\u65afMPM\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "HEMC-CPS\u6a21\u578b\u4e3a\u590d\u6742\u56fe\u50cf\uff08\u5982\u4e09\u7ef4\u6216\u591a\u4f20\u611f\u5668\u591a\u5206\u8fa8\u7387\u56fe\u50cf\uff09\u7684\u5efa\u6a21\u548c\u5206\u5272\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002\u8be5\u6a21\u578b\u4e0d\u4ec5\u9650\u4e8e\u56fe\u50cf\u5206\u5272\uff0c\u8fd8\u53ef\u7528\u4e8e\u4efb\u4f55\u7c7b\u578b\u7684\u7a7a\u95f4\u76f8\u5173\u6570\u636e\u5206\u6790\u3002"}}
{"id": "2512.12203", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12203", "abs": "https://arxiv.org/abs/2512.12203", "authors": ["Eric J. Elias", "Michael Esswein", "Jonathan P. How", "David W. Miller"], "title": "Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion", "comment": "18 pages, 11 figures. To be published in proceedings of AIAA SCITECH 2026 Forum", "summary": "As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u50cf\u7d20\u7ea7\u878d\u5408\u53ef\u89c1\u5149\u548c\u70ed\u7ea2\u5916\u56fe\u50cf\uff0c\u63d0\u5347\u5728\u8f68\u64cd\u4f5c\u4e2d\u672a\u77e5\u7a7a\u95f4\u76ee\u6807\u7684\u5bfc\u822a\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u5355\u4e00\u4f20\u611f\u5668\u663e\u8457\u6539\u5584SLAM\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5728\u8f68\u64cd\u4f5c\u9700\u6c42\u589e\u957f\uff0c\u9700\u8981\u7cbe\u786e\u5bfc\u822a\u672a\u77e5\u7a7a\u95f4\u76ee\u6807\u3002\u4f20\u7edf\u76f8\u673a\u5728\u9634\u5f71\u671f\u6027\u80fd\u53d7\u9650\uff0c\u6fc0\u5149\u96f7\u8fbe\u867d\u4e0d\u53d7\u5149\u7167\u5f71\u54cd\u4f46\u7b28\u91cd\u8017\u7535\uff0c\u70ed\u7ea2\u5916\u76f8\u673a\u80fd\u5728\u56f0\u96be\u5149\u7167\u6761\u4ef6\u4e0b\u5de5\u4f5c\u4f46\u5206\u8fa8\u7387\u8f83\u4f4e\u3002\u9700\u8981\u7ed3\u5408\u53ef\u89c1\u5149\u548c\u70ed\u7ea2\u5916\u76f8\u673a\u7684\u4f18\u52bf\u6765\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002", "method": "\u5728\u4f4e\u5730\u7403\u8f68\u9053\u4e0a\u5bf9\u76ee\u6807\u536b\u661f\u8fdb\u884c\u53ef\u89c1\u5149\u548c\u70ed\u7ea2\u5916\u6ce2\u6bb5\u7684\u5149\u771f\u5b9e\u611f\u6a21\u62df\uff0c\u4f7f\u7528\u50cf\u7d20\u7ea7\u878d\u5408\u65b9\u6cd5\u521b\u5efa\u53ef\u89c1\u5149/\u70ed\u7ea2\u5916\u590d\u5408\u56fe\u50cf\uff0c\u901a\u8fc7\u5355\u76eeSLAM\u7b97\u6cd5\u6bd4\u8f83\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u548c\u8f68\u8ff9\u4e0b\u7684\u5bfc\u822a\u8bef\u5dee\u3002", "result": "\u878d\u5408\u56fe\u50cf\u76f8\u6bd4\u4ec5\u4f7f\u7528\u53ef\u89c1\u5149\u6216\u4ec5\u4f7f\u7528\u70ed\u7ea2\u5916\u7684\u65b9\u6cd5\uff0c\u5bfc\u822a\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u53ef\u89c1\u5149\u4e0e\u70ed\u7ea2\u5916\u56fe\u50cf\u7684\u50cf\u7d20\u7ea7\u878d\u5408\u80fd\u591f\u6709\u6548\u7ed3\u5408\u4e24\u79cd\u4f20\u611f\u5668\u7684\u4f18\u52bf\uff0c\u4e3a\u5728\u8f68\u64cd\u4f5c\u4e2d\u7684\u672a\u77e5\u7a7a\u95f4\u76ee\u6807\u5bfc\u822a\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12817", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12817", "abs": "https://arxiv.org/abs/2512.12817", "authors": ["Mengqian Wu", "Jiayi Zhang", "Raymond Z. Zhang"], "title": "Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles", "comment": null, "summary": "Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22GPT-4\u5728\u82f1\u8bed\u8bed\u8a00\u827a\u672f\u8fa9\u8bba\u4e2d\u751f\u6210\u8bba\u636e\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u6bd4\u8f83AI\u4e0e\u4eba\u7c7b\u8fa9\u624b\u5236\u4f5c\u7684\u8bc1\u636e\u5361\u8d28\u91cf\uff0c\u57fa\u4e8e\u4e9a\u91cc\u58eb\u591a\u5fb7\u4fee\u8f9e\u539f\u5219\u5206\u6790\u4f18\u52a3\u3002", "motivation": "\u8fa9\u8bba\u662f\u57f9\u517b\u6279\u5224\u6027\u601d\u7ef4\u7684\u91cd\u8981\u7b56\u7565\uff0c\u4f46\u8fa9\u8bba\u8bad\u7ec3\u4f9d\u8d56\u4eba\u5de5\u6307\u5bfc\uff0c\u52b3\u52a8\u5bc6\u96c6\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u5728\u751f\u6210\u6709\u6548\u8fa9\u8bba\u8bba\u636e\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u5b66\u751f\u51c6\u5907\u8fa9\u8bba\u3002", "method": "\u4f7f\u7528GPT-4\u751f\u6210\u8bc1\u636e\u5361\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8fa9\u624b\u5236\u4f5c\u7684\u8bc1\u636e\u5361\u8fdb\u884c\u6bd4\u8f83\u3002\u8bc1\u636e\u5361\u5305\u542b\u6587\u5b66\u8bc1\u636e\u5f15\u7528\u3001\u6838\u5fc3\u601d\u60f3\u603b\u7ed3\u3001\u6717\u8bfb\u811a\u672c\u548c\u8bba\u70b9\u6807\u9898\u7b49\u7ec4\u4ef6\u3002\u91c7\u7528\u57fa\u4e8e\u4e9a\u91cc\u58eb\u591a\u5fb7\u4fee\u8f9e\u539f\u5219\uff08ethos\u3001pathos\u3001logos\uff09\u7684\u7cfb\u7edf\u6027\u8d28\u6027\u548c\u5b9a\u91cf\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4fee\u8f9e\u539f\u5219\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u4eba\u7c7b\u548cGPT\u5728\u8fa9\u8bba\u63a8\u7406\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\uff0c\u63ed\u793a\u4e86AI\u5173\u6ce8\u70b9\u548c\u8bba\u8bc1\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u5dee\u5f02\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aAI\u8f85\u52a9\u5b66\u4e60\u5e72\u9884\u7684\u6f14\u8fdb\u89d2\u8272\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e2e\u52a9\u5b66\u751f\u8fa9\u624b\u5236\u5b9a\u589e\u5f3a\u8bba\u8fa9\u548c\u63a8\u7406\u80fd\u529b\u7684\u7b56\u7565\uff0c\u4e3a\u8fa9\u8bba\u6559\u80b2\u4e2d\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2512.11941", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11941", "abs": "https://arxiv.org/abs/2512.11941", "authors": ["Jingmin Zhu", "Anqi Zhu", "James Bailey", "Jun Liu", "Hossein Rahmani", "Mohammed Bennamoun", "Farid Boussaid", "Qiuhong Ke"], "title": "DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition", "comment": null, "summary": "Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \\textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS", "AI": {"tldr": "DynaPURLS\u662f\u4e00\u4e2a\u7528\u4e8e\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u7acb\u591a\u5c3a\u5ea6\u89c6\u89c9-\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\u5e76\u5728\u63a8\u7406\u65f6\u52a8\u6001\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9aa8\u67b6\u7279\u5f81\u4e0e\u9759\u6001\u7c7b\u522b\u7ea7\u8bed\u4e49\u7684\u5bf9\u9f50\uff0c\u8fd9\u79cd\u7c97\u7c92\u5ea6\u5bf9\u9f50\u65e0\u6cd5\u5f25\u5408\u53ef\u89c1\u7c7b\u522b\u548c\u672a\u89c1\u7c7b\u522b\u4e4b\u95f4\u7684\u9886\u57df\u504f\u79fb\uff0c\u963b\u788d\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u77e5\u8bc6\u7684\u6709\u6548\u8fc1\u79fb\u3002", "method": "1. \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5305\u542b\u5168\u5c40\u8fd0\u52a8\u548c\u5c40\u90e8\u8eab\u4f53\u90e8\u4f4d\u52a8\u6001\u7684\u5206\u5c42\u6587\u672c\u63cf\u8ff0\uff1b2. \u81ea\u9002\u5e94\u5206\u533a\u6a21\u5757\u901a\u8fc7\u8bed\u4e49\u5206\u7ec4\u9aa8\u67b6\u5173\u8282\u4ea7\u751f\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8868\u793a\uff1b3. \u52a8\u6001\u4f18\u5316\u6a21\u5757\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u6295\u5f71\u5c06\u6587\u672c\u7279\u5f81\u9002\u914d\u5230\u8f93\u5165\u89c6\u89c9\u6d41\uff1b4. \u7f6e\u4fe1\u611f\u77e5\u7684\u7c7b\u522b\u5e73\u8861\u8bb0\u5fc6\u5e93\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\uff0c\u51cf\u5c11\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u9519\u8bef\u4f20\u64ad\u3002", "result": "\u5728NTU RGB+D 60/120\u548cPKU-MMD\u4e09\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDynaPURLS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u8bb0\u5f55\u3002", "conclusion": "DynaPURLS\u901a\u8fc7\u5efa\u7acb\u9c81\u68d2\u7684\u591a\u5c3a\u5ea6\u89c6\u89c9-\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\u5e76\u5728\u63a8\u7406\u65f6\u52a8\u6001\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u66f4\u597d\u6cdb\u5316\u3002"}}
{"id": "2512.12211", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12211", "abs": "https://arxiv.org/abs/2512.12211", "authors": ["Longchao Da", "David Isele", "Hua Wei", "Manish Saroya"], "title": "Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving", "comment": "9 Pages, 8 Figures", "summary": "Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u9002\u5e94\u8bc4\u4f30\u8f68\u8ff9\u9884\u6d4b\u5668\u6027\u80fd\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u4e24\u4e2a\u7ef4\u5ea6\u52a8\u6001\u7ed3\u5408\u6765\u8bc4\u4f30\u9884\u6d4b\u5668\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b9e\u9645\u9a7e\u9a76\u6027\u80fd\u7684\u8d21\u732e\u3002", "motivation": "\u5f53\u524d\u8f68\u8ff9\u9884\u6d4b\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u8bef\u5dee\u6307\u6807\uff08\u5982ADE\u3001FDE\uff09\uff0c\u8fd9\u4e9b\u6307\u6807\u53ea\u5173\u6ce8\u4e8b\u540e\u51c6\u786e\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u9884\u6d4b\u5668\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5f71\u54cd\u3002\u9ad8\u8d28\u91cf\u7684\u9884\u6d4b\u5668\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u6027\uff0c\u8fd8\u5e94\u6355\u6349\u90bb\u5c45\u8f66\u8f86\u6240\u6709\u53ef\u80fd\u7684\u79fb\u52a8\u65b9\u5411\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8c28\u614e\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u7ba1\u9053\uff0c\u901a\u8fc7\u4e24\u4e2a\u7ef4\u5ea6\u81ea\u9002\u5e94\u8bc4\u4f30\u9884\u6d4b\u5668\u6027\u80fd\uff1a\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002\u57fa\u4e8e\u9a7e\u9a76\u573a\u666f\u7684\u5173\u952e\u6027\uff0c\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u88ab\u52a8\u6001\u7ed3\u5408\uff0c\u4ea7\u751f\u9884\u6d4b\u5668\u6027\u80fd\u7684\u6700\u7ec8\u8bc4\u5206\u3002\u5728\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8bc4\u4f30\u7ba1\u9053\u6bd4\u4f20\u7edf\u6307\u6807\u4ea7\u751f\u66f4\u5408\u7406\u7684\u8bc4\u4f30\uff0c\u80fd\u66f4\u597d\u5730\u53cd\u6620\u9884\u6d4b\u5668\u8bc4\u4f30\u4e0e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9a7e\u9a76\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u8be5\u7ba1\u9053\u4e3a\u9009\u62e9\u6700\u80fd\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9a7e\u9a76\u6027\u80fd\u7684\u9884\u6d4b\u5668\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bc4\u4f30\u7ba1\u9053\u901a\u8fc7\u7efc\u5408\u8003\u8651\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u5e76\u6839\u636e\u573a\u666f\u5173\u952e\u6027\u52a8\u6001\u8c03\u6574\u6743\u91cd\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u8f68\u8ff9\u9884\u6d4b\u5668\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b9e\u9645\u9a7e\u9a76\u6027\u80fd\u7684\u8d21\u732e\uff0c\u4e3a\u9884\u6d4b\u5668\u9009\u62e9\u63d0\u4f9b\u4e86\u66f4\u5408\u7406\u7684\u6807\u51c6\u3002"}}
{"id": "2512.12891", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12891", "abs": "https://arxiv.org/abs/2512.12891", "authors": ["Mahsa Nasri", "Mahnoosh Jahanian", "Wei Wu", "Binyan Xu", "Casper Harteveld"], "title": "Tangible Intangibles: Exploring Embodied Emotion in Mixed Reality for Art Therapy", "comment": null, "summary": "This in-person studio explores how mixed reality (MR) and biometrics can make intangible emotional states tangible through embodied art practices. We begin with two well-established modalities, clay sculpting and free-form 2D drawing, to ground participants in somatic awareness and manual, reflective expression. Building on this baseline, we introduce an MR prototype that maps physiological signals (e.g., breath, heart rate variability, eye movement dynamics) to visual and spatial parameters (color saturation, pulsing, motion qualities), generating ''3D emotional artifacts.'' The full-day program balances theory (somatic psychology, embodied cognition, expressive biosignals), hands-on making, and comparative reflection to interrogate what analog and digital modalities respectively afford for awareness, expression, and meaning-making. Participants will (1) experience and compare analog and MR-based journaling of emotion; (2) prototype and critique mappings from biosignals to visual/spatial feedback; and (3) articulate design principles for trauma-informed, hybrid workflows that amplify interoceptive literacy without overwhelming the user. The expected contributions include a shared design vocabulary for biometric expressivity, a set of generative constraints for future TEI work on emotional archiving, and actionable insights into when automated translation supports or hinders embodied connection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6df7\u5408\u73b0\u5b9e\u548c\u751f\u7269\u8bc6\u522b\u6280\u672f\uff0c\u5c06\u65e0\u5f62\u7684\u60c5\u611f\u72b6\u6001\u8f6c\u5316\u4e3a\u6709\u5f62\u76843D\u60c5\u611f\u827a\u672f\u54c1\uff0c\u63a2\u7d22\u4e86\u4f20\u7edf\u827a\u672f\u5b9e\u8df5\u4e0e\u6570\u5b57\u6280\u672f\u7ed3\u5408\u5728\u60c5\u611f\u8868\u8fbe\u548c\u521b\u4f24\u7597\u6108\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6df7\u5408\u73b0\u5b9e\u548c\u751f\u7269\u8bc6\u522b\u6280\u672f\u4f7f\u65e0\u5f62\u7684\u60c5\u611f\u72b6\u6001\u53d8\u5f97\u6709\u5f62\uff0c\u4e3a\u60c5\u611f\u8868\u8fbe\u3001\u81ea\u6211\u610f\u8bc6\u548c\u521b\u4f24\u7597\u6108\u521b\u9020\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u5728\u589e\u5f3a\u5185\u611f\u53d7\u6027\u7d20\u517b\u65b9\u9762\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a1) \u4f20\u7edf\u7c98\u571f\u96d5\u5851\u548c2D\u7ed8\u753b\u5efa\u7acb\u8eab\u4f53\u610f\u8bc6\u548c\u8868\u8fbe\u57fa\u7840\uff1b2) \u5f00\u53d1MR\u539f\u578b\uff0c\u5c06\u751f\u7406\u4fe1\u53f7\uff08\u547c\u5438\u3001\u5fc3\u7387\u53d8\u5f02\u6027\u3001\u773c\u52a8\u52a8\u6001\uff09\u6620\u5c04\u5230\u89c6\u89c9\u548c\u7a7a\u95f4\u53c2\u6570\uff1b3) \u5168\u5929\u5de5\u4f5c\u574a\u7ed3\u5408\u7406\u8bba\uff08\u8eaf\u4f53\u5fc3\u7406\u5b66\u3001\u5177\u8eab\u8ba4\u77e5\u3001\u8868\u8fbe\u6027\u751f\u7269\u4fe1\u53f7\uff09\u3001\u5b9e\u8df5\u5236\u4f5c\u548c\u6bd4\u8f83\u53cd\u601d\u3002", "result": "\u53c2\u4e0e\u8005\u5c06\uff1a1) \u4f53\u9a8c\u548c\u6bd4\u8f83\u4f20\u7edf\u4e0eMR\u60c5\u611f\u65e5\u5fd7\u8bb0\u5f55\uff1b2) \u539f\u578b\u8bbe\u8ba1\u548c\u6279\u5224\u751f\u7269\u4fe1\u53f7\u5230\u89c6\u89c9/\u7a7a\u95f4\u53cd\u9988\u7684\u6620\u5c04\uff1b3) \u63d0\u51fa\u521b\u4f24\u77e5\u60c5\u6df7\u5408\u5de5\u4f5c\u6d41\u7a0b\u7684\u8bbe\u8ba1\u539f\u5219\u3002\u9884\u671f\u8d21\u732e\u5305\u62ec\u751f\u7269\u8bc6\u522b\u8868\u8fbe\u6027\u7684\u5171\u4eab\u8bbe\u8ba1\u8bcd\u6c47\u3001\u60c5\u611f\u5b58\u6863\u7684\u751f\u6210\u7ea6\u675f\u6846\u67b6\uff0c\u4ee5\u53ca\u81ea\u52a8\u5316\u7ffb\u8bd1\u5bf9\u5177\u8eab\u8fde\u63a5\u652f\u6301/\u963b\u788d\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u6df7\u5408\u73b0\u5b9e\u548c\u751f\u7269\u8bc6\u522b\u6280\u672f\u5982\u4f55\u4e0e\u4f20\u7edf\u827a\u672f\u5b9e\u8df5\u7ed3\u5408\uff0c\u4e3a\u60c5\u611f\u8868\u8fbe\u548c\u521b\u4f24\u7597\u6108\u521b\u9020\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5f3a\u8c03\u5728\u589e\u5f3a\u5185\u611f\u53d7\u6027\u7d20\u517b\u7684\u540c\u65f6\u907f\u514d\u7528\u6237\u8fc7\u5ea6\u8d1f\u8377\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u60c5\u611f\u5b58\u6863\u6280\u672f\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5bfc\u539f\u5219\u3002"}}
{"id": "2512.11977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11977", "abs": "https://arxiv.org/abs/2512.11977", "authors": ["Sushmita Nath"], "title": "A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer", "comment": "submit/7075585. 5 pages with 5 figures", "summary": "Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6570\u636e\u53d7\u9650\u6761\u4ef6\u4e0b\u4f7f\u7528DeiT\uff08\u6570\u636e\u9ad8\u6548\u56fe\u50cf\u53d8\u6362\u5668\uff09\u8fdb\u884c\u6676\u5706\u7f3a\u9677\u5206\u7c7b\uff0c\u76f8\u6bd4\u4f20\u7edfCNN\u6a21\u578b\uff08VGG-19\u3001Xception\u3001Squeeze-Net\uff09\u5728\u51c6\u786e\u7387\u3001F1\u5206\u6570\u548c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u9884\u6d4b\u6027\u7ef4\u62a4\u5728\u73b0\u4ee3\u5de5\u4e1a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u534a\u5bfc\u4f53\u5236\u9020\u8fd9\u79cd\u654f\u611f\u9886\u57df\u3002\u867d\u7136CNN\u6a21\u578b\u5728\u6676\u5706\u7f3a\u9677\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6570\u636e\u6709\u9650\u4e14\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u4f1a\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6570\u636e\u9ad8\u6548\u56fe\u50cf\u53d8\u6362\u5668\uff08DeiT\uff09\u6a21\u578b\u5bf9\u6676\u5706\u7f3a\u9677\u56fe\u8fdb\u884c\u5206\u7c7b\uff0c\u5728\u6570\u636e\u53d7\u9650\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4e0eVGG-19\u3001Xception\u3001Squeeze-Net\u7b49CNN\u6a21\u578b\u4ee5\u53ca\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "DeiT\u6a21\u578b\u53d6\u5f97\u4e8690.83%\u7684\u6700\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8eVGG-19\uff0865%\uff09\u3001SqueezeNet\uff0882%\uff09\u3001Xception\uff0866%\uff09\u548c\u6df7\u5408\u6a21\u578b\uff0867%\uff09\u3002\u540c\u65f6\uff0cDeiT\u7684F1\u5206\u6570\u8fbe\u523090.78%\uff0c\u8bad\u7ec3\u6536\u655b\u66f4\u5feb\uff0c\u5728\u68c0\u6d4b\u5c11\u6570\u7f3a\u9677\u7c7b\u522b\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DeiT\u7b49\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u6a21\u578b\u5728\u534a\u5bfc\u4f53\u6676\u5706\u7f3a\u9677\u68c0\u6d4b\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u652f\u6301\u534a\u5bfc\u4f53\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u53d7\u9650\u548c\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.12228", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12228", "abs": "https://arxiv.org/abs/2512.12228", "authors": ["Huichang Yun", "Seungho Yoo"], "title": "Semantic Zone based 3D Map Management for Mobile Robot", "comment": "12 pages, 11 figures", "summary": "Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u5206\u533a\u76843D\u5730\u56fe\u7ba1\u7406\u65b9\u6cd5\uff0c\u5c06\u73af\u5883\u5212\u5206\u4e3a\u6709\u610f\u4e49\u7684\u7a7a\u95f4\u5355\u5143\uff08\u5982\u5927\u5385\u3001\u8d70\u5eca\uff09\uff0c\u4f5c\u4e3a\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u672c\u5355\u4f4d\uff0c\u52a8\u6001\u52a0\u8f7d\u4efb\u52a1\u76f8\u5173\u533a\u57df\u5230\u5de5\u4f5c\u5185\u5b58\uff0c\u5378\u8f7d\u975e\u6d3b\u8dc3\u533a\u57df\u5230\u957f\u671f\u5185\u5b58\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u4e2d\u9700\u8981\u7cbe\u786e\u76843D\u7a7a\u95f4\u8868\u793a\uff0c\u4f463D\u5730\u56fe\u5360\u7528\u5927\u91cf\u5185\u5b58\uff0c\u96be\u4ee5\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e2d\u7ef4\u62a4\u5b8c\u6574\u5730\u56fe\u6570\u636e\u3002\u73b0\u6709SLAM\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u51e0\u4f55\u8ddd\u79bb\u6216\u65f6\u95f4\u6307\u6807\u8fdb\u884c\u5185\u5b58\u7ba1\u7406\uff0c\u5728\u7a7a\u95f4\u5206\u9694\u73af\u5883\u4e2d\u6570\u636e\u68c0\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u5206\u533a\u4e3a\u57fa\u7840\u76843D\u5730\u56fe\u7ba1\u7406\u65b9\u6cd5\uff0c\u5c06\u73af\u5883\u5212\u5206\u4e3a\u6709\u610f\u4e49\u7684\u7a7a\u95f4\u5355\u5143\uff08\u8bed\u4e49\u533a\u57df\uff09\uff0c\u5e76\u5c06\u8fd9\u4e9b\u533a\u57df\u4f5c\u4e3a\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u672c\u5355\u4f4d\u3002\u7cfb\u7edf\u52a8\u6001\u52a0\u8f7d\u4efb\u52a1\u76f8\u5173\u533a\u57df\u5230\u5de5\u4f5c\u5185\u5b58\uff0c\u5378\u8f7d\u975e\u6d3b\u8dc3\u533a\u57df\u5230\u957f\u671f\u5185\u5b58\uff0c\u4e25\u683c\u5f3a\u5236\u6267\u884c\u7528\u6237\u5b9a\u4e49\u7684\u5185\u5b58\u9608\u503c\u3002\u8be5\u65b9\u6cd5\u5728RTAB-Map\u6846\u67b6\u4e2d\u5b9e\u73b0\u3002", "result": "\u4e0e\u6807\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u7b7e\u540d\u52a0\u8f7d/\u5378\u8f7d\u5468\u671f\u548c\u7d2f\u79ef\u5185\u5b58\u4f7f\u7528\u3002\u8bed\u4e49\u5206\u533a\u7ba1\u7406\u786e\u4fdd\u4e86\u7a33\u5b9a\u3001\u53ef\u9884\u6d4b\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u5bfc\u822a\u6240\u9700\u7684\u5730\u56fe\u53ef\u7528\u6027\u3002", "conclusion": "\u8bed\u4e49\u5206\u533a\u7ba1\u7406\u65b9\u6cd5\u5c06\u5730\u56fe\u7ba1\u7406\u8303\u5f0f\u4ece\u51e0\u4f55\u4e2d\u5fc3\u8f6c\u5411\u8bed\u4e49\u4e2d\u5fc3\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u4e2d3D\u5730\u56fe\u5185\u5b58\u7ba1\u7406\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u5185\u5b58\u4f7f\u7528\u548c\u9ad8\u6548\u7684\u6570\u636e\u68c0\u7d22\u3002"}}
{"id": "2512.11988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11988", "abs": "https://arxiv.org/abs/2512.11988", "authors": ["Xianghui Xie", "Bowen Wen", "Yan Chang", "Hesam Rabeti", "Jiefeng Li", "Ye Yuan", "Gerard Pons-Moll", "Stan Birchfield"], "title": "CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction", "comment": "14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/", "summary": "Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.", "AI": {"tldr": "CARI4D\uff1a\u9996\u4e2a\u4ece\u5355\u76eeRGB\u89c6\u9891\u4e2d\u91cd\u5efa4D\u4eba-\u7269\u4ea4\u4e92\u7684\u7c7b\u522b\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u59ff\u6001\u5047\u8bbe\u9009\u62e9\u7b97\u6cd5\u548c\u6e32\u67d3-\u6bd4\u8f83\u8303\u5f0f\u5b9e\u73b0\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u50cf\u7d20\u5bf9\u9f50\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e36%", "motivation": "\u4ece\u5355\u76eeRGB\u89c6\u9891\u51c6\u786e\u6355\u6349\u4eba-\u7269\u4ea4\u4e92\u5bf9\u4e8e\u4eba\u7c7b\u7406\u89e3\u3001\u6e38\u620f\u548c\u673a\u5668\u4eba\u5b66\u4e60\u5e94\u7528\u5f88\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u672a\u77e5\u7684\u7269\u4f53\u548c\u4eba\u4f53\u4fe1\u606f\u3001\u6df1\u5ea6\u6a21\u7cca\u3001\u906e\u6321\u548c\u590d\u6742\u8fd0\u52a8\uff0c\u4ece\u5355\u89c6\u56fe\u63a8\u65ad4D\u4ea4\u4e92\u6781\u5177\u6311\u6218\u6027\u3002\u5148\u524d\u65b9\u6cd5\u901a\u8fc7\u5047\u8bbe\u771f\u5b9e\u7269\u4f53\u6a21\u677f\u6216\u9650\u5236\u5230\u6709\u9650\u7269\u4f53\u7c7b\u522b\u6765\u7b80\u5316\u8bbe\u7f6e\u3002", "method": "\u63d0\u51faCARI4D\u65b9\u6cd5\uff1a1\uff09\u59ff\u6001\u5047\u8bbe\u9009\u62e9\u7b97\u6cd5\uff0c\u9c81\u68d2\u5730\u6574\u5408\u57fa\u7840\u6a21\u578b\u7684\u4e2a\u4f53\u9884\u6d4b\uff1b2\uff09\u901a\u8fc7\u5b66\u4e60\u7684\u6e32\u67d3-\u6bd4\u8f83\u8303\u5f0f\u8054\u5408\u4f18\u5316\uff0c\u786e\u4fdd\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u50cf\u7d20\u5bf9\u9f50\uff1b3\uff09\u63a8\u7406\u590d\u6742\u63a5\u89e6\u70b9\u8fdb\u884c\u8fdb\u4e00\u6b65\u7ec6\u5316\uff0c\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u3002", "result": "\u5728\u5206\u5e03\u5185\u6570\u636e\u96c6\u4e0a\u91cd\u5efa\u8bef\u5dee\u6bd4\u5148\u524d\u65b9\u6cd5\u964d\u4f4e38%\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u964d\u4f4e36%\u3002\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u8bad\u7ec3\u7c7b\u522b\u4e4b\u5916\uff0c\u53ef\u4ee5\u96f6\u6837\u672c\u5e94\u7528\u4e8e\u91ce\u5916\u4e92\u8054\u7f51\u89c6\u9891\u3002", "conclusion": "CARI4D\u662f\u9996\u4e2a\u4ece\u5355\u76eeRGB\u89c6\u9891\u91cd\u5efa\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u76844D\u4eba-\u7269\u4ea4\u4e92\u7684\u7c7b\u522b\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u548c\u6e32\u67d3-\u6bd4\u8f83\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u8d85\u8d8a\u8bad\u7ec3\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.12230", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12230", "abs": "https://arxiv.org/abs/2512.12230", "authors": ["Jonathan Spraggett"], "title": "Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy", "comment": "Accepted at 28th RoboCup International Symposium", "summary": "Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u591f\u4f7f7\u79cd\u4e0d\u540c\u5f62\u6001\u7684\u4eba\u5f62\u673a\u5668\u4eba\u4ece\u8dcc\u5012\u72b6\u6001\u6062\u590d\uff0c\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u673a\u5668\u4eba\u5355\u72ec\u8bad\u7ec3", "motivation": "\u5728RoboCup\u7b49\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8dcc\u5012\u6062\u590d\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u79cd\u673a\u5668\u4eba\u5f62\u6001\u8bad\u7ec3\u5355\u72ec\u7684\u7b56\u7565\uff0c\u8fd9\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027", "method": "\u4f7f\u7528CrossQ\u8bad\u7ec3\u7edf\u4e00\u7684DRL\u7b56\u7565\uff0c\u6db5\u76d67\u79cd\u4e0d\u540c\u9ad8\u5ea6\u3001\u91cd\u91cf\u548c\u52a8\u529b\u5b66\u7279\u6027\u7684\u4eba\u5f62\u673a\u5668\u4eba\u3002\u901a\u8fc7\u7559\u4e00\u6cd5\u5b9e\u9a8c\u3001\u5f62\u6001\u7f29\u653e\u5206\u6790\u548c\u591a\u6837\u6027\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u65b9\u6cd5", "result": "\u7edf\u4e00\u7b56\u7565\u5728\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba\u5f62\u6001\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u6210\u529f\u7387\u9ad8\u8fbe86\u00b17%\uff0895%\u7f6e\u4fe1\u533a\u95f4[81,89]\uff09\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5171\u4eab\u7b56\u7565\u751a\u81f3\u8d85\u8d8a\u4e86\u4e13\u95e8\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u7814\u7a76\u8868\u660e\u6709\u9488\u5bf9\u6027\u7684\u5f62\u6001\u8986\u76d6\u53ef\u4ee5\u6539\u5584\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5f62\u6001\u65e0\u5173\u63a7\u5236\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u901a\u7528\u63a7\u5236\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2512.13253", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13253", "abs": "https://arxiv.org/abs/2512.13253", "authors": ["Julian Berger", "Jason W. Burton", "Ralph Hertwig", "Thomas Kosch", "Ralf H. J. M. Kurvers", "Benito Kurzenberger", "Christopher Lazik", "Linda Onnasch", "Tobias Rieger", "Anna I. Thoma", "Dirk U. Wulff", "Stefan M. Herzog"], "title": "Fostering human learning is crucial for boosting human-AI synergy", "comment": null, "summary": "The collaboration between humans and artificial intelligence (AI) holds the promise of achieving superior outcomes compared to either acting alone. Nevertheless, our understanding of the conditions that facilitate such human-AI synergy remains limited. A recent meta-analysis showed that, on average, human-AI combinations do not outperform the better individual agent, indicating overall negative human-AI synergy. We argue that this pessimistic conclusion arises from insufficient attention to human learning in the experimental designs used. To substantiate this claim, we re-analyzed all 74 studies included in the original meta-analysis, which yielded two new findings. First, most previous research overlooked design features that foster human learning, such as providing trial-by-trial outcome feedback to participants. Second, our re-analysis, using robust Bayesian meta-regressions, demonstrated that studies providing outcome feedback show relatively higher synergy than those without outcome feedback. Crucially, when feedback is paired with AI explanations we tend to find positive human-AI synergy, while AI explanations provided without feedback were strongly linked to negative synergy, indicating that explanations are useful for synergy only when humans can learn to verify the AI's reliability through feedback. We conclude that the current literature underestimates the potential for human-AI collaboration because it predominantly relies on experimental designs that do not facilitate human learning, thus hindering humans from effectively adapting their collaboration strategies. We therefore advocate for a paradigm shift in human-AI interaction research that explicitly incorporates and tests human learning mechanisms to enhance our understanding of and support for successful human-AI collaboration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u91cd\u65b0\u5206\u6790\u4e8674\u9879\u4eba\u673a\u534f\u4f5c\u7814\u7a76\uff0c\u53d1\u73b0\u73b0\u6709\u6587\u732e\u4f4e\u4f30\u4e86\u4eba\u673a\u534f\u4f5c\u6f5c\u529b\uff0c\u56e0\u4e3a\u5b9e\u9a8c\u8bbe\u8ba1\u7f3a\u4e4f\u4fc3\u8fdb\u4eba\u7c7b\u5b66\u4e60\u7684\u8981\u7d20\u3002\u7814\u7a76\u663e\u793a\uff0c\u63d0\u4f9b\u7ed3\u679c\u53cd\u9988\u80fd\u63d0\u9ad8\u4eba\u673a\u534f\u540c\u6548\u679c\uff0c\u800cAI\u89e3\u91ca\u53ea\u6709\u5728\u914d\u5408\u53cd\u9988\u65f6\u624d\u80fd\u4ea7\u751f\u6b63\u534f\u540c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u4eba\u673a\u7ec4\u5408\u5e73\u5747\u8868\u73b0\u5e76\u4e0d\u4f18\u4e8e\u5355\u72ec\u7684\u6700\u4f73\u4e2a\u4f53\uff0c\u5448\u73b0\u8d1f\u534f\u540c\u6548\u5e94\u3002\u7814\u7a76\u8005\u8ba4\u4e3a\u8fd9\u4e00\u60b2\u89c2\u7ed3\u8bba\u6e90\u4e8e\u5b9e\u9a8c\u8bbe\u8ba1\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u4eba\u673a\u534f\u4f5c\u7684\u771f\u6b63\u6f5c\u529b\u3002", "method": "\u91cd\u65b0\u5206\u6790\u4e86\u539f\u59cb\u5143\u5206\u6790\u4e2d\u5305\u542b\u768474\u9879\u7814\u7a76\uff0c\u4f7f\u7528\u7a33\u5065\u7684\u8d1d\u53f6\u65af\u5143\u56de\u5f52\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u5b9e\u9a8c\u8bbe\u8ba1\u7279\u5f81\uff08\u5982\u662f\u5426\u63d0\u4f9b\u9010\u6b21\u7ed3\u679c\u53cd\u9988\u3001AI\u89e3\u91ca\u7b49\uff09\u5bf9\u4eba\u673a\u534f\u540c\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "1. \u5927\u591a\u6570\u5148\u524d\u7814\u7a76\u7f3a\u4e4f\u4fc3\u8fdb\u4eba\u7c7b\u5b66\u4e60\u7684\u8bbe\u8ba1\u7279\u5f81\uff1b2. \u63d0\u4f9b\u7ed3\u679c\u53cd\u9988\u7684\u7814\u7a76\u663e\u793a\u51fa\u76f8\u5bf9\u66f4\u9ad8\u7684\u534f\u540c\u6548\u5e94\uff1b3. \u5f53\u53cd\u9988\u4e0eAI\u89e3\u91ca\u7ed3\u5408\u65f6\uff0c\u503e\u5411\u4e8e\u53d1\u73b0\u6b63\u4eba\u673a\u534f\u540c\uff0c\u800c\u4ec5\u6709AI\u89e3\u91ca\u65e0\u53cd\u9988\u5219\u4e0e\u8d1f\u534f\u540c\u5f3a\u76f8\u5173\u3002", "conclusion": "\u5f53\u524d\u6587\u732e\u4f4e\u4f30\u4e86\u4eba\u673a\u534f\u4f5c\u6f5c\u529b\uff0c\u56e0\u4e3a\u5b9e\u9a8c\u8bbe\u8ba1\u672a\u80fd\u4fc3\u8fdb\u4eba\u7c7b\u5b66\u4e60\u3002\u9700\u8981\u8303\u5f0f\u8f6c\u53d8\uff0c\u5728\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u4e2d\u660e\u786e\u7eb3\u5165\u548c\u6d4b\u8bd5\u4eba\u7c7b\u5b66\u4e60\u673a\u5236\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u652f\u6301\u6210\u529f\u7684\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2512.11995", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11995", "abs": "https://arxiv.org/abs/2512.11995", "authors": ["Chenrui Fan", "Yijun Liang", "Shweta Bhardwaj", "Kwesi Cobbina", "Ming Li", "Tianyi Zhou"], "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions", "comment": "28 pages", "summary": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.", "AI": {"tldr": "V-REX\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u6b65\u63a2\u7d22\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u6d4b\u5957\u4ef6\uff0c\u5305\u542b\u6311\u6218\u6027\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u5c06\u591a\u6b65\u63a2\u7d22\u5206\u89e3\u4e3a\u95ee\u9898\u94fe\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u591a\u8f6e\u63a2\u7d22\u548c\u63a8\u7406\u7684\u590d\u6742\u5f00\u653e\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8fd9\u7c7b\u4efb\u52a1\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u5e38\u89c1\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u6a21\u578b\u591a\u6b65\u63a2\u7d22\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1V-REX\u8bc4\u6d4b\u5957\u4ef6\uff0c\u5c06\u591a\u6b65\u63a2\u7d22\u63a8\u7406\u8f6c\u5316\u4e3a\u95ee\u9898\u94fe\uff08Chain-of-Questions\uff09\uff0c\u901a\u8fc7\u6709\u9650\u9009\u9879\u8bbe\u8ba1\u5b9e\u73b0\u53ef\u9760\u7684\u5b9a\u91cf\u5206\u6790\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\uff08\u5206\u89e3\u4efb\u52a1\u9009\u62e9\u95ee\u9898\u94fe\uff09\u548c\u6267\u884c\u80fd\u529b\uff08\u6309\u987a\u5e8f\u56de\u7b54\u95ee\u9898\u6536\u96c6\u4fe1\u606f\uff09\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u4e13\u6709\u548c\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u4e86\u6a21\u578b\u6027\u80fd\u7684\u6269\u5c55\u8d8b\u52bf\u3001\u89c4\u5212\u80fd\u529b\u548c\u6267\u884c\u80fd\u529b\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\uff0c\u4ee5\u53ca\u591a\u6b65\u63a2\u7d22\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "V-REX\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u6b65\u63a2\u7d22\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2512.12233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12233", "abs": "https://arxiv.org/abs/2512.12233", "authors": ["Murad Mehrab Abrar", "Trevor W. Harrison"], "title": "Robust Underwater Localization of Buoyancy Driven microFloats Using Acoustic Time-of-Flight Measurements", "comment": "9 pages", "summary": "Accurate underwater localization remains a challenge for inexpensive autonomous platforms that require highfrequency position updates. In this paper, we present a robust, low-cost localization pipeline for buoyancy-driven microFloats operating in coastal waters. We build upon previous work by introducing a bidirectional acoustic Time-of-Flight (ToF) localization framework, which incorporates both float-to-buoy and buoy-to-float transmissions, thereby increasing the number of usable measurements. The method integrates nonlinear trilateration with a filtering of computed position estimates based on geometric cost and Cramer-Rao Lower Bounds (CRLB). This approach removes outliers caused by multipath effects and other acoustic errors from the ToF estimation and improves localization robustness without relying on heavy smoothing. We validate the framework in two field deployments in Puget Sound, Washington, USA. The localization pipeline achieves median positioning errors below 4 m relative to GPS positions. The filtering technique shows a reduction in mean error from 139.29 m to 12.07 m, and improved alignment of trajectories with GPS paths. Additionally, we demonstrate a Time-Difference-of-Arrival (TDoA) localization for unrecovered floats that were transmitting during the experiment. Range-based acoustic localization techniques are widely used and generally agnostic to hardware-this work aims to maximize their utility by improving positioning frequency and robustness through careful algorithmic design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6cbf\u6d77\u6c34\u57df\u6d6e\u6807\u9a71\u52a8\u5fae\u578b\u6d6e\u6807\u7cfb\u7edf\u7684\u4f4e\u6210\u672c\u3001\u9c81\u68d2\u6027\u6c34\u4e0b\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u58f0\u5b66\u98de\u884c\u65f6\u95f4\u6d4b\u91cf\u6846\u67b6\u63d0\u9ad8\u6d4b\u91cf\u6570\u91cf\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u4e09\u8fb9\u6d4b\u91cf\u548c\u57fa\u4e8e\u51e0\u4f55\u6210\u672c\u4e0eCRLB\u7684\u6ee4\u6ce2\u6280\u672f\uff0c\u5728\u5b9e\u5730\u90e8\u7f72\u4e2d\u5b9e\u73b0\u4e86\u4f4e\u4e8e4\u7c73\u7684\u5b9a\u4f4d\u8bef\u5dee\u3002", "motivation": "\u5ec9\u4ef7\u81ea\u4e3b\u5e73\u53f0\u9700\u8981\u9ad8\u9891\u4f4d\u7f6e\u66f4\u65b0\uff0c\u4f46\u7cbe\u786e\u7684\u6c34\u4e0b\u5b9a\u4f4d\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6cbf\u6d77\u6c34\u57df\u4e2d\u9762\u4e34\u591a\u5f84\u6548\u5e94\u548c\u58f0\u5b66\u8bef\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u8981\u63d0\u9ad8\u5b9a\u4f4d\u9891\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u5411\u58f0\u5b66\u98de\u884c\u65f6\u95f4\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u6d6e\u6807\u5230\u6d6e\u6807\u548c\u6d6e\u6807\u5230\u6d6e\u6807\u7684\u53cc\u5411\u4f20\u8f93\u589e\u52a0\u53ef\u7528\u6d4b\u91cf\u6570\u91cf\u3002\u91c7\u7528\u975e\u7ebf\u6027\u4e09\u8fb9\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u51e0\u4f55\u6210\u672c\u548c\u514b\u62c9\u7f8e-\u7f57\u4e0b\u754c\u5bf9\u8ba1\u7b97\u7684\u4f4d\u7f6e\u4f30\u8ba1\u8fdb\u884c\u6ee4\u6ce2\uff0c\u4ee5\u6d88\u9664\u591a\u5f84\u6548\u5e94\u548c\u58f0\u5b66\u8bef\u5dee\u5f15\u8d77\u7684\u5f02\u5e38\u503c\u3002", "result": "\u5728\u534e\u76db\u987f\u666e\u5409\u7279\u6d77\u6e7e\u7684\u4e24\u6b21\u5b9e\u5730\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u3002\u5b9a\u4f4d\u7ba1\u9053\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8eGPS\u4f4d\u7f6e\u7684\u4e2d\u4f4d\u6570\u5b9a\u4f4d\u8bef\u5dee\u4f4e\u4e8e4\u7c73\u3002\u6ee4\u6ce2\u6280\u672f\u5c06\u5e73\u5747\u8bef\u5dee\u4ece139.29\u7c73\u964d\u4f4e\u523012.07\u7c73\uff0c\u5e76\u6539\u5584\u4e86\u8f68\u8ff9\u4e0eGPS\u8def\u5f84\u7684\u5bf9\u9f50\u3002\u8fd8\u5c55\u793a\u4e86\u672a\u56de\u6536\u6d6e\u6807\u7684\u5230\u8fbe\u65f6\u95f4\u5dee\u5b9a\u4f4d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7b97\u6cd5\u63d0\u9ad8\u4e86\u57fa\u4e8e\u8ddd\u79bb\u7684\u58f0\u5b66\u5b9a\u4f4d\u6280\u672f\u7684\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u6cbf\u6d77\u6c34\u57df\u4e2d\u7684\u4f4e\u6210\u672c\u81ea\u4e3b\u5e73\u53f0\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u91cd\u578b\u5e73\u6ed1\u6280\u672f\u3002"}}
{"id": "2512.12012", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12012", "abs": "https://arxiv.org/abs/2512.12012", "authors": ["Antonio Guillen-Perez"], "title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus", "comment": null, "summary": "The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.", "AI": {"tldr": "Semantic-Drive\u662f\u4e00\u4e2a\u672c\u5730\u4f18\u5148\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u89c6\u9891\u65e5\u5fd7\u4e2d\u6316\u6398\u7f55\u89c1\u7684\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\uff0c\u901a\u8fc7\u89e3\u8026\u611f\u77e5\u548c\u8ba4\u77e5\u5206\u6790\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u641c\u7d22\uff0c\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5f00\u53d1\u9762\u4e34\"\u957f\u5c3e\"\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u74f6\u9888\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u7cbe\u5ea6\u4e0d\u8db3\uff08\u7c97\u7c92\u5ea6\u5143\u6570\u636e\u641c\u7d22\uff09\uff0c\u8981\u4e48\u5b58\u5728\u9690\u79c1\u4fb5\u72af\u548c\u6210\u672c\u9ad8\u6602\u95ee\u9898\uff08\u57fa\u4e8e\u4e91\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u89e3\u8026\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u7b26\u53f7\u63a5\u5730\uff1a\u901a\u8fc7\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\uff08YOLOE\uff09\u951a\u5b9a\u6ce8\u610f\u529b\uff1b2\uff09\u8ba4\u77e5\u5206\u6790\uff1a\u901a\u8fc7\u63a8\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6cd5\u533b\u573a\u666f\u5206\u6790\u3002\u4f7f\u7528\"\u7cfb\u7edf2\"\u63a8\u7406\u65f6\u95f4\u5bf9\u9f50\u7b56\u7565\u548c\u591a\u6a21\u578b\"\u6cd5\u5b98-\u4fa6\u5bdf\u5458\"\u5171\u8bc6\u673a\u5236\u6765\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4Waymo Open Dataset\u5206\u7c7b\u6cd5\uff0cSemantic-Drive\u5b9e\u73b0\u4e860.966\u7684\u53ec\u56de\u7387\uff08CLIP\u4e3a0.475\uff09\uff0c\u98ce\u9669\u8bc4\u4f30\u9519\u8bef\u51cf\u5c11\u4e8640%\u3002\u7cfb\u7edf\u5b8c\u5168\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\uff08NVIDIA RTX 3090\uff09\u4e0a\u8fd0\u884c\u3002", "conclusion": "Semantic-Drive\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9690\u79c1\u4fdd\u62a4\u7684\u672c\u5730\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u4ece\u6d77\u91cf\u89c6\u9891\u65e5\u5fd7\u4e2d\u6316\u6398\u7f55\u89c1\u7684\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2512.12243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12243", "abs": "https://arxiv.org/abs/2512.12243", "authors": ["HT To", "S Nguyen", "NH Pham"], "title": "CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \\textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \\textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \\textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \\emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\\% and 50\\%) demonstrates a geometric mean speedup of 2.46$\\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\\% to 84.8\\% (+6.9 percentage points), reduce total runtime by 70.1\\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.", "AI": {"tldr": "CAR-CHASE\u901a\u8fc7\u51b2\u7a81\u611f\u77e5\u542f\u53d1\u5f0f\u7f13\u5b58\u548c\u81ea\u9002\u5e94\u6df7\u5408\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u8f66\u673a\u5668\u4eba\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6700\u4f18\u89e3\u7684\u540c\u65f6\u5b9e\u73b0\u4e862.46\u500d\u7684\u5e73\u5747\u52a0\u901f\u3002", "motivation": "\u7c7b\u8f66\u673a\u5668\u4eba\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u9762\u4e34\u8ba1\u7b97\u6311\u6218\uff0c\u4f20\u7edf\u542f\u53d1\u5f0f\u7f13\u5b58\u65b9\u6cd5\u5728CBS\u4e2d\u5931\u6548\uff0c\u56e0\u4e3a\u7ea6\u675f\u4f7f\u641c\u7d22\u7a7a\u95f4\u5177\u6709\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u542f\u53d1\u5f0f\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCAR-CHASE\u65b9\u6cd5\uff1a1\uff09\u51b2\u7a81\u611f\u77e5\u542f\u53d1\u5f0f\u7f13\u5b58\uff0c\u57fa\u4e8e\u72b6\u6001\u548c\u76f8\u5173\u7ea6\u675f\u4e0a\u4e0b\u6587\u7f13\u5b58\u542f\u53d1\u5f0f\u503c\uff1b2\uff09\u81ea\u9002\u5e94\u6df7\u5408\u542f\u53d1\u5f0f\uff0c\u667a\u80fd\u5207\u6362\u5feb\u901f\u8fd1\u4f3c\u548c\u7cbe\u786e\u8ba1\u7b97\uff1b3\uff09\u51b2\u7a81\u6307\u7eb9\u7f16\u7801\u7ea6\u675f\u5f71\u54cd\uff1b4\uff09\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u51e0\u4f55\u76f8\u5173\u6027\u8fc7\u6ee4\uff1b5\uff09\u5177\u6709\u7406\u8bba\u8d28\u91cf\u754c\u7684\u81ea\u9002\u5e94\u5207\u6362\u7b56\u7565\u3002", "result": "\u5728480\u4e2a\u57fa\u51c6\u5b9e\u4f8b\u4e0a\u6d4b\u8bd5\uff0c\u51e0\u4f55\u5e73\u5747\u52a0\u901f2.46\u500d\uff0c\u6210\u529f\u7387\u4ece77.9%\u63d0\u5347\u523084.8%\uff08+6.9\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u603b\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1170.1%\uff0c\u89e3\u51b3\u4e8633\u4e2a\u4e4b\u524d\u8d85\u65f6\u7684\u5b9e\u4f8b\uff0c\u572830\u667a\u80fd\u4f53\u969c\u788d\u573a\u666f\u4e2d\u6700\u9ad8\u8fbe\u52304.06\u500d\u52a0\u901f\u3002", "conclusion": "CAR-CHASE\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u8f66\u673a\u5668\u4ebaMAPF\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u901a\u8fc7\u51b2\u7a81\u611f\u77e5\u7f13\u5b58\u548c\u81ea\u9002\u5e94\u542f\u53d1\u5f0f\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6CBS\u53d8\u4f53\u3002"}}
{"id": "2512.12320", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12320", "abs": "https://arxiv.org/abs/2512.12320", "authors": ["Canqi Meng", "Weibang Bai"], "title": "Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy", "comment": null, "summary": "Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\\circ}$ (N=2), tilting of $18^{\\circ}$ (N=1), and twisting of $115^{\\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5728\u591a\u5b54\u6ce1\u6cab\u4f53\u4e0a\u5207\u5272\u7279\u5b9a\u56fe\u6848\u5b9e\u73b0\u53ef\u7f16\u7a0b\u53d8\u5f62\u7684\u8f6f\u591a\u5b54\u81f4\u52a8\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u771f\u7a7a\u9a71\u52a8\u5b9e\u73b0\u5f2f\u66f2\u3001\u503e\u659c\u548c\u626d\u8f6c\u7b49\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u8f6f\u6c14\u52a8\u81f4\u52a8\u5668\u901a\u5e38\u57fa\u4e8e\u4e2d\u7a7a\u5f39\u6027\u4f53\u8154\u5ba4\uff0c\u5b58\u5728\u7ed3\u6784\u652f\u6491\u5f31\u3001\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u529f\u80fd\u8fdb\u884c\u6602\u8d35\u51e0\u4f55\u91cd\u65b0\u8bbe\u8ba1\u7684\u95ee\u9898\u3002\u591a\u5b54\u6750\u6599\u5982\u6ce1\u6cab\u53ef\u4ee5\u63d0\u4f9b\u7ed3\u6784\u7a33\u5b9a\u6027\uff0c\u4f46\u5982\u4f55\u901a\u8fc7\u5b9a\u5236\u591a\u5b54\u4f53\u672c\u8eab\u5b9e\u73b0\u53ef\u7f16\u7a0b\u53d8\u5f62\u4ecd\u7f3a\u4e4f\u7814\u7a76\u3002", "method": "\u5728\u591a\u5b54\u6ce1\u6cab\u4f53\u4e0a\u5207\u5272\u7279\u5b9a\u56fe\u6848\uff08\u6a2a\u5411\u3001\u7eb5\u5411\u3001\u5bf9\u89d2\u7ebf\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u7ed3\u6784\u5404\u5411\u5f02\u6027\uff0c\u5728\u5168\u5c40\u771f\u7a7a\u8f93\u5165\u4e0b\u5f15\u5bfc\u6750\u6599\u53d8\u5f62\u3002\u5efa\u7acb\u6709\u9650\u5143\u5206\u6790\u8ba1\u7b97\u6a21\u578b\u7814\u7a76\u5207\u53e3\u56fe\u6848\u65b9\u6cd5\u7684\u673a\u7406\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e0d\u540c\u56fe\u6848\u9635\u5217\u6570\u91cfN\u7684\u6700\u4f18\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u56fe\u6848\u9635\u5217\u6570\u91cfN\uff0c\u81f4\u52a8\u5668\u53ef\u5b9e\u73b0\u5f2f\u66f2\u8fbe80\u00b0\uff08N=2\uff09\u3001\u503e\u659c18\u00b0\uff08N=1\uff09\u548c\u626d\u8f6c115\u00b0\uff08N=8\uff09\u3002\u65b9\u6cd5\u5177\u6709\u56fe\u6848\u53ef\u8f6c\u79fb\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u65e0\u9700\u6a21\u5177\u7684\u5feb\u901f\u539f\u578b\u5236\u4f5c\u80fd\u529b\u3002\u901a\u8fc7\u5c06\u4eba\u624b\u8936\u76b1\u56fe\u8f6c\u5316\u4e3a\u529f\u80fd\u5207\u53e3\u56fe\u6848\uff0c\u521b\u5efa\u4e86\u80fd\u591f\u8fdb\u884c\u7c7b\u4eba\u81ea\u9002\u5e94\u6293\u53d6\u7684\u4eff\u751f\u8f6f\u673a\u5668\u4eba\u624b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u591a\u529f\u80fd\u8f6f\u591a\u5b54\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u591a\u5b54\u6ce1\u6cab\u4f53\u4e0a\u5207\u5272\u56fe\u6848\u5b9e\u73b0\u53ef\u7f16\u7a0b\u53d8\u5f62\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u8f6f\u6c14\u52a8\u81f4\u52a8\u5668\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.13674", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13674", "abs": "https://arxiv.org/abs/2512.13674", "authors": ["Yiyi Cai", "Xuangeng Chu", "Xiwei Gao", "Sitong Gong", "Yifei Huang", "Caixin Kang", "Kunhang Li", "Haiyang Liu", "Ruicong Liu", "Yun Liu", "Dianwen Ng", "Zixiong Su", "Erwin Wu", "Yuhan Wu", "Dingkun Yan", "Tianyu Yan", "Chang Zeng", "Bo Zheng", "You Zhou"], "title": "Towards Interactive Intelligence for Digital Humans", "comment": null, "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.", "AI": {"tldr": "Mio\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6570\u5b57\u4eba\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e13\u95e8\u6a21\u5757\u5b9e\u73b0\u4e2a\u6027\u8868\u8fbe\u3001\u81ea\u9002\u5e94\u4ea4\u4e92\u548c\u81ea\u6211\u8fdb\u5316\u7684\u4ea4\u4e92\u667a\u80fd", "motivation": "\u63a8\u52a8\u6570\u5b57\u4eba\u4ece\u8868\u9762\u6a21\u4eff\u5411\u667a\u80fd\u4ea4\u4e92\u53d1\u5c55\uff0c\u5b9e\u73b0\u4e2a\u6027\u5bf9\u9f50\u8868\u8fbe\u3001\u81ea\u9002\u5e94\u4ea4\u4e92\u548c\u81ea\u6211\u8fdb\u5316\u7684\u4ea4\u4e92\u667a\u80fd", "method": "\u63d0\u51faMio\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u4e13\u95e8\u6a21\u5757\uff1aThinker\uff08\u601d\u8003\u8005\uff09\u3001Talker\uff08\u8bf4\u8bdd\u8005\uff09\u3001Face Animator\uff08\u9762\u90e8\u52a8\u753b\uff09\u3001Body Animator\uff08\u8eab\u4f53\u52a8\u753b\uff09\u548cRenderer\uff08\u6e32\u67d3\u5668\uff09\uff0c\u7edf\u4e00\u67b6\u6784\u6574\u5408\u8ba4\u77e5\u63a8\u7406\u4e0e\u5b9e\u65f6\u591a\u6a21\u6001\u4f53\u73b0", "result": "\u5728\u6240\u6709\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u4ea4\u4e92\u667a\u80fd\u8bc4\u4f30\u57fa\u51c6", "conclusion": "Mio\u6846\u67b6\u5c06\u6570\u5b57\u4eba\u4ece\u8868\u9762\u6a21\u4eff\u63a8\u5411\u667a\u80fd\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e86\u4ea4\u4e92\u667a\u80fd\u7684\u65b0\u8303\u5f0f"}}
{"id": "2512.12053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12053", "abs": "https://arxiv.org/abs/2512.12053", "authors": ["Tran-Vu La", "Minh-Tan Pham", "Yu Li", "Patrick Matgen", "Marco Chini"], "title": "Adaptive federated learning for ship detection across diverse satellite imagery sources", "comment": "5 pages, IGARSS 2025", "summary": "We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u536b\u661f\u8239\u8236\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u56db\u79cdFL\u6a21\u578b\uff08FedAvg\u3001FedProx\u3001FedOpt\u3001FedMedian\uff09\u4e0e\u672c\u5730\u8bad\u7ec3\u57fa\u7ebf\u5bf9\u6bd4\uff0c\u8bc1\u660eFL\u80fd\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u63a5\u8fd1\u4f7f\u7528\u5168\u90e8\u6570\u636e\u7684\u5168\u5c40\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u536b\u661f\u8239\u8236\u68c0\u6d4b\u4e2d\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u7279\u522b\u662f\u5546\u4e1a\u536b\u661f\u56fe\u50cf\u548c\u654f\u611f\u8239\u8236\u6807\u6ce8\u6570\u636e\u4e0d\u80fd\u5171\u4eab\u6216\u96c6\u4e2d\u6536\u96c6\u7684\u6311\u6218\uff0c\u540c\u65f6\u63d0\u5347\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528YOLOv8\u4f5c\u4e3a\u8239\u8236\u68c0\u6d4b\u6a21\u578b\uff0c\u8bc4\u4f30\u56db\u79cdFL\u7b97\u6cd5\uff08FedAvg\u3001FedProx\u3001FedOpt\u3001FedMedian\uff09\uff0c\u5e76\u4e0e\u4ec5\u5728\u5404\u6570\u636e\u96c6\u4e0a\u72ec\u7acb\u8bad\u7ec3\u7684\u672c\u5730\u57fa\u7ebf\u6a21\u578b\u5bf9\u6bd4\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u901a\u4fe1\u8f6e\u6570\u548c\u672c\u5730\u8bad\u7ec3\u8f6e\u6570\u7b49\u914d\u7f6e\u53c2\u6570\u7684\u5f71\u54cd\u3002", "result": "FL\u6a21\u578b\u76f8\u6bd4\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u672c\u5730\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u6027\u80fd\u63a5\u8fd1\u4f7f\u7528\u6240\u6709\u6570\u636e\u96c6\u7684\u5168\u5c40\u8bad\u7ec3\u3002\u540c\u65f6\u53d1\u73b0\u9009\u62e9\u5408\u9002\u7684FL\u914d\u7f6e\uff08\u5982\u901a\u4fe1\u8f6e\u6570\u548c\u672c\u5730\u8bad\u7ec3\u8f6e\u6570\uff09\u5bf9\u4f18\u5316\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e3a\u536b\u661f\u8239\u8236\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u63a5\u8fd1\u5168\u5c40\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u9009\u62e9FL\u914d\u7f6e\u53c2\u6570\u4ee5\u8fbe\u5230\u6700\u4f73\u6548\u679c\u3002"}}
{"id": "2512.12377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12377", "abs": "https://arxiv.org/abs/2512.12377", "authors": ["Haichuan Li", "Changda Tian", "Panos Trahanias", "Tomi Westerlund"], "title": "INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset", "comment": null, "summary": "We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.", "AI": {"tldr": "INDOOR-LIDAR\u662f\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u5ba4\u51853D LiDAR\u70b9\u4e91\u6df7\u5408\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e86\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754c\u626b\u63cf\u6570\u636e\uff0c\u65e8\u5728\u63a8\u52a8\u673a\u5668\u4eba\u611f\u77e5\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u5ba4\u5185LiDAR\u6570\u636e\u96c6\u5b58\u5728\u89c4\u6a21\u6709\u9650\u3001\u6807\u6ce8\u683c\u5f0f\u4e0d\u4e00\u81f4\u3001\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\u4eba\u4e3a\u5f15\u5165\u7684\u53d8\u5f02\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u4e00\u81f4\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6574\u5408\u6a21\u62df\u73af\u5883\u548c\u81ea\u4e3b\u5730\u9762\u673a\u5668\u4eba\u91c7\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u626b\u63cf\u6570\u636e\uff0c\u63d0\u4f9b\u4e00\u81f4\u7684\u8986\u76d6\u8303\u56f4\u548c\u53d7\u63a7\u53d8\u5316\u4e0b\u7684\u771f\u5b9e\u4f20\u611f\u5668\u884c\u4e3a\u3002\u6570\u636e\u96c6\u5305\u542b\u5bc6\u96c6\u70b9\u4e91\u6570\u636e\u3001\u5f3a\u5ea6\u6d4b\u91cf\u548cKITTI\u98ce\u683c\u6807\u6ce8\u3002", "result": "INDOOR-LIDAR\u6570\u636e\u96c6\u5305\u542b\u6a21\u62df\u5b50\u96c6\uff08\u53ef\u7075\u6d3b\u914d\u7f6e\u5e03\u5c40\u3001\u70b9\u5bc6\u5ea6\u548c\u906e\u6321\uff09\u548c\u771f\u5b9e\u4e16\u754c\u5b50\u96c6\uff08\u6355\u83b7\u771f\u5b9e\u4f20\u611f\u5668\u566a\u58f0\u3001\u6742\u4e71\u548c\u7279\u5b9a\u9886\u57df\u4f2a\u5f71\uff09\uff0c\u652f\u6301\u591a\u79cd\u5ba4\u5185\u7269\u4f53\u7c7b\u522b\u3002", "conclusion": "INDOOR-LIDAR\u901a\u8fc7\u5f25\u5408\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u611f\u77e5\u7814\u7a76\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u3001\u771f\u5b9e\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2512.12056", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12056", "abs": "https://arxiv.org/abs/2512.12056", "authors": ["Maria Rodriguez", "Minh-Tan Pham", "Martin Sudmanns", "Quentin Poterek", "Oscar Narvaez"], "title": "Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management", "comment": "5 pages, IGARSS 2025", "summary": "After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5de5\u4f5c\u6d41\uff0c\u65e8\u5728\u63d0\u5347\u706b\u707e\u540e\u70e7\u6bc1\u533a\u57df\uff08BA\uff09\u7684\u7ed8\u5236\u6027\u80fd\u548c\u6548\u7387\uff0c\u9488\u5bf9SPOT-6/7\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83U-Net\u548cSegFormer\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u4e86\u571f\u5730\u8986\u76d6\u6570\u636e\u8f85\u52a9\u4efb\u52a1\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u70e7\u6bc1\u533a\u57df\u7ed8\u5236\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5904\u7406\u707e\u540e\u9065\u611f\u5f71\u50cf\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u5176\u5728\u65f6\u95f4\u7d27\u8feb\u7684\u5e94\u6025\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u8bc1\u6027\u80fd\u53c8\u80fd\u63d0\u9ad8\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5de5\u4f5c\u6d41\uff0c\u9488\u5bf9SPOT-6/7\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\u3002\u6bd4\u8f83\u4e86U-Net\u548cSegFormer\u6a21\u578b\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u7684\u8868\u73b0\u3002\u5f15\u5165\u4e86\u571f\u5730\u8986\u76d6\u6570\u636e\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u6765\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002\u6d4b\u8bd5\u4e86\u6d4b\u8bd5\u65f6\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u7b49\u4f18\u5316\u65b9\u6cd5\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "result": "U-Net\u548cSegFormer\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46SegFormer\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u5e94\u6025\u573a\u666f\u4e2d\u5b9e\u7528\u6027\u53d7\u9650\u3002\u52a0\u5165\u571f\u5730\u8986\u76d6\u8f85\u52a9\u4efb\u52a1\u80fd\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u3002\u6d4b\u8bd5\u65f6\u589e\u5f3a\u80fd\u63d0\u5347\u7ed8\u5236\u6027\u80fd\u4f46\u4f1a\u589e\u52a0\u63a8\u7406\u65f6\u95f4\uff0c\u53ef\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u7b49\u4f18\u5316\u65b9\u6cd5\u7f13\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5e94\u6025\u7ba1\u7406\u573a\u666f\u4e0b\u7684\u70e7\u6bc1\u533a\u57df\u7ed8\u5236\u63d0\u4f9b\u4e86\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\uff0cU-Net\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u66f4\u5177\u5b9e\u7528\u6027\uff0c\u8f85\u52a9\u4efb\u52a1\u548c\u4f18\u5316\u6280\u672f\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2512.12427", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12427", "abs": "https://arxiv.org/abs/2512.12427", "authors": ["Rudolf Reiter", "Chao Qin", "Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models", "comment": null, "summary": "Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.", "AI": {"tldr": "Unique\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00MPC\u6846\u67b6\uff0c\u901a\u8fc7\u7ea7\u8054\u4e0d\u540c\u7cbe\u5ea6\u7684\u6a21\u578b\u5b9e\u73b0\u5373\u65f6\u53cd\u5e94\u6027\u548c\u957f\u65f6\u57df\u89c4\u5212\uff1a\u77ed\u65f6\u57df\u4f7f\u7528\u9ad8\u7cbe\u5ea6\u6a21\u578b\u8fdb\u884c\u7cbe\u786e\u63a7\u5236\uff0c\u957f\u65f6\u57df\u4f7f\u7528\u4f4e\u7cbe\u5ea6\u6a21\u578b\u8fdb\u884c\u89c4\u5212\u3002", "motivation": "\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4efb\u52a1\u9700\u8981\u540c\u65f6\u5177\u5907\u5373\u65f6\u53cd\u5e94\u6027\u548c\u957f\u65f6\u57df\u89c4\u5212\u80fd\u529b\u3002\u9ad8\u7cbe\u5ea6\u6a21\u578b\u63a7\u5236\u51c6\u786e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u5408\u957f\u65f6\u57df\u89c4\u5212\uff1b\u4f4e\u7cbe\u5ea6\u89c4\u5212\u5668\u53ef\u6269\u5c55\u4f46\u95ed\u73af\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u3002", "method": "1. \u5728\u5355\u4e00\u4f18\u5316\u4e2d\u7ea7\u8054\u4e0d\u540c\u7cbe\u5ea6\u6a21\u578b\uff1a\u77ed\u65f6\u57df\u9ad8\u7cbe\u5ea6\u6a21\u578b\u7528\u4e8e\u7cbe\u786e\u63a7\u5236\uff0c\u957f\u65f6\u57df\u4f4e\u7cbe\u5ea6\u6a21\u578b\u7528\u4e8e\u89c4\u5212\uff1b2. \u8de8\u65f6\u57df\u5bf9\u9f50\u6210\u672c\u51fd\u6570\uff1b3. \u4e3a\u70b9\u8d28\u91cf\u6a21\u578b\u63a8\u5bfc\u4fdd\u6301\u53ef\u884c\u6027\u7684\u63a8\u529b\u548c\u4f53\u901f\u7387\u7ea6\u675f\uff1b4. \u5f15\u5165\u5339\u914d\u4e0d\u540c\u72b6\u6001\u3001\u63a8\u529b\u8bf1\u5bfc\u52a0\u901f\u5ea6\u548c\u6025\u52a8-\u4f53\u901f\u7387\u5173\u7cfb\u7684\u8fc7\u6e21\u7ea6\u675f\uff1b5. \u63d0\u51fa3D\u6e10\u8fdb\u5e73\u6ed1\u8c03\u5ea6\u9632\u6b62\u975e\u5e73\u6ed1\u969c\u788d\u7269\u5bfc\u81f4\u7684\u5c40\u90e8\u6700\u5c0f\u503c\uff1b6. \u90e8\u7f72\u5e76\u884c\u968f\u673a\u521d\u59cb\u5316MPC\u6c42\u89e3\u5668\u5728\u957f\u65f6\u57df\u4f4e\u7cbe\u5ea6\u6a21\u578b\u4e0a\u53d1\u73b0\u66f4\u4f4e\u6210\u672c\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "result": "\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cUnique\u76f8\u6bd4\u6807\u51c6MPC\u548c\u5206\u5c42\u89c4\u5212\u5668-\u8ddf\u8e2a\u5668\u57fa\u7ebf\uff0c\u5c06\u95ed\u73af\u4f4d\u7f6e\u6216\u901f\u5ea6\u8ddf\u8e2a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe75%\u3002\u6d88\u878d\u7814\u7a76\u548c\u5e15\u7d2f\u6258\u5206\u6790\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u65f6\u57df\u53d8\u5316\u3001\u7ea6\u675f\u8fd1\u4f3c\u548c\u5e73\u6ed1\u8c03\u5ea6\u65b9\u9762\u7684\u7a33\u5065\u589e\u76ca\u3002", "conclusion": "Unique\u901a\u8fc7\u7edf\u4e00MPC\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4efb\u52a1\u4e2d\u5373\u65f6\u53cd\u5e94\u6027\u548c\u957f\u65f6\u57df\u89c4\u5212\u7684\u77db\u76fe\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u95ed\u73af\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u7a7a\u4e2d\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12060", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.12060", "abs": "https://arxiv.org/abs/2512.12060", "authors": ["Tejas Panambur", "Ishan Rajendrakumar Dave", "Chongjian Ge", "Ersin Yumer", "Xue Bai"], "title": "CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos", "comment": "The first two authors contributed equally", "summary": "Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.\n  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.\n  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.", "AI": {"tldr": "CreativeVR\u662f\u4e00\u4e2a\u9488\u5bf9AI\u751f\u6210\u89c6\u9891\u548c\u771f\u5b9e\u89c6\u9891\u4e2d\u4e25\u91cd\u7ed3\u6784/\u65f6\u5e8f\u4f2a\u5f71\u7684\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u7cbe\u5ea6\u63a7\u5236\u65cb\u94ae\u5728\u7cbe\u786e\u4fee\u590d\u4e0e\u7ed3\u6784/\u8fd0\u52a8\u6821\u6b63\u4e4b\u95f4\u5e73\u6ed1\u6743\u8861\uff0c\u5728AIGC54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u7cbe\u7ec6\u7ed3\u6784\u4e0a\u5b58\u5728\u7f3a\u9677\uff08\u626d\u66f2\u7684\u9762\u90e8/\u624b\u90e8\u3001\u53d8\u5f62\u80cc\u666f\u3001\u65f6\u5e8f\u4e0d\u4e00\u81f4\uff09\uff0c\u800c\u4f20\u7edf\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5408\u6210\u9000\u5316\uff08\u6a21\u7cca\u3001\u4e0b\u91c7\u6837\uff09\uff0c\u6269\u6563\u5148\u9a8c\u4fee\u590d\u5668\u901a\u5e38\u9488\u5bf9\u5149\u5ea6\u566a\u58f0\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5bf9\u611f\u77e5\u8d28\u91cf\u4e0e\u4fdd\u771f\u5ea6\u6743\u8861\u7684\u63a7\u5236\u3002", "method": "\u63d0\u51faCreativeVR\u6846\u67b6\uff0c\u91c7\u7528\u6df1\u5ea6\u9002\u914d\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00\u7cbe\u5ea6\u63a7\u5236\u65cb\u94ae\u8c03\u8282\u6a21\u578b\u5bf9\u8f93\u5165\u7684\u8ddf\u968f\u7a0b\u5ea6\uff1b\u5173\u952e\u521b\u65b0\u662f\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u65f6\u5e8f\u4e00\u81f4\u9000\u5316\u6a21\u5757\uff0c\u5e94\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53d8\u6362\u6765\u4ea7\u751f\u771f\u5b9e\u7684\u7ed3\u6784\u5931\u6548\u3002", "result": "\u5728AIGC54\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542bFIQA\u3001\u8bed\u4e49\u548c\u611f\u77e5\u6307\u6807\uff09\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff1b\u5728\u6807\u51c6\u89c6\u9891\u4fee\u590d\u57fa\u51c6\u4e0a\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff1b\u5728\u5355\u5f2080GB A100\u4e0a\u4ee5720p\u5206\u8fa8\u7387\u5b9e\u73b0\u7ea613FPS\u7684\u5b9e\u7528\u541e\u5410\u91cf\u3002", "conclusion": "CreativeVR\u4e3aAI\u751f\u6210\u5185\u5bb9\u548c\u771f\u5b9e\u89c6\u9891\u4e2d\u7684\u4e25\u91cd\u7ed3\u6784\u53ca\u65f6\u5e8f\u4f2a\u5f71\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4fee\u590d\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u4e00\u63a7\u5236\u53c2\u6570\u5b9e\u73b0\u4e86\u4fee\u590d\u7cbe\u5ea6\u4e0e\u521b\u9020\u6027\u6821\u6b63\u4e4b\u95f4\u7684\u7075\u6d3b\u5e73\u8861\u3002"}}
{"id": "2512.12437", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12437", "abs": "https://arxiv.org/abs/2512.12437", "authors": ["Jonathan Spraggett"], "title": "Sim2Real Reinforcement Learning for Soccer skills", "comment": "Undergrad Thesis", "summary": "This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\uff0c\u4f7f\u7528\u8bfe\u7a0b\u8bad\u7ec3\u548c\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u6280\u672f\uff0c\u5728\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u52a8\u6001\u8fd0\u52a8\u6027\u80fd\uff0c\u4f46\u672a\u80fd\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u73af\u5883\u3001\u5904\u7406\u590d\u6742\u6027\u548c\u751f\u6210\u81ea\u7136\u8fd0\u52a8\u3002\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u8bad\u7ec3\u7b56\u7565\u548c\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\uff08AMP\uff09\u6280\u672f\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5b66\u4e60\u548c\u8fd0\u52a8\u5148\u9a8c\u7ea6\u675f\u6765\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u8e22\u7403\u3001\u884c\u8d70\u548c\u8df3\u8dc3\u7b49\u63a7\u5236\u4efb\u52a1\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u5f00\u53d1\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8868\u73b0\u51fa\u66f4\u597d\u7684\u52a8\u6001\u6027\u3001\u9002\u5e94\u6027\u548c\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u65b9\u6cd5\u3002\u4f46\u5c06\u5b66\u4e60\u5230\u7684\u7b56\u7565\u4ece\u6a21\u62df\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u65f6\u672a\u80fd\u6210\u529f\u3002", "conclusion": "\u867d\u7136\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4f46\u6a21\u62df\u5230\u771f\u5b9e\u7684\u8fc1\u79fb\u5931\u8d25\u63ed\u793a\u4e86\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5b8c\u5168\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u573a\u666f\u65b9\u9762\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002"}}
{"id": "2512.12080", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12080", "abs": "https://arxiv.org/abs/2512.12080", "authors": ["Ryan Po", "Eric Ryan Chan", "Changan Chen", "Gordon Wetzstein"], "title": "BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models", "comment": "Project page here: https://ryanpo.com/bagger", "summary": "Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.", "AI": {"tldr": "BAgger\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u4ece\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u8f68\u8ff9\u4e2d\u6784\u5efa\u7ea0\u6b63\u8def\u5f84\uff0c\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u957f\u671f\u751f\u6210\u7a33\u5b9a\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6a21\u578b\u901a\u8fc7\u4e0b\u4e00\u5e27\u9884\u6d4b\u8fdb\u884c\u4e16\u754c\u5efa\u6a21\uff0c\u4f46\u5b58\u5728\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff1a\u8bad\u7ec3\u65f6\u4f7f\u7528\u5e72\u51c0\u4e0a\u4e0b\u6587\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u81ea\u751f\u6210\u5e27\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u548c\u8d28\u91cf\u6f02\u79fb\u3002", "method": "\u63d0\u51faBackwards Aggregation (BAgger)\u65b9\u6848\uff0c\u4ece\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u8f68\u8ff9\u4e2d\u6784\u5efa\u7ea0\u6b63\u8def\u5f84\uff0c\u6559\u5bfc\u6a21\u578b\u4ece\u9519\u8bef\u4e2d\u6062\u590d\u3002\u4f7f\u7528\u6807\u51c6\u5206\u6570\u5339\u914d\u6216\u6d41\u5339\u914d\u76ee\u6807\u8bad\u7ec3\uff0c\u907f\u514d\u5927\u578b\u6559\u5e08\u6a21\u578b\u548c\u957f\u65f6\u95f4\u94fe\u7684\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728\u56e0\u679c\u6269\u6563\u53d8\u6362\u5668\u4e0a\u5b9e\u73b0BAgger\uff0c\u5728\u6587\u672c\u5230\u89c6\u9891\u3001\u89c6\u9891\u6269\u5c55\u548c\u591a\u63d0\u793a\u751f\u6210\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u89c2\u5bdf\u5230\u66f4\u7a33\u5b9a\u7684\u957f\u671f\u8fd0\u52a8\u548c\u66f4\u597d\u7684\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4e86\u6f02\u79fb\u73b0\u8c61\u3002", "conclusion": "BAgger\u901a\u8fc7\u81ea\u76d1\u7763\u7684\u7ea0\u6b63\u8f68\u8ff9\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6a21\u578b\u7684\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u751f\u6210\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.12468", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12468", "abs": "https://arxiv.org/abs/2512.12468", "authors": ["Tina Tian", "Xinyu Wang", "Andrew L. Orekhov", "Fujun Ruan", "Lu Li", "Oliver Kroemer", "Howie Choset"], "title": "Autonomously Unweaving Multiple Cables Using Visual Feedback", "comment": "6 pages, 5 figures", "summary": "Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u53cd\u9988\u7684\u591a\u7535\u7f06\u89e3\u7f20\u65b9\u6cd5\uff0c\u5c06\u7535\u7f06\u89e3\u7f20\u5efa\u6a21\u4e3a\u6293\u53d6\u653e\u7f6e\u95ee\u9898\uff0c\u4f7f\u7528\u56fe\u8868\u793a\u7535\u7f06\u72b6\u6001\uff0c\u901a\u8fc7\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u9884\u6d4b\u672a\u6765\u72b6\u6001\u5e76\u9009\u62e9\u52a8\u4f5c\uff0c\u5b9e\u9a8c\u5e73\u5747\u6210\u529f\u738784%", "motivation": "\u7535\u7f06\u7ba1\u7406\u4efb\u52a1\u4e2d\uff0c\u591a\u6839\u7535\u7f06\u76f8\u4e92\u7f20\u7ed5\u7684\u89e3\u7f20\u95ee\u9898\u5177\u6709\u6311\u6218\u6027\u3002\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u6839\u7535\u7f06\u7684\u89e3\u7ed3\uff0c\u800c\u672c\u6587\u805a\u7126\u4e8e\u591a\u7535\u7f06\u89e3\u7f20\u8fd9\u4e00\u5b50\u4efb\u52a1\uff0c\u5373\u5206\u79bb\u591a\u6839\u76f8\u4e92\u4ea4\u7ec7\u7684\u7535\u7f06\u4ee5\u4fbf\u540e\u7eed\u64cd\u4f5c\u3002", "method": "1) \u5c06\u7535\u7f06\u89e3\u7f20\u5efa\u6a21\u4e3a\u6293\u53d6\u653e\u7f6e\u95ee\u9898\uff1b2) \u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u7535\u7f06\u72b6\u6001\u8868\u793a\uff0c\u4ece\u89c6\u89c9\u56fe\u50cf\u7f16\u7801\u62d3\u6251\u548c\u51e0\u4f55\u4fe1\u606f\uff1b3) \u63d0\u51fa\u8003\u8651\u7535\u7f06\u5728\u64cd\u4f5c\u4e2d\u4f38\u76f4\u548c\u5f2f\u66f2\u7684\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\uff1b4) \u4f7f\u7528\u8be5\u6a21\u578b\u9009\u62e9\u4e24\u79cd\u9ad8\u5c42\u52a8\u4f5c\u57fa\u5143\u5e76\u8ba1\u7b97\u9884\u6d4b\u5373\u65f6\u6210\u672c\u4ee5\u4f18\u5316\u5e95\u5c42\u52a8\u4f5c\uff1b5) \u8fed\u4ee3\u611f\u77e5-\u89c4\u5212-\u884c\u52a8\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u89e3\u7f20\u7535\u6e90\u7ebf\u548c\u978b\u5e26\uff0c\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523084%\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u89c6\u89c9\u53cd\u9988\u7684\u591a\u7535\u7f06\u89e3\u7f20\u65b9\u6cd5\u6709\u6548\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u548c\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u5b9e\u73b0\u4e86\u5bf9\u7535\u7f06\u53d8\u5f62\u884c\u4e3a\u7684\u5efa\u6a21\uff0c\u8fed\u4ee3\u611f\u77e5-\u89c4\u5212-\u884c\u52a8\u8fc7\u7a0b\u80fd\u591f\u6210\u529f\u5206\u79bb\u76f8\u4e92\u7f20\u7ed5\u7684\u7535\u7f06\u3002"}}
{"id": "2512.12083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12083", "abs": "https://arxiv.org/abs/2512.12083", "authors": ["Guanfang Dong", "Luke Schultz", "Negar Hassanpour", "Chao Gao"], "title": "RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer", "comment": null, "summary": "The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.", "AI": {"tldr": "RePack\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9ad8\u7ef4\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8868\u793a\u6295\u5f71\u5230\u4f4e\u7ef4\u6d41\u5f62\uff0c\u89e3\u51b3\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u52a0\u901f\u6269\u6563\u53d8\u6362\u5668\u6536\u655b\u5e76\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u7ef4\u8868\u793a\u867d\u7136\u80fd\u589e\u5f3a\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u8fc7\u8f7d\uff0c\u7279\u522b\u662f\u5f53VFM\u7279\u5f81\u5c3a\u5bf8\u8d85\u8fc7\u539f\u59cb\u56fe\u50cf\u89e3\u7801\u9700\u6c42\u65f6\uff0c\u5f71\u54cd\u6a21\u578b\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faRePack\u6846\u67b6\uff0c\u5c06\u9ad8\u7ef4VFM\u8868\u793a\u6295\u5f71\u5230\u4f4e\u7ef4\u6d41\u5f62\uff0c\u8f6c\u5316\u4e3a\u66f4\u7d27\u51d1\u3001\u89e3\u7801\u5668\u53cb\u597d\u7684\u8868\u793a\uff0c\u8fc7\u6ee4\u975e\u8bed\u4e49\u566a\u58f0\u540c\u65f6\u4fdd\u7559\u6838\u5fc3\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u5728DiT-XL/2\u4e0a\uff0cRePack\u4ec5\u752864\u4e2aepoch\u5c31\u8fbe\u5230FID 3.66\uff0c\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5feb35%\u6536\u655b\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u6ce8\u5165\u539f\u59cbVFM\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "conclusion": "RePack\u6210\u529f\u63d0\u53d6\u4e86VFM\u8868\u793a\u7684\u6838\u5fc3\u8bed\u4e49\uff0c\u540c\u65f6\u907f\u514d\u4e86\u9ad8\u7ef4\u5ea6\u7684\u526f\u4f5c\u7528\uff0c\u4e3a\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.12089", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12089", "abs": "https://arxiv.org/abs/2512.12089", "authors": ["Zihu Wang", "Boxun Xu", "Yuxuan Xia", "Peng Li"], "title": "VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering", "comment": null, "summary": "Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.", "AI": {"tldr": "VEGAS\uff1a\u901a\u8fc7\u5c06\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6ce8\u610f\u529b\u56fe\u6ce8\u5165\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u6765\u51cf\u5c11LVLM\u5e7b\u89c9\u7684\u63a8\u7406\u65f6\u65b9\u6cd5", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u867d\u7136\u80fd\u8054\u5408\u63a8\u7406\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u4e0e\u89c6\u89c9\u8bc1\u636e\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9\u8f93\u51fa\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u89e3\u51b3\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u95ee\u9898\u3002", "method": "\u63d0\u51faVEGAS\u65b9\u6cd5\uff1a1\uff09\u53d1\u73b0\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6ce8\u610f\u529b\u56fe\u6bd4\u6700\u7ec8\u89c6\u89c9\u6ce8\u610f\u529b\u56fe\u66f4\u96c6\u4e2d\uff1b2\uff09\u5206\u6790\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u89c6\u89c9-\u6587\u672c\u51b2\u7a81\uff0c\u53d1\u73b0\u51b2\u7a81\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u8fbe\u5230\u5cf0\u503c\uff1b3\uff09\u5c06\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6ce8\u610f\u529b\u56fe\u6ce8\u5165\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\uff1b4\uff09\u81ea\u9002\u5e94\u5730\u5f15\u5bfc\u672a\u80fd\u805a\u7126\u5173\u952e\u56fe\u50cf\u5bf9\u8c61\u7684token\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVEGAS\u5728\u51cf\u5c11\u5e7b\u89c9\u65b9\u9762\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6ce8\u610f\u529b\u56fe\u80fd\u6709\u6548\u6291\u5236LVLM\u7684\u5e7b\u89c9\uff0cVEGAS\u4f5c\u4e3a\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6ce8\u5165\u673a\u5236\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u7684\u89c6\u89c9\u4e00\u81f4\u6027\u3002"}}
{"id": "2512.12090", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12090", "abs": "https://arxiv.org/abs/2512.12090", "authors": ["Samar Fares", "Nurbek Tastan", "Karthik Nandakumar"], "title": "SPDMark: Selective Parameter Displacement for Robust Video Watermarking", "comment": null, "summary": "The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.", "AI": {"tldr": "SPDMark\u662f\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u53c2\u6570\u4f4d\u79fb\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u751f\u6210\u6a21\u578b\u53c2\u6570\u5b50\u96c6\u5d4c\u5165\u6c34\u5370\uff0c\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5b9e\u73b0\u53c2\u6570\u6548\u7387\uff0c\u80fd\u591f\u751f\u6210\u4e0d\u53ef\u611f\u77e5\u7684\u6c34\u5370\u5e76\u62b5\u6297\u591a\u79cd\u89c6\u9891\u4fee\u6539\u3002", "motivation": "\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5174\u8d77\u589e\u52a0\u4e86\u5bf9\u53ef\u9760\u6c34\u5370\u65b9\u6848\u7684\u9700\u6c42\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u8ffd\u8e2a\u751f\u6210\u89c6\u9891\u7684\u6765\u6e90\u3002\u73b0\u6709\u89c6\u9891\u6c34\u5370\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u4e0d\u53ef\u611f\u77e5\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u9009\u62e9\u6027\u53c2\u6570\u4f4d\u79fb\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u751f\u6210\u6a21\u578b\u53c2\u6570\u5b50\u96c6\u5d4c\u5165\u6c34\u5370\u3002\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5b9e\u73b0\u53c2\u6570\u6548\u7387\uff0c\u5c06\u4f4d\u79fb\u5efa\u6a21\u4e3a\u5c42\u95f4\u57fa\u7840\u4f4d\u79fb\u7684\u52a0\u6027\u7ec4\u5408\u3002\u8bad\u7ec3\u9636\u6bb5\u8054\u5408\u5b66\u4e60\u57fa\u7840\u4f4d\u79fb\u548c\u6c34\u5370\u63d0\u53d6\u5668\uff0c\u6700\u5c0f\u5316\u6d88\u606f\u6062\u590d\u3001\u611f\u77e5\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\u3002\u4f7f\u7528\u5bc6\u7801\u54c8\u5e0c\u51fd\u6570\u4ece\u57fa\u7840\u6c34\u5370\u5bc6\u94a5\u6d3e\u751f\u5e27\u7279\u5b9a\u6c34\u5370\u6d88\u606f\uff0c\u901a\u8fc7\u6700\u5927\u4e8c\u5206\u56fe\u5339\u914d\u6062\u590d\u6b63\u786e\u5e27\u987a\u5e8f\u3002", "result": "\u5728\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSPDMark\u80fd\u591f\u751f\u6210\u4e0d\u53ef\u611f\u77e5\u7684\u6c34\u5370\uff0c\u5e76\u4ee5\u9ad8\u51c6\u786e\u7387\u6062\u590d\u6c34\u5370\uff0c\u540c\u65f6\u5bf9\u5404\u79cd\u5e38\u89c1\u89c6\u9891\u4fee\u6539\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "SPDMark\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u6c34\u5370\u65b9\u6cd5\u5728\u4e0d\u53ef\u611f\u77e5\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u751f\u6210\u89c6\u9891\u7684\u6eaf\u6e90\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12101", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12101", "abs": "https://arxiv.org/abs/2512.12101", "authors": ["Swarn S. Warshaneyan", "Maksims Ivanovs", "Bla\u017e Cugmas", "Inese B\u0113rzi\u0146a", "Laura Goldberga", "Mindaugas Tamosiunas", "Roberts Kadi\u0137is"], "title": "AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging", "comment": "10 pages, 10 figures, 2 tables, 22 references. Journal submission undergoing peer review", "summary": "We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u5149\u5b66\u663e\u5fae\u955c\u548c\u6570\u5b57\u540c\u8f74\u5168\u606f\u663e\u5fae\u955c(DIHM)\u5728\u82b1\u7c89\u81ea\u52a8\u8bc6\u522b\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4f7f\u7528GAN\u751f\u6210\u5408\u6210DIHM\u56fe\u50cf\u6765\u6539\u5584\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3DIHM\u56fe\u50cf\u4e2d\u82b1\u7c89\u8bc6\u522b\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5305\u62ec\u6563\u6591\u566a\u58f0\u3001\u5b6a\u751f\u50cf\u4f2a\u5f71\u4ee5\u53ca\u4e0e\u660e\u573a\u56fe\u50cf\u7684\u663e\u8457\u5dee\u5f02\uff0c\u63a8\u52a8\u5168\u81ea\u52a8DIHM\u5de5\u4f5c\u6d41\u7a0b\u5728\u517d\u533b\u6210\u50cf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528YOLOv8s\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548cMobileNetV3L\u8fdb\u884c\u5206\u7c7b\uff0c\u5728\u53cc\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff1b\u91c7\u7528Wasserstein GAN with spectral normalization (WGAN-SN)\u751f\u6210\u5408\u6210DIHM\u56fe\u50cf\uff1b\u901a\u8fc7\u6df7\u5408\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u6765\u6539\u5584\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5149\u5b66\u6570\u636e\u68c0\u6d4bmAP50\u8fbe91.3%\uff0c\u5206\u7c7b\u51c6\u786e\u738797%\uff1bDIHM\u6570\u636e\u68c0\u6d4bmAP50\u4ec58.15%\uff0c\u5206\u7c7b\u51c6\u786e\u738750%\uff1b\u4f7f\u7528GAN\u5408\u6210\u6570\u636e\u6df7\u5408\u8bad\u7ec3\u540e\uff0cDIHM\u68c0\u6d4bmAP50\u63d0\u5347\u81f315.4%\u3002", "conclusion": "GAN\u6570\u636e\u589e\u5f3a\u80fd\u7f29\u5c0f\u5149\u5b66\u548cDIHM\u56fe\u50cf\u8bc6\u522b\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u5168\u81ea\u52a8DIHM\u5de5\u4f5c\u6d41\u7a0b\u5728\u517d\u533b\u6210\u50cf\u4e2d\u7684\u5e94\u7528\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.12722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12722", "abs": "https://arxiv.org/abs/2512.12722", "authors": ["Tarik Viehmann", "Daniel Swoboda", "Samridhi Kalra", "Himanshu Grover", "Gerhard Lakemeyer"], "title": "Making Robots Play by the Rules: The ROS 2 CLIPS-Executive", "comment": null, "summary": "CLIPS is a rule-based programming language for building knowledge-driven applications, well suited for the complex task of coordinating autonomous robots. Inspired by the CLIPS-Executive originally developed for the lesser known Fawkes robotics framework, we present an Integration of CLIPS into the ROS ecosystem. Additionally, we show the flexibility of CLIPS by describing a PDDL-based planning framework integration.", "AI": {"tldr": "\u5c06CLIPS\u89c4\u5219\u7f16\u7a0b\u8bed\u8a00\u96c6\u6210\u5230ROS\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u5e76\u5c55\u793a\u4e86\u4e0ePDDL\u89c4\u5212\u6846\u67b6\u7684\u96c6\u6210", "motivation": "CLIPS\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u9a71\u52a8\u7f16\u7a0b\u8bed\u8a00\uff0c\u975e\u5e38\u9002\u5408\u534f\u8c03\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u590d\u6742\u4efb\u52a1\u3002\u53d7\u6700\u521d\u4e3aFawkes\u673a\u5668\u4eba\u6846\u67b6\u5f00\u53d1\u7684CLIPS-Executive\u542f\u53d1\uff0c\u9700\u8981\u5c06\u5176\u96c6\u6210\u5230\u66f4\u5e7f\u6cdb\u4f7f\u7528\u7684ROS\u751f\u6001\u7cfb\u7edf\u4e2d", "method": "\u5c06CLIPS\u96c6\u6210\u5230ROS\u751f\u6001\u7cfb\u7edf\uff0c\u5e76\u63cf\u8ff0\u4e86\u4e00\u4e2a\u57fa\u4e8ePDDL\u7684\u89c4\u5212\u6846\u67b6\u96c6\u6210\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86CLIPS\u7684\u7075\u6d3b\u6027", "result": "\u6210\u529f\u5b9e\u73b0\u4e86CLIPS\u5728ROS\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\uff0c\u5e76\u5c55\u793a\u4e86\u4e0ePDDL\u89c4\u5212\u6846\u67b6\u7684\u96c6\u6210\u80fd\u529b", "conclusion": "CLIPS\u53ef\u4ee5\u6709\u6548\u5730\u96c6\u6210\u5230ROS\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u673a\u5668\u4eba\u534f\u8c03\u63d0\u4f9b\u77e5\u8bc6\u9a71\u52a8\u7684\u89c4\u5219\u7f16\u7a0b\u80fd\u529b\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u4e0e\u89c4\u5212\u6846\u67b6\u7684\u826f\u597d\u517c\u5bb9\u6027"}}
{"id": "2512.12107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12107", "abs": "https://arxiv.org/abs/2512.12107", "authors": ["Yuheng Li", "Yue Zhang", "Abdoul Aziz Amadou", "Yuxiang Lai", "Jike Zhong", "Tiziano Passerini", "Dorin Comaniciu", "Puneet Sharma"], "title": "EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography", "comment": null, "summary": "Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u6d4b\u91cf\u7684\u591a\u6a21\u6001\u8d85\u58f0\u5fc3\u52a8\u56fe\u6570\u636e\u96c6EchoGround-MIMIC\u548c\u76f8\u5e94\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bEchoVLM\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u5728\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u8d85\u58f0\u5fc3\u52a8\u56fe\u662f\u5fc3\u810f\u75c5\u5b66\u4e2d\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u6210\u50cf\u65b9\u5f0f\uff0c\u4f46\u5176\u89e3\u8bfb\u4ecd\u7136\u52b3\u52a8\u5bc6\u96c6\u4e14\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u7684\uff0c\u9700\u8981\u89c6\u56fe\u8bc6\u522b\u3001\u5b9a\u91cf\u6d4b\u91cf\u3001\u5b9a\u6027\u8bc4\u4f30\u548c\u57fa\u4e8e\u6307\u5357\u7684\u63a8\u7406\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8d85\u58f0\u5fc3\u52a8\u56fe\u9886\u57df\u7684\u5e94\u7528\u53d7\u5230\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e34\u5e8a\u57fa\u7840\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\u4ee5\u53ca\u7f3a\u5c11\u6d4b\u91cf\u57fa\u7840\u63a8\u7406\u7684\u9650\u5236\u3002", "method": "1. \u521b\u5efaEchoGround-MIMIC\u6570\u636e\u96c6\uff1a\u5305\u542b19,065\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u6765\u81ea1,572\u540d\u60a3\u8005\uff0c\u5177\u6709\u6807\u51c6\u5316\u89c6\u56fe\u3001\u7ed3\u6784\u5316\u6d4b\u91cf\u3001\u57fa\u4e8e\u6d4b\u91cf\u7684\u63cf\u8ff0\u548c\u6307\u5357\u884d\u751f\u7684\u75be\u75c5\u6807\u7b7e\u30022. \u5f00\u53d1EchoVLM\u6a21\u578b\uff1a\u91c7\u7528\u4e24\u79cd\u65b0\u9896\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff1a(i) \u89c6\u56fe\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\uff0c\u7f16\u7801\u8d85\u58f0\u5fc3\u52a8\u56fe\u6210\u50cf\u7684\u89c6\u56fe\u4f9d\u8d56\u7ed3\u6784\uff1b(ii) \u5426\u5b9a\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\uff0c\u533a\u5206\u4e34\u5e8a\u5173\u952e\u7684\u9634\u6027\u53d1\u73b0\u4e0e\u9633\u6027\u53d1\u73b0\u3002", "result": "\u5728\u6db5\u76d6\u591a\u6a21\u6001\u75be\u75c5\u5206\u7c7b\u3001\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u3001\u89c6\u56fe\u5206\u7c7b\u3001\u8154\u5ba4\u5206\u5272\u548c\u6807\u5fd7\u70b9\u68c0\u6d4b\u768436\u4e2a\u4efb\u52a1\u4e2d\uff0cEchoVLM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1a\u96f6\u6837\u672c\u75be\u75c5\u5206\u7c7b\u7684AUC\u8fbe\u523086.5%\uff0c\u89c6\u56fe\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523095.1%\u3002\u4e34\u5e8a\u57fa\u7840\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u4ea7\u751f\u4e86\u53ef\u8fc1\u79fb\u7684\u89c6\u89c9\u8868\u793a\u3002", "conclusion": "EchoVLM\u88ab\u786e\u7acb\u4e3a\u7aef\u5230\u7aef\u8d85\u58f0\u5fc3\u52a8\u56fe\u89e3\u8bfb\u7684\u57fa\u7840\u6a21\u578b\uff0cEchoGround-MIMIC\u6570\u636e\u96c6\u548c\u6570\u636e\u5904\u7406\u4ee3\u7801\u7684\u53d1\u5e03\u5c06\u4fc3\u8fdb\u591a\u6a21\u6001\u8d85\u58f0\u5fc3\u52a8\u56fe\u89e3\u8bfb\u7684\u53ef\u91cd\u590d\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2512.12793", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12793", "abs": "https://arxiv.org/abs/2512.12793", "authors": ["Mizuho Aoki", "Kohei Honda", "Yasuhiro Yoshimura", "Takeshi Ishita", "Ryo Yonetani"], "title": "VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps", "comment": null, "summary": "This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.", "AI": {"tldr": "VLG-Loc\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5168\u5c40\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f7f\u7528\u4ec5\u5305\u542b\u89c6\u89c9\u5730\u6807\u540d\u79f0\u548c\u533a\u57df\u7684\u6807\u6ce8\u8db3\u8ff9\u5730\u56fe\uff0c\u901a\u8fc7\u591a\u65b9\u5411\u56fe\u50cf\u89c2\u6d4b\u641c\u7d22\u5730\u56fe\u4e2d\u7684\u5730\u6807\uff0c\u5e76\u5728\u8499\u7279\u5361\u6d1b\u5b9a\u4f4d\u6846\u67b6\u4e2d\u8bc4\u4f30\u4f4d\u59ff\u5047\u8bbe\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u4f7f\u7528\u4ec5\u5305\u542b\u5730\u6807\u540d\u79f0\u548c\u533a\u57df\u7684\u7b80\u5355\u5730\u56fe\u8fdb\u884c\u5b9a\u4f4d\uff0c\u4f46\u5c06\u8fd9\u79cd\u80fd\u529b\u79fb\u690d\u5230\u673a\u5668\u4eba\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u51e0\u4f55\u548c\u5916\u89c2\u7ec6\u8282\u65f6\u96be\u4ee5\u5efa\u7acb\u89c2\u6d4b\u5730\u6807\u4e0e\u5730\u56fe\u5730\u6807\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u7684\u591a\u65b9\u5411\u56fe\u50cf\u89c2\u6d4b\u4e2d\u641c\u7d22\u5730\u56fe\u4e2d\u6807\u6ce8\u7684\u5730\u6807\uff0c\u7136\u540e\u5728\u8499\u7279\u5361\u6d1b\u5b9a\u4f4d\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u627e\u5230\u7684\u5730\u6807\u8bc4\u4f30\u6bcf\u4e2a\u4f4d\u59ff\u5047\u8bbe\u7684\u4f3c\u7136\u5ea6\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u96f6\u552e\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u7684\u57fa\u4e8e\u626b\u63cf\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u73af\u5883\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002\u901a\u8fc7\u89c6\u89c9\u548c\u626b\u63cf\u5b9a\u4f4d\u7684\u6982\u7387\u878d\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "VLG-Loc\u6210\u529f\u5b9e\u73b0\u4e86\u4f7f\u7528\u4eba\u7c7b\u53ef\u8bfb\u7684\u7b80\u5355\u5730\u56fe\u8fdb\u884c\u673a\u5668\u4eba\u5168\u5c40\u5b9a\u4f4d\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u5730\u6807\u5bf9\u5e94\u95ee\u9898\uff0c\u5728\u73af\u5883\u53d8\u5316\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.12108", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12108", "abs": "https://arxiv.org/abs/2512.12108", "authors": ["Dashti A. Ali", "Aras T. Asaad", "Jacob J. Peoples", "Mohammad Hamghalam", "Alex Robins", "Mane Piliposyan", "Richard K. G. Do", "Natalie Gangai", "Yun S. Chun", "Ahmad Bashir Barekzai", "Jayasree Chakraborty", "Hala Khasawneh", "Camila Vilela", "Natally Horvat", "Jo\u00e3o Miranda", "Alice C. Wei", "Amber L. Simpson"], "title": "A Novel Patch-Based TDA Approach for Computed Tomography", "comment": null, "summary": "The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf93D CT\u56fe\u50cf\u7684\u57fa\u4e8epatch\u7684\u6301\u4e45\u540c\u8c03\u6784\u9020\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u76843D\u7acb\u65b9\u4f53\u590d\u5f62\u65b9\u6cd5\uff0c\u5728\u5206\u7c7b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e3D\u7acb\u65b9\u4f53\u590d\u5f62\u8fc7\u6ee4\u7684\u6301\u4e45\u540c\u8c03\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387CT\u56fe\u50cf\u65f6\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u62d3\u6251\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8epatch\u7684\u6301\u4e45\u540c\u8c03\u6784\u9020\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u4f53\u79ef\u533b\u5b66\u6210\u50cf\u6570\u636e\uff08\u7279\u522b\u662fCT\u6a21\u6001\uff09\u8bbe\u8ba1\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684Python\u5305Patch-TDA\u3002", "result": "\u5728\u591a\u4e2a3D CT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cpatch-based TDA\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u3001AUC\u3001\u654f\u611f\u6027\u3001\u7279\u5f02\u6027\u548cF1\u5206\u6570\u4e0a\u5e73\u5747\u5206\u522b\u63d0\u5347\u4e8610.38%\u30016.94%\u30012.06%\u300111.58%\u548c8.51%\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u6548\u7387\u4e0a\u4e5f\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8epatch\u7684\u6301\u4e45\u540c\u8c03\u65b9\u6cd5\u5728CT\u56fe\u50cf\u5206\u6790\u4e2d\u6bd4\u4f20\u7edf\u76843D\u7acb\u65b9\u4f53\u590d\u5f62\u65b9\u6cd5\u66f4\u4f18\u8d8a\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5206\u7c7b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u6709\u671b\u4fc3\u8fdb\u62d3\u6251\u6570\u636e\u5206\u6790\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2512.12842", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12842", "abs": "https://arxiv.org/abs/2512.12842", "authors": ["Kuan Fang", "Yuxin Chen", "Xinghao Zhu", "Farzad Niroui", "Lingfeng Sun", "Jiuguang Wang"], "title": "SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding", "comment": "9 pages, 7 figures", "summary": "We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.", "AI": {"tldr": "SAGA\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7684\u901a\u7528\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9ad8\u7ea7\u8bed\u4e49\u610f\u56fe\u4e0e\u4f4e\u7ea7\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u89e3\u8026\uff0c\u4f7f\u7528\u57fa\u4e8e\u53ef\u4f9b\u6027\u7684\u4efb\u52a1\u8868\u793a\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5c06\u4efb\u52a1\u8868\u793a\u6620\u5c04\u52303D\u53ef\u4f9b\u6027\u70ed\u56fe\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u73af\u5883\u3001\u4efb\u52a1\u76ee\u6807\u548c\u7528\u6237\u89c4\u8303\u7684\u6cdb\u5316\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u5728\u5404\u79cd\u73af\u5883\u3001\u4efb\u52a1\u76ee\u6807\u548c\u7528\u6237\u89c4\u8303\u4e4b\u95f4\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u8026\u9ad8\u7ea7\u8bed\u4e49\u610f\u56fe\u4e0e\u4f4e\u7ea7\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7684\u6846\u67b6\uff0c\u4ee5\u6709\u6548\u5b66\u4e60\u901a\u7528\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u53ef\u4f9b\u6027\u7684\u4efb\u52a1\u8868\u793a\u6765\u8868\u8fbe\u591a\u6837\u590d\u6742\u884c\u4e3a\uff1b\u5229\u7528\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5c06\u4efb\u52a1\u8868\u793a\u6620\u5c04\u5230\u673a\u5668\u4eba\u89c6\u89c9\u89c2\u5bdf\u4e2d\u76843D\u53ef\u4f9b\u6027\u70ed\u56fe\uff1b\u57fa\u4e8e\u8fd9\u4e9b\u63a5\u5730\u7684\u53ef\u4f9b\u6027\u8bad\u7ec3\u6761\u4ef6\u7b56\u7565\u8fdb\u884c\u5168\u8eab\u63a7\u5236\u3002", "result": "\u5728\u56db\u8db3\u673a\u68b0\u81c2\u4e0a\u5b9e\u73b0\u4e86SAGA\uff0c\u5e76\u572811\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u3002SAGA\u5728\u96f6\u6837\u672c\u6267\u884c\u548c\u5c11\u6837\u672c\u9002\u5e94\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u59cb\u7ec8\u5927\u5e45\u4f18\u4e8e\u7aef\u5230\u7aef\u548c\u6a21\u5757\u5316\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u6784\u5316\u53ef\u4f9b\u6027\u63a5\u5730\u65b9\u6848\u4e3a\u901a\u7528\u79fb\u52a8\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u9014\u5f84\uff0c\u80fd\u591f\u5904\u7406\u8bed\u8a00\u6307\u4ee4\u3001\u9009\u5b9a\u70b9\u548c\u793a\u4f8b\u6f14\u793a\u7b49\u591a\u79cd\u4efb\u52a1\u6307\u5b9a\u5f62\u5f0f\u3002"}}
{"id": "2512.12128", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12128", "abs": "https://arxiv.org/abs/2512.12128", "authors": ["Thomas Manzini", "Priyankari Perali", "Raisa Karnik", "Robin R. Murphy"], "title": "A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery", "comment": "11 pages, 6 figures, 6 tables. To appear AAAI'26", "summary": "This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\\% Macro IoU. If spatial alignment is not considered, approximately 8\\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u76ee\u524d\u6700\u5927\u7684\u9053\u8def\u635f\u574f\u8bc4\u4f30\u4e0e\u9053\u8def\u5bf9\u9f50\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b10\u6b21\u8054\u90a6\u5ba3\u5e03\u707e\u5bb3\u540e\u7684\u65e0\u4eba\u673a\u5f71\u50cf\uff0c\u63d0\u4f9b\u4e8618\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u6570\u636e\u96c6\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u707e\u5bb3\u9053\u8def\u635f\u574f\u8bc4\u4f30\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u6216\u4f9d\u8d56\u4f4e\u5206\u8fa8\u7387\u5f71\u50cf\uff0c\u65e0\u6cd5\u68c0\u6d4b\u5e94\u6025\u7ba1\u7406\u8005\u5173\u5fc3\u7684\u73b0\u8c61\uff0c\u4e14\u5148\u524d\u5f00\u53d1\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7f3a\u4e4f\u5b9e\u9645\u8fd0\u8425\u9a8c\u8bc1\u3002\u6b64\u5916\uff0c\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u9053\u8def\u7ebf\u9519\u4f4d\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u6807\u6ce8\u4e86657.25\u516c\u91cc\u9053\u8def\uff0c\u91c7\u752810\u7c7b\u6807\u6ce8\u65b9\u6848\uff1b\u57282024\u5e74\u98d3\u98ceDebby\u548cHelene\u7684\u5e94\u6025\u54cd\u5e94\u4e2d\u8bad\u7ec3\u5e76\u90e8\u7f72\u4e8618\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1b\u63d0\u4f9b\u4e869,184\u4e2a\u9053\u8def\u7ebf\u8c03\u6574\u7528\u4e8e\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "\u5f5318\u4e2a\u57fa\u7ebf\u6a21\u578b\u90e8\u7f72\u5230\u5b9e\u9645\u9519\u4f4d\u7684\u9053\u8def\u7ebf\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u5e73\u5747\u4e0b\u964d5.596% Macro IoU\uff1b\u5982\u679c\u4e0d\u8003\u8651\u7a7a\u95f4\u5bf9\u9f50\uff0c\u7ea68%\uff0811\u516c\u91cc\uff09\u7684\u9053\u8def\u4e0d\u826f\u6761\u4ef6\u4f1a\u88ab\u9519\u8bef\u6807\u6ce8\uff0c\u7ea69%\uff0859\u516c\u91cc\uff09\u7684\u9053\u8def\u7ebf\u4f1a\u504f\u79bb\u5b9e\u9645\u9053\u8def\u3002", "conclusion": "\u7a7a\u95f4\u5bf9\u9f50\u662f\u707e\u5bb3\u54cd\u5e94\u4e2d\u9053\u8def\u635f\u574f\u8bc4\u4f30\u7684\u5173\u952e\u95ee\u9898\uff0cML\u3001CV\u548c\u673a\u5668\u4eba\u793e\u533a\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4ee5\u652f\u6301\u66f4\u6709\u6548\u548c\u660e\u667a\u7684\u707e\u5bb3\u51b3\u7b56\u3002"}}
{"id": "2512.12855", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12855", "abs": "https://arxiv.org/abs/2512.12855", "authors": ["Patrick Kostelac", "Xuerui Wang", "Anahita Jamshidnejad"], "title": "MPC-Guided Safe Reinforcement Learning and Lipschitz-Based Filtering for Structured Nonlinear Systems", "comment": null, "summary": "Modern engineering systems, such as autonomous vehicles, flexible robotics, and intelligent aerospace platforms, require controllers that are robust to uncertainties, adaptive to environmental changes, and safety-aware under real-time constraints. RL offers powerful data-driven adaptability for systems with nonlinear dynamics that interact with uncertain environments. RL, however, lacks built-in mechanisms for dynamic constraint satisfaction during exploration. MPC offers structured constraint handling and robustness, but its reliance on accurate models and computationally demanding online optimization may pose significant challenges. This paper proposes an integrated MPC-RL framework that combines stability and safety guarantees of MPC with the adaptability of RL. During training, MPC defines safe control bounds that guide the RL component and that enable constraint-aware policy learning. At deployment, the learned policy operates in real time with a lightweight safety filter based on Lipschitz continuity to ensure constraint satisfaction without heavy online optimizations. The approach, which is validated on a nonlinear aeroelastic wing system, demonstrates improved disturbance rejection, reduced actuator effort, and robust performance under turbulence. The architecture generalizes to other domains with structured nonlinearities and bounded disturbances, offering a scalable solution for safe artificial-intelligence-driven control in engineering applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u96c6\u6210\u6846\u67b6\uff0c\u5c06MPC\u7684\u7a33\u5b9a\u6027\u3001\u5b89\u5168\u6027\u4fdd\u8bc1\u4e0eRL\u7684\u9002\u5e94\u6027\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5177\u6709\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u7684\u5de5\u7a0b\u7cfb\u7edf\u63a7\u5236\u3002", "motivation": "\u73b0\u4ee3\u5de5\u7a0b\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3001\u67d4\u6027\u673a\u5668\u4eba\u3001\u667a\u80fd\u822a\u7a7a\u822a\u5929\u5e73\u53f0\uff09\u9700\u8981\u80fd\u591f\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u3001\u9002\u5e94\u73af\u5883\u53d8\u5316\u5e76\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u4fdd\u6301\u5b89\u5168\u6027\u7684\u63a7\u5236\u5668\u3002RL\u5177\u6709\u5f3a\u5927\u7684\u6570\u636e\u9a71\u52a8\u9002\u5e94\u6027\uff0c\u4f46\u7f3a\u4e4f\u52a8\u6001\u7ea6\u675f\u6ee1\u8db3\u673a\u5236\uff1bMPC\u5177\u6709\u7ed3\u6784\u5316\u7ea6\u675f\u5904\u7406\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u4f9d\u8d56\u7cbe\u786e\u6a21\u578b\u4e14\u5728\u7ebf\u8ba1\u7b97\u91cf\u5927\u3002", "method": "\u63d0\u51fa\u96c6\u6210MPC-RL\u6846\u67b6\uff1a\u8bad\u7ec3\u9636\u6bb5\uff0cMPC\u5b9a\u4e49\u5b89\u5168\u63a7\u5236\u8fb9\u754c\u6765\u6307\u5bfcRL\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u7ea6\u675f\u611f\u77e5\u7684\u7b56\u7565\u5b66\u4e60\uff1b\u90e8\u7f72\u9636\u6bb5\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5b9e\u65f6\u8fd0\u884c\uff0c\u5e76\u57fa\u4e8eLipschitz\u8fde\u7eed\u6027\u7684\u8f7b\u91cf\u7ea7\u5b89\u5168\u6ee4\u6ce2\u5668\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\uff0c\u65e0\u9700\u7e41\u91cd\u7684\u5728\u7ebf\u4f18\u5316\u3002", "result": "\u5728\u975e\u7ebf\u6027\u6c14\u52a8\u5f39\u6027\u673a\u7ffc\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u6270\u52a8\u6291\u5236\u3001\u51cf\u5c11\u7684\u6267\u884c\u5668\u52aa\u529b\u4ee5\u53ca\u5728\u6e4d\u6d41\u4e0b\u7684\u9c81\u68d2\u6027\u80fd\u3002\u8be5\u67b6\u6784\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u5177\u6709\u7ed3\u6784\u5316\u975e\u7ebf\u6027\u548c\u6709\u754c\u6270\u52a8\u7684\u9886\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86MPC\u7684\u5b89\u5168\u4fdd\u8bc1\u548cRL\u7684\u9002\u5e94\u6027\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5b89\u5168\u63a7\u5236\u3002"}}
{"id": "2512.12142", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2512.12142", "abs": "https://arxiv.org/abs/2512.12142", "authors": ["Bj\u00f6rn L\u00fctjens", "Patrick Alexander", "Raf Antwerpen", "Til Widmann", "Guido Cervone", "Marco Tedesco"], "title": "MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater", "comment": null, "summary": "The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as \"ground truth\", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.", "AI": {"tldr": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u878d\u5408\u591a\u6e90\u9065\u611f\u6570\u636e\uff0c\u751f\u6210\u683c\u9675\u5170\u51b0\u76d6\u6bcf\u65e5100\u7c73\u5206\u8fa8\u7387\u7684\u5730\u8868\u878d\u6c34\u5206\u5e03\u56fe\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u7cbe\u5ea6\u663e\u8457\u63d0\u5347", "motivation": "\u683c\u9675\u5170\u51b0\u76d6\u52a0\u901f\u878d\u5316\u8fc7\u7a0b\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u4e14\u96be\u4ee5\u6d4b\u91cf\uff0c\u73b0\u6709\u878d\u6c34\u5206\u5e03\u56fe\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0a\u5b58\u5728\u6743\u8861\uff0c\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387", "method": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u878d\u5408\u533a\u57df\u6c14\u5019\u6a21\u578b\u8f93\u51fa\u3001\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u3001\u88ab\u52a8\u5fae\u6ce2\u548c\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u6570\u636e\uff0c\u5bf9Helheim\u51b0\u5ddd\u533a\u57df\u8fdb\u884c\u65f6\u7a7a\u964d\u5c3a\u5ea6\u5904\u7406", "result": "\u6df1\u5ea6\u5b66\u4e60\u878d\u5408\u65b9\u6cd5\u7cbe\u5ea6\u8fbe95%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u533a\u57df\u6c14\u5019\u6a21\u578b\u7684\u65b9\u6cd5\uff0883%\uff09\u548c\u4ec5\u7528\u88ab\u52a8\u5fae\u6ce2\u7684\u65b9\u6cd5\uff0872%\uff09\uff1b\u540c\u65f6\u53d1\u5e03\u57fa\u51c6\u6570\u636e\u96c6MeltwaterBench", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u878d\u5408\u591a\u6e90\u6570\u636e\u80fd\u6709\u6548\u751f\u6210\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u878d\u6c34\u5206\u5e03\u56fe\uff0c\u4e3a\u7406\u89e3\u51b0\u76d6\u878d\u5316\u8fc7\u7a0b\u63d0\u4f9b\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u4fc3\u8fdb\u65b9\u6cd5\u6bd4\u8f83"}}
{"id": "2512.12945", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12945", "abs": "https://arxiv.org/abs/2512.12945", "authors": ["Anja Sheppard", "Parker Ewen", "Joey Wilson", "Advaith V. Sethuraman", "Benard Adewole", "Anran Li", "Yuzhen Chen", "Ram Vasudevan", "Katherine A. Skinner"], "title": "SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework", "comment": "Accepted into R-AL", "summary": "This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.", "AI": {"tldr": "SLIM-VDB\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5efa\u56fe\u7cfb\u7edf\uff0c\u5229\u7528OpenVDB\u6570\u636e\u7ed3\u6784\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u66f4\u65b0\u6846\u67b6\u540c\u65f6\u652f\u6301\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u8bed\u4e49\u878d\u5408\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u548c\u5efa\u56fe\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u5efa\u56fe\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u867d\u7136OpenVDB\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u5df2\u8bc1\u660e\u80fd\u663e\u8457\u63d0\u9ad8\u4f53\u7d20\u573a\u666f\u8868\u793a\u7684\u6548\u7387\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u4f46\u5728\u673a\u5668\u4eba\u8bed\u4e49\u5efa\u56fe\u4e2d\u5c1a\u672a\u5e94\u7528\uff1b2) \u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u540c\u65f6\u96c6\u6210\u56fa\u5b9a\u7c7b\u522b\u548c\u5f00\u653e\u8bed\u8a00\u6807\u7b7e\u9884\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faSLIM-VDB\u7cfb\u7edf\uff0c\u5229\u7528OpenVDB\u6570\u636e\u7ed3\u6784\u8fdb\u884c3D\u8bed\u4e49\u5efa\u56fe\uff0c\u5e76\u8bbe\u8ba1\u7edf\u4e00\u7684\u8d1d\u53f6\u65af\u66f4\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u5c01\u95ed\u96c6\uff08\u56fa\u5b9a\u7c7b\u522b\uff09\u548c\u5f00\u653e\u96c6\uff08\u5f00\u653e\u8bed\u8a00\u6807\u7b7e\uff09\u7684\u8bed\u4e49\u878d\u5408\u3002", "result": "\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bed\u4e49\u5efa\u56fe\u65b9\u6cd5\u76f8\u6bd4\uff0cSLIM-VDB\u5728\u4fdd\u6301\u53ef\u6bd4\u5efa\u56fe\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\u548c\u5efa\u56fe\u65f6\u95f4\u3002\u7cfb\u7edf\u63d0\u4f9b\u5f00\u6e90C++\u4ee3\u7801\u5e93\u548cPython\u63a5\u53e3\u3002", "conclusion": "SLIM-VDB\u6210\u529f\u5730\u5c06OpenVDB\u6570\u636e\u7ed3\u6784\u5e94\u7528\u4e8e\u8bed\u4e49\u5efa\u56fe\u9886\u57df\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u5de5\u5177\u3002"}}
{"id": "2512.12165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12165", "abs": "https://arxiv.org/abs/2512.12165", "authors": ["Daniel Adebi", "Sagnik Majumder", "Kristen Grauman"], "title": "Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video", "comment": null, "summary": "Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u5229\u7528\u97f3\u9891\u8f85\u52a9\u89c6\u89c9\u8fdb\u884c\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u58f0\u6e90\u65b9\u5411\u8c31\u548c\u53cc\u8033\u5d4c\u5165\u5230\u89c6\u89c9\u6a21\u578b\u4e2d\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u89c6\u89c9\u65b9\u6cd5\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u906e\u6321\u7b49\u89c6\u89c9\u9000\u5316\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u88ab\u52a8\u573a\u666f\u58f0\u97f3\u63d0\u4f9b\u4e86\u4e92\u8865\u7ebf\u7d22\uff0c\u53ef\u4ee5\u589e\u5f3a\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u97f3\u9891-\u89c6\u89c9\u6846\u67b6\uff0c\u5c06\u58f0\u6e90\u65b9\u5411\uff08DOA\uff09\u8c31\u548c\u53cc\u8033\u5316\u5d4c\u5165\u6574\u5408\u5230\u6700\u5148\u8fdb\u7684\u7eaf\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u578b\u6570\u636e\u96c6\u4e0a\u7684\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7ebf\u65b9\u6cd5\u6709\u6301\u7eed\u63d0\u5347\uff0c\u4e14\u5728\u89c6\u89c9\u4fe1\u606f\u53d7\u635f\u65f6\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u6210\u529f\u5229\u7528\u97f3\u9891\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u5de5\u4f5c\uff0c\u786e\u7acb\u4e86\u65e5\u5e38\u97f3\u9891\u4f5c\u4e3a\u7ecf\u5178\u7a7a\u95f4\u6311\u6218\u7684\u4e00\u4e2a\u610f\u5916\u4f46\u6709\u524d\u666f\u7684\u4fe1\u53f7\u3002"}}
{"id": "2512.12993", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12993", "abs": "https://arxiv.org/abs/2512.12993", "authors": ["Guillermo A. Castillo", "Himanshu Lodha", "Ayonga Hereid"], "title": "Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations", "comment": null, "summary": "This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5730\u5f62\u611f\u77e5\u53cc\u8db3\u8fd0\u52a8\u7684\u5206\u5c42\u7b56\u7565\uff0c\u901a\u8fc7\u964d\u7ef4\u611f\u77e5\u8868\u793a\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u9ad8\u5c42\u7b56\u7565\uff0c\u63d0\u9ad8\u5b9e\u65f6\u6b65\u6001\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7aef\u5230\u7aef\u65b9\u6cd5\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u53cc\u8db3\u8fd0\u52a8\u63a7\u5236\u5b58\u5728\u6548\u7387\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u611f\u77e5\u8868\u793a\u548c\u51b3\u7b56\u6846\u67b6\u6765\u63d0\u5347\u786c\u4ef6\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\uff1a1) \u4f7f\u7528CNN-VAE\u63d0\u53d6\u5730\u5f62\u6f5c\u5728\u7f16\u7801\uff1b2) \u7ed3\u5408\u964d\u9636\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6784\u5efa\u7d27\u51d1\u72b6\u6001\uff1b3) \u5f15\u5165\u5386\u53f2\u611f\u77e5\u673a\u5236\uff0c\u6574\u5408\u8fd1\u671f\u5730\u5f62\u89c2\u6d4b\u5e8f\u5217\uff1b4) \u63d0\u51fa\u84b8\u998f\u65b9\u6cd5\u76f4\u63a5\u4ece\u6df1\u5ea6\u76f8\u673a\u56fe\u50cf\u5b66\u4e60\u6f5c\u5728\u8868\u793a\uff1b5) \u5728\u9ad8\u4fdd\u771fAR\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\uff0c\u5305\u542b\u4f20\u611f\u5668\u566a\u58f0\u3001\u72b6\u6001\u4f30\u8ba1\u548c\u9a71\u52a8\u5668\u52a8\u6001\u3002", "result": "\u7cfb\u7edf\u5206\u6790\u4e86\u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6\u5bf9\u5b66\u4e60\u6548\u7387\u548c\u7b56\u7565\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u6a21\u62df\u4e0e\u771f\u5b9e\u4f20\u611f\u5668\u6570\u636e\u5bf9\u6bd4\u63d0\u4f9b\u4e86\u521d\u6b65\u786c\u4ef6\u9a8c\u8bc1\uff0c\u786e\u8ba4\u4e86\u786c\u4ef6\u90e8\u7f72\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5206\u5c42\u6846\u67b6\u901a\u8fc7\u964d\u7ef4\u611f\u77e5\u8868\u793a\u663e\u8457\u63d0\u5347\u4e86\u5730\u5f62\u611f\u77e5\u53cc\u8db3\u8fd0\u52a8\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u5b9e\u9645\u786c\u4ef6\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13009", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13009", "abs": "https://arxiv.org/abs/2512.13009", "authors": ["O\u011fuzhan Akb\u0131y\u0131k", "Naseem Alhousani", "Fares J. Abu-Dakka"], "title": "K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots", "comment": null, "summary": "Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.", "AI": {"tldr": "K-VARK\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4f20\u611f\u5668\u529b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6838\u5316\u6982\u7387\u6a21\u578b\u548c\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u57286\u81ea\u7531\u5ea6\u534f\u4f5c\u673a\u68b0\u81c2\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u8d85\u8fc720%\u7684RMSE\u964d\u4f4e\u3002", "motivation": "\u673a\u5668\u4eba\u5b89\u5168\u7cbe\u786e\u5730\u4e0e\u65e0\u7ed3\u6784\u73af\u5883\u4ea4\u4e92\u9700\u8981\u53ef\u9760\u63a5\u89e6\u529b\u4f30\u8ba1\uff0c\u4f46\u73b0\u6709\u4f20\u611f\u5668\u529b\u4f30\u8ba1\u65b9\u6cd5\u9762\u4e34\u5efa\u6a21\u8bef\u5dee\u3001\u590d\u6742\u6b8b\u4f59\u52a8\u529b\u5b66\u548c\u6469\u64e6\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faK-VARK\u65b9\u6cd5\uff0c\u5c06\u5173\u8282\u6b8b\u4f59\u626d\u77e9\u7684\u6838\u5316\u6982\u7387\u6a21\u578b\u96c6\u6210\u5230\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u6838\u5316\u8fd0\u52a8\u57fa\u5143\u6355\u83b7\u6b8b\u4f59\u626d\u77e9\u7684\u9884\u6d4b\u5747\u503c\u548c\u8f93\u5165\u76f8\u5173\u5f02\u65b9\u5dee\u65b9\u5dee\uff0c\u5e76\u91c7\u7528\u65b9\u5dee\u611f\u77e5\u865a\u62df\u6d4b\u91cf\u66f4\u65b0\u548c\u5728\u7ebf\u53d8\u5206\u8d1d\u53f6\u65af\u4f18\u5316\u9002\u5e94\u8fc7\u7a0b\u566a\u58f0\u3002", "result": "\u57286\u81ea\u7531\u5ea6\u534f\u4f5c\u673a\u68b0\u81c2\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cK-VARK\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u4f20\u611f\u5668\u529b\u4f30\u8ba1\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc720%\u7684\u5747\u65b9\u6839\u8bef\u5dee\u964d\u4f4e\uff0c\u80fd\u591f\u8fdb\u884c\u9c81\u68d2\u51c6\u786e\u7684\u5916\u90e8\u529b/\u626d\u77e9\u4f30\u8ba1\u3002", "conclusion": "K-VARK\u65b9\u6cd5\u901a\u8fc7\u6838\u5316\u6982\u7387\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u611f\u5668\u529b\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u629b\u5149\u3001\u88c5\u914d\u7b49\u9ad8\u7ea7\u4efb\u52a1\u3002"}}
{"id": "2512.12199", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12199", "abs": "https://arxiv.org/abs/2512.12199", "authors": ["Ercan Erkalkan", "Vedat Topuz", "Ay\u00e7a Ak"], "title": "Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms", "comment": "Conference paper in 17th International Scientific Studies Congress proceedings. Topic: thermal+RGB rule level fusion, RDP boundary simplification, leader follower guidance, sub 50ms embedded SoC, minimal communications for wildfire perimeter tracking. Thermal RGB Fusion for Micro-UAV", "summary": "This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5fae\u578b\u65e0\u4eba\u673a\u7fa4\u5728\u6709\u9650\u5e26\u5bbd\u6761\u4ef6\u4e0b\u5bf9\u91ce\u706b\u73af\u5883\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5468\u754c\u8ddf\u8e2a\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u70ed\u6210\u50cf\u548cRGB\u56fe\u50cf\u5904\u7406\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u3001\u5f62\u6001\u5b66\u4f18\u5316\u548c\u68af\u5ea6\u6ee4\u6ce2\u5b9e\u73b0\u8fb9\u754c\u68c0\u6d4b\uff0c\u5728\u5d4c\u5165\u5f0fSoC\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u8ddf\u8e2a\u3002", "motivation": "\u9488\u5bf9\u91ce\u706b\u5e94\u6025\u4fa6\u5bdf\u573a\u666f\u4e2d\u5fae\u578b\u65e0\u4eba\u673a\u7fa4\u5728\u6709\u9650\u5e26\u5bbd\u6761\u4ef6\u4e0b\u7684\u5468\u754c\u8ddf\u8e2a\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u4f4e\u5ef6\u8fdf\u7684\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u80fd\u591f\u5728GPS\u4fe1\u53f7\u9000\u5316\u65f6\u4fdd\u6301\u8f68\u8ff9\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "method": "1) \u70ed\u6210\u50cf\u5e27\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u548c\u5f62\u6001\u5b66\u7ec6\u5316\u751f\u6210\u7c97\u7565\u7684\u70ed\u533a\u57df\u63a9\u7801\uff1b2) RGB\u5e27\u63d0\u4f9b\u8fb9\u7f18\u7ebf\u7d22\uff0c\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u6ee4\u6ce2\u6291\u5236\u7eb9\u7406\u76f8\u5173\u7684\u8bef\u68c0\u6d4b\uff1b3) \u89c4\u5219\u7ea7\u5408\u5e76\u7b56\u7565\u9009\u62e9\u8fb9\u754c\u5019\u9009\u70b9\uff0c\u901a\u8fc7Ramer-Douglas-Peucker\u7b97\u6cd5\u7b80\u5316\uff1b4) \u7cfb\u7edf\u5305\u542b\u5468\u671f\u6027\u4fe1\u6807\u548c\u60ef\u6027\u53cd\u9988\u56de\u8def\uff0c\u5728GPS\u9000\u5316\u65f6\u4fdd\u6301\u8f68\u8ff9\u7a33\u5b9a\u6027\uff1b5) \u5728\u5d4c\u5165\u5f0fSoC\u5e73\u53f0\u4e0a\u901a\u8fc7\u9650\u5236\u6bcf\u5e27\u50cf\u7d20\u64cd\u4f5c\u548c\u9884\u8ba1\u7b97\u68af\u5ea6\u8868\u5b9e\u73b0\u4f4e\u4e8e50ms\u7684\u5ef6\u8fdf\u3002", "result": "\u5c0f\u89c4\u6a21\u4eff\u771f\u663e\u793a\uff1a\u4e0e\u7eaf\u8fb9\u7f18\u8ddf\u8e2a\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5e73\u5747\u8def\u5f84\u957f\u5ea6\u548c\u8fb9\u754c\u6296\u52a8\u51cf\u5c11\uff0c\u540c\u65f6\u901a\u8fc7\u4ea4\u96c6\u5408\u5e76\u5206\u6790\u4fdd\u6301\u73af\u5883\u8986\u76d6\uff1b\u7535\u6c60\u6d88\u8017\u548c\u8ba1\u7b97\u5229\u7528\u7387\u8bc1\u5b9e\u4e86\u5728\u6807\u51c6\u5fae\u578b\u5e73\u53f0\u4e0a\u5b9e\u73b010-15m/s\u524d\u5411\u8fd0\u52a8\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5feb\u901f\u73b0\u573a\u90e8\u7f72\uff0c\u4ec5\u9700\u9c81\u68d2\u7684\u4f20\u611f\u548c\u6700\u5c0f\u5316\u7684\u901a\u4fe1\uff0c\u9002\u7528\u4e8e\u5e94\u6025\u4fa6\u5bdf\u5e94\u7528\uff0c\u4e3a\u6709\u9650\u5e26\u5bbd\u6761\u4ef6\u4e0b\u5fae\u578b\u65e0\u4eba\u673a\u7fa4\u7684\u91ce\u706b\u5468\u754c\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13080", "abs": "https://arxiv.org/abs/2512.13080", "authors": ["Yicheng Feng", "Wanpeng Zhang", "Ye Wang", "Hao Luo", "Haoqi Yuan", "Sipeng Zheng", "Zongqing Lu"], "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos", "comment": null, "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u611f\u77e5\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5c062D\u89c6\u89c9\u89c2\u5bdf\u4e0e3D\u7a7a\u95f4\u63a8\u7406\u5bf9\u9f50\uff0c\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u57283D\u7269\u7406\u73af\u5883\u4e2d\u6267\u884c\u52a8\u4f5c\u65f6\u7684\u611f\u77e5\u4e0e\u52a8\u4f5c\u57fa\u7840\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5927\u591a\u4f9d\u8d562D\u89c6\u89c9\u8f93\u5165\u57283D\u7269\u7406\u73af\u5883\u4e2d\u6267\u884c\u52a8\u4f5c\uff0c\u5bfc\u81f4\u611f\u77e5\u4e0e\u52a8\u4f5c\u57fa\u7840\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5efa\u7acb\u89c6\u89c9\u7a7a\u95f4\u4e0e\u7269\u7406\u7a7a\u95f4\u4e4b\u95f4\u7684\u660e\u786e\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u611f\u77e5VLA\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u5927\u89c4\u6a21\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u63d0\u53d63D\u89c6\u89c9\u548c3D\u52a8\u4f5c\u6807\u6ce8\uff0c\u5f62\u6210\u65b0\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u5177\u4f53\u5b9e\u73b0\u4e3aVIPA-VLA\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5305\u542b3D\u89c6\u89c9\u7f16\u7801\u5668\u4ee5\u589e\u5f3a\u8bed\u4e49\u89c6\u89c9\u8868\u793a\u4e0e3D\u611f\u77e5\u7279\u5f81\u7684\u7ed3\u5408\u3002", "result": "VIPA-VLA\u5728\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e862D\u89c6\u89c9\u4e0e3D\u52a8\u4f5c\u4e4b\u95f4\u663e\u8457\u6539\u8fdb\u7684\u57fa\u7840\u5bf9\u9f50\uff0c\u4ea7\u751f\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5efa\u7acb\u89c6\u89c9\u7a7a\u95f4\u4e0e\u7269\u7406\u7a7a\u95f4\u7684\u660e\u786e\u5bf9\u9f50\uff0c\u7a7a\u95f4\u611f\u77e5VLA\u9884\u8bad\u7ec3\u8303\u5f0f\u80fd\u591f\u4f7f\u6a21\u578b\u5728\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e4b\u524d\u83b7\u5f973D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5f25\u5408\u611f\u77e5\u4e0e\u52a8\u4f5c\u57fa\u7840\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2512.12205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12205", "abs": "https://arxiv.org/abs/2512.12205", "authors": ["Peizheng Li", "Ioannis Mavromatis", "Ajith Sahadevan", "Tim Farnham", "Adnan Aijaz", "Aftab Khan"], "title": "A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection", "comment": "10 pages, 7 figures. Submitted to Data in Brief (Elsevier)", "summary": "We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.", "AI": {"tldr": "\u82f1\u56fd\u5e03\u91cc\u65af\u6258\u5c14\u57ce\u5e02\u8def\u706f\u5927\u89c4\u6a21\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u5305\u542b22\u4e2a\u56fa\u5b9a\u6444\u50cf\u5934\u57282021-2025\u5e74\u95f4\u6bcf\u5c0f\u65f6\u91c7\u96c6\u768452.6\u4e07\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6\u4e0d\u540c\u5149\u7167\u3001\u5929\u6c14\u548c\u5b63\u8282\u6761\u4ef6\uff0c\u7528\u4e8e\u7814\u7a76\u89c6\u89c9\u6f02\u79fb\u548c\u667a\u80fd\u57ce\u5e02MLOps\u7b56\u7565\u3002", "motivation": "\u4e3a\u667a\u80fd\u57ce\u5e02\u90e8\u7f72\u4e2d\u7684\u89c6\u89c9\u6f02\u79fb\u3001\u5f02\u5e38\u68c0\u6d4b\u548cMLOps\u7b56\u7565\u7814\u7a76\u63d0\u4f9b\u771f\u5b9e\u4e16\u754c\u3001\u7ec6\u7c92\u5ea6\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u957f\u671f\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u90e8\u7f72\u5c31\u7eea\u89c6\u89c9\u7cfb\u7edf\u7684\u8bc4\u4f30\u9700\u6c42\u3002", "method": "\u4f7f\u752822\u4e2a\u56fa\u5b9a\u89d2\u5ea6\u6444\u50cf\u5934\u6bcf\u5c0f\u65f6\u91c7\u96c6\u56fe\u50cf\uff0c\u63d0\u4f9b\u5305\u542b\u65f6\u95f4\u6233\u3001GPS\u5750\u6807\u548c\u8bbe\u5907\u6807\u8bc6\u7684\u4e30\u5bcc\u5143\u6570\u636e\u3002\u91c7\u7528\u57fa\u4e8e\u5377\u79ef\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CNN-VAEs\uff09\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u4e3a\u6bcf\u4e2a\u6444\u50cf\u5934\u8282\u70b9\u548c\u65e5/\u591c\u56fe\u50cf\u96c6\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9a\u4e49\u76f8\u5bf9\u8d28\u5fc3\u6f02\u79fb\u548c\u76f8\u5bf9\u91cd\u5efa\u8bef\u5dee\u4e24\u79cd\u6f02\u79fb\u5ea6\u91cf\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u8d85\u8fc7526,000\u5f20\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u7eb5\u5411\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u4ee5JPEG\u548cCSV\u683c\u5f0f\u516c\u5f00\uff0c\u652f\u6301\u8857\u9053\u7167\u660e\u76d1\u63a7\u3001\u5929\u6c14\u63a8\u65ad\u548c\u57ce\u5e02\u573a\u666f\u7406\u89e3\u7b49\u4e0b\u6e38\u5e94\u7528\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u957f\u671f\u6a21\u578b\u7a33\u5b9a\u6027\u3001\u6f02\u79fb\u611f\u77e5\u5b66\u4e60\u548c\u90e8\u7f72\u5c31\u7eea\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u73b0\u5b9e\u3001\u7ec6\u7c92\u5ea6\u7684\u57fa\u51c6\uff0c\u4fc3\u8fdb\u4e86\u667a\u80fd\u57ce\u5e02\u89c6\u89c9\u7cfb\u7edf\u7684\u53ef\u91cd\u590d\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2512.13090", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13090", "abs": "https://arxiv.org/abs/2512.13090", "authors": ["Jebeom Chae", "Junwoo Chang", "Seungho Yeom", "Yujin Kim", "Jongeun Choi"], "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion", "comment": null, "summary": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.", "AI": {"tldr": "LCHD\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408CLIP\u8bed\u4e49\u5148\u9a8c\u548c\u78b0\u649e\u907f\u514d\u6269\u6563\u6838\uff0c\u751f\u6210\u8bed\u8a00\u6761\u4ef6\u5316\u7684\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u591a\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u9700\u8981\u663e\u5f0f\u73af\u5883\u8868\u793a\u3001\u7f3a\u4e4f\u51e0\u4f55\u53ef\u8fbe\u6027\u63a8\u7406\u673a\u5236\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u8bed\u8a00\u6761\u4ef6\u5316\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u8bed\u8a00\u6761\u4ef6\u5316\u70ed\u542f\u53d1\u6269\u6563(LCHD)\u6846\u67b6\uff0c\u96c6\u6210CLIP\u8bed\u4e49\u5148\u9a8c\u4f5c\u4e3a\u78b0\u649e\u907f\u514d\u6269\u6563\u6838\u7684\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f7f\u89c4\u5212\u5668\u80fd\u5728\u53ef\u8fbe\u5de5\u4f5c\u7a7a\u95f4\u5185\u4e25\u683c\u89e3\u91ca\u8bed\u8a00\u6307\u4ee4\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u7684\u663e\u5f0f\u969c\u788d\u7269\u4fe1\u606f\u3002", "result": "\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u542f\u53d1\u7684\u5730\u56fe\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cLCHD\u5728\u6210\u529f\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u89c4\u5212\u5668\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u89c4\u5212\u5ef6\u8fdf\u3002", "conclusion": "LCHD\u901a\u8fc7\u8bed\u4e49\u5148\u9a8c\u4e0e\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\u7684\u7ed3\u5408\uff0c\u6709\u6548\u5904\u7406\u5206\u5e03\u5916\u573a\u666f\uff0c\u5f15\u5bfc\u673a\u5668\u4eba\u671d\u5411\u8bed\u4e49\u610f\u56fe\u5339\u914d\u7684\u53ef\u8fbe\u66ff\u4ee3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u6cdb\u5316\u7684\u591a\u673a\u5668\u4eba\u8bed\u8a00\u6761\u4ef6\u5316\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2512.12206", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12206", "abs": "https://arxiv.org/abs/2512.12206", "authors": ["Jeongjun Park", "Sunwook Hwang", "Hyeonho Noh", "Jin Mo Yang", "Hyun Jong Yang", "Saewoong Bahk"], "title": "ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB", "comment": null, "summary": "Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.\n  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.\n  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.", "AI": {"tldr": "\u63d0\u51faALERT\u6570\u636e\u96c6\u548cISA-ViT\u6846\u67b6\uff0c\u89e3\u51b3UWB\u96f7\u8fbe\u5728\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u96c6\u4e0d\u8db3\u548cViT\u8f93\u5165\u5c3a\u5bf8\u56fa\u5b9a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u5206\u5fc3\u9a7e\u9a76\u5bfc\u81f4\u5168\u7403\u81f4\u547d\u4e8b\u6545\uff0cUWB\u96f7\u8fbe\u5177\u6709\u6297\u5e72\u6270\u3001\u4f4e\u529f\u8017\u548c\u9690\u79c1\u4fdd\u62a4\u4f18\u52bf\uff0c\u4f46\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754cUWB\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u56fa\u5b9a\u8f93\u5165\u5c3a\u5bf8\u7684Vision Transformers\u96be\u4ee5\u9002\u5e94\u975e\u6807\u51c6\u7ef4\u5ea6\u7684UWB\u96f7\u8fbe\u6570\u636e\u3002", "method": "1) \u63d0\u51faALERT\u6570\u636e\u96c6\uff1a\u5305\u542b10,220\u4e2a\u771f\u5b9e\u9a7e\u9a76\u6761\u4ef6\u4e0b\u7684\u96f7\u8fbe\u6837\u672c\uff0c\u6db5\u76d67\u79cd\u5206\u5fc3\u9a7e\u9a76\u884c\u4e3a\uff1b2) \u63d0\u51faISA-ViT\u6846\u67b6\uff1a\u901a\u8fc7\u8c03\u6574\u8865\u4e01\u914d\u7f6e\u548c\u5229\u7528\u9884\u8bad\u7ec3\u4f4d\u7f6e\u5d4c\u5165\u5411\u91cf\uff0c\u5728\u8c03\u6574UWB\u6570\u636e\u5c3a\u5bf8\u65f6\u4fdd\u7559\u591a\u666e\u52d2\u9891\u79fb\u548c\u76f8\u4f4d\u7279\u5f81\u7b49\u96f7\u8fbe\u7279\u5b9a\u4fe1\u606f\uff1b3) \u91c7\u7528\u57df\u878d\u5408\u7b56\u7565\uff1a\u7ed3\u5408\u8ddd\u79bb\u57df\u548c\u9891\u57df\u7279\u5f81\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "result": "ISA-ViT\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8eViT\u7684UWB-DAR\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u534722.68%\u3002ALERT\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\uff0c\u8f93\u5165\u5c3a\u5bf8\u65e0\u5173\u7b56\u7565\u8be6\u7ec6\u516c\u5f00\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9bALERT\u6570\u636e\u96c6\u548cISA-ViT\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\uff0c\u6709\u52a9\u4e8e\u5b9e\u9645\u90e8\u7f72\u5e94\u7528\u3002"}}
{"id": "2512.13093", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13093", "abs": "https://arxiv.org/abs/2512.13093", "authors": ["Mingqi Yuan", "Tao Yu", "Haolin Song", "Bo Li", "Xin Jin", "Hua Chen", "Wenjun Zeng"], "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations", "comment": "13 pages, 12 figures", "summary": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.", "AI": {"tldr": "PvP\u6846\u67b6\u901a\u8fc7\u672c\u4f53\u611f\u77e5\u4e0e\u7279\u6743\u72b6\u6001\u7684\u5bf9\u6bd4\u5b66\u4e60\uff0c\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\uff0c\u65e0\u9700\u624b\u5de5\u6570\u636e\u589e\u5f3a", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u9700\u8981\u9ad8\u6548\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u7531\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u590d\u6742\u7684\u52a8\u529b\u5b66\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027", "method": "\u63d0\u51faPvP\uff08\u672c\u4f53\u611f\u77e5-\u7279\u6743\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff09\uff0c\u5229\u7528\u672c\u4f53\u611f\u77e5\u72b6\u6001\u4e0e\u7279\u6743\u72b6\u6001\u7684\u5185\u5728\u4e92\u8865\u6027\uff0c\u5b66\u4e60\u7d27\u51d1\u4e14\u4efb\u52a1\u76f8\u5173\u7684\u6f5c\u5728\u8868\u793a\uff1b\u540c\u65f6\u5f00\u53d1SRL4Humanoid\u6846\u67b6\uff0c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u9ad8\u8d28\u91cf\u5b9e\u73b0", "result": "\u5728LimX Oli\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u901f\u5ea6\u8ddf\u8e2a\u548c\u8fd0\u52a8\u6a21\u4eff\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPvP\u76f8\u6bd4\u57fa\u7ebfSRL\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd", "conclusion": "PvP\u6846\u67b6\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u8fdb\u4e00\u6b65\u63d0\u4f9b\u4e86\u5c06SRL\u4e0eRL\u96c6\u6210\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u7684\u5b9e\u7528\u89c1\u89e3"}}
{"id": "2512.13094", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13094", "abs": "https://arxiv.org/abs/2512.13094", "authors": ["Xiang Li", "Gang Liu", "Weitao Zhou", "Hongyi Zhu", "Zhong Cao"], "title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation", "comment": null, "summary": "Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.", "AI": {"tldr": "\u63d0\u51faSequence of Experts (SoE)\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u5e8f\u4ea4\u66ff\u7b56\u7565\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u95ed\u73af\u6027\u80fd\uff0c\u65e0\u9700\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u6216\u6570\u636e\u9700\u6c42", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u867d\u7136\u80fd\u5728\u5f00\u73af\u73af\u5883\u4e0b\u5339\u914d\u4e13\u5bb6\u884c\u4e3a\uff0c\u4f46\u5728\u95ed\u73af\u73af\u5883\u4e2d\u7531\u4e8e\u5fae\u5c0f\u8bef\u5dee\u7684\u7d2f\u79ef\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u65f6\u95f4\u70b9\u7684\u72b6\u6001\u7ea7\u9c81\u68d2\u6027\uff0c\u800c\u81ea\u52a8\u9a7e\u9a76\u662f\u8fde\u7eed\u65f6\u95f4\u8fc7\u7a0b\uff0c\u5229\u7528\u65f6\u95f4\u5c3a\u5ea6\u63d0\u5347\u9c81\u68d2\u6027\u53ef\u80fd\u63d0\u4f9b\u65b0\u7684\u89e3\u51b3\u89c6\u89d2\u3002", "method": "\u63d0\u51faSequence of Experts (SoE)\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65f6\u5e8f\u4ea4\u66ff\u7b56\u7565\uff0c\u901a\u8fc7\u65f6\u95f4\u5c3a\u5ea6\u7684\u4ea4\u66ff\u6765\u589e\u5f3a\u95ed\u73af\u6027\u80fd\uff0c\u4e0d\u9700\u8981\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u6216\u6570\u636e\u9700\u6c42\u3002", "result": "\u5728nuPlan\u5927\u89c4\u6a21\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSoE\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "SoE\u6a21\u5757\u53ef\u80fd\u4e3a\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u63d0\u4f9b\u5173\u952e\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u652f\u6301\uff0c\u901a\u8fc7\u65f6\u95f4\u5c3a\u5ea6\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u5728\u95ed\u73af\u73af\u5883\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002"}}
{"id": "2512.12209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12209", "abs": "https://arxiv.org/abs/2512.12209", "authors": ["Zahra Dehghanian", "Morteza Abolghasemi", "Hamid Beigy", "Hamid R. Rabiee"], "title": "CineLOG: A Training Free Approach for Cinematic Long Video Generation", "comment": null, "summary": "Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.", "AI": {"tldr": "CineLOG\uff1a\u4e00\u4e2a\u5305\u542b5000\u4e2a\u9ad8\u8d28\u91cf\u89c6\u9891\u7247\u6bb5\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u53ef\u63a7\u89c6\u9891\u5408\u6210\uff0c\u6bcf\u4e2a\u7247\u6bb5\u90fd\u6807\u6ce8\u4e86\u8be6\u7ec6\u573a\u666f\u63cf\u8ff0\u3001\u57fa\u4e8e\u6807\u51c6\u7535\u5f71\u5206\u7c7b\u7684\u76f8\u673a\u6307\u4ee4\u548c\u7c7b\u578b\u6807\u7b7e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u6570\u636e\u4e0d\u5e73\u8861\u3001\u566a\u58f0\u6807\u7b7e\u548c\u6a21\u62df\u5230\u771f\u5b9e\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u53ef\u63a7\u89c6\u9891\u5408\u6210\u6a21\u578b\u96be\u4ee5\u5b9e\u73b0\u8d85\u8d8a\u6587\u672c\u63d0\u793a\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u7279\u522b\u662f\u5728\u76f8\u673a\u8f68\u8ff9\u548c\u7535\u5f71\u7c7b\u578b\u7b49\u7535\u5f71\u5c5e\u6027\u65b9\u9762\u3002\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u4e0d\u5e73\u8861\u3001\u566a\u58f0\u6807\u7b7e\u6216\u663e\u8457\u7684\u6a21\u62df\u5230\u771f\u5b9e\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86CineLOG\u6570\u636e\u96c6\uff0c\u5305\u542b5000\u4e2a\u9ad8\u8d28\u91cf\u3001\u5e73\u8861\u3001\u672a\u526a\u8f91\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u6bcf\u4e2a\u7247\u6bb5\u90fd\u6709\u8be6\u7ec6\u573a\u666f\u63cf\u8ff0\u3001\u57fa\u4e8e\u6807\u51c6\u7535\u5f71\u5206\u7c7b\u7684\u76f8\u673a\u6307\u4ee4\u548c\u7c7b\u578b\u6807\u7b7e\u3002\u5f00\u53d1\u4e86\u65b0\u7684\u6d41\u6c34\u7ebf\uff0c\u5c06\u590d\u6742\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u89e3\u8026\u4e3a\u56db\u4e2a\u66f4\u7b80\u5355\u7684\u9636\u6bb5\uff0c\u5e76\u5f15\u5165\u4e86\u8f68\u8ff9\u5f15\u5bfc\u8fc7\u6e21\u6a21\u5757\u6765\u751f\u6210\u5e73\u6ed1\u7684\u65f6\u7a7a\u63d2\u503c\u3002", "result": "\u5e7f\u6cdb\u7684\u4eba\u7c7b\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6d41\u6c34\u7ebf\u5728\u9075\u5faa\u7279\u5b9a\u76f8\u673a\u548c\u5267\u672c\u6307\u4ee4\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e13\u4e1a\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "CineLOG\u6570\u636e\u96c6\u548c\u65b0\u6d41\u6c34\u7ebf\u4e3a\u53ef\u63a7\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u5e73\u8861\u7684\u6570\u636e\u8d44\u6e90\u548c\u6709\u6548\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u63a7\u5236\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.13100", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13100", "abs": "https://arxiv.org/abs/2512.13100", "authors": ["Guanhua Ji", "Harsha Polavaram", "Lawrence Yunliang Chen", "Sandeep Bajamahal", "Zehan Ma", "Simeon Adebola", "Chenfeng Xu", "Ken Goldberg"], "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning", "comment": null, "summary": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot--scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $\u03c0_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot--gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.", "AI": {"tldr": "OXE-AugE\u6570\u636e\u96c6\u901a\u8fc7\u673a\u5668\u4eba\u589e\u5f3a\u6280\u672f\u5c06OXE\u6570\u636e\u96c6\u89c4\u6a21\u6269\u5927\u4e09\u500d\uff0c\u5305\u542b\u8d85\u8fc7440\u4e07\u8f68\u8ff9\u548c9\u79cd\u673a\u5668\u4eba\u6784\u578b\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6784\u578b\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u96c6\uff08\u5982OXE\uff09\u5b58\u5728\u4e25\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u524d\u56db\u79cd\u673a\u5668\u4eba\u7c7b\u578b\u5360\u771f\u5b9e\u6570\u636e\u768485%\u4ee5\u4e0a\uff0c\u8fd9\u5bfc\u81f4\u7b56\u7565\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u7279\u5b9a\u673a\u5668\u4eba-\u573a\u666f\u7ec4\u5408\uff0c\u9650\u5236\u4e86\u8de8\u6784\u578b\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51faAugE-Toolkit\u53ef\u6269\u5c55\u673a\u5668\u4eba\u589e\u5f3a\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u4e3aOXE\u6570\u636e\u96c6\u589e\u52a09\u79cd\u4e0d\u540c\u7684\u673a\u5668\u4eba\u6784\u578b\uff0c\u521b\u5efaOXE-AugE\u9ad8\u8d28\u91cf\u5f00\u6e90\u6570\u636e\u96c6", "result": "OXE-AugE\u5305\u542b\u8d85\u8fc7440\u4e07\u8f68\u8ff9\uff0c\u662f\u539f\u59cbOXE\u7684\u4e09\u500d\u591a\uff1b\u589e\u5f3a\u6570\u636e\u4e0d\u4ec5\u63d0\u5347\u589e\u5f3a\u673a\u5668\u4eba\u7684\u6027\u80fd\uff0c\u8fd8\u80fd\u6539\u5584\u672a\u89c1\u673a\u5668\u4eba\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u4f7fOpenVLA\u548c\u03c0_0\u7b49\u7b56\u7565\u5728\u672a\u89c1\u673a\u5668\u4eba-\u5939\u722a\u7ec4\u5408\u4e0a\u7684\u6210\u529f\u7387\u63d0\u534724-45%", "conclusion": "\u901a\u8fc7\u673a\u5668\u4eba\u6570\u636e\u589e\u5f3a\u6269\u5c55\u6570\u636e\u96c6\u591a\u6837\u6027\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8de8\u6784\u578b\u5b66\u4e60\u6548\u679c\uff0c\u4e3a\u8bad\u7ec3\u66f4\u901a\u7528\u7684\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.12218", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12218", "abs": "https://arxiv.org/abs/2512.12218", "authors": ["Rheeya Uppaal", "Phu Mon Htut", "Min Bai", "Nikolaos Pappas", "Zheng Qi"], "title": "Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking", "comment": "Preprint", "summary": "Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u94fe\u89c6\u89c9\u5fe0\u5b9e\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u81ea\u53cd\u601d\u7a0b\u5e8f\u6765\u68c0\u6d4b\u548c\u4fee\u590d\u4e0d\u5fe0\u5b9e\u7684\u611f\u77e5\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u663e\u5f0f\u7684\u601d\u7ef4\u94fe\uff0c\u4f46\u5b58\u5728\u65b0\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u89c6\u89c9\u4e0a\u4e0d\u5fe0\u5b9e\u7684\u4e2d\u95f4\u6b65\u9aa4\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u6216\u8005\u63a8\u7406\u5fe0\u5b9e\u4f46\u6700\u7ec8\u9884\u6d4b\u5931\u8d25\u3002\u4ec5\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u7684\u6807\u51c6\u65b9\u6cd5\u65e0\u6cd5\u533a\u5206\u8fd9\u4e9b\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u548c\u53c2\u8003\u7684\u6846\u67b6\uff1a1) \u5c06\u63a8\u7406\u94fe\u5206\u89e3\u4e3a\u611f\u77e5\u6b65\u9aa4\u548c\u63a8\u7406\u6b65\u9aa4\uff1b2) \u4f7f\u7528\u73b0\u6210\u7684VLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8fdb\u884c\u6b65\u9aa4\u7ea7\u5fe0\u5b9e\u6027\u8bc4\u4f30\uff1b3) \u901a\u8fc7\u4eba\u7c7b\u5143\u8bc4\u4f30\u9a8c\u8bc1\u8be5\u65b9\u6cd5\uff1b4) \u57fa\u4e8e\u8be5\u6307\u6807\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u81ea\u53cd\u601d\u7a0b\u5e8f\uff0c\u68c0\u6d4b\u5e76\u5c40\u90e8\u91cd\u65b0\u751f\u6210\u4e0d\u5fe0\u5b9e\u7684\u611f\u77e5\u6b65\u9aa4\u3002", "result": "\u5728\u591a\u4e2a\u7ecf\u8fc7\u63a8\u7406\u8bad\u7ec3\u7684VLM\u548c\u611f\u77e5\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u4e0d\u5fe0\u5b9e\u611f\u77e5\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u89c6\u89c9\u63a8\u7406\u94fe\u7684\u5fe0\u5b9e\u6027\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u63d0\u51fa\u7684\u8bad\u7ec3\u548c\u53c2\u8003\u65e0\u5173\u7684\u8bc4\u4f30\u6846\u67b6\u4ee5\u53ca\u81ea\u53cd\u601d\u7a0b\u5e8f\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u591a\u6a21\u6001\u63a8\u7406\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2512.13153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13153", "abs": "https://arxiv.org/abs/2512.13153", "authors": ["Ruiqi Yu", "Qianshi Wang", "Hongyi Li", "Zheng Jun", "Zhicheng Wang", "Jun Wu", "Qiuguo Zhu"], "title": "START: Traversing Sparse Footholds with Terrain Reconstruction", "comment": null, "summary": "Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.", "AI": {"tldr": "START\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u5728\u9ad8\u5ea6\u7a00\u758f\u548c\u968f\u673a\u7684\u7acb\u8db3\u70b9\u4e0a\u5b9e\u73b0\u654f\u6377\u7a33\u5b9a\u7684\u8fd0\u52a8\uff0c\u4ec5\u4f7f\u7528\u4f4e\u6210\u672c\u673a\u8f7d\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u6765\u91cd\u5efa\u5c40\u90e8\u5730\u5f62\u9ad8\u5ea6\u56fe\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u6a21\u578b\u7684\u5206\u5c42\u63a7\u5236\u5668\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u884c\u4e3a\u4fdd\u5b88\uff1b\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u566a\u58f0\u5927\u7684\u9ad8\u5ea6\u56fe\uff0c\u8981\u4e48\u4ece\u6df1\u5ea6\u56fe\u50cf\u9690\u5f0f\u63a8\u65ad\u5730\u5f62\u7279\u5f81\uff0c\u7f3a\u4e4f\u51c6\u786e\u7684\u51e0\u4f55\u7ebf\u7d22\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u548c\u6b65\u6001\u50f5\u786c\u3002", "method": "\u63d0\u51faSTART\u5355\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4f4e\u6210\u672c\u673a\u8f7d\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u51c6\u786e\u91cd\u5efa\u5c40\u90e8\u5730\u5f62\u9ad8\u5ea6\u56fe\uff0c\u63d0\u4f9b\u660e\u786e\u7684\u4e2d\u95f4\u8868\u793a\u6765\u4f20\u8fbe\u7a00\u758f\u7acb\u8db3\u70b9\u533a\u57df\u7684\u5173\u952e\u7279\u5f81\uff0c\u652f\u6301\u5168\u9762\u7684\u73af\u5883\u7406\u89e3\u548c\u7cbe\u786e\u7684\u5730\u5f62\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSTART\u5b9e\u73b0\u4e86\u5728\u591a\u6837\u5316\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u3001\u7cbe\u786e\u7684\u7acb\u8db3\u70b9\u653e\u7f6e\u548c\u9c81\u68d2\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "START\u901a\u8fc7\u660e\u786e\u7684\u4e2d\u95f4\u5730\u5f62\u8868\u793a\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a2\u7d22\u6210\u672c\u5e76\u52a0\u901f\u4e86\u6280\u80fd\u83b7\u53d6\uff0c\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u5728\u7a00\u758f\u7acb\u8db3\u70b9\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12219", "abs": "https://arxiv.org/abs/2512.12219", "authors": ["Zhi Chen", "Jingcai Guo", "Taotao Cai", "Yuxiang Cai"], "title": "Fine-Grained Zero-Shot Learning with Attribute-Centric Representations", "comment": "Preprint", "summary": "Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.", "AI": {"tldr": "\u63d0\u51faACR\u6846\u67b6\uff0c\u901a\u8fc7\u5c5e\u6027\u89e3\u7f20\u5b66\u4e60\u6765\u89e3\u51b3\u96f6\u6837\u672c\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4e2d\u7684\u5c5e\u6027\u7ea0\u7f20\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u7ec6\u7c92\u5ea6\u7c7b\u522b\u8bc6\u522b\u9700\u8981\u533a\u5206\u7ec6\u5fae\u89c6\u89c9\u5dee\u5f02\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u989c\u8272\u3001\u5f62\u72b6\u3001\u7eb9\u7406\u7b49\u4e0d\u540c\u5c5e\u6027\u538b\u7f29\u5230\u5355\u4e00\u89c6\u89c9\u5d4c\u5165\u4e2d\uff0c\u5bfc\u81f4\u5c5e\u6027\u7ea0\u7f20\u548c\u5e72\u6270\uff0c\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u5df2\u6df7\u5408\u7684\u8868\u5f81\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5c5e\u6027\u4e2d\u5fc3\u8868\u793a(ACR)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6df7\u5408\u4e13\u5bb6\u7ec4\u4ef6\uff1aMoPE(\u8865\u4e01\u4e13\u5bb6\u6df7\u5408)\u548cMoAE(\u5c5e\u6027\u4e13\u5bb6\u6df7\u5408)\u3002MoPE\u901a\u8fc7\u53cc\u7ea7\u8def\u7531\u673a\u5236\u5c06\u56fe\u50cf\u8865\u4e01\u5206\u6d3e\u7ed9\u4e13\u95e8\u4e13\u5bb6\u5904\u7406\u8fde\u8d2f\u5c5e\u6027\u65cf\uff0cMoAE\u5934\u5c06\u4e13\u5bb6\u7cbe\u70bc\u7279\u5f81\u6295\u5f71\u4e3a\u7a00\u758f\u3001\u90e8\u5206\u611f\u77e5\u7684\u5c5e\u6027\u6620\u5c04\u3002", "result": "\u5728CUB\u3001AwA2\u548cSUN\u7b49\u96f6\u6837\u672c\u5b66\u4e60\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cACR\u6846\u67b6\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5728\u8868\u793a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u65bd\u52a0\u5c5e\u6027\u89e3\u7f20\uff0cACR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5c5e\u6027\u7ea0\u7f20\u95ee\u9898\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u96f6\u6837\u672c\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13170", "categories": ["cs.RO", "cs.LG", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13170", "abs": "https://arxiv.org/abs/2512.13170", "authors": ["Deepak Ingole", "Valentin Bhend", "Shiva Ganesh Murali", "Oliver Dobrich", "Alisa Rupenayan"], "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks", "comment": null, "summary": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fed\u4ee3\u5b66\u4e60\u7684NMPC\u6743\u91cd\u77e9\u9635\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7ea7\u6027\u80fd\u53cd\u9988\u81ea\u9002\u5e94\u8c03\u6574Q\u548cR\u6743\u91cd\uff0c\u5728\u91cd\u590d\u6027\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u5230\u8fd1\u6700\u4f18\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u5236\u9020\u8fc7\u7a0b\u5e38\u53d7\u73af\u5883\u6f02\u79fb\u548c\u7cfb\u7edf\u78e8\u635f\u5f71\u54cd\uff0c\u5373\u4f7f\u5728\u91cd\u590d\u64cd\u4f5c\u4e2d\u4e5f\u9700\u8981\u91cd\u65b0\u8c03\u6574\u63a7\u5236\u53c2\u6570\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u8d1d\u53f6\u65af\u4f18\u5316\u9700\u8981\u5927\u91cf\u79bb\u7ebf\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5728\u7ebf\u81ea\u9002\u5e94\u80fd\u529b\u3002", "method": "\u53d7\u8303\u6570\u6700\u4f18\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u542f\u53d1\uff0c\u6784\u5efa\u7ecf\u9a8c\u7075\u654f\u5ea6\u77e9\u9635\u800c\u975e\u901a\u8fc7NMPC\u6c42\u89e3\u5668\u6c42\u5bfc\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u6743\u91cd\u66f4\u65b0\u3002\u901a\u8fc7\u4efb\u52a1\u91cd\u590d\u8fed\u4ee3\u8c03\u6574NMPC\u6743\u91cd\u77e9\u9635Q\u548cR\uff0c\u6700\u5c0f\u5316\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u63a7\u5236\u52aa\u529b\u548c\u9971\u548c\u7b49\u5173\u952e\u6027\u80fd\u6307\u6807\u3002", "result": "\u5728UR10e\u673a\u5668\u4eba\u78b3\u7ea4\u7ef4\u7f20\u7ed5\u4efb\u52a1\u4eff\u771f\u9a8c\u8bc1\u4e2d\uff0c\u4ec5\u97004\u6b21\u5728\u7ebf\u91cd\u590d\u5373\u6536\u655b\u5230\u8fd1\u6700\u4f18\u8ddf\u8e2a\u6027\u80fd\uff08RMSE\u5728\u79bb\u7ebf\u8d1d\u53f6\u65af\u4f18\u5316\u76840.3%\u5185\uff09\uff0c\u800cBO\u7b97\u6cd5\u9700\u8981100\u6b21\u79bb\u7ebf\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u91cd\u590d\u6027\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u81ea\u9002\u5e94NMPC\u8c03\u4f18\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u7cbe\u5fc3\u4f18\u5316\u63a7\u5236\u5668\u7684\u7cbe\u5ea6\u548c\u5728\u7ebf\u81ea\u9002\u5e94\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2512.12220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12220", "abs": "https://arxiv.org/abs/2512.12220", "authors": ["Minheng Ni", "Zhengyuan Yang", "Yaowen Zhang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Zhendong Wang", "Xiaofei Wang", "Shujie Liu", "Lei Zhang", "Wangmeng Zuo", "Lijuan Wang"], "title": "ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation", "comment": null, "summary": "We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86ProImage-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e13\u4e1a\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u79d1\u5b66\u63d2\u56fe\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u79d1\u5b66\u51c6\u786e\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u53cd\u9988\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4e13\u4e1a\u56fe\u50cf\u751f\u6210\u9700\u8981\u4ece\u6280\u672f\u63cf\u8ff0\u4e2d\u5408\u6210\u4fe1\u606f\u5bc6\u96c6\u3001\u79d1\u5b66\u7cbe\u786e\u7684\u63d2\u56fe\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u56fe\u7247\u3002\u73b0\u6709\u6a21\u578b\u5728\u79d1\u5b66\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u91cf\u5316\u8bc4\u4f30\u5de5\u5177\u6765\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "1. \u6784\u5efaProImage-Bench\u57fa\u51c6\uff1a\u6536\u96c6654\u4e2a\u771f\u5b9e\u6559\u79d1\u4e66\u548c\u6280\u672f\u62a5\u544a\u4e2d\u7684\u56fe\u8868\uff0c\u521b\u5efa\u8be6\u7ec6\u56fe\u50cf\u6307\u4ee4\uff1b2. \u5efa\u7acb\u5206\u5c42\u8bc4\u5206\u6807\u51c6\uff1a\u5c06\u6b63\u786e\u6027\u5206\u89e3\u4e3a6,076\u4e2a\u6807\u51c6\u548c44,131\u4e2a\u4e8c\u5143\u68c0\u67e5\uff1b3. \u4f7f\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4ece\u6587\u672c\u548c\u53c2\u8003\u56fe\u4e2d\u63a8\u5bfc\u8bc4\u5206\u6807\u51c6\uff1b4. \u5f00\u53d1\u81ea\u52a8\u5316LMM\u8bc4\u4f30\u5668\uff0c\u91c7\u7528\u539f\u5219\u6027\u60e9\u7f5a\u65b9\u6848\u5c06\u5b50\u95ee\u9898\u7ed3\u679c\u805a\u5408\u6210\u53ef\u89e3\u91ca\u7684\u6807\u51c6\u5206\u6570\u3002", "result": "1. \u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5c3d\u7ba1\u5728\u5f00\u653e\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6700\u4f73\u57fa\u7840\u6a21\u578b\u4ec5\u8fbe\u52300.791\u7684\u8bc4\u5206\u51c6\u786e\u7387\u548c0.553\u7684\u6807\u51c6\u5206\u6570\uff1b2. \u63ed\u793a\u4e86\u5728\u7ec6\u7c92\u5ea6\u79d1\u5b66\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1b3. \u53cd\u9988\u673a\u5236\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u5931\u8d25\u7684\u68c0\u67e5\u53cd\u9988\u7ed9\u7f16\u8f91\u6a21\u578b\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u53ef\u5c06\u8bc4\u5206\u51c6\u786e\u7387\u4ece0.653\u63d0\u5347\u52300.865\uff0c\u6807\u51c6\u5206\u6570\u4ece0.388\u63d0\u5347\u52300.697\u3002", "conclusion": "ProImage-Bench\u4e3a\u4e13\u4e1a\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u5e76\u4e3a\u6539\u8fdb\u89c4\u8303\u5fe0\u5b9e\u5ea6\u7684\u79d1\u5b66\u63d2\u56fe\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u4fe1\u53f7\u3002\u8be5\u57fa\u51c6\u4e0d\u4ec5\u80fd\u591f\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u80fd\u901a\u8fc7\u53cd\u9988\u673a\u5236\u6307\u5bfc\u6a21\u578b\u6539\u8fdb\uff0c\u63a8\u52a8\u79d1\u5b66\u63d2\u56fe\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.13183", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13183", "abs": "https://arxiv.org/abs/2512.13183", "authors": ["Alfredo Gonz\u00e1lez-Calvin", "Juan F. Jim\u00e9nez", "H\u00e9ctor Garc\u00eda de Marina"], "title": "Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification", "comment": null, "summary": "Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u78e8\u5149\u6b63\u5219\u5316\u5904\u7406\u4e0d\u53ef\u5fae\u8def\u5f84\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u53ef\u5fae\u8def\u5f84\u4ee5\u517c\u5bb9\u6807\u51c6\u8f68\u8ff9\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u540c\u65f6\u4fdd\u8bc1\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8\u673a\u5668\u4eba\u8def\u5f84\u8ddf\u8e2a\u7b97\u6cd5\u901a\u5e38\u8981\u6c42\u8def\u5f84\u81f3\u5c11\u4e8c\u9636\u8fde\u7eed\u53ef\u5fae\uff0c\u4f46\u5b9e\u9645\u4efb\u52a1\u4e2d\u5e38\u4f7f\u7528\u5206\u6bb5\u51fd\u6570\u7b49\u4e0d\u53ef\u5fae\u8def\u5f84\u3002\u4f20\u7edf\u5e73\u6ed1\u65b9\u6cd5\u8981\u4e48\u4ea7\u751f\u590d\u6742\u8f68\u8ff9\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u78e8\u5149\uff08mollification\uff09\u65b9\u6cd5\u6b63\u5219\u5316\u4e0d\u53ef\u5fae\u51fd\u6570\uff0c\u901a\u8fc7\u8fd1\u4f3c\u751f\u6210\u53ef\u5fae\u8def\u5f84\uff0c\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u66f2\u7387\u7ea6\u675f\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7531\u822a\u70b9\u5e8f\u5217\u8fde\u63a5\u5f62\u6210\u7684\u8def\u5f84\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u4efb\u610f\u7cbe\u5ea6\u903c\u8fd1\u539f\u59cb\u8def\u5f84\u7684\u53ef\u5fae\u8def\u5f84\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u53ef\u5728\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u65f6\u5b9e\u73b0\uff0c\u5e76\u4e0e\u6807\u51c6\u8f68\u8ff9\u8ddf\u8e2a\u548c\u8def\u5f84\u8ddf\u968f\u7b97\u6cd5\u517c\u5bb9\u3002", "conclusion": "\u63d0\u51fa\u7684\u78e8\u5149\u6b63\u5219\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u53ef\u5fae\u8def\u5f84\u5728\u79fb\u52a8\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u8def\u5f84\u63cf\u8ff0\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12222", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12222", "abs": "https://arxiv.org/abs/2512.12222", "authors": ["Nathalie Alexander", "Arnaud Gucciardi", "Umberto Michelucci"], "title": "Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs", "comment": null, "summary": "Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86SynthSeg\u548cSamSeg\u4e24\u79cd\u65b9\u6cd5\u5728\u5a74\u513f\u8111MRI\u5206\u5272\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u53d1\u73b0SynthSeg\u5728\u6240\u6709\u8d28\u91cf\u6307\u6807\u4e0a\u5747\u4f18\u4e8eSamSeg\uff0c\u80fd\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u5a74\u513f\u8111MRI\u7684\u51c6\u786e\u5206\u5272\u5bf9\u4e8e\u91cf\u5316\u7ed3\u6784\u53d1\u80b2\u53d8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u9ad3\u9798\u5316\u8fc7\u7a0b\u4e2d\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\u964d\u4f4e\uff0c\u81ea\u52a8\u5206\u5272\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u5206\u5272\u65b9\u6cd5\u5bf9\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Baby Open Brains\u6570\u636e\u96c6\uff0871\u4e2a\u626b\u63cf\uff0c1-9\u4e2a\u6708\uff09\uff0c\u6bd4\u8f83SynthSeg\u548cSamSeg\u4e24\u79cd\u5206\u5272\u65b9\u6cd5\u3002\u901a\u8fc7Dice\u7cfb\u6570\u3001IoU\u300195% Hausdorff\u8ddd\u79bb\u548c\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\u7b49\u6307\u6807\u8bc4\u4f30\u5206\u5272\u51c6\u786e\u6027\uff0c\u5e76\u5206\u6790\u5bf9\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "result": "SynthSeg\u5728\u6240\u6709\u8d28\u91cf\u6307\u6807\u4e0a\u5747\u4f18\u4e8eSamSeg\uff08\u4e3b\u8981\u533a\u57df\u5e73\u5747Dice > 0.8\uff09\uff0c\u4f53\u79ef\u4f30\u8ba1\u4e0e\u624b\u52a8\u53c2\u8003\u63a5\u8fd1\uff08\u5e73\u5747+4%\uff09\u3002SamSeg\u7cfb\u7edf\u6027\u5730\u9ad8\u4f30\u8111\u5ba4\u548c\u5168\u8111\u4f53\u79ef\uff08\u5e73\u5747+76%\uff09\u3002\u5206\u5272\u51c6\u786e\u6027\u968f\u5e74\u9f84\u589e\u957f\u800c\u63d0\u9ad8\uff0c\u4e0e\u9ad3\u9798\u5316\u8fc7\u7a0b\u4e2d\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\u589e\u52a0\u4e00\u81f4\u3002\u5206\u5f62\u7ef4\u5ea6\u5206\u6790\u663e\u793aSynthSeg\u4e0e\u4e13\u5bb6\u5206\u5272\u5b58\u5728\u663e\u8457\u533a\u57df\u5dee\u5f02\uff0c\u5206\u5272\u76f8\u5173\u7684FD\u53d8\u5f02\u6027\u8d85\u8fc7\u4e86\u5927\u591a\u6570\u53d1\u80b2\u961f\u5217\u62a5\u544a\u7684\u7ec4\u95f4\u5dee\u5f02\u3002", "conclusion": "SynthSeg\u4e3a\u513f\u79d1MRI\u63d0\u4f9b\u4e86\u6700\u53ef\u9760\u7684\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u7ed3\u679c\uff0c\u4f46\u7531\u4e8e\u5206\u5272\u76f8\u5173\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5bf9\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u7684\u5fae\u5c0f\u5f62\u6001\u5b66\u5dee\u5f02\u5e94\u8c28\u614e\u89e3\u91ca\u3002\u5206\u5272\u504f\u5dee\u76f4\u63a5\u5f71\u54cdFD\u4f30\u8ba1\uff0c\u5206\u5272\u51c6\u786e\u6027\u968f\u5a74\u513f\u5e74\u9f84\u589e\u957f\u800c\u6539\u5584\u3002"}}
{"id": "2512.13198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13198", "abs": "https://arxiv.org/abs/2512.13198", "authors": ["Hyun-Gi Lee", "Jaekyeong Han", "Minjun Kwon", "Hyeonuk Kwon", "Jooha Park", "Hoe Jin Ha", "Dong-Hwa Seo"], "title": "ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation", "comment": null, "summary": "As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 \u03a9 in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.", "AI": {"tldr": "\u5f00\u53d1\u4e86ALBATROSS\u81ea\u52a8\u5316\u9502\u79bb\u5b50\u7535\u6c60\u6d4b\u8bd5\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u53ef\u5728\u6c29\u6c14\u624b\u5957\u7bb1\u5185\u5b9e\u73b0\u7535\u89e3\u6db2\u914d\u5236\u3001\u7ebd\u6263\u7535\u6c60\u7ec4\u88c5\u548c\u7535\u5316\u5b66\u6d4b\u8bd5\u7684\u5168\u81ea\u52a8\u5316\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740\u7535\u6c60\u6280\u672f\u5411\u66f4\u9ad8\u7a33\u5b9a\u6027\u548c\u80fd\u91cf\u5bc6\u5ea6\u53d1\u5c55\uff0c\u9700\u8981\u5bf9\u4e0d\u540c\u7ec4\u4ef6\u914d\u7f6e\u8fdb\u884c\u5927\u91cf\u7535\u6c60\u7ea7\u6d4b\u8bd5\u3002\u4f20\u7edf\u7684\u7ebd\u6263\u7535\u6c60\u7ec4\u88c5\u548c\u6d4b\u8bd5\u8fc7\u7a0b\u8017\u65f6\u8017\u529b\uff0c\u963b\u788d\u4e86\u9ad8\u901a\u91cf\u7b5b\u9009\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86ALBATROSS\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u96c6\u6210\u5728\u6c29\u6c14\u624b\u5957\u7bb1\u5185\uff0c\u5305\u542b\u5b9a\u5236\u8bbe\u8ba1\u7684\u673a\u5668\u4eba\u5939\u722a\u548c3D\u6253\u5370\u7ed3\u6784\uff0c\u53ef\u7cbe\u786e\u5904\u7406\u7535\u6c60\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u7535\u89e3\u6db2\u914d\u5236\u3001\u7ebd\u6263\u7535\u6c60\u7ec4\u88c5\u548c\u7535\u5316\u5b66\u6d4b\u8bd5\u7684\u5168\u81ea\u52a8\u5316\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u8fbe48\u4e2a\u7535\u6c60\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u7ec4\u88c5\u548c\u6d4b\u8bd5\uff0c\u7ec4\u88c5\u53ef\u9760\u6027\u9ad8\uff0cNCM811||Li\u534a\u7535\u6c60\u7684\u653e\u7535\u5bb9\u91cf\u76f8\u5bf9\u6807\u51c6\u504f\u5dee\u5c0f\u4e8e1.2%\uff0cEIS\u6d4b\u91cf\u6807\u51c6\u504f\u5dee\u5c0f\u4e8e3\u03a9\u3002", "conclusion": "ALBATROSS\u7cfb\u7edf\u51ed\u501f\u9ad8\u53ef\u9760\u6027\u548c\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u80fd\u591f\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u7ebd\u6263\u7535\u6c60\u6570\u636e\u96c6\uff0c\u6709\u671b\u52a0\u901f\u4e0b\u4e00\u4ee3\u7535\u89e3\u6db2\u7684\u5f00\u53d1\u3002"}}
{"id": "2512.13214", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13214", "abs": "https://arxiv.org/abs/2512.13214", "authors": ["Diego Bolliger", "Gabriele Fadini", "Markus Bambach", "Alisa Rupenyan"], "title": "Differentiable Material Point Method for the Control of Deformable Objects", "comment": "7 Pages, 4 Figures, 1 Table", "summary": "Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u6750\u6599\u70b9\u6cd5\uff08MPM\uff09\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u67d4\u6027\u7269\u4f53\u7684\u63a7\u5236\u4f18\u5316\uff0c\u5728\u8d85\u5f39\u6027\u7ef3\u7d22\u7684\u4e3b\u52a8\u963b\u5c3c\u95ee\u9898\u4e2d\u6bd4\u57fa\u7ebfMPPI\u65b9\u6cd5\u5feb2\u500d\u3001\u80fd\u91cf\u964d\u4f4e20%\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ec5\u97003%\u3002", "motivation": "\u67d4\u6027\u7269\u4f53\u7684\u53d8\u5f62\u63a7\u5236\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5176\u5177\u6709\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u8fd9\u7c7b\u590d\u6742\u63a7\u5236\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u6750\u6599\u70b9\u6cd5\uff08MPM\uff09\u6a21\u62df\u5668\uff0c\u5229\u7528\u5176\u53ef\u5fae\u7279\u6027\u5728\u4e3b\u52a8\u963b\u5c3c\u95ee\u9898\u4e2d\u4f18\u5316\u63a7\u5236\u8f68\u8ff9\u3002", "result": "\u5728\u8d85\u5f39\u6027\u7ef3\u7d22\u7684\u4e3b\u52a8\u963b\u5c3c\u95ee\u9898\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebfMPPI\u65b9\u6cd5\u5feb\u7ea62\u500d\uff0c\u5c06\u7ef3\u7d22\u52a8\u80fd\u964d\u4f4e\u5230\u6bd4\u57fa\u7ebf\u4f4e20%\u7684\u6c34\u5e73\uff0c\u4e14\u4ec5\u9700\u7ea63%\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "\u53ef\u5fae\u5206MPM\u6a21\u62df\u5668\u80fd\u6709\u6548\u4f18\u5316\u67d4\u6027\u7269\u4f53\u7684\u63a7\u5236\u8f68\u8ff9\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u67d4\u6027\u7269\u4f53\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2512.12246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12246", "abs": "https://arxiv.org/abs/2512.12246", "authors": ["I Putu Andika Bagas Jiwanta", "Ayu Purwarianti"], "title": "Moment and Highlight Detection via MLLM Frame Segmentation", "comment": null, "summary": "Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous \"0\" and/or \"1\" characters, with one character per frame. The \"0\"/\"1\" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u65f6\u523b\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e8c\u8fdb\u5236\u5b57\u7b26\u5e8f\u5217\uff080/1\uff09\u6765\u540c\u65f6\u5b9e\u73b0\u65f6\u523b\u5b9a\u4f4d\u548c\u9ad8\u4eae\u68c0\u6d4b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6587\u672c\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5e27\u7ea7\u68af\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u6587\u672c\u751f\u6210\u65b9\u5f0f\u65e0\u6cd5\u4e3a\u5e27\u7ea7\u9884\u6d4b\u63d0\u4f9b\u76f4\u63a5\u68af\u5ea6\u3002\u5c3d\u7ba1\u6700\u8fd1\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c1d\u8bd5\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u4f5c\u8005\u63d0\u51fa\u4e86\u66f4\u76f4\u63a5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8ba9LLM\u63a5\u6536\u56fa\u5b9a\u6570\u91cf\u7684\u5e27\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u4f7f\u5176\u8f93\u51fa\u8fde\u7eed\u7684\"0\"\u548c/\u6216\"1\"\u5b57\u7b26\u5e8f\u5217\uff08\u6bcf\u4e2a\u5b57\u7b26\u5bf9\u5e94\u4e00\u5e27\uff09\u3002\u8fd9\u4e9b\u5b57\u7b26\u65e2\u5229\u7528\u4e86LLM\u7684\u8bed\u8a00\u80fd\u529b\uff0c\u53c8\u4f5c\u4e3a\u80cc\u666f\u548c\u524d\u666f\u6982\u7387\u3002\u8bad\u7ec3\u65f6\u7ed3\u5408\u5206\u5272\u635f\u5931\u548c\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u635f\u5931\u3002", "result": "\u5728QVHighlights\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8656.74 HIT@1\u7684\u9ad8\u4eae\u68c0\u6d4b\u6027\u80fd\uff0c\u4ec5\u91c7\u683725\u5e27\uff08\u5c11\u4e8e\u540c\u7c7b\u65b9\u6cd5\u7684\u4e00\u534a\uff09\u3002\u5728\u65f6\u523b\u68c0\u7d22\u4efb\u52a1\u4e0a\u4e5f\u8d85\u8fc7\u4e86\u57fa\u7ebf\uff0835.28 MAP\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u5272\u635f\u5931\u4e3aLLM\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u8865\u5145\u5b66\u4e60\u4fe1\u53f7\uff0c\u5373\u4f7f\u5728\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u635f\u5931\u5e73\u53f0\u671f\u4e5f\u80fd\u6709\u6548\u8bad\u7ec3\u3002\u4e8c\u8fdb\u5236\u5b57\u7b26\u8868\u793a\u65e2\u5229\u7528\u4e86LLM\u7684\u8bed\u8a00\u80fd\u529b\uff0c\u53c8\u5b9e\u73b0\u4e86\u5e27\u7ea7\u9884\u6d4b\u3002"}}
{"id": "2512.12268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12268", "abs": "https://arxiv.org/abs/2512.12268", "authors": ["Yuqing Lei", "Yingjun Du", "Yawen Huang", "Xiantong Zhen", "Ling Shao"], "title": "MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models", "comment": "NeurIPS 2025 Workshop", "summary": "Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.", "AI": {"tldr": "MetaTPT\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8f85\u52a9\u4efb\u52a1\u52a8\u6001\u5b66\u4e60\u53c2\u6570\u5316\u589e\u5f3a\u6765\u6307\u5bfc\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u57df\u504f\u79fb\u4e0b\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u589e\u5f3a\u7b56\u7565\uff0c\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u57df\u504f\u79fb\u573a\u666f\u4e2d\u53ef\u80fd\u5931\u6548\u3002\u9700\u8981\u66f4\u7075\u6d3b\u3001\u66f4\u5177\u8868\u8fbe\u529b\u7684\u589e\u5f3a\u65b9\u6cd5\u6765\u6355\u83b7\u76ee\u6807\u57df\u7684\u5173\u952e\u7279\u5f81\u3002", "method": "\u91c7\u7528\u5143\u5b66\u4e60\u53cc\u5faa\u73af\u4f18\u5316\u8303\u5f0f\uff1a\u5185\u5faa\u73af\u5b66\u4e60\u81ea\u76d1\u7763\u4efb\u52a1\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u56fe\uff0c\u5916\u5faa\u73af\u901a\u8fc7\u5f3a\u5236\u8fd9\u4e9b\u89c6\u56fe\u95f4\u7684\u4e00\u81f4\u6027\u8fdb\u884c\u63d0\u793a\u8c03\u4f18\u3002\u901a\u8fc7\u5c06\u589e\u5f3a\u5b66\u4e60\u4e0e\u63d0\u793a\u8c03\u4f18\u8026\u5408\uff0c\u52a8\u6001\u5b66\u4e60\u6bcf\u4e2a\u6837\u672c\u7684\u53c2\u6570\u5316\u589e\u5f3a\u3002", "result": "\u5728\u57df\u6cdb\u5316\u548c\u8de8\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetaTPT\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "MetaTPT\u901a\u8fc7\u5143\u5b66\u4e60\u81ea\u76d1\u7763\u8f85\u52a9\u4efb\u52a1\u52a8\u6001\u751f\u6210\u53c2\u6570\u5316\u589e\u5f3a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u9762\u5bf9\u57df\u504f\u79fb\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2512.13219", "categories": ["cs.RO", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.13219", "abs": "https://arxiv.org/abs/2512.13219", "authors": ["Christoph Hartmann", "Marios Demetriades", "Kevin Pr\u00fcfer", "Zichen Zhang", "Klaus Spindler", "Stefan Weltge"], "title": "A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization", "comment": "Code available at https://github.com/TUM-utg/PyCAALP (repository will be made public prior to publication)", "summary": "This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.", "AI": {"tldr": "PyCAALP\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u88c5\u914d\u7ebf\u89c4\u5212\u6846\u67b6\uff0c\u91c7\u7528\u56fe\u8bba\u65b9\u6cd5\u5efa\u6a21\u751f\u4ea7\u6a21\u5757\u4e2d\u7684\u7ec4\u4ef6\u548c\u8fde\u63a5\uff0c\u96c6\u6210\u8fd0\u52a8\u5b66\u8fb9\u754c\u6761\u4ef6\u786e\u4fdd\u81ea\u52a8\u5316\u88c5\u914d\u89c4\u5212\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u51cf\u5c11\u7ec4\u5408\u590d\u6742\u5ea6\uff0c\u5e76\u4f7f\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\u5e73\u8861\u5236\u9020\u7ad9\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u88c5\u914d\u5e8f\u5217\u89c4\u5212(ASP)\u548c\u751f\u4ea7\u7ebf\u89c4\u5212(PLP)\u4e2d\u7684\u9ad8\u7ec4\u5408\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u786e\u4fdd\u81ea\u52a8\u5316\u88c5\u914d\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u5e73\u8861\u5236\u9020\u7ad9\u65f6\u95f4\uff0c\u4e3a\u590d\u6742\u88c5\u914d\u63d0\u4f9b\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u56fe\u8bba\u65b9\u6cd5\u5efa\u6a21\u7ec4\u4ef6\u548c\u8fde\u63a5\uff0c\u96c6\u6210\u8fd0\u52a8\u5b66\u8fb9\u754c\u6761\u4ef6\uff08\u5982\u96f6\u4ef6\u78b0\u649e\u68c0\u6d4b\uff09\uff0c\u5f00\u53d1\u7b97\u6cd5\u8ba1\u7b97\u6240\u6709\u53ef\u884c\u751f\u4ea7\u5e8f\u5217\uff0c\u7ed3\u5408\u7a7a\u95f4\u5173\u7cfb\u68c0\u6d4b\u548c\u51e0\u4f55\u7ea6\u675f\u5236\u5b9a\u6a21\u5757\uff0c\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u5982\u5355\u4ef6\u6d41\u88c5\u914d\u548c\u51e0\u4f55\u7ea6\u675f\u6267\u884c\uff09\u51cf\u5c11\u89e3\u7a7a\u95f4\uff0c\u5c06PLP\u9636\u6bb5\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u4e86PyCAALP\u5f00\u6e90\u6846\u67b6\uff0c\u80fd\u591f\u8ba1\u7b97\u6240\u6709\u53ef\u884c\u7684\u751f\u4ea7\u5e8f\u5217\uff0c\u6709\u6548\u7ba1\u7406\u88c5\u914d\u5e8f\u5217\u751f\u6210\u7684\u9ad8\u7ec4\u5408\u590d\u6742\u5ea6\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u663e\u8457\u51cf\u5c11MIP\u8ba1\u7b97\u65f6\u95f4\uff0c\u652f\u6301\u5de5\u7a0b\u7ea6\u675f\u5b9a\u5236\u548cASP\u4e0ePLP\u4e4b\u95f4\u7684\u7075\u6d3b\u6743\u8861\u3002", "conclusion": "PyCAALP\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u88c5\u914d\u5e8f\u5217\u89c4\u5212\u548c\u751f\u4ea7\u7ebf\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u96c6\u6210\u8fd0\u52a8\u5b66\u7ea6\u675f\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u5e73\u8861\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u4e0e\u6700\u4f18\u6027\uff0c\u5f00\u6e90\u7279\u6027\u4fc3\u8fdb\u4e86\u5de5\u4e1a\u548c\u751f\u4ea7\u7814\u7a76\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u5408\u4f5c\u4e0e\u5e94\u7528\u3002"}}
{"id": "2512.12277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12277", "abs": "https://arxiv.org/abs/2512.12277", "authors": ["Thibault Geoffroy", "Myriam Maumy", "Lionel Prevost"], "title": "Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions", "comment": "28 pages, 8 figures, chapter for \"Emotion and Facial Recognition in Artificial Intelligence: Sustainable Multidisciplinary Perspectives and Applications\" (2026)", "summary": "As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u8fde\u7eed\u5b66\u4e60\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5377\u79ef\u7279\u5f81\u548c\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5728CFEE\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u4ece\u57fa\u672c\u8868\u60c5\u5230\u590d\u5408\u8868\u60c5\u7684\u6e10\u8fdb\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u65e5\u76ca\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u8bc6\u522b\u548c\u9002\u5e94\u4eba\u7c7b\u60c5\u611f\u5bf9\u4e8e\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u662f\u63a8\u65ad\u60c5\u611f\u72b6\u6001\u7684\u4e3b\u8981\u6e20\u9053\uff0c\u4f46\u60c5\u611f\u5177\u6709\u52a8\u6001\u6027\u548c\u6587\u5316\u5dee\u5f02\u6027\uff0c\u9700\u8981\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u800c\u4e0d\u9057\u5fd8\u5148\u524d\u77e5\u8bc6\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u6574\u5408\u4e24\u79cd\u4e92\u8865\u6a21\u6001\uff1a\u6df1\u5ea6\u5377\u79ef\u7279\u5f81\u548c\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u3002\u901a\u8fc7\u8d1d\u53f6\u65af\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5bf9\u7ec4\u5408\u8868\u793a\u8fdb\u884c\u5efa\u6a21\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u6982\u7387\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u540c\u65f6\u4fdd\u6301\u5f3a\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728CFEE\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u80fd\u591f\u5148\u5b66\u4e60\u57fa\u672c\u8868\u60c5\uff0c\u7136\u540e\u9010\u6b65\u8bc6\u522b\u590d\u5408\u8868\u60c5\u3002\u5b9e\u9a8c\u663e\u793a\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u589e\u5f3a\u4e86\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u9057\u5fd8\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u5f00\u53d1\u5177\u6709\u60c5\u611f\u667a\u80fd\u7684AI\u7cfb\u7edf\uff0c\u53ef\u5e94\u7528\u4e8e\u6559\u80b2\u3001\u533b\u7597\u4fdd\u5065\u548c\u81ea\u9002\u5e94\u7528\u6237\u754c\u9762\u7b49\u9886\u57df\u3002"}}
{"id": "2512.13262", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13262", "abs": "https://arxiv.org/abs/2512.13262", "authors": ["Hyunki Seong", "Jeong-Kyun Lee", "Heesoo Myeong", "Yongho Shin", "Hyun-Mook Cho", "Duck Hoon Kim", "Pranav Desai", "Monu Surana"], "title": "Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving", "comment": "11 pages, 5 figures", "summary": "Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u884c\u4e3a\u5b66\u4e60\u7684\u6311\u6218\uff1aGRBO\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\u548cWarm-K\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u884c\u4e3a\u5b66\u4e60\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u4ece\u6570\u636e\u96c6\u7ee7\u627f\u504f\u89c1\uff0c\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u9c81\u68d2\u6027\u4e0d\u8db3\uff1b\u5927\u591a\u6570\u7814\u7a76\u4f9d\u8d56\u5f00\u73af\u8bc4\u4f30\uff0c\u5ffd\u7565\u4e86\u95ed\u73af\u6267\u884c\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a1) GRBO\uff08\u7fa4\u4f53\u76f8\u5bf9\u884c\u4e3a\u4f18\u5316\uff09- \u901a\u8fc7\u7fa4\u4f53\u76f8\u5bf9\u4f18\u52bf\u6700\u5927\u5316\u548c\u4eba\u7c7b\u6b63\u5219\u5316\u5bf9\u9884\u8bad\u7ec3\u884c\u4e3a\u6a21\u578b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff1b2) Warm-K - \u57fa\u4e8e\u70ed\u542f\u52a8\u7684Top-K\u91c7\u6837\u7b56\u7565\uff0c\u5e73\u8861\u8fd0\u52a8\u9009\u62e9\u7684\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u3002", "result": "GRBO\u4ec5\u4f7f\u752810%\u8bad\u7ec3\u6570\u636e\u5c31\u5c06\u5b89\u5168\u6027\u80fd\u63d0\u534740%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u884c\u4e3a\u771f\u5b9e\u6027\uff1bWarm-K\u65b9\u6cd5\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7f29\u653e\u589e\u5f3a\u4e86\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u53cd\u5e94\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u63d0\u51fa\u7684GRBO\u548cWarm-K\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u884c\u4e3a\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\u548c\u95ed\u73af\u6267\u884c\u95ee\u9898\uff0cGRBO\u63d0\u5347\u5b89\u5168\u6027\u80fd\uff0cWarm-K\u7f13\u89e3\u534f\u53d8\u91cf\u504f\u79fb\u548c\u6027\u80fd\u5dee\u5f02\u3002"}}
{"id": "2512.13271", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13271", "abs": "https://arxiv.org/abs/2512.13271", "authors": ["Fangju Yang", "Hang Yang", "Ibrahim Alsarraj", "Yuhao Wang", "Ke Wu"], "title": "Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation", "comment": null, "summary": "Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.", "AI": {"tldr": "\u63d0\u51faLASEM\u6846\u67b6\uff0c\u901a\u8fc7\u9a71\u52a8\u7a7a\u95f4\u80fd\u91cf\u5efa\u6a21\u5b9e\u73b0\u7535\u7f06\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7684\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u52a8\u6001\u5efa\u6a21\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u534762.3%", "motivation": "\u7535\u7f06\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u9700\u8981\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u52a8\u6001\u6a21\u578b\u7528\u4e8e\u9ad8\u901f\u52a8\u6001\u9884\u6d4b\u6216\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u4e14\u6548\u7387\u4e0d\u8db3", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u9a71\u52a8\u7a7a\u95f4\u80fd\u91cf\u5efa\u6a21\u6846\u67b6\uff0c\u5728\u9a71\u52a8\u7a7a\u95f4\u76f4\u63a5\u5efa\u7acb\u9a71\u52a8\u52bf\u80fd\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u5bfc\u5c06\u52a8\u6001\u65b9\u7a0b\u7b80\u5316\u4e3a\u5355\u4e2a\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u7535\u7f06-\u9aa8\u67b6\u63a5\u89e6\u529b\uff0c\u652f\u6301\u529b\u548c\u4f4d\u79fb\u4e24\u79cd\u9a71\u52a8\u6a21\u5f0f", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5b9e\u65f6\u52a8\u6001\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e73\u5747\u8ba1\u7b97\u901f\u5ea6\u63d0\u534762.3%\uff0c\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u7cbe\u5ea6\u548c\u7269\u7406\u4e00\u81f4\u6027", "conclusion": "LASEM\u6846\u67b6\u4e3a\u7535\u7f06\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u52a8\u6001\u5efa\u6a21\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u9a71\u52a8\u6a21\u5f0f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.12287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12287", "abs": "https://arxiv.org/abs/2512.12287", "authors": ["Ahmad Zafarani", "Zahra Dehghanian", "Mohammadreza Davoodi", "Mohsen Shadroo", "MohammadAmin Fazli", "Hamid R. Rabiee"], "title": "RealDrag: The First Dragging Benchmark with Real Target Image", "comment": null, "summary": "The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \\textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.\n  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.", "AI": {"tldr": "RealDrag\u662f\u9996\u4e2a\u5305\u542b\u771f\u5b9e\u76ee\u6807\u56fe\u50cf\u7684\u57fa\u4e8e\u70b9\u62d6\u62fd\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\uff0c\u5305\u542b400+\u4eba\u5de5\u6807\u6ce8\u6837\u672c\u548c\u56db\u4e2a\u65b0\u6307\u6807\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f3017\u4e2aSOTA\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u62d6\u62fd\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u8bc4\u4f30\u4e0d\u53ef\u9760\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u548c\u6307\u6807\uff0c\u6ca1\u6709\u5305\u542b\u771f\u5b9e\u76ee\u6807\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u65b9\u6cd5\u95f4\u96be\u4ee5\u5ba2\u89c2\u6bd4\u8f83\u3002", "method": "\u521b\u5efaRealDrag\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b400+\u4eba\u5de5\u6807\u6ce8\u6837\u672c\uff08\u6765\u6e90/\u76ee\u6807\u56fe\u50cf\u3001\u63a7\u5236\u70b9/\u76ee\u6807\u70b9\u3001\u53ef\u7f16\u8f91\u533a\u57df\u63a9\u7801\u3001\u63cf\u8ff0\u6027\u6807\u6ce8\uff09\uff1b\u63d0\u51fa\u56db\u4e2a\u65b0\u6307\u6807\uff1a\u8bed\u4e49\u8ddd\u79bb(SeD)\u3001\u5916\u90e8\u63a9\u7801\u4fdd\u6301\u5206\u6570(OMPS)\u3001\u5185\u90e8\u8865\u4e01\u4fdd\u6301\u5206\u6570(IPPS)\u3001\u65b9\u5411\u76f8\u4f3c\u6027(DiS)\u3002", "result": "\u4f7f\u7528\u8be5\u57fa\u51c6\u5bf917\u4e2aSOTA\u6a21\u578b\u8fdb\u884c\u4e86\u9996\u6b21\u5927\u89c4\u6a21\u7cfb\u7edf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u95f4\u7684\u660e\u786e\u6743\u8861\uff0c\u5efa\u7acb\u4e86\u7a33\u5065\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u3002", "conclusion": "RealDrag\u4e3a\u57fa\u4e8e\u70b9\u62d6\u62fd\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u9996\u4e2a\u5305\u542b\u771f\u5b9e\u76ee\u6807\u56fe\u50cf\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\uff0c\u63d0\u51fa\u7684\u6307\u6807\u80fd\u5168\u9762\u91cf\u5316\u7f16\u8f91\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\u3002"}}
{"id": "2512.13293", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13293", "abs": "https://arxiv.org/abs/2512.13293", "authors": ["Hao Fua", "Wei Liu", "Shuai Zhoua"], "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration", "comment": null, "summary": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5185\u5728\u6fc0\u52b1\u63a2\u7d22\u7684\u534f\u540c\u63a2\u7d22\u591a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u673a\u5668\u4eba\u793e\u4f1a\u7f16\u961f\u5bfc\u822a\u4e2d\u884c\u4eba\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u793e\u4f1a\u7f16\u961f\u5bfc\u822a\u5bf9\u4e8e\u5b9e\u73b0\u4eba\u673a\u65e0\u7f1d\u5171\u5b58\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u884c\u4eba\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u975e\u5408\u4f5c\u6027\u7ed9\u673a\u5668\u4eba\u534f\u540c\u63a2\u7d22\u6548\u7387\u5e26\u6765\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u534f\u540c\u63a2\u7d22\u591a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u6838\u5fc3\u662f\u81ea\u5b66\u4e60\u5185\u5728\u5956\u52b1\u673a\u5236\u6765\u7f13\u89e3\u7b56\u7565\u4fdd\u5b88\u4e3b\u4e49\uff1b\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u6846\u67b6\u4e0b\u7684\u53cc\u91c7\u6837\u6a21\u5f0f\uff0c\u901a\u8fc7\u4e24\u65f6\u95f4\u5c3a\u5ea6\u66f4\u65b0\u89c4\u5219\u89e3\u8026\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728\u793e\u4f1a\u7f16\u961f\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7b97\u6cd5\u5728\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5185\u5728\u6fc0\u52b1\u63a2\u7d22\u7684\u534f\u540c\u63a2\u7d22\u591a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u793e\u4f1a\u7f16\u961f\u5bfc\u822a\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u63d0\u5347\u4eba\u673a\u5171\u5b58\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u5bfc\u822a\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.13304", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13304", "abs": "https://arxiv.org/abs/2512.13304", "authors": ["Sait Sovukluk", "Johannes Englsberger", "Christian Ott"], "title": "Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories", "comment": "Accepted for publication in Biomimetic Intelligence and Robotics. Supplemental video: https://youtu.be/HlAg2nbNct4", "summary": "This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5f39\u7c27\u8d28\u91cf\u8f68\u8ff9\u548c\u6b7b\u533a\u63a7\u5236\u589e\u76ca\u5e93\u7684\u6b65\u6001\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u8dd1\u6b65\u63a7\u5236\uff0c\u901a\u8fc7\u5168\u8eab\u4f53\u63a7\u5236\u5b9e\u73b0\u591a\u79cd\u654f\u6377\u884c\u4e3a", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u590d\u6742\u5730\u5f62\u548c\u5e72\u6270\u7684\u8dd1\u6b65\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u9002\u5e94\u5404\u79cd\u6311\u6218\u6027\u73af\u5883\uff0c\u540c\u65f6\u4fdd\u6301\u63a7\u5236\u53c2\u6570\u7684\u7edf\u4e00\u6027", "method": "\u5305\u542b\u56db\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1) \u81ea\u52a8\u751f\u6210\u5f39\u7c27\u8d28\u91cf\u8f68\u8ff9\u5e93\uff1b2) \u901a\u8fc7\u4e3b\u52a8\u63a7\u5236\u6a21\u677f\u6a21\u578b\u751f\u6210\u6b7b\u533a\u63a7\u5236\u589e\u76ca\u5e93\uff1b3) \u5f00\u53d1\u6b65\u6001\u9002\u5e94\u7684\u8f68\u8ff9\u9009\u62e9\u7b56\u7565\uff1b4) \u901a\u8fc7\u5168\u8eab\u4f53\u63a7\u5236\u6846\u67b6\u5c06\u5f39\u7c27\u8d28\u91cf\u8f68\u8ff9\u6620\u5c04\u5230\u4eba\u5f62\u6a21\u578b", "result": "\u6846\u67b6\u5728MuJoCo\u7269\u7406\u6a21\u62df\u5668\u4e2d\u5c55\u793a\u4e86\u5305\u5bb9\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5904\u7406\u968f\u673a\u751f\u6210\u7684\u8e0f\u811a\u77f3\u3001\u8df3\u8dc3\u969c\u788d\u3001\u56de\u8f6c\u8fd0\u52a8\u3001\u7a81\u7136\u6539\u53d8\u65b9\u5411\u7b49\u590d\u6742\u884c\u4e3a\uff0c315\u4e2a\u4e0d\u540c\u8f68\u8ff9\u7684\u603b\u8ba1\u7b97\u65f6\u95f4\u4ec5\u4e3a4.5\u79d2", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u4f7f\u7528\u5355\u4e00\u5e93\u548c\u76f8\u540c\u7684\u63a7\u5236\u53c2\u6570\u5b9e\u73b0\u591a\u79cd\u654f\u6377\u884c\u4e3a\uff0c\u5bf9\u73b0\u5b9e\u4e16\u754c\u6311\u6218\uff08\u5982\u4fe1\u53f7\u566a\u58f0\u3001\u5efa\u6a21\u8bef\u5dee\u7b49\uff09\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u8dd1\u6b65\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.12302", "categories": ["cs.CV", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12302", "abs": "https://arxiv.org/abs/2512.12302", "authors": ["Huan Zheng", "Yucheng Zhou", "Tianyi Yan", "Jiayi Su", "Hongjun Chen", "Dubing Chen", "Wencheng Han", "Runzhou Tao", "Zhongying Qiu", "Jianfei Yang", "Jianbing Shen"], "title": "From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving", "comment": null, "summary": "Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Intention-Drive\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4ece\u9ad8\u7ea7\u4eba\u7c7b\u610f\u56fe\u5230\u7cbe\u786e\u9a7e\u9a76\u52a8\u4f5c\u7684\u8f6c\u6362\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u610f\u56fe\u7406\u89e3\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4ec5\u80fd\u6267\u884c\u4f4e\u7ea7\u8f6c\u5411\u6307\u4ee4\uff0c\u7f3a\u4e4f\u7406\u89e3\u5e76\u5b9e\u73b0\u9ad8\u7ea7\u4eba\u7c7b\u62bd\u8c61\u610f\u56fe\u7684\u80fd\u529b\u3002\u5b9e\u73b0\u771f\u6b63\u667a\u80fd\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u4ece\"\u6307\u4ee4\u8ddf\u968f\u8005\"\u5411\"\u610f\u56fe\u5b9e\u73b0\u8005\"\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8861\u91cf\u548c\u63a8\u52a8\u8fd9\u4e00\u590d\u6742\u4efb\u52a1\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faIntention-Drive\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u8d21\u732e\uff1a1)\u5305\u542b\u590d\u6742\u573a\u666f\u548c\u5bf9\u5e94\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u7684\u65b0\u6570\u636e\u96c6\uff1b2)\u4ee5\u610f\u56fe\u6210\u529f\u7387(ISR)\u4e3a\u4e2d\u5fc3\u7684\u65b0\u8bc4\u4f30\u534f\u8bae\uff0c\u8bc4\u4f30\u4eba\u7c7b\u76ee\u6807\u7684\u8bed\u4e49\u5b9e\u73b0\u7a0b\u5ea6\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u51e0\u4f55\u7cbe\u5ea6\u3002", "result": "\u901a\u8fc7\u5bf9\u4e00\u7cfb\u5217\u57fa\u7ebf\u6a21\u578b\u5728Intention-Drive\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u7f3a\u9677\uff0c\u8868\u660e\u57fa\u7ebf\u6a21\u578b\u96be\u4ee5\u8fbe\u5230\u8fd9\u4e00\u9ad8\u7ea7\u4efb\u52a1\u6240\u9700\u7684\u5168\u9762\u573a\u666f\u548c\u610f\u56fe\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "Intention-Drive\u586b\u8865\u4e86\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30\u4ece\u9ad8\u7ea7\u610f\u56fe\u5230\u9a7e\u9a76\u52a8\u4f5c\u7684\u8f6c\u6362\u80fd\u529b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u5728\u610f\u56fe\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.13356", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13356", "abs": "https://arxiv.org/abs/2512.13356", "authors": ["Zeyad Gamal", "Youssef Mahran", "Ayman El-Badawy"], "title": "Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)", "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53cc\u65cb\u7ffc\u6c14\u52a8\u7cfb\u7edf\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u7528TD3\u7b97\u6cd5\u5b9e\u73b0\u59ff\u6001\u7a33\u5b9a\u548c\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u76f8\u6bd4\u4f20\u7edfPID\u63a7\u5236\u5668\u5728\u6297\u5e72\u6270\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u53cc\u65cb\u7ffc\u6c14\u52a8\u7cfb\u7edf\u5177\u6709\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\uff0c\u4f20\u7edf\u63a7\u5236\u7b97\u6cd5\u96be\u4ee5\u6709\u6548\u63a7\u5236\u3002\u8fd1\u5e74\u6765\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u65cb\u7ffc\u63a7\u5236\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u5f15\u8d77\u4e86\u7814\u7a76\u5174\u8da3\u3002", "method": "\u91c7\u7528Twin Delayed Deep Deterministic Policy Gradient (TD3)\u7b97\u6cd5\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883\uff0c\u4e14\u4e0d\u9700\u8981\u7cfb\u7edf\u6a21\u578b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\uff0c\u5728\u98ce\u6270\u7b49\u5916\u90e8\u5e72\u6270\u4e0b\u6bd4\u4f20\u7edfPID\u63a7\u5236\u5668\u8868\u73b0\u66f4\u597d\uff0c\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u63a7\u5236\u53cc\u65cb\u7ffc\u6c14\u52a8\u7cfb\u7edf\uff0c\u5728\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\u548c\u5b58\u5728\u5916\u90e8\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2512.12303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12303", "abs": "https://arxiv.org/abs/2512.12303", "authors": ["Yang Ou", "Xiongwei Zhao", "Xinye Yang", "Yihan Wang", "Yicheng Di", "Rong Yuan", "Xieyuanli Chen", "Xu Zhu"], "title": "OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation", "comment": "Submitted to TMM", "summary": "Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.", "AI": {"tldr": "OMUDA\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u63a9\u7801\u7b56\u7565\u89e3\u51b3\u8de8\u57df\u4e0a\u4e0b\u6587\u6a21\u7cca\u3001\u7279\u5f81\u8868\u793a\u4e0d\u4e00\u81f4\u548c\u7c7b\u522b\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u4ecd\u96be\u4ee5\u5f25\u5408\u57df\u95f4\u5dee\u8ddd\uff0c\u4e3b\u8981\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a\u8de8\u57df\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u3001\u4e0d\u4e00\u81f4\u7684\u7279\u5f81\u8868\u793a\u4ee5\u53ca\u7c7b\u522b\u7ea7\u522b\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u3002", "method": "\u63d0\u51faOMUDA\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5206\u5c42\u63a9\u7801\u7b56\u7565\uff1a1) \u4e0a\u4e0b\u6587\u611f\u77e5\u63a9\u7801(CAM)\u81ea\u9002\u5e94\u533a\u5206\u524d\u666f\u4e0e\u80cc\u666f\u4ee5\u5e73\u8861\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u7ec6\u8282\uff1b2) \u7279\u5f81\u84b8\u998f\u63a9\u7801(FDM)\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u589e\u5f3a\u9c81\u68d2\u4e14\u4e00\u81f4\u7684\u7279\u5f81\u5b66\u4e60\uff1b3) \u7c7b\u522b\u89e3\u8026\u63a9\u7801(CDM)\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7c7b\u522b\u4e0d\u786e\u5b9a\u6027\u6765\u51cf\u8f7b\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u5f71\u54cd\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u57df\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86OMUDA\u7684\u6709\u6548\u6027\u3002\u5728SYNTHIA->Cityscapes\u548cGTA5->Cityscapes\u4efb\u52a1\u4e2d\uff0cOMUDA\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709UDA\u65b9\u6cd5\u4e2d\uff0c\u5e73\u5747\u63d0\u53477%\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "OMUDA\u901a\u8fc7\u5206\u5c42\u63a9\u7801\u8303\u5f0f\u5728\u4e0a\u4e0b\u6587\u3001\u8868\u793a\u548c\u7c7b\u522b\u4e09\u4e2a\u5c42\u9762\u6709\u6548\u51cf\u5c11\u57df\u504f\u79fb\uff0c\u4e3a\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13359", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13359", "abs": "https://arxiv.org/abs/2512.13359", "authors": ["S\u00fcmer Tun\u00e7ay", "Alain Andres", "Ignacio Carlucho"], "title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles", "comment": null, "summary": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eJAX\u548cMuJoCo-XLA\u7684GPU\u52a0\u901f\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u5b9e\u73b02\u5206\u949f\u5185\u5b8c\u6210AUV\u516d\u81ea\u7531\u5ea6\u4f4d\u7f6e\u63a7\u5236\u7b56\u7565\u8bad\u7ec3\uff0c\u5e76\u5728\u771f\u5b9e\u6c34\u4e0b\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u7684\u9c81\u68d2\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfAUV\u63a7\u5236\u5668\u5728\u672a\u5efa\u6a21\u52a8\u6001\u548c\u73af\u5883\u6270\u52a8\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u7f13\u6162\u4e14\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u56f0\u96be\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u4f7f\u7528JAX\u548cMuJoCo-XLA\u6784\u5efaGPU\u52a0\u901f\u8bad\u7ec3\u7ba1\u9053\uff0c\u901a\u8fc7\u8054\u5408JIT\u7f16\u8bd1\u5927\u89c4\u6a21\u5e76\u884c\u7269\u7406\u4eff\u771f\u548c\u5b66\u4e60\u66f4\u65b0\uff0c\u5b9e\u73b0\u5feb\u901f\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cdRL\u7b97\u6cd5\u3002", "result": "\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u81f32\u5206\u949f\u4ee5\u5185\uff0c\u5728\u771f\u5b9e\u6c34\u4e0b\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u516d\u81ea\u7531\u5ea6\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6709\u6548\u7684\u6270\u52a8\u6291\u5236\uff0c\u7b56\u7565\u4ece\u4eff\u771f\u96f6\u6837\u672c\u8fc1\u79fb\u6210\u529f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5c55\u793a\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684AUV\u516d\u81ea\u7531\u5ea6\u4f4d\u7f6e\u63a7\u5236\uff0c\u4e3a\u5feb\u901f\u8bad\u7ec3\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12307", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.12307", "abs": "https://arxiv.org/abs/2512.12307", "authors": ["Benjamin Beilharz", "Thomas S. A. Wallis"], "title": "MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding", "comment": "18 pages, 6 figures. Supplementary material and code will be provided at the end of January", "summary": "While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.", "AI": {"tldr": "MRD\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u6765\u63a2\u7d22\u89c6\u89c9\u6a21\u578b\u5bf93D\u573a\u666f\u5c5e\u6027\u7684\u9690\u5f0f\u7406\u89e3\uff0c\u901a\u8fc7\u5bfb\u627e\u4ea7\u751f\u76f8\u540c\u6a21\u578b\u6fc0\u6d3b\u7684\u4e0d\u540c3D\u573a\u666f\u53c2\u6570\uff08\u6a21\u578b\u540c\u6784\u4f53\uff09\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6210\u529f\uff0c\u4f46\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u7684\u8868\u793a\u548c\u51b3\u7b56\u4ecd\u7136\u56f0\u96be\u3002\u89c6\u89c9\u6a21\u578b\u901a\u5e38\u57fa\u4e8e2D\u8f93\u5165\u8bad\u7ec3\uff0c\u4f46\u88ab\u8ba4\u4e3a\u53d1\u5c55\u4e86\u5bf9\u5e95\u5c423D\u573a\u666f\u7684\u9690\u5f0f\u7406\u89e3\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63a2\u6d4b\u6a21\u578b\u5bf9\u751f\u6210\u60273D\u573a\u666f\u5c5e\u6027\u7684\u7406\u89e3\u3002", "method": "MRD\uff08\u53ef\u5fae\u5206\u6e32\u67d3\u7684\u540c\u6784\u4f53\uff09\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u5fae\u5206\u6e32\u67d3\uff0c\u901a\u8fc7\u5bfb\u627e\u5728\u7269\u7406\u4e0a\u4e0d\u540c\u4f46\u4ea7\u751f\u76f8\u540c\u6a21\u578b\u6fc0\u6d3b\u76843D\u573a\u666f\u53c2\u6570\u6765\u63a2\u6d4b\u89c6\u89c9\u6a21\u578b\u3002\u4e0e\u4e4b\u524d\u57fa\u4e8e\u50cf\u7d20\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u8fd9\u4e9b\u91cd\u5efa\u7ed3\u679c\u59cb\u7ec8\u57fa\u4e8e\u7269\u7406\u573a\u666f\u63cf\u8ff0\uff0c\u53ef\u4ee5\u72ec\u7acb\u63a2\u6d4b\u6a21\u578b\u5bf9\u7279\u5b9a\u573a\u666f\u5c5e\u6027\uff08\u5982\u5f62\u72b6\u3001\u6750\u8d28\uff09\u7684\u654f\u611f\u6027\u3002", "result": "\u4f5c\u4e3a\u539f\u7406\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u4e86\u591a\u4e2a\u6a21\u578b\u5728\u6062\u590d\u573a\u666f\u51e0\u4f55\uff08\u5f62\u72b6\uff09\u548c\u53cc\u5411\u53cd\u5c04\u5206\u5e03\u51fd\u6570\uff08\u6750\u8d28\uff09\u53c2\u6570\u65b9\u9762\u7684\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\u76ee\u6807\u573a\u666f\u548c\u4f18\u5316\u573a\u666f\u4e4b\u95f4\u7684\u6a21\u578b\u6fc0\u6d3b\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u4f46\u89c6\u89c9\u7ed3\u679c\u5404\u5f02\u3002\u8fd9\u4e9b\u91cd\u5efa\u6709\u52a9\u4e8e\u5b9a\u6027\u7814\u7a76\u6a21\u578b\u5bf9\u54ea\u4e9b\u7269\u7406\u573a\u666f\u5c5e\u6027\u654f\u611f\u6216\u4e0d\u654f\u611f\u3002", "conclusion": "MRD\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u7269\u7406\u573a\u666f\u53c2\u6570\u5982\u4f55\u9a71\u52a8\u6a21\u578b\u54cd\u5e94\u53d8\u5316\uff0c\u6709\u671b\u589e\u8fdb\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u4eba\u7c7b\u89c6\u89c9\u7684\u7406\u89e3\uff0c\u4e3a\u6a21\u578b\u89e3\u91ca\u6027\u63d0\u4f9b\u65b0\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2512.13380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13380", "abs": "https://arxiv.org/abs/2512.13380", "authors": ["Chuan Mao", "Haoqi Yuan", "Ziye Huang", "Chaoyi Xu", "Kai Ma", "Zongqing Lu"], "title": "Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning", "comment": "19 pages", "summary": "Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.", "AI": {"tldr": "DemoFunGrasp\u901a\u8fc7\u5c06\u529f\u80fd\u6293\u53d6\u6761\u4ef6\u5206\u89e3\u4e3a\u6293\u53d6\u98ce\u683c\u548c\u53ef\u64cd\u4f5c\u6027\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff0c\u7ed3\u5408\u5355\u6b21\u6f14\u793a\u7f16\u8f91\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u901a\u7528\u7075\u5de7\u529f\u80fd\u6293\u53d6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u5728\u7075\u5de7\u6293\u53d6\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u7ec6\u7c92\u5ea6\u7684\u529f\u80fd\u6293\u53d6\uff08\u5bf9\u4e0b\u6e38\u64cd\u4f5c\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff09\u4ecd\u9762\u4e34\u6311\u6218\uff1a\u4e3a\u4e0d\u540c\u5bf9\u8c61\u6307\u5b9a\u529f\u80fd\u6293\u53d6\u76ee\u6807\u548c\u5956\u52b1\u51fd\u6570\u7684\u590d\u6742\u6027\u3001\u591a\u4efb\u52a1RL\u63a2\u7d22\u7684\u56f0\u96be\u4ee5\u53ca\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u8fc1\u79fb\u7684\u6311\u6218\u3002", "method": "\u5c06\u529f\u80fd\u6293\u53d6\u6761\u4ef6\u5206\u89e3\u4e3a\u6293\u53d6\u98ce\u683c\u548c\u53ef\u64cd\u4f5c\u6027\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff0c\u5e76\u96c6\u6210\u5230RL\u6846\u67b6\u4e2d\uff1b\u5229\u7528\u5355\u6b21\u6293\u53d6\u6f14\u793a\uff0c\u5c06RL\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u6b65\u6f14\u793a\u7f16\u8f91\uff0c\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff1b\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c4\u5212\uff0c\u5b9e\u73b0\u81ea\u4e3b\u6307\u4ee4\u8ddf\u968f\u6293\u53d6\u6267\u884c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cDemoFunGrasp\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5bf9\u8c61\u3001\u53ef\u64cd\u4f5c\u6027\u548c\u6293\u53d6\u98ce\u683c\u7ec4\u5408\uff0c\u5728\u6210\u529f\u7387\u548c\u529f\u80fd\u6293\u53d6\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1b\u7cfb\u7edf\u5177\u5907\u5f3a\u5927\u7684\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "DemoFunGrasp\u901a\u8fc7\u521b\u65b0\u7684\u6761\u4ef6\u5206\u89e3\u548c\u6f14\u793a\u7f16\u8f91\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u901a\u7528\u7075\u5de7\u529f\u80fd\u6293\u53d6\u7684\u591a\u4e2a\u6311\u6218\uff0c\u4e3a\u4e0b\u6e38\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u529f\u80fd\u6293\u53d6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12309", "abs": "https://arxiv.org/abs/2512.12309", "authors": ["Shenghao Fu", "Yukun Su", "Fengyun Rao", "Jing Lyu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "comment": null, "summary": "Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \\ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.", "AI": {"tldr": "WeDetect\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u54f2\u5b66\u7684\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5bb6\u65cf\uff0c\u5305\u542b\u4e09\u4e2a\u53d8\u4f53\uff1a\u57fa\u7840\u7248\u5b9e\u73b0\u5b9e\u65f6\u68c0\u6d4b\uff0cUni\u7248\u652f\u6301\u5386\u53f2\u6570\u636e\u56de\u6eaf\uff0cRef\u7248\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u6307\u4ee3\u8868\u8fbe\u3002\u8be5\u7cfb\u5217\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u5177\u6709\u9ad8\u6548\u63a8\u7406\u4f18\u52bf\u3002", "motivation": "\u63a2\u7d22\u65e0\u8de8\u6a21\u6001\u878d\u5408\u5c42\u7684\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u68c0\u7d22\u54f2\u5b66\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u548c\u591a\u529f\u80fd\u5e94\u7528\uff0c\u5305\u62ec\u5b9e\u65f6\u68c0\u6d4b\u3001\u5386\u53f2\u6570\u636e\u56de\u6eaf\u548c\u590d\u6742\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u3002", "method": "\u91c7\u7528\u53cc\u5854\u67b6\u6784\uff0c\u5c06\u76ee\u6807\u68c0\u6d4b\u89c6\u4e3a\u68c0\u7d22\u95ee\u9898\uff1a\u5339\u914d\u533a\u57df\u4e0e\u6587\u672c\u67e5\u8be2\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u3002WeDetect\u4e3a\u57fa\u7840\u68c0\u6d4b\u5668\uff0cWeDetect-Uni\u51bb\u7ed3\u68c0\u6d4b\u5668\u5e76\u5fae\u8c03\u76ee\u6807\u6027\u63d0\u793a\u751f\u6210\u901a\u7528\u5efa\u8bae\uff0cWeDetect-Ref\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u6307\u4ee3\u8868\u8fbe\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b9e\u73b0\u5b9e\u65f6\u68c0\u6d4b\uff0c\u652f\u6301\u5386\u53f2\u6570\u636e\u4e2d\u7684\u5bf9\u8c61\u68c0\u7d22\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u6307\u4ee3\u8868\u8fbe\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "WeDetect\u5bb6\u65cf\u5728\u7edf\u4e00\u7684\u68c0\u7d22\u6846\u67b6\u4e0b\u96c6\u6210\u4e86\u68c0\u6d4b\u3001\u5efa\u8bae\u751f\u6210\u3001\u5bf9\u8c61\u68c0\u7d22\u548c\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff0c\u8bc1\u660e\u4e86\u68c0\u7d22\u54f2\u5b66\u5728\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\u4f18\u52bf\u3002"}}
{"id": "2512.13477", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13477", "abs": "https://arxiv.org/abs/2512.13477", "authors": ["Timothy A. Brumfiel", "Revanth Konda", "Drew Elliott", "Jaydev P. Desai"], "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model", "comment": "Presented at the 14th Conference on New Technologies for Computer and Robot Assisted Surgery (CRAS 2025)", "summary": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u7b80\u5316\u7248COAST\u5bfc\u4e1d\u673a\u5668\u4eba\u5728\u8109\u52a8\u6d41\u89e3\u5256\u6a21\u578b\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u901a\u8fc7\u590d\u6742\u8840\u7ba1\u7ed3\u6784\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u5bfc\u822a\u5bfc\u4e1d\u5728\u8840\u7ba1\u4ecb\u5165\u624b\u672f\u4e2d\u5b58\u5728\u64cd\u4f5c\u56f0\u96be\u7684\u95ee\u9898\uff0c\u673a\u5668\u4eba\u53ef\u64cd\u63a7\u5bfc\u4e1d\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u64cd\u4f5c\u6027\u548c\u5bfc\u822a\u80fd\u529b\u3002COAST\u5bfc\u4e1d\u673a\u5668\u4eba\u80fd\u4ea7\u751f\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u5176\u5728\u771f\u5b9e\u89e3\u5256\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7b80\u5316\u7248COAST\u5bfc\u4e1d\u673a\u5668\u4eba\uff08\u4ece\u4e09\u7ba1\u7ed3\u6784\u7b80\u5316\u4e3a\u4e24\u7ba1\u7ed3\u6784\uff09\uff0c\u5728\u5177\u6709\u8109\u52a8\u6d41\u7684\u89e3\u5256\u6a21\u578b\u4e2d\u8fdb\u884c\u5bfc\u822a\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5176\u5728\u590d\u6742\u8840\u7ba1\u7ed3\u6784\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7b80\u5316\u7248COAST\u5bfc\u4e1d\u673a\u5668\u4eba\u80fd\u591f\u6709\u6548\u5bfc\u822a\u590d\u6742\u7684\u6a21\u578b\u8840\u7ba1\u7ed3\u6784\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u89e3\u5256\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u7b80\u5316\u7248COAST\u5bfc\u4e1d\u673a\u5668\u4eba\u5728\u8109\u52a8\u6d41\u89e3\u5256\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u4e3a\u8840\u7ba1\u4ecb\u5165\u624b\u672f\u7684\u673a\u5668\u4eba\u8f85\u52a9\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12339", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12339", "abs": "https://arxiv.org/abs/2512.12339", "authors": ["Maurya Goyal", "Anuj Singh", "Hadi Jamali-Rad"], "title": "Unified Control for Inference-Time Guidance of Denoising Diffusion Models", "comment": null, "summary": "Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe", "AI": {"tldr": "UniCoDe\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7b97\u6cd5\uff0c\u5c06\u91c7\u6837\u65b9\u6cd5\u548c\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u7ed3\u5408\uff0c\u901a\u8fc7\u6574\u5408\u5c40\u90e8\u68af\u5ea6\u4fe1\u53f7\u63d0\u9ad8\u91c7\u6837\u6548\u7387\uff0c\u5728\u5956\u52b1\u5bf9\u9f50\u548c\u6269\u6563\u6a21\u578b\u5148\u9a8c\u4e4b\u95f4\u5b9e\u73b0\u66f4\u597d\u7684\u6743\u8861\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4e0e\u4e0b\u6e38\u76ee\u6807\u7684\u5bf9\u9f50\u5bf9\u4e8e\u63d0\u9ad8\u4efb\u52a1\u7279\u5b9a\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u63a8\u7406\u65f6\u65e0\u8bad\u7ec3\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u91c7\u6837\u65b9\u6cd5\u548c\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\uff0c\u4f46\u5404\u81ea\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u6765\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faUniCoDe\u7edf\u4e00\u7b97\u6cd5\uff0c\u5c06\u91c7\u6837\u65b9\u6cd5\u548c\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u7ed3\u5408\u5230\u4e00\u4e2a\u6846\u67b6\u4e2d\u3002\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u6574\u5408\u5c40\u90e8\u68af\u5ea6\u4fe1\u53f7\uff0c\u89e3\u51b3\u590d\u6742\u5956\u52b1\u91c7\u6837\u65b9\u6cd5\u7684\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniCoDe\u5728\u4e00\u7cfb\u5217\u4efb\u52a1\u4e2d\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5728\u5956\u52b1\u5bf9\u9f50\u548c\u6269\u6563\u6a21\u578b\u65e0\u6761\u4ef6\u5148\u9a8c\u504f\u79bb\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6743\u8861\u3002", "conclusion": "UniCoDe\u6210\u529f\u5730\u5c06\u91c7\u6837\u548c\u68af\u5ea6\u5f15\u5bfc\u4e24\u79cd\u8303\u5f0f\u7edf\u4e00\u8d77\u6765\uff0c\u901a\u8fc7\u6574\u5408\u5c40\u90e8\u68af\u5ea6\u4fe1\u53f7\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13514", "abs": "https://arxiv.org/abs/2512.13514", "authors": ["Aman Arora", "Matteo El-Hariry", "Miguel Olivares-Mendez"], "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM", "comment": "Presented at AI4OPA Workshop at the International Conference on Space Robotics (iSpaRo) 2025 at Sendai, Japan", "summary": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3JAXA Int-Ball2\u673a\u5668\u4eba\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u65e5\u672c\u5b9e\u9a8c\u8231\u5185\u8fdb\u884c\u516d\u81ea\u7531\u5ea6\u7cbe\u786e\u5bf9\u63a5\uff0c\u901a\u8fc7PPO\u7b97\u6cd5\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\u4e2d\u5904\u7406\u4f20\u611f\u566a\u58f0\u3001\u6267\u884c\u5668\u5931\u914d\u548c\u73af\u5883\u53d8\u5316\u7b49\u6311\u6218\u3002", "motivation": "\u56fd\u9645\u7a7a\u95f4\u7ad9\u5185\u7684\u81ea\u4e3b\u81ea\u7531\u98de\u884c\u5668\u5728\u6267\u884c\u8231\u5185\u4efb\u52a1\u65f6\u9762\u4e34\u7cbe\u786e\u5bf9\u63a5\u7684\u6311\u6218\uff0c\u5305\u62ec\u4f20\u611f\u566a\u58f0\u3001\u5c0f\u578b\u6267\u884c\u5668\u5931\u914d\u548c\u73af\u5883\u53d8\u5316\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4f7f\u5f97\u5728\u53d7\u9650\u5fae\u91cd\u529b\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u5bf9\u63a5\u6210\u4e3a\u975e\u5e73\u51e1\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\uff0c\u5728\u9ad8\u4fdd\u771f\u7684Isaac Sim\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u516d\u81ea\u7531\u5ea6\u5bf9\u63a5\u63a7\u5236\u5668\u3002\u901a\u8fc7\u57df\u968f\u673a\u5316\u52a8\u529b\u5b66\u548c\u6709\u754c\u89c2\u6d4b\u566a\u58f0\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u663e\u5f0f\u5efa\u6a21\u87ba\u65cb\u6868\u963b\u529b\u626d\u77e9\u6548\u5e94\u548c\u6781\u6027\u7ed3\u6784\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u5bf9\u63a5\uff0c\u4e3a\u7814\u7a76Int-Ball2\u63a8\u8fdb\u7269\u7406\u7279\u6027\u5982\u4f55\u5f71\u54cd\u5f3a\u5316\u5b66\u4e60\u5bf9\u63a5\u6027\u80fd\u63d0\u4f9b\u4e86\u53d7\u63a7\u7814\u7a76\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aInt-Ball2\u673a\u5668\u4eba\u5728\u78b0\u649e\u611f\u77e5\u5bfc\u822a\u3001\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u3001\u63a8\u8fdb\u7cbe\u786e\u7684\u6a21\u62df\u5230\u771f\u5b9e\u8f6c\u79fb\u4ee5\u53ca\u57fa\u4e8e\u89c6\u89c9\u7684\u7aef\u5230\u7aef\u5bf9\u63a5\u7b49\u672a\u6765\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.12357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12357", "abs": "https://arxiv.org/abs/2512.12357", "authors": ["Zishen Song", "Yongjian Zhu", "Dong Wang", "Hongzhan Liu", "Lingyu Jiang", "Yongxing Duan", "Zehua Zhang", "Sihan Li", "Jiarui Li"], "title": "TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection", "comment": null, "summary": "Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.", "AI": {"tldr": "\u63d0\u51faTCLeaf-Net\uff0c\u4e00\u79cd\u9488\u5bf9\u7530\u95f4\u53f6\u7247\u75c5\u5bb3\u68c0\u6d4b\u7684Transformer-CNN\u6df7\u5408\u68c0\u6d4b\u5668\uff0c\u914d\u5408\u65b0\u53d1\u5e03\u7684Daylily-Leaf\u6570\u636e\u96c6\uff0c\u5728\u590d\u6742\u80cc\u666f\u3001\u57df\u504f\u79fb\u548c\u6709\u9650\u75c5\u53d8\u6570\u636e\u4e0b\u5b9e\u73b0\u9c81\u68d2\u68c0\u6d4b\u3002", "motivation": "\u7530\u95f4\u53f6\u7247\u75c5\u5bb3\u68c0\u6d4b\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u590d\u6742\u80cc\u666f\u5e72\u6270\u3001\u57df\u504f\u79fb\u95ee\u9898\uff08\u7406\u60f3\u6761\u4ef6\u4e0e\u7530\u95f4\u6761\u4ef6\u5dee\u5f02\uff09\u3001\u4ee5\u53ca\u75c5\u53d8\u7ea7\u6570\u636e\u96c6\u6709\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u7530\u95f4\u73af\u5883\u4e0b\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\u3002", "method": "1) \u63d0\u51faTCLeaf-Net\u6df7\u5408\u68c0\u6d4b\u5668\uff1b2) Transformer-\u5377\u79ef\u6a21\u5757(TCM)\u7ed3\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u5377\u79ef\u6291\u5236\u975e\u53f6\u7247\u533a\u57df\uff1b3) \u539f\u59cb\u5c3a\u5ea6\u7279\u5f81\u53ec\u56de\u4e0e\u91c7\u6837(RSFRS)\u5757\u901a\u8fc7\u53cc\u7ebf\u6027\u91cd\u91c7\u6837\u548c\u5377\u79ef\u4fdd\u7559\u7a7a\u95f4\u7ec6\u8282\uff1b4) \u53ef\u53d8\u5f62\u5bf9\u9f50FPN(DFPN)\u4f7f\u7528\u504f\u79fb\u5bf9\u9f50\u548c\u591a\u611f\u53d7\u91ce\u611f\u77e5\u589e\u5f3a\u591a\u5c3a\u5ea6\u878d\u5408\u3002", "result": "\u5728Daylily-Leaf\u6570\u636e\u96c6\u7530\u95f4\u5206\u5272\u4e0a\uff0cmAP@50\u63d0\u53475.4\u4e2a\u767e\u5206\u70b9\u81f378.2%\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c117.5 GFLOPs\uff0cGPU\u5185\u5b58\u4f7f\u7528\u964d\u4f4e8.7%\u3002\u4f18\u4e8eYOLO\u548cRT-DETR\u7cfb\u5217\uff0c\u5728PlantDoc\u3001Tomato-Leaf\u548cRice-Leaf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "TCLeaf-Net\u80fd\u6709\u6548\u5e94\u5bf9\u7530\u95f4\u53f6\u7247\u75c5\u5bb3\u68c0\u6d4b\u7684\u4e09\u5927\u6311\u6218\uff0c\u5728\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13561", "abs": "https://arxiv.org/abs/2512.13561", "authors": ["Li-Wei Shih", "Ruo-Syuan Mei", "Jesse Heidrich", "Hui-Ping Wang", "Joel Hooton", "Joshua Solomon", "Jorge Arinez", "Guangze Li", "Chenhui Shao"], "title": "Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments", "comment": "Submitted to the 54th SME North American Manufacturing Research Conference (NAMRC 54)", "summary": "Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u4e09\u5c42\u8fd1\u573a\u611f\u77e5\u6846\u67b6\uff0c\u5305\u62ec\u5149\u95f4\u65ad\u68c0\u6d4b\u3001\u5149\u4f4d\u79fb\u6d4b\u91cf\u548c\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u5728\u6811\u8393\u6d3e5\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd", "motivation": "\u4f20\u7edf\u6d4b\u8ddd\u4f20\u611f\u5668\uff08\u5982LiDAR\u548c\u8d85\u58f0\u6ce2\uff09\u5728\u5236\u9020\u73af\u5883\u4e2d\u4e3a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u5e7f\u6cdb\u7684\u60c5\u5883\u611f\u77e5\uff0c\u4f46\u7ecf\u5e38\u65e0\u6cd5\u68c0\u6d4b\u5230\u673a\u5668\u4eba\u5e95\u5ea7\u9644\u8fd1\u7684\u5c0f\u7269\u4f53\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u5b89\u5168\u64cd\u4f5c\u80fd\u529b", "method": "\u63d0\u51fa\u4e09\u5c42\u8fd1\u573a\u611f\u77e5\u6846\u67b6\uff1a1\uff09\u5149\u95f4\u65ad\u68c0\u6d4b\uff1a\u5728\u8fd1\u573a\u533a\u57df\u6295\u5c04\u6fc0\u5149\u6761\u7eb9\uff0c\u901a\u8fc7\u68c0\u6d4b\u6761\u7eb9\u4e2d\u65ad\u5b9e\u73b0\u5feb\u901f\u4e8c\u8fdb\u5236\u969c\u788d\u7269\u5b58\u5728\u68c0\u6d4b\uff1b2\uff09\u5149\u4f4d\u79fb\u6d4b\u91cf\uff1a\u901a\u8fc7\u5206\u6790\u76f8\u673a\u56fe\u50cf\u4e2d\u6295\u5f71\u6761\u7eb9\u7684\u51e0\u4f55\u4f4d\u79fb\u6765\u4f30\u8ba1\u7269\u4f53\u9ad8\u5ea6\uff1b3\uff09\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff1a\u5728\u5d4c\u5165\u5f0fAI\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5bf9\u8c61\u5206\u7c7b\uff0c\u652f\u6301\u8bed\u4e49\u611f\u77e5\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b89\u5168\u51b3\u7b56", "result": "\u6240\u6709\u65b9\u6cd5\u5728\u6811\u8393\u6d3e5\u7cfb\u7edf\u4e0a\u5b9e\u73b0\uff0c\u8fbe\u523025\u621650\u5e27/\u79d2\u7684\u5b9e\u65f6\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc4\u4f30\u548c\u6bd4\u8f83\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5c42\u6b21\u7ed3\u6784\u5728\u7cbe\u5ea6\u3001\u8ba1\u7b97\u548c\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5236\u9020\u73af\u5883\u4e2d\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5b89\u5168\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5c42\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fd1\u573a\u5c0f\u7269\u4f53\u68c0\u6d4b\u7684\u6311\u6218"}}
{"id": "2512.13644", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13644", "abs": "https://arxiv.org/abs/2512.13644", "authors": ["Raktim Gautam Goswami", "Amir Bar", "David Fan", "Tsung-Yen Yang", "Gaoyue Zhou", "Prashanth Krishnamurthy", "Michael Rabbat", "Farshad Khorrami", "Yann LeCun"], "title": "World Models Can Leverage Human Videos for Dexterous Manipulation", "comment": null, "summary": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.", "AI": {"tldr": "DexWM\u662f\u4e00\u4e2a\u7075\u5de7\u64cd\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4b\u73af\u5883\u7684\u4e0b\u4e00\u4e2a\u6f5c\u5728\u72b6\u6001\u6765\u7406\u89e3\u624b\u90e8\u8fd0\u52a8\u5982\u4f55\u901a\u8fc7\u7269\u4f53\u63a5\u89e6\u5f71\u54cd\u73af\u5883\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7075\u5de7\u64cd\u4f5c\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7406\u89e3\u7ec6\u5fae\u7684\u624b\u90e8\u8fd0\u52a8\u5982\u4f55\u901a\u8fc7\u7269\u4f53\u63a5\u89e6\u5f71\u54cd\u73af\u5883\u3002\u73b0\u6709\u7075\u5de7\u64cd\u4f5c\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u4e14\u4ec5\u9884\u6d4b\u89c6\u89c9\u7279\u5f81\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u7cbe\u7ec6\u7075\u5de7\u6027\u3002", "method": "\u63d0\u51faDexWM\u6a21\u578b\uff0c\u57fa\u4e8e\u8fc7\u53bb\u72b6\u6001\u548c\u7075\u5de7\u52a8\u4f5c\u9884\u6d4b\u73af\u5883\u7684\u4e0b\u4e00\u4e2a\u6f5c\u5728\u72b6\u6001\u3002\u4e3a\u89e3\u51b3\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\uff0c\u4f7f\u7528\u8d85\u8fc7900\u5c0f\u65f6\u7684\u4eba\u7c7b\u548c\u975e\u7075\u5de7\u673a\u5668\u4eba\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\u3002\u5f15\u5165\u8f85\u52a9\u624b\u90e8\u4e00\u81f4\u6027\u635f\u5931\u6765\u786e\u4fdd\u51c6\u786e\u7684\u624b\u90e8\u914d\u7f6e\uff0c\u4ee5\u589e\u5f3a\u7cbe\u7ec6\u7075\u5de7\u6027\u3002", "result": "DexWM\u5728\u9884\u6d4b\u672a\u6765\u72b6\u6001\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u57fa\u4e8e\u6587\u672c\u3001\u5bfc\u822a\u548c\u5168\u8eab\u52a8\u4f5c\u7684\u4e16\u754c\u6a21\u578b\u3002\u5728\u914d\u5907Allegro\u5939\u722a\u7684Franka Panda\u673a\u68b0\u81c2\u4e0a\u90e8\u7f72\u65f6\uff0c\u5728\u6293\u53d6\u3001\u653e\u7f6e\u548c\u5230\u8fbe\u4efb\u52a1\u4e2d\u5e73\u5747\u6bd4Diffusion Policy\u9ad8\u51fa50%\u4ee5\u4e0a\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DexWM\u901a\u8fc7\u7ed3\u5408\u5927\u89c4\u6a21\u89c6\u9891\u8bad\u7ec3\u548c\u624b\u90e8\u4e00\u81f4\u6027\u635f\u5931\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u6311\u6218\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u64cd\u4f5c\u6280\u80fd\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u7075\u5de7\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e16\u754c\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12372", "abs": "https://arxiv.org/abs/2512.12372", "authors": ["Peixuan Zhang", "Zijian Jia", "Kaiqi Liu", "Shuchen Weng", "Si Li", "Boxin Shi"], "title": "STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative", "comment": null, "summary": "While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.", "AI": {"tldr": "STAGE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6545\u4e8b\u677f\u7684\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u9884\u6d4b\u7ed3\u6784\u5316\u6545\u4e8b\u677f\u800c\u975e\u7a00\u758f\u5173\u952e\u5e27\uff0c\u7ed3\u5408\u591a\u955c\u5934\u8bb0\u5fc6\u5305\u548c\u53cc\u7f16\u7801\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u591a\u955c\u5934\u53d9\u4e8b\u4e2d\u8de8\u955c\u5934\u4e00\u81f4\u6027\u548c\u7535\u5f71\u8bed\u8a00\u6355\u6349\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u5728\u89c6\u9891\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u521b\u5efa\u8fde\u8d2f\u7684\u591a\u955c\u5934\u53d9\u4e8b\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u5173\u952e\u5e27\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u66f4\u9ad8\u6548\u7387\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u8de8\u955c\u5934\u4e00\u81f4\u6027\u5e76\u6355\u6349\u7535\u5f71\u8bed\u8a00\u3002", "method": "\u63d0\u51fa\u4e86STAGE\u5de5\u4f5c\u6d41\uff0c\u4f7f\u7528STEP2\u9884\u6d4b\u6bcf\u4e2a\u955c\u5934\u7684\u7ed3\u6784\u5316\u6545\u4e8b\u677f\uff08\u8d77\u59cb-\u7ed3\u675f\u5e27\u5bf9\uff09\u3002\u5f15\u5165\u4e86\u591a\u955c\u5934\u8bb0\u5fc6\u5305\u786e\u4fdd\u957f\u8ddd\u79bb\u5b9e\u4f53\u4e00\u81f4\u6027\uff0c\u53cc\u7f16\u7801\u7b56\u7565\u4fdd\u8bc1\u955c\u5934\u5185\u8fde\u8d2f\u6027\uff0c\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u5b66\u4e60\u7535\u5f71\u5316\u7684\u955c\u5934\u95f4\u8fc7\u6e21\u3002\u540c\u65f6\u8d21\u732e\u4e86ConStoryBoard\u6570\u636e\u96c6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSTAGE\u5728\u7ed3\u6784\u5316\u53d9\u4e8b\u63a7\u5236\u548c\u8de8\u955c\u5934\u8fde\u8d2f\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "STAGE\u901a\u8fc7\u6545\u4e8b\u677f\u951a\u5b9a\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8de8\u955c\u5934\u4e00\u81f4\u6027\u548c\u7535\u5f71\u8bed\u8a00\u6355\u6349\u95ee\u9898\uff0c\u4e3a\u8fde\u8d2f\u53d9\u4e8b\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.13660", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13660", "abs": "https://arxiv.org/abs/2512.13660", "authors": ["Enshen Zhou", "Cheng Chi", "Yibo Li", "Jingkun An", "Jiayuan Zhang", "Shanyu Rong", "Yi Han", "Yuheng Ji", "Mengzhen Liu", "Pengwei Wang", "Zhongyuan Wang", "Lu Sheng", "Shanghang Zhang"], "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics", "comment": "Project page: https://zhoues.github.io/RoboTracer", "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.", "AI": {"tldr": "RoboTracer\u662f\u4e00\u4e2a3D\u611f\u77e5\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u7f16\u7801\u5668\u548c\u56de\u5f52\u76d1\u7763\u89e3\u7801\u5668\u5b9e\u73b03D\u7a7a\u95f4\u6307\u4ee3\u548c\u6d4b\u91cf\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5fae\u8c03\u8fdb\u884c\u591a\u6b65\u5ea6\u91cf\u63a8\u7406\uff0c\u5728\u7a7a\u95f4\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7a7a\u95f4\u8ffd\u8e2a\u4f5c\u4e3a\u673a\u5668\u4eba\u7684\u57fa\u672c\u5177\u8eab\u4ea4\u4e92\u80fd\u529b\uff0c\u9700\u8981\u591a\u6b65\u5ea6\u91cf\u63a8\u7406\u4e0e\u590d\u6742\u7a7a\u95f4\u6307\u4ee3\u548c\u771f\u5b9e\u4e16\u754c\u5ea6\u91cf\u6d4b\u91cf\u7684\u7ed3\u5408\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u7ec4\u5408\u4efb\u52a1\u3002", "method": "1) \u63d0\u51faRoboTracer 3D\u611f\u77e5VLM\uff0c\u901a\u8fc7\u901a\u7528\u7a7a\u95f4\u7f16\u7801\u5668\u548c\u56de\u5f52\u76d1\u7763\u89e3\u7801\u5668\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u589e\u5f3a\u5c3a\u5ea6\u611f\u77e5\uff1b2) \u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u4e0e\u5ea6\u91cf\u654f\u611f\u8fc7\u7a0b\u5956\u52b1\u63a8\u8fdb\u591a\u6b65\u5ea6\u91cf\u63a8\u7406\uff1b3) \u6784\u5efaTraceSpatial\u6570\u636e\u96c6(30M QA\u5bf9)\u548cTraceSpatial-Bench\u57fa\u51c6\u3002", "result": "RoboTracer\u5728\u7a7a\u95f4\u7406\u89e3\u3001\u6d4b\u91cf\u548c\u6307\u4ee3\u65b9\u9762\u8d85\u8d8a\u57fa\u7ebf\uff0c\u5e73\u5747\u6210\u529f\u738779.1%\uff1b\u5728TraceSpatial-Bench\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8d85\u8fc7Gemini-2.5-Pro 36%\u51c6\u786e\u7387\uff1b\u53ef\u4e0e\u591a\u79cd\u63a7\u5236\u7b56\u7565\u96c6\u6210\uff0c\u5728\u6742\u4e71\u771f\u5b9e\u573a\u666f\u4e2d\u6267\u884c\u957f\u65f6\u7a0b\u52a8\u6001\u4efb\u52a1\u3002", "conclusion": "RoboTracer\u901a\u8fc7\u521b\u65b0\u76843D\u611f\u77e5\u67b6\u6784\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a7a\u95f4\u8ffd\u8e2a\u7684\u7ec4\u5408\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u5e94\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u548c\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2512.12375", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12375", "abs": "https://arxiv.org/abs/2512.12375", "authors": ["Hyunkoo Lee", "Wooseok Jang", "Jini Yang", "Taehwan Kim", "Sangoh Kim", "Sangwon Jung", "Seungryong Kim"], "title": "V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping", "comment": "Project Page: https://cvlab-kaist.github.io/V-Warper", "summary": "Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.", "AI": {"tldr": "V-Warper\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u5916\u89c2\u9002\u5e94\u548c\u7ec6\u7c92\u5ea6\u5916\u89c2\u6ce8\u5165\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6587\u672c\u5bf9\u9f50\u548c\u8fd0\u52a8\u52a8\u6001\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5916\u89c2\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4e2a\u6027\u5316\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u89c6\u9891\u5fae\u8c03\u6216\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u540c\u65f6\u5728\u8de8\u5e27\u4fdd\u6301\u7ec6\u7c92\u5ea6\u5916\u89c2\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff1a1) \u8f7b\u91cf\u7ea7\u7c97\u7c92\u5ea6\u5916\u89c2\u9002\u5e94\u9636\u6bb5\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u53c2\u8003\u56fe\u50cf\uff0c\u901a\u8fc7\u56fe\u50cf\u7ea7LoRA\u548c\u4e3b\u4f53\u5d4c\u5165\u9002\u5e94\u7f16\u7801\u5168\u5c40\u4e3b\u4f53\u8eab\u4efd\uff1b2) \u63a8\u7406\u65f6\u7ec6\u7c92\u5ea6\u5916\u89c2\u6ce8\u5165\u9636\u6bb5\uff0c\u901a\u8fc7\u8ba1\u7b97RoPE-free\u4e2d\u95f4\u5c42\u67e5\u8be2-\u952e\u7279\u5f81\u7684\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\uff0c\u5f15\u5bfc\u5916\u89c2\u4e30\u5bcc\u7684\u503c\u8868\u793a\u5230\u751f\u6210\u8fc7\u7a0b\u7684\u8bed\u4e49\u5bf9\u9f50\u533a\u57df\u3002", "result": "V-Warper\u663e\u8457\u63d0\u9ad8\u4e86\u5916\u89c2\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63d0\u793a\u5bf9\u9f50\u548c\u8fd0\u52a8\u52a8\u6001\uff0c\u4e14\u65e0\u9700\u5927\u89c4\u6a21\u89c6\u9891\u5fae\u8c03\u5373\u53ef\u9ad8\u6548\u5b9e\u73b0\u8fd9\u4e9b\u6539\u8fdb\u3002", "conclusion": "V-Warper\u4e3a\u57fa\u4e8etransformer\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u5916\u89c2\u4e00\u81f4\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.12378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12378", "abs": "https://arxiv.org/abs/2512.12378", "authors": ["Junqiao Fan", "Yunjiao Zhou", "Yizhuo Yang", "Xinyuan Cui", "Jiarui Zhang", "Lihua Xie", "Jianfei Yang", "Chris Xiaoxuan Lu", "Fangqiang Ding"], "title": "M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction", "comment": null, "summary": "Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.", "AI": {"tldr": "M4Human\u662f\u76ee\u524d\u6700\u5927\u89c4\u6a21\u7684\u591a\u6a21\u6001\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b66.1\u4e07\u5e27\u9ad8\u5206\u8fa8\u7387\u6beb\u7c73\u6ce2\u96f7\u8fbe\u3001RGB\u548c\u6df1\u5ea6\u6570\u636e\uff0c\u63d0\u4f9b\u539f\u59cb\u96f7\u8fbe\u5f20\u91cf\u548c\u5904\u7406\u540e\u7684\u96f7\u8fbe\u70b9\u4e91\uff0c\u652f\u6301\u4e0d\u540c\u7c92\u5ea6RF\u4fe1\u53f7\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u6570\u636e\u96c6\u4e3b\u8981\u4f9d\u8d56\u89c6\u7ebfRGB\u8f93\u5165\uff0c\u4f46\u89c6\u89c9\u4f20\u611f\u53d7\u906e\u6321\u3001\u5149\u7167\u53d8\u5316\u548c\u9690\u79c1\u95ee\u9898\u9650\u5236\u3002\u6beb\u7c73\u6ce2\u96f7\u8fbe\u80fd\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u5ba4\u5185\u4eba\u4f53\u611f\u77e5\uff0c\u4f46\u73b0\u6709\u96f7\u8fbe\u6570\u636e\u96c6\u5b58\u5728\u9aa8\u67b6\u6807\u7b7e\u7a00\u758f\u3001\u89c4\u6a21\u6709\u9650\u3001\u52a8\u4f5c\u7b80\u5355\u7b49\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b20\u540d\u53d7\u8bd5\u8005\u548c50\u79cd\u591a\u6837\u5316\u52a8\u4f5c\u7684M4Human\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u6beb\u7c73\u6ce2\u96f7\u8fbe\u3001RGB\u548c\u6df1\u5ea6\u6570\u636e\uff0c\u5305\u62ec\u539f\u59cb\u96f7\u8fbe\u5f20\u91cf\u548c\u5904\u7406\u540e\u7684\u96f7\u8fbe\u70b9\u4e91\uff0c\u5e76\u914d\u6709\u9ad8\u8d28\u91cf\u52a8\u4f5c\u6355\u6349\u6807\u6ce8\uff083D\u7f51\u683c\u548c\u5168\u5c40\u8f68\u8ff9\uff09\u3002", "result": "M4Human\u662f\u76ee\u524d\u6700\u5927\u89c4\u6a21\u7684\u96f7\u8fbe\u4eba\u4f53\u5efa\u6a21\u6570\u636e\u96c6\uff08\u6bd4\u4e4b\u524d\u6700\u5927\u6570\u636e\u96c6\u59279\u500d\uff09\uff0c\u5efa\u7acb\u4e86\u96f7\u8fbe\u5f20\u91cf\u548c\u96f7\u8fbe\u70b9\u4e91\u4e24\u79cd\u6a21\u6001\u7684\u57fa\u51c6\uff0c\u5e76\u63a2\u7d22\u4e86\u4e0eRGB-D\u6a21\u6001\u7684\u591a\u6a21\u6001\u878d\u5408\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6570\u636e\u96c6\u5bf9\u96f7\u8fbe\u4eba\u4f53\u5efa\u6a21\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5728\u5feb\u901f\u3001\u65e0\u7ea6\u675f\u8fd0\u52a8\u4e0b\u7684\u6301\u7eed\u6311\u6218\u3002", "conclusion": "M4Human\u4e3a\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u514b\u670d\u4e86\u73b0\u6709\u96f7\u8fbe\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u4e0d\u540c\u7c92\u5ea6RF\u4fe1\u53f7\u7814\u7a76\uff0c\u5c06\u4fc3\u8fdb\u9690\u79c1\u4fdd\u62a4\u4eba\u4f53\u611f\u77e5\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.12386", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12386", "abs": "https://arxiv.org/abs/2512.12386", "authors": ["Swayam Bhanded"], "title": "Speedrunning ImageNet Diffusion", "comment": null, "summary": "Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.", "AI": {"tldr": "SR-DiT\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u591a\u79cd\u6280\u672f\uff08token\u8def\u7531\u3001\u67b6\u6784\u6539\u8fdb\u3001\u8bad\u7ec3\u4fee\u6539\u548c\u8868\u793a\u5bf9\u9f50\uff09\uff0c\u5728140M\u53c2\u6570\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e3a\u6269\u6563\u53d8\u6362\u5668\u63d0\u4f9b\u4e86\u9ad8\u6548\u8bad\u7ec3\u57fa\u51c6\u3002", "motivation": "\u867d\u7136\u6269\u6563\u53d8\u6362\u5668\u7684\u8bad\u7ec3\u6548\u7387\u5df2\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u73b0\u6709\u6280\u672f\u591a\u88ab\u5b64\u7acb\u7814\u7a76\uff0c\u7f3a\u4e4f\u5bf9\u591a\u79cd\u65b9\u6cd5\u7ec4\u5408\u534f\u540c\u6548\u5e94\u7684\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6574\u5408\u4e0d\u540c\u6280\u672f\uff0c\u53d1\u6398\u5176\u6f5c\u5728\u534f\u540c\u4f5c\u7528\uff0c\u4e3a\u6269\u6563\u53d8\u6362\u5668\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u63d0\u51faSR-DiT\uff08Speedrun Diffusion Transformer\uff09\u6846\u67b6\uff0c\u5728\u8868\u793a\u5bf9\u9f50\u57fa\u7840\u4e0a\u7cfb\u7edf\u6574\u5408\uff1a1\uff09token\u8def\u7531\u6280\u672f\uff1b2\uff09\u67b6\u6784\u6539\u8fdb\uff1b3\uff09\u8bad\u7ec3\u4fee\u6539\u3002\u901a\u8fc7\u5927\u91cf\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e0d\u540c\u6280\u672f\u7ec4\u5408\u7684\u6709\u6548\u6027\uff0c\u8bc6\u522b\u534f\u540c\u4f5c\u7528\u548c\u517c\u5bb9\u6027\u95ee\u9898\u3002", "result": "\u5728ImageNet-256\u4e0a\u4ec5\u4f7f\u7528140M\u53c2\u6570\u6a21\u578b\u3001400K\u8fed\u4ee3\u3001\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230FID 3.49\u548cKDD 0.319\uff0c\u6027\u80fd\u4e0e\u8bad\u7ec3\u65f6\u95f4\u66f4\u957f\u3001\u53c2\u6570\u66f4\u591a\uff08685M\uff09\u7684\u6a21\u578b\u76f8\u5f53\u3002\u8fd9\u662f\u8be5\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "SR-DiT\u6846\u67b6\u6210\u529f\u5c55\u793a\u4e86\u591a\u79cd\u6280\u672f\u6574\u5408\u7684\u534f\u540c\u6548\u5e94\uff0c\u4e3a\u6269\u6563\u53d8\u6362\u5668\u63d0\u4f9b\u4e86\u8ba1\u7b97\u53ef\u8bbf\u95ee\u7684\u57fa\u51c6\u3002\u7814\u7a76\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u5c0f\u6a21\u578b\u7684\u9ad8\u6027\u80fd\uff0c\u8fd8\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6280\u672f\u7ec4\u5408\u7684\u6709\u6548\u6027\u6307\u5bfc\u3002"}}
{"id": "2512.12395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12395", "abs": "https://arxiv.org/abs/2512.12395", "authors": ["Haowen Wang", "Xiaoping Yuan", "Fugang Zhang", "Rui Jian", "Yuanwei Zhu", "Xiuquan Qiao", "Yakun Huang"], "title": "ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States", "comment": null, "summary": "Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.", "AI": {"tldr": "ArtGen\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u80fd\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u6216\u6587\u672c\u63cf\u8ff0\u751f\u6210\u5177\u6709\u51c6\u786e\u51e0\u4f55\u7ed3\u6784\u548c\u8fde\u8d2f\u8fd0\u52a8\u5b66\u7684\u94f0\u63a5\u5f0f3D\u7269\u4f53\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u51e0\u4f55\u5f62\u72b6\u4e0e\u5173\u8282\u52a8\u529b\u5b66\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u8868\u793a\u95ed\u5408\u72b6\u6001\u7684\u5355\u89c6\u56fe\u8f93\u5165\uff0c\u5bfc\u81f4\u51e0\u4f55\u5f62\u72b6\u4e0e\u5173\u8282\u52a8\u529b\u5b66\u7ea0\u7f20\uff0c\u4ea7\u751f\u6a21\u7cca\u6216\u4e0d\u73b0\u5b9e\u7684\u8fd0\u52a8\u5b66\u7ed3\u6784\u3002\u94f0\u63a5\u5f0f\u8d44\u4ea7\u751f\u6210\u5bf9\u673a\u5668\u4eba\u3001\u6570\u5b57\u5b6a\u751f\u548c\u5177\u8eab\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u4f7f\u7528\u8de8\u72b6\u6001\u8499\u7279\u5361\u6d1b\u91c7\u6837\u660e\u786e\u5f3a\u5236\u6267\u884c\u5168\u5c40\u8fd0\u52a8\u5b66\u4e00\u81f4\u6027\uff1b2. \u96c6\u6210\u601d\u7ef4\u94fe\u63a8\u7406\u6a21\u5757\u63a8\u65ad\u7ed3\u6784\u5148\u9a8c\uff08\u90e8\u4ef6\u8bed\u4e49\u3001\u5173\u8282\u7c7b\u578b\u3001\u8fde\u63a5\u6027\uff09\uff1b3. \u7a00\u758f\u4e13\u5bb6\u6269\u6563\u53d8\u6362\u5668\u4e13\u95e8\u5904\u7406\u591a\u6837\u8fd0\u52a8\u5b66\u4ea4\u4e92\uff1b4. \u5c40\u90e8-\u5168\u5c40\u6ce8\u610f\u529b\u589e\u5f3a\u7684\u7ec4\u5408\u5f0f3D-VAE\u6f5c\u5728\u5148\u9a8c\u3002", "result": "\u5728PartNet-Mobility\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cArtGen\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "ArtGen\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u5f62\u72b6\u4e0e\u8fd0\u52a8\u5b66\uff0c\u80fd\u591f\u4ece\u5355\u89c6\u56fe\u8f93\u5165\u751f\u6210\u5177\u6709\u51c6\u786e\u51e0\u4f55\u548c\u8fde\u8d2f\u8fd0\u52a8\u5b66\u7684\u94f0\u63a5\u5f0f3D\u7269\u4f53\uff0c\u4e3a\u673a\u5668\u4eba\u3001\u6570\u5b57\u5b6a\u751f\u548c\u5177\u8eab\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12424", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12424", "abs": "https://arxiv.org/abs/2512.12424", "authors": ["Tue-Thu Van-Dinh", "Hoang-Duy Tran", "Truong-Binh Duong", "Mai-Hanh Pham", "Binh-Nam Le-Nguyen", "Quoc-Thai Nguyen"], "title": "ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics", "comment": "10 pages, 4 figures, Accepted to AI4Research @ AAAI", "summary": "Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.", "AI": {"tldr": "ViInfographicVQA\u662f\u9996\u4e2a\u8d8a\u5357\u8bed\u4fe1\u606f\u56fe\u8868\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b6747\u4e2a\u771f\u5b9e\u4fe1\u606f\u56fe\u8868\u548c20409\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u5355\u56fe\u548c\u591a\u56fe\u63a8\u7406\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4fe1\u606f\u56fe\u8868\u89c6\u89c9\u95ee\u7b54\u9700\u8981\u6a21\u578b\u7406\u89e3\u6570\u636e\u4e30\u5bcc\u3001\u5e03\u5c40\u590d\u6742\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u7ed3\u5408\u6587\u672c\u3001\u56fe\u8868\u3001\u56fe\u6807\u548c\u8bbe\u8ba1\u5143\u7d20\u3002\u76f8\u6bd4\u573a\u666f\u6587\u672c\u6216\u81ea\u7136\u56fe\u50cfVQA\uff0c\u4fe1\u606f\u56fe\u8868\u9700\u8981\u66f4\u5f3a\u7684OCR\u3001\u5e03\u5c40\u7406\u89e3\u4ee5\u53ca\u6570\u503c\u548c\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002\u76ee\u524d\u7f3a\u4e4f\u8d8a\u5357\u8bed\u7684\u4fe1\u606f\u56fe\u8868VQA\u57fa\u51c6\uff0c\u7279\u522b\u662f\u5728\u8de8\u56fe\u50cf\u63a8\u7406\u65b9\u9762\u3002", "method": "\u521b\u5efaViInfographicVQA\u57fa\u51c6\uff0c\u5305\u542b6747\u4e2a\u771f\u5b9e\u4e16\u754c\u4fe1\u606f\u56fe\u8868\u548c20409\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u7ecf\u6d4e\u3001\u533b\u7597\u3001\u6559\u80b2\u7b49\u591a\u4e2a\u9886\u57df\u3002\u57fa\u51c6\u5305\u542b\u4e24\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff1a\u5355\u56fe\u4efb\u52a1\uff08\u4f20\u7edf\u8bbe\u7f6e\uff09\u548c\u591a\u56fe\u4efb\u52a1\uff08\u9700\u8981\u8de8\u591a\u4e2a\u8bed\u4e49\u76f8\u5173\u4fe1\u606f\u56fe\u8868\u5408\u6210\u8bc1\u636e\uff09\u3002\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u6700\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u6700\u4e25\u91cd\u7684\u9519\u8bef\u53d1\u751f\u5728\u591a\u56fe\u95ee\u9898\u4e0a\uff0c\u7279\u522b\u662f\u6d89\u53ca\u8de8\u56fe\u50cf\u6574\u5408\u548c\u975e\u8de8\u5ea6\u63a8\u7406\u7684\u4efb\u52a1\u3002\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\u3002", "conclusion": "ViInfographicVQA\u4e3a\u8d8a\u5357\u8bed\u4fe1\u606f\u56fe\u8868VQA\u63d0\u4f9b\u4e86\u57fa\u51c6\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u9f13\u52b1\u672a\u6765\u63a2\u7d22\u5e03\u5c40\u611f\u77e5\u548c\u8de8\u56fe\u50cf\u63a8\u7406\u65b9\u6cd5\u3002"}}
{"id": "2512.12622", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12622", "abs": "https://arxiv.org/abs/2512.12622", "authors": ["Zihan Wang", "Seungjun Lee", "Guangzhao Dai", "Gim Hee Lee"], "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation", "comment": null, "summary": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.", "AI": {"tldr": "D3D-VLP\u6a21\u578b\u901a\u8fc7\u52a8\u60013D\u601d\u7ef4\u94fe\u548c\u534f\u540c\u5b66\u4e60\u7b56\u7565\uff0c\u7edf\u4e00\u4e86\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u89c4\u5212\u3001\u5bfc\u822a\u3001\u95ee\u7b54\u7b49\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u7aef\u5230\u7aef\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u663e\u5f0f3D\u63a8\u7406\u80fd\u529b\uff0c\u800c\u6a21\u5757\u5316\u7cfb\u7edf\u5ffd\u7565\u4e86\u8de8\u7ec4\u4ef6\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u548c\u534f\u540c\u6548\u5e94\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u53c8\u80fd\u5b9e\u73b0\u8de8\u4efb\u52a1\u534f\u540c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u52a8\u60013D\u89c6\u89c9-\u8bed\u8a00-\u89c4\u5212\u6a21\u578b(D3D-VLP)\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u52a8\u60013D\u601d\u7ef4\u94fe(3D CoT)\uff0c\u5c06\u89c4\u5212\u3001\u5b9a\u4f4d\u3001\u5bfc\u822a\u548c\u95ee\u7b54\u7edf\u4e00\u5230\u5355\u4e2a3D-VLM\u548cCoT\u6d41\u7a0b\u4e2d\uff1b2) \u788e\u7247\u5316\u76d1\u7763\u534f\u540c\u5b66\u4e60(SLFS)\u7b56\u7565\uff0c\u4f7f\u7528\u63a9\u7801\u81ea\u56de\u5f52\u635f\u5931\u4ece\u5927\u89c4\u6a21\u90e8\u5206\u6807\u6ce8\u7684\u6df7\u5408\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u8ba9\u4e0d\u540cCoT\u7ec4\u4ef6\u76f8\u4e92\u589e\u5f3a\u548c\u9690\u5f0f\u76d1\u7763\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b10M\u6df7\u5408\u6837\u672c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6db5\u76d65K\u771f\u5b9e\u626b\u63cf\u548c20K\u5408\u6210\u573a\u666f\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u7ed3\u679c\uff1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a(R2R-CE, REVERIE-CE, NavRAG-CE)\u3001\u76ee\u6807\u5bfc\u822a(HM3D-OVON)\u3001\u4efb\u52a1\u5bfc\u5411\u987a\u5e8f\u5b9a\u4f4d\u548c\u5bfc\u822a(SG3D)\u3002\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u64cd\u4f5c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "D3D-VLP\u6210\u529f\u89e3\u51b3\u4e86\u5177\u8eab\u667a\u80fd\u4e2d\u53ef\u89e3\u91ca\u6027\u4e0e\u534f\u540c\u5b66\u4e60\u7684\u77db\u76fe\uff0c\u901a\u8fc7\u7edf\u4e00\u76843D\u601d\u7ef4\u94fe\u6846\u67b6\u5b9e\u73b0\u4e86\u8de8\u4efb\u52a1\u7684\u534f\u540c\u589e\u5f3a\uff0c\u5728\u591a\u4e2a\u5bfc\u822a\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12425", "abs": "https://arxiv.org/abs/2512.12425", "authors": ["Hangwei Zhang", "Armando Teles Fortes", "Tianyi Wei", "Xingang Pan"], "title": "BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation", "comment": null, "summary": "Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.", "AI": {"tldr": "BokehDepth\uff1a\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5c06\u6563\u666f\u5408\u6210\u4e0e\u6df1\u5ea6\u9884\u6d4b\u89e3\u8026\uff0c\u5229\u7528\u6563\u7126\u4f5c\u4e3a\u65e0\u76d1\u7763\u7684\u51e0\u4f55\u7ebf\u7d22\uff0c\u63d0\u5347\u6563\u666f\u6e32\u67d3\u8d28\u91cf\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6563\u666f\u4e0e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e4b\u95f4\u7684\u7d27\u5bc6\u8054\u7cfb\u3002\u9ad8\u8d28\u91cf\u7684\u6563\u666f\u6e32\u67d3\u4f9d\u8d56\u566a\u58f0\u6df1\u5ea6\u56fe\uff0c\u800c\u73b0\u4ee3\u5355\u76ee\u6df1\u5ea6\u6a21\u578b\u5728\u5f31\u7eb9\u7406\u3001\u8fdc\u8ddd\u79bb\u548c\u51e0\u4f55\u6a21\u7cca\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e9b\u533a\u57df\u6070\u6070\u662f\u6563\u7126\u7ebf\u7d22\u6700\u4e30\u5bcc\u7684\u5730\u65b9\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u9aa8\u5e72\u7684\u7269\u7406\u5f15\u5bfc\u53ef\u63a7\u6563\u666f\u751f\u6210\u5668\uff0c\u4ece\u5355\u5f20\u6e05\u6670\u8f93\u5165\u751f\u6210\u65e0\u6df1\u5ea6\u6563\u666f\u5806\u6808\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6563\u7126\u611f\u77e5\u805a\u5408\u6a21\u5757\uff0c\u5c06\u6563\u7126\u7ef4\u5ea6\u7279\u5f81\u878d\u5408\u5230\u73b0\u6709\u5355\u76ee\u6df1\u5ea6\u7f16\u7801\u5668\u4e2d\uff0c\u66b4\u9732\u7a33\u5b9a\u7684\u6df1\u5ea6\u654f\u611f\u53d8\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBokehDepth\u76f8\u6bd4\u57fa\u4e8e\u6df1\u5ea6\u56fe\u7684\u6563\u666f\u57fa\u7ebf\u63d0\u5347\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5e76\u6301\u7eed\u589e\u5f3a\u4e86\u5f3a\u5355\u76ee\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u5ea6\u91cf\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6563\u666f\u5408\u6210\u4e0e\u6df1\u5ea6\u9884\u6d4b\u89e3\u8026\uff0c\u5e76\u5c06\u6563\u7126\u4f5c\u4e3a\u8f85\u52a9\u7684\u65e0\u76d1\u7763\u51e0\u4f55\u7ebf\u7d22\uff0cBokehDepth\u6846\u67b6\u80fd\u591f\u540c\u65f6\u63d0\u5347\u6563\u666f\u6e32\u67d3\u8d28\u91cf\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u5145\u5206\u5229\u7528\u4e86\u955c\u5934\u6210\u50cf\u51e0\u4f55\u7684\u5185\u5728\u8054\u7cfb\u3002"}}
{"id": "2512.12884", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12884", "abs": "https://arxiv.org/abs/2512.12884", "authors": ["Xiangzhong Liu", "Jiajie Zhang", "Hao Shen"], "title": "Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection", "comment": "6 pages, 3 figures, accepted at IV2025", "summary": "In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4f20\u611f\u5668\u878d\u5408\u7684\u7aef\u5230\u7aef\u8de8\u5c42\u7ea7Transformer\u878d\u5408\u65b9\u6cd5\uff0c\u5c06\u9ad8\u5ea6\u62bd\u8c61\u7684\u76ee\u6807\u5217\u8868\u4fe1\u606f\u4e0e\u539f\u59cb\u76f8\u673a\u56fe\u50cf\u7ed3\u5408\u8fdb\u884c3D\u76ee\u6807\u68c0\u6d4b\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u89c6\u89c9\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u6c7d\u8f66\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u4e2d\uff0c\u667a\u80fd\u4f20\u611f\u5668\u548cV2X\u6a21\u5757\u901a\u5e38\u53ea\u63d0\u4f9b\u5904\u7406\u540e\u7684\u76ee\u6807\u5217\u8868\u800c\u975e\u539f\u59cb\u6570\u636e\u3002\u4f20\u7edf\u65b9\u6cd5\u5206\u522b\u5904\u7406\u539f\u59cb\u6570\u636e\u540e\u5728\u76ee\u6807\u5c42\u7ea7\u878d\u5408\uff0c\u5b58\u5728\u6548\u7387\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u6574\u5408\u9ad8\u5ea6\u62bd\u8c61\u76ee\u6807\u5217\u8868\u4fe1\u606f\u4e0e\u539f\u59cb\u76f8\u673a\u56fe\u50cf\u7684\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u8de8\u5c42\u7ea7\u878d\u5408\u6982\u5ff5\uff1a1) \u5c06\u76ee\u6807\u5217\u8868\u4f5c\u4e3a\u53bb\u566a\u67e5\u8be2\u8f93\u5165Transformer\uff0c\u4e0e\u53ef\u5b66\u4e60\u67e5\u8be2\u4e00\u8d77\u4f20\u64ad\uff1b2) \u4ece\u76ee\u6807\u5217\u8868\u7684\u4f4d\u7f6e\u548c\u5c3a\u5bf8\u5148\u9a8c\u4e2d\u5bfc\u51fa\u53ef\u53d8\u5f62\u9ad8\u65af\u63a9\u7801\uff0c\u96c6\u6210\u5230Transformer\u89e3\u7801\u5668\u4e2d\uff0c\u5f15\u5bfc\u6ce8\u610f\u529b\u5230\u611f\u5174\u8da3\u533a\u57df\u5e76\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff1b3) \u7531\u4e8e\u7f3a\u4e4f\u5305\u542b\u76ee\u6807\u5217\u8868\u4f5c\u4e3a\u72ec\u7acb\u6a21\u6001\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4ece\u771f\u5b9e\u8fb9\u754c\u6846\u751f\u6210\u4f2a\u76ee\u6807\u5217\u8868\u7684\u65b9\u6cd5\uff0c\u6a21\u62df\u72b6\u6001\u566a\u58f0\u548c\u8bef\u62a5/\u6f0f\u62a5\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u4e8e\u89c6\u89c9\u7684\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5bf9\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u7684\u6a21\u62df\u76ee\u6807\u5217\u8868\u548c\u771f\u5b9e\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u8fdb\u884c\u8de8\u5c42\u7ea7\u878d\u5408\u7684\u5de5\u4f5c\uff0c\u6210\u529f\u5c06\u9ad8\u5ea6\u62bd\u8c61\u7684\u76ee\u6807\u5217\u8868\u4fe1\u606f\u4e0e\u539f\u59cb\u76f8\u673a\u56fe\u50cf\u878d\u5408\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4f20\u611f\u5668\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.12430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12430", "abs": "https://arxiv.org/abs/2512.12430", "authors": ["Ke Zhang", "Yiqun Mei", "Jiacong Xu", "Vishal M. Patel"], "title": "Endless World: Real-Time 3D-Aware Long Video Generation", "comment": "10 pages,7 figures", "summary": "Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.", "AI": {"tldr": "Endless World\u662f\u4e00\u4e2a\u5b9e\u65f6\u65e0\u96503D\u4e00\u81f4\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u81ea\u56de\u5f52\u8bad\u7ec3\u548c\u5168\u5c403D\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u957f\u5e8f\u5217\u3001\u51e0\u4f55\u4e00\u81f4\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u957f\u3001\u8fde\u8d2f\u89c6\u9891\u5e8f\u5217\u65f6\u9762\u4e34\u76843D\u7ed3\u6784\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6d41\u5f0f\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5b9e\u73b0\u65e0\u9650\u30013D\u4e00\u81f4\u7684\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u3002", "method": "1. \u6761\u4ef6\u81ea\u56de\u5f52\u8bad\u7ec3\u7b56\u7565\uff1a\u5bf9\u9f50\u65b0\u751f\u6210\u5185\u5bb9\u4e0e\u73b0\u6709\u89c6\u9891\u5e27\uff0c\u4fdd\u6301\u957f\u7a0b\u4f9d\u8d56\u4e14\u8ba1\u7b97\u9ad8\u6548\uff1b2. \u5168\u5c403D\u611f\u77e5\u6ce8\u610f\u529b\uff1a\u63d0\u4f9b\u8de8\u65f6\u95f4\u7684\u8fde\u7eed\u51e0\u4f55\u6307\u5bfc\uff1b3. 3D\u6ce8\u5165\u673a\u5236\uff1a\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEndless World\u80fd\u591f\u751f\u6210\u957f\u3001\u7a33\u5b9a\u3001\u89c6\u89c9\u8fde\u8d2f\u7684\u89c6\u9891\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u652f\u6301\u5355GPU\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "Endless World\u6210\u529f\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u89c6\u9891\u751f\u6210\u76843D\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9650\u3001\u5b9e\u65f6\u3001\u51e0\u4f55\u4e00\u81f4\u7684\u89c6\u9891\u751f\u6210\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12885", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12885", "abs": "https://arxiv.org/abs/2512.12885", "authors": ["Minghao Zhu", "Zhihao Zhang", "Anmol Sidhu", "Keith Redmill"], "title": "SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition", "comment": "Submitted to IV 2026", "summary": "Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u96f6\u6837\u672c\u8def\u6807\u8bc6\u522b\u6846\u67b6\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u68c0\u7d22\u76f8\u5173\u5019\u9009\u6807\u5fd7\uff0c\u518d\u7531\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc6\u522b\uff0c\u5728303\u4e2a\u76d1\u7ba1\u6807\u5fd7\u4e0a\u53d6\u5f97\u4f18\u5f02\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u8def\u6807\u7c7b\u522b\u7e41\u591a\u548c\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u5c31\u80fd\u51c6\u786e\u8bc6\u522b\u5927\u91cf\u8def\u6807\u7c7b\u522b\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528RAG\u8303\u5f0f\uff1a1) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u8f93\u5165\u56fe\u50cf\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff1b2) \u4ece\u53c2\u8003\u8bbe\u8ba1\u7684\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u6700\u76f8\u5173\u7684\u5019\u9009\u6807\u5fd7\uff1b3) \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u68c0\u7d22\u5230\u7684\u5019\u9009\u8fdb\u884c\u63a8\u7406\uff0c\u505a\u51fa\u6700\u7ec8\u7ec6\u7c92\u5ea6\u8bc6\u522b\u3002", "result": "\u5728\u4fc4\u4ea5\u4fc4\u5ddeMUTCD\u7684303\u4e2a\u76d1\u7ba1\u6807\u5fd7\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5728\u7406\u60f3\u53c2\u8003\u56fe\u50cf\u4e0a\u8fbe\u523095.58%\u7684\u51c6\u786e\u7387\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u9053\u8def\u6570\u636e\u4e0a\u8fbe\u523082.45%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u57fa\u4e8eRAG\u7684\u67b6\u6784\u5728\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u521b\u5efa\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u8def\u6807\u8bc6\u522b\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.12459", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.12459", "abs": "https://arxiv.org/abs/2512.12459", "authors": ["Jiachen Tao", "Benjamin Planche", "Van Nguyen Nguyen", "Junyi Wu", "Yuchun Liu", "Haoxuan Wang", "Zhongpai Gao", "Gengyu Zhang", "Meng Zheng", "Feiran Wang", "Anwesa Choudhuri", "Zhenghao Zhao", "Weitai Kang", "Terrence Chen", "Yan Yan", "Ziyan Wu"], "title": "From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields", "comment": null, "summary": "Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9ad8\u65af\u5149\u5b50\u573a(GPF)\uff0c\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u5149\u5b50\u6620\u5c04\u91cd\u65b0\u8868\u8ff0\u4e3a\u8fde\u7eed\u53ef\u91cd\u7528\u7684\u8f90\u5c04\u51fd\u6570\uff0c\u7528\u4e8e\u52a0\u901f\u591a\u89c6\u89d2\u6e32\u67d3\u3002", "motivation": "\u5149\u5b50\u6620\u5c04\u867d\u7136\u80fd\u63d0\u4f9b\u7269\u7406\u4e0a\u51c6\u786e\u7684\u5149\u7ebf\u4f20\u8f93\u6a21\u62df\uff0c\u4f46\u5728\u6e32\u67d3\u540c\u4e00\u573a\u666f\u7684\u591a\u4e2a\u89c6\u89d2\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u89c6\u89d2\u90fd\u9700\u8981\u72ec\u7acb\u7684\u5149\u5b50\u8ffd\u8e2a\u548c\u968f\u673a\u6838\u4f30\u8ba1\uff0c\u5bfc\u81f4\u5927\u91cf\u5197\u4f59\u8ba1\u7b97\u3002", "method": "\u5f15\u5165\u9ad8\u65af\u5149\u5b50\u573a(GPF)\uff0c\u5c06\u5149\u5b50\u5206\u5e03\u7f16\u7801\u4e3a\u5404\u5411\u5f02\u6027\u76843D\u9ad8\u65af\u57fa\u5143\uff0c\u53c2\u6570\u5305\u62ec\u4f4d\u7f6e\u3001\u65cb\u8f6c\u3001\u5c3a\u5ea6\u548c\u5149\u8c31\u3002GPF\u4ece\u7b2c\u4e00\u4e2aSPPM\u8fed\u4ee3\u7684\u7269\u7406\u8ffd\u8e2a\u5149\u5b50\u521d\u59cb\u5316\uff0c\u5e76\u4f7f\u7528\u6700\u7ec8\u8f90\u5c04\u5ea6\u7684\u591a\u89c6\u89d2\u76d1\u7763\u8fdb\u884c\u4f18\u5316\uff0c\u5c06\u57fa\u4e8e\u5149\u5b50\u7684\u5149\u7ebf\u4f20\u8f93\u63d0\u70bc\u4e3a\u8fde\u7eed\u573a\u3002", "result": "\u5728\u5177\u6709\u590d\u6742\u5149\u7ebf\u4f20\u8f93\u7684\u573a\u666f\uff08\u5982\u7126\u6563\u548c\u955c\u9762-\u6f2b\u53cd\u5c04\u4ea4\u4e92\uff09\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eGPF\u5728\u4fdd\u6301\u5149\u5b50\u7ea7\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e86\u6570\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "GPF\u5c06\u57fa\u4e8e\u5149\u5b50\u7684\u6e32\u67d3\u7684\u7269\u7406\u4e25\u8c28\u6027\u4e0e\u795e\u7ecf\u573a\u666f\u8868\u793a\u7684\u6548\u7387\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u53ef\u5fae\u5206\u7684\u8f90\u5c04\u5ea6\u8bc4\u4f30\uff0c\u65e0\u9700\u91cd\u590d\u5149\u5b50\u8ffd\u8e2a\u6216\u8fed\u4ee3\u4f18\u5316\u3002"}}
{"id": "2512.13030", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13030", "abs": "https://arxiv.org/abs/2512.13030", "authors": ["Hongzhe Bi", "Hengkai Tan", "Shenghao Xie", "Zeyuan Wang", "Shuhe Huang", "Haitian Liu", "Ruowen Zhao", "Yao Feng", "Chendong Xiang", "Yinze Rong", "Hongyan Zhao", "Hanyu Liu", "Zhizhong Su", "Lei Ma", "Hang Su", "Jun Zhu"], "title": "Motus: A Unified Latent Action World Model", "comment": null, "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.", "AI": {"tldr": "Motus\u662f\u4e00\u4e2a\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408Transformer\u67b6\u6784\u6574\u5408\u7406\u89e3\u3001\u89c6\u9891\u751f\u6210\u548c\u52a8\u4f5c\u4e09\u4e2a\u4e13\u5bb6\u6a21\u5757\uff0c\u5229\u7528\u5149\u6d41\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5177\u8eab\u667a\u80fd\u65b9\u6cd5\u91c7\u7528\u5b64\u7acb\u7684\u7406\u89e3\u3001\u4e16\u754c\u5efa\u6a21\u548c\u63a7\u5236\u6a21\u578b\uff0c\u8fd9\u79cd\u788e\u7247\u5316\u963b\u788d\u4e86\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\u7684\u7edf\u4e00\uff0c\u4e5f\u59a8\u788d\u4e86\u4ece\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u4e2d\u5b66\u4e60\u3002\u9700\u8981\u6784\u5efa\u7edf\u4e00\u7684\u7cfb\u7edf\u6765\u6574\u5408\u8fd9\u4e9b\u529f\u80fd\u3002", "method": "\u63d0\u51faMotus\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff1a1\uff09\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784\u6574\u5408\u4e09\u4e2a\u4e13\u5bb6\u6a21\u5757\uff08\u7406\u89e3\u3001\u89c6\u9891\u751f\u6210\u3001\u52a8\u4f5c\uff09\uff1b2\uff09\u4f7f\u7528UniDiffuser\u98ce\u683c\u8c03\u5ea6\u5668\u5b9e\u73b0\u4e0d\u540c\u5efa\u6a21\u6a21\u5f0f\u7684\u7075\u6d3b\u5207\u6362\uff1b3\uff09\u5229\u7528\u5149\u6d41\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\uff0c\u63d0\u53d6\u50cf\u7d20\u7ea7\"delta\u52a8\u4f5c\"\uff1b4\uff09\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u548c\u516d\u5c42\u6570\u636e\u91d1\u5b57\u5854\u8fdb\u884c\u5927\u89c4\u6a21\u52a8\u4f5c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u573a\u666f\u4e2d\uff1a\u6bd4X-VLA\u63d0\u534715%\uff0c\u6bd4Pi0.5\u63d0\u534745%\uff1b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff1a\u63d0\u534711-48%\u3002\u8bc1\u660e\u4e86\u7edf\u4e00\u5efa\u6a21\u6240\u6709\u529f\u80fd\u548c\u5148\u9a8c\u5bf9\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u6709\u663e\u8457\u76ca\u5904\u3002", "conclusion": "Motus\u901a\u8fc7\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\u6210\u529f\u6574\u5408\u4e86\u7406\u89e3\u3001\u4e16\u754c\u5efa\u6a21\u548c\u63a7\u5236\u529f\u80fd\uff0c\u5229\u7528\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u4e30\u5bcc\u7684\u53ef\u5171\u4eab\u8fd0\u52a8\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u5efa\u6a21\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.12487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12487", "abs": "https://arxiv.org/abs/2512.12487", "authors": ["Hoang Anh Just", "Yifei Fan", "Handong Zhao", "Jiuxiang Gu", "Ruiyi Zhang", "Simon Jenni", "Kushal Kafle", "Ruoxi Jia", "Jing Shi"], "title": "More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models", "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.", "AI": {"tldr": "PeRL-VL\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u522b\u6539\u8fdb\u89c6\u89c9\u611f\u77e5\u548c\u6587\u672c\u63a8\u7406\u6765\u63d0\u5347RLVR\u8bad\u7ec3\u6548\u679c\uff0c\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u8bad\u7ec3\u65b9\u6cd5\u867d\u7136\u4ece\u7eaf\u6587\u672c\u6269\u5c55\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u89c6\u89c9\u63d0\u53d6\u4e0d\u51c6\u786e\uff08\u9057\u6f0f\u6216\u5e7b\u89c9\u7ec6\u8282\uff09\u548c\u601d\u7ef4\u94fe\u903b\u8f91\u4e0d\u4e00\u81f4\uff0c\u56e0\u4e3a\u53ef\u9a8c\u8bc1\u4fe1\u53f7\u53ea\u76d1\u7763\u6700\u7ec8\u7b54\u6848\u3002", "method": "PeRL-VL\u91c7\u7528\u89e3\u8026\u6846\u67b6\uff0c\u5206\u522b\u6539\u8fdb\u89c6\u89c9\u611f\u77e5\u548c\u6587\u672c\u63a8\u7406\u3002\u5bf9\u4e8e\u611f\u77e5\uff0c\u5f15\u5165\u57fa\u4e8eVLM\u7684\u63cf\u8ff0\u5956\u52b1\uff0c\u8bc4\u4f30\u6a21\u578b\u81ea\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\u7684\u5fe0\u5b9e\u6027\u548c\u5145\u5206\u6027\uff1b\u5bf9\u4e8e\u63a8\u7406\uff0c\u589e\u52a0\u7eaf\u6587\u672c\u63a8\u7406SFT\u9636\u6bb5\uff0c\u5728\u903b\u8f91\u4e30\u5bcc\u7684\u601d\u7ef4\u94fe\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u72ec\u7acb\u4e8e\u89c6\u89c9\u63d0\u5347\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u6837\u5316\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPeRL-VL\u5c06\u5e73\u5747Pass@1\u51c6\u786e\u7387\u4ece63.3%\uff08\u57fa\u7840Qwen2.5-VL-7B\uff09\u63d0\u5347\u523068.8%\uff0c\u4f18\u4e8e\u6807\u51c6RLVR\u3001\u7eaf\u6587\u672c\u63a8\u7406SFT\u548c\u4eceGPT-4o\u7684\u6734\u7d20\u591a\u6a21\u6001\u84b8\u998f\u3002", "conclusion": "PeRL-VL\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u611f\u77e5\u548c\u6587\u672c\u63a8\u7406\u7684\u6539\u8fdb\uff0c\u6709\u6548\u89e3\u51b3\u4e86RLVR\u8bad\u7ec3\u4e2d\u89c6\u89c9\u63d0\u53d6\u4e0d\u51c6\u786e\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.13177", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13177", "abs": "https://arxiv.org/abs/2512.13177", "authors": ["Minghui Hou", "Wei-Hsing Huang", "Shaofeng Liang", "Daizong Liu", "Tai-Hao Wen", "Gang Wang", "Runwei Guan", "Weiping Ding"], "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion", "comment": null, "summary": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.", "AI": {"tldr": "MMDrive\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u5c06\u4f20\u7edf2D\u56fe\u50cf\u7406\u89e3\u6269\u5c55\u52303D\u573a\u666f\u7406\u89e3\uff0c\u878d\u5408\u5360\u636e\u56fe\u3001LiDAR\u70b9\u4e91\u548c\u6587\u672c\u63cf\u8ff0\u4e09\u79cd\u6a21\u6001\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u878d\u5408\u548c\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u63d0\u5347\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e2D\u5e73\u9762\u56fe\u50cf\u7406\u89e3\u8303\u5f0f\uff0c\u96be\u4ee5\u611f\u77e53D\u7a7a\u95f4\u4fe1\u606f\u5e76\u8fdb\u884c\u6df1\u5ea6\u8bed\u4e49\u878d\u5408\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u7a81\u7834\u4f20\u7edf\u56fe\u50cf\u7406\u89e3\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMMDrive\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u4e92\u8865\u6a21\u6001\uff1a\u5360\u636e\u56fe\u3001LiDAR\u70b9\u4e91\u548c\u6587\u672c\u573a\u666f\u63cf\u8ff0\u3002\u5f15\u5165\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u9762\u5411\u6587\u672c\u7684\u591a\u6a21\u6001\u8c03\u5236\u5668\uff0c\u6839\u636e\u95ee\u9898\u8bed\u4e49\u52a8\u6001\u52a0\u6743\u5404\u6a21\u6001\u8d21\u732e\uff1b2) \u8de8\u6a21\u6001\u62bd\u8c61\u5668\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u62bd\u8c61\u4ee4\u724c\u751f\u6210\u7d27\u51d1\u7684\u8de8\u6a21\u6001\u6458\u8981\uff0c\u7a81\u51fa\u5173\u952e\u533a\u57df\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728DriveLM\u548cNuScenes-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aDriveLM\u4e0aBLEU-4\u5f97\u520654.56\uff0cMETEOR\u5f97\u520641.78\uff1bNuScenes-QA\u4e0a\u51c6\u786e\u738762.7%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "MMDrive\u6709\u6548\u7a81\u7834\u4e86\u4f20\u7edf\u4ec5\u56fe\u50cf\u7406\u89e3\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u57fa\u7840\u3002"}}
{"id": "2512.12492", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12492", "abs": "https://arxiv.org/abs/2512.12492", "authors": ["Shengkai Xu", "Hsiang Lun Kao", "Tianxiang Xu", "Honghui Zhang", "Junqiao Wang", "Runmeng Ding", "Guanyu Liu", "Tianyu Shi", "Zhenyu Yu", "Guofeng Pan", "Ziqian Bi", "Yuqi Ouyang"], "title": "Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings", "comment": null, "summary": "Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.", "AI": {"tldr": "\u63d0\u51faAdaptiveDetector\u6846\u67b6\uff0c\u901a\u8fc7YOLOv11\u68c0\u6d4b\u5668\u548cVLM\u9a8c\u8bc1\u5668\u7684\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u9608\u503c\u8c03\u6574\u548c\u6210\u672c\u654f\u611f\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u5185\u955c\u606f\u8089\u68c0\u6d4b\u5728\u771f\u5b9e\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u53ec\u56de\u7387\u3002", "motivation": "\u73b0\u6709\u606f\u8089\u68c0\u6d4b\u5668\u5728\u5e72\u51c0\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4f46\u5728\u771f\u5b9e\u5185\u955c\u73af\u5883\u4e2d\uff08\u5149\u7167\u53d8\u5316\u3001\u8fd0\u52a8\u6a21\u7cca\u3001\u906e\u6321\u7b49\u6076\u52a3\u6210\u50cf\u6761\u4ef6\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e0e\u5b9e\u9a8c\u5ba4\u6761\u4ef6\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u68c0\u6d4b\u5668-\u9a8c\u8bc1\u5668\u6846\u67b6\uff1a1\uff09YOLOv11\u68c0\u6d4b\u5668\u5728VLM\u6307\u5bfc\u4e0b\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u5e27\u7f6e\u4fe1\u5ea6\u9608\u503c\uff1b2\uff09VLM\u9a8c\u8bc1\u5668\u4f7f\u7528\u975e\u5bf9\u79f0\u6210\u672c\u654f\u611f\u5956\u52b1\u51fd\u6570\u901a\u8fc7GRPO\u8fdb\u884c\u5fae\u8c03\uff0c\u4e13\u95e8\u8bbe\u8ba1\u6765\u51cf\u5c11\u6f0f\u68c0\u3002\u6784\u5efa\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\u7cfb\u7edf\u6027\u5730\u6a21\u62df\u4e34\u5e8a\u6076\u52a3\u6761\u4ef6\u3002", "result": "\u5728\u5408\u6210\u9000\u5316\u7684CVC-ClinicDB\u548cKvasir-SEG\u56fe\u50cf\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u53ec\u56de\u7387\u6bd4\u5355\u72ec\u4f7f\u7528YOLO\u63d0\u9ad814-22\u4e2a\u767e\u5206\u70b9\uff0c\u7cbe\u5ea6\u4fdd\u6301\u5728\u57fa\u7ebf\u4ee5\u4e0b0.7\u70b9\u5230\u4ee5\u4e0a1.7\u70b9\u8303\u56f4\u5185\u3002\u663e\u8457\u51cf\u5c11\u5047\u9634\u6027\uff0c\u964d\u4f4e\u6f0f\u68c0\u764c\u524d\u606f\u8089\u98ce\u9669\u3002", "conclusion": "\u81ea\u9002\u5e94\u9608\u503c\u8c03\u6574\u4e0e\u6210\u672c\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u5b9e\u73b0\u4e86\u4e34\u5e8a\u5bf9\u9f50\u7684\u5f00\u653e\u4e16\u754c\u606f\u8089\u68c0\u6d4b\uff0c\u901a\u8fc7\u5927\u5e45\u51cf\u5c11\u5047\u9634\u6027\u6539\u5584\u4e86\u60a3\u8005\u9884\u540e\uff0c\u4e3a\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u606f\u8089\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13636", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13636", "abs": "https://arxiv.org/abs/2512.13636", "authors": ["Haoyu Fu", "Diankun Zhang", "Zongchuang Zhao", "Jianfeng Cui", "Hongwei Xie", "Bing Wang", "Guang Chen", "Dingkang Liang", "Xiang Bai"], "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning", "comment": "16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/", "summary": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.", "AI": {"tldr": "MindDrive\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684VLA\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u6620\u5c04\u5230\u6709\u9650\u7684\u8bed\u8a00\u51b3\u7b56\u7a7a\u95f4\u6765\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u8303\u5f0f\u4e3b\u8981\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u5b58\u5728\u5206\u5e03\u504f\u79fb\u548c\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\u3002\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u80fd\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "MindDrive\u91c7\u7528\u5177\u6709\u4e24\u7ec4\u4e0d\u540cLoRA\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff1a\u4e00\u4e2a\u4f5c\u4e3a\u51b3\u7b56\u4e13\u5bb6\u8fdb\u884c\u573a\u666f\u63a8\u7406\u548c\u9a7e\u9a76\u51b3\u7b56\uff0c\u53e6\u4e00\u4e2a\u4f5c\u4e3a\u52a8\u4f5c\u4e13\u5bb6\u5c06\u8bed\u8a00\u51b3\u7b56\u52a8\u6001\u6620\u5c04\u4e3a\u53ef\u884c\u8f68\u8ff9\u3002\u901a\u8fc7\u5c06\u8f68\u8ff9\u7ea7\u5956\u52b1\u53cd\u9988\u5230\u63a8\u7406\u7a7a\u95f4\uff0c\u5728\u6709\u9650\u7684\u8bed\u8a00\u51b3\u7b56\u7a7a\u95f4\u800c\u975e\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8bd5\u9519\u5b66\u4e60\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMindDrive\u53d6\u5f97\u4e8678.04\u7684\u9a7e\u9a76\u5206\u6570\u548c55.09%\u7684\u6210\u529f\u7387\uff0c\u9996\u6b21\u8bc1\u660e\u4e86\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76VLA\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MindDrive\u901a\u8fc7\u5c06\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u6620\u5c04\u5230\u79bb\u6563\u8bed\u8a00\u51b3\u7b56\u7a7a\u95f4\uff0c\u6709\u6548\u5e73\u8861\u4e86\u590d\u6742\u573a\u666f\u4e2d\u7684\u6700\u4f18\u51b3\u7b56\u3001\u7c7b\u4eba\u9a7e\u9a76\u884c\u4e3a\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u6548\u63a2\u7d22\uff0c\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76VLA\u6a21\u578b\u7684\u63a2\u7d22\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.12508", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12508", "abs": "https://arxiv.org/abs/2512.12508", "authors": ["Jinfan Zhou", "Lixin Luo", "Sungmin Eum", "Heesung Kwon", "Jeong Joon Park"], "title": "Generative Spatiotemporal Data Augmentation", "comment": null, "summary": "We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.", "AI": {"tldr": "\u5229\u7528\u89c6\u9891\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u65f6\u7a7a\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u771f\u5b9e\u76843D\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u5316\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u7b80\u5355\u7684\u51e0\u4f55\u53d8\u6362\u6216\u5916\u89c2\u6270\u52a8\uff0c\u65e0\u6cd5\u6709\u6548\u751f\u62103D\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u5316\u3002\u5728\u65e0\u4eba\u673a\u56fe\u50cf\u7b49\u6807\u6ce8\u7a00\u7f3a\u7684\u4f4e\u6570\u636e\u573a\u666f\u4e2d\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u589e\u5f3a\u65b9\u6cd5\u6765\u6269\u5c55\u6570\u636e\u5206\u5e03", "method": "\u4f7f\u7528\u73b0\u6210\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ece\u7ed9\u5b9a\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u751f\u6210\u771f\u5b9e\u76843D\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u5316\uff0c\u5c06\u5408\u6210\u7684\u89c6\u9891\u7247\u6bb5\u4f5c\u4e3a\u8865\u5145\u8bad\u7ec3\u6570\u636e\u3002\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\uff1a\u9009\u62e9\u9002\u5f53\u7684\u65f6\u7a7a\u751f\u6210\u8bbe\u7f6e\u3001\u5c06\u6807\u6ce8\u8f6c\u79fb\u5230\u5408\u6210\u5e27\u3001\u5904\u7406\u65b0\u66b4\u9732\u533a\u57df\u7684\u906e\u6321\u95ee\u9898", "result": "\u5728COCO\u5b50\u96c6\u548c\u65e0\u4eba\u673a\u6355\u83b7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65f6\u7a7a\u589e\u5f3a\u80fd\u591f\u6cbf\u7740\u4f20\u7edf\u548c\u5148\u524d\u751f\u6210\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u4ee3\u8868\u7684\u8f74\u6269\u5c55\u6570\u636e\u5206\u5e03\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd", "conclusion": "\u65f6\u7a7a\u6570\u636e\u589e\u5f3a\u901a\u8fc7\u89c6\u9891\u57fa\u7840\u6a21\u578b\u80fd\u591f\u6709\u6548\u6269\u5c55\u6570\u636e\u5206\u5e03\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u624b\u6bb5\u3002\u9700\u8981\u8c28\u614e\u5e94\u7528\uff0c\u5e76\u8003\u8651\u906e\u6321\u7b49\u5b9e\u9645\u95ee\u9898"}}
{"id": "2512.12534", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12534", "abs": "https://arxiv.org/abs/2512.12534", "authors": ["Qi Sun", "Can Wang", "Jiaxiang Shang", "Wensen Feng", "Jing Liao"], "title": "Animus3D: Text-driven 3D Animation via Motion Score Distillation", "comment": "SIGGRAPH Asia 2025", "summary": "We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.", "AI": {"tldr": "Animus3D\uff1a\u57fa\u4e8e\u6587\u672c\u9a71\u52a8\u76843D\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7Motion Score Distillation\uff08MSD\uff09\u66ff\u4ee3\u4f20\u7edfSDS\uff0c\u7ed3\u5408LoRA\u589e\u5f3a\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u65f6\u7a7a\u6b63\u5219\u5316\uff0c\u4e3a\u9759\u60013D\u8d44\u4ea7\u751f\u6210\u9ad8\u8d28\u91cf\u8fd0\u52a8\u573a", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528vanilla Score Distillation Sampling\uff08SDS\uff09\u4ece\u9884\u8bad\u7ec3\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u8fd0\u52a8\uff0c\u5bfc\u81f4\u52a8\u753b\u8fd0\u52a8\u5e45\u5ea6\u5c0f\u6216\u5b58\u5728\u660e\u663e\u6296\u52a8\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8fd0\u52a8\u751f\u6210\u65b9\u6848", "method": "\u63d0\u51faMotion Score Distillation\uff08MSD\uff09\u66ff\u4ee3SDS\uff0c\u5305\u542b\uff1a1\uff09LoRA\u589e\u5f3a\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u9759\u6001\u6e90\u5206\u5e03\u800c\u975e\u7eaf\u566a\u58f0\u4f5c\u4e3a\u8d77\u70b9\uff1b2\uff09\u57fa\u4e8e\u53cd\u8f6c\u7684\u566a\u58f0\u4f30\u8ba1\u6280\u672f\u4fdd\u6301\u5916\u89c2\u4e00\u81f4\u6027\uff1b3\uff09\u65f6\u7a7a\u6b63\u5219\u5316\u9879\u51cf\u5c11\u51e0\u4f55\u5931\u771f\uff1b4\uff09\u8fd0\u52a8\u7ec6\u5316\u6a21\u5757\u63d0\u5347\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7ec6\u8282", "result": "\u5b9e\u9a8c\u8868\u660eAnimus3D\u80fd\u6210\u529f\u4e3a\u591a\u6837\u6587\u672c\u63d0\u793a\u7684\u9759\u60013D\u8d44\u4ea7\u751f\u6210\u52a8\u753b\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u4ea7\u751f\u66f4\u663e\u8457\u3001\u66f4\u8be6\u7ec6\u7684\u8fd0\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u5b8c\u6574\u6027", "conclusion": "Animus3D\u901a\u8fc7\u521b\u65b0\u7684MSD\u65b9\u6cd5\u548c\u7efc\u5408\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u9a71\u52a83D\u52a8\u753b\u4e2d\u7684\u8fd0\u52a8\u5e45\u5ea6\u4e0d\u8db3\u548c\u6296\u52a8\u95ee\u9898\uff0c\u4e3a\u9759\u60013D\u8d44\u4ea7\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u753b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848"}}
{"id": "2512.12539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12539", "abs": "https://arxiv.org/abs/2512.12539", "authors": ["Huan Huang", "Michele Esposito", "Chen Zhao"], "title": "Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling", "comment": "6 figures", "summary": "Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u51a0\u72b6\u52a8\u8109\u5206\u5272\u6846\u67b6\uff0c\u6574\u5408\u5fc3\u808c\u89e3\u5256\u5148\u9a8c\u3001\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u7f16\u7801\u548c\u4e09\u7ef4\u5c0f\u6ce2\u53d8\u6362\uff0c\u5728\u590d\u6742\u51e0\u4f55\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u7a33\u5b9a\u4e00\u81f4\u7684\u51a0\u72b6\u52a8\u8109\u5206\u5272\u3002", "motivation": "\u51a0\u72b6\u52a8\u8109CT\u8840\u7ba1\u6210\u50cf\u7684\u51c6\u786e\u5206\u5272\u5bf9\u5b9a\u91cf\u5206\u6790\u548c\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8840\u7ba1\u7ba1\u5f84\u5c0f\u3001\u5206\u652f\u590d\u6742\u3001\u8fb9\u754c\u6a21\u7cca\u548c\u5fc3\u808c\u5e72\u6270\u7b49\u56e0\u7d20\uff0c\u53ef\u9760\u5206\u5272\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u51a0\u72b6\u52a8\u8109\u5206\u5272\u6846\u67b6\uff0c\u6574\u5408\u5fc3\u808c\u89e3\u5256\u5148\u9a8c\u3001\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u7f16\u7801\u548c\u4e09\u7ef4\u5c0f\u6ce2-\u9006\u5c0f\u6ce2\u53d8\u6362\u3002\u7f16\u7801\u9636\u6bb5\u52a0\u5165\u5fc3\u808c\u5148\u9a8c\u548c\u6b8b\u5dee\u6ce8\u610f\u529b\u7279\u5f81\u589e\u5f3a\uff0c\u5c0f\u6ce2-\u9006\u5c0f\u6ce2\u4e0b\u91c7\u6837\u548c\u4e0a\u91c7\u6837\u5b9e\u73b0\u8054\u5408\u7a7a\u95f4\u9891\u7387\u5efa\u6a21\uff0c\u89e3\u7801\u9636\u6bb5\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\u6574\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5728\u516c\u5f00ImageCAS\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u91c7\u75283D\u91cd\u53e0\u8865\u4e01\u7b56\u7565\uff0c\u8bad\u7ec3/\u9a8c\u8bc1/\u6d4b\u8bd5\u6bd4\u4f8b\u4e3a7:1:2\u3002Dice\u7cfb\u65700.8082\uff0c\u654f\u611f\u60270.7946\uff0c\u7cbe\u786e\u5ea60.8471\uff0cHD95\u4e3a9.77mm\uff0c\u4f18\u4e8e\u591a\u4e2a\u4e3b\u6d41\u5206\u5272\u6a21\u578b\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u5404\u7ec4\u4ef6\u4e92\u8865\u8d21\u732e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u51e0\u4f55\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u7a33\u5b9a\u4e00\u81f4\u7684\u51a0\u72b6\u52a8\u8109\u5206\u5272\uff0c\u4e3a\u540e\u7eed\u51a0\u72b6\u52a8\u8109\u7ed3\u6784\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u53ef\u9760\u5206\u5272\u7ed3\u679c\u3002"}}
{"id": "2512.12560", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12560", "abs": "https://arxiv.org/abs/2512.12560", "authors": ["Xinqi Jin", "Hanxun Yu", "Bohan Yu", "Kebin Liu", "Jian Liu", "Keda Tao", "Yixuan Pei", "Huan Wang", "Fan Dang", "Jiangchuan Liu", "Weiqiang Wang"], "title": "StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding", "comment": null, "summary": "Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891token\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u7a7a\u95f4\u76f8\u90bb\u89c6\u9891token\u76f8\u4f3c\u5ea6\uff08MSSAVT\uff09\u6307\u6807\u548c\u63a9\u7801\u526a\u679d\u7b56\u7565\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u5173\u952e\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u5728\u516c\u5171\u76d1\u63a7\u548cAI\u773c\u955c\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u89c6\u9891\u65f6\u9762\u4e34\u5e27\u6570\u8fc7\u591a\u5bfc\u81f4\u7684GPU\u5185\u5b58\u5360\u7528\u9ad8\u548c\u8ba1\u7b97\u5ef6\u8fdf\u5927\u7684\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684token\u526a\u679d\u65b9\u6cd5\u6765\u51cf\u5c11\u4e0a\u4e0b\u6587\u957f\u5ea6\u540c\u65f6\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6700\u5927\u7a7a\u95f4\u76f8\u90bb\u89c6\u9891token\u76f8\u4f3c\u5ea6\uff08MSSAVT\uff09\u7684\u5197\u4f59\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8003\u8651token\u76f8\u4f3c\u5ea6\u548c\u7a7a\u95f4\u4f4d\u7f6e\uff1b\u8bbe\u8ba1\u63a9\u7801\u526a\u679d\u7b56\u7565\u89e3\u51b3\u526a\u679d\u4e0e\u5197\u4f59\u4e4b\u95f4\u7684\u53cc\u5411\u4f9d\u8d56\u95ee\u9898\uff1b\u7ed3\u5408\u73b0\u6709\u7684\u65f6\u95f4\u5197\u4f59\u526a\u679d\u65b9\u6cd5\u6d88\u9664\u89c6\u9891\u6a21\u6001\u7684\u65f6\u95f4\u5197\u4f59\u3002", "result": "\u5728\u591a\u4e2a\u5728\u7ebf\u548c\u79bb\u7ebf\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\uff08\u6700\u591a\u63d0\u53474%\uff09\uff0c\u540c\u65f6\u526a\u679d\u5ef6\u8fdf\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff08\u5c0f\u4e8e1\u6beb\u79d2\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684token\u526a\u679d\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\u7684\u8054\u5408\u526a\u679d\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2512.12586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12586", "abs": "https://arxiv.org/abs/2512.12586", "authors": ["Lixin Chen", "Chaomeng Chen", "Jiale Zhou", "Zhijian Wu", "Xun Lin"], "title": "StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis", "comment": "13 pages, 10 figures. This is the extended version of the paper accepted at AAAI 2026, including related works and appendix", "summary": "Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.", "AI": {"tldr": "StegaVAR\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u9690\u79c1\u4fdd\u62a4\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6846\u67b6\uff0c\u9996\u6b21\u5728\u9690\u5199\u57df\u4e2d\u76f4\u63a5\u6267\u884c\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\uff0c\u5c06\u52a8\u4f5c\u89c6\u9891\u5d4c\u5165\u5230\u666e\u901a\u5c01\u9762\u89c6\u9891\u4e2d\uff0c\u65e2\u4fdd\u62a4\u9690\u79c1\u53c8\u4fdd\u6301\u65f6\u7a7a\u7279\u5f81\u5b8c\u6574\u6027\u3002", "motivation": "\u5f53\u524d\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1)\u4f4e\u9690\u853d\u6027\uff0c\u4ea7\u751f\u89c6\u89c9\u626d\u66f2\u7684\u89c6\u9891\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u5438\u5f15\u653b\u51fb\u8005\u6ce8\u610f\uff1b(2)\u65f6\u7a7a\u7834\u574f\uff0c\u964d\u4f4e\u51c6\u786eVAR\u6240\u9700\u7684\u57fa\u672c\u65f6\u7a7a\u7279\u5f81\u3002\u9700\u8981\u89e3\u51b3\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002", "method": "\u63d0\u51faStegaVAR\u6846\u67b6\uff0c\u5c06\u52a8\u4f5c\u89c6\u9891\u5d4c\u5165\u666e\u901a\u5c01\u9762\u89c6\u9891\u4e2d\uff0c\u9996\u6b21\u5728\u9690\u5199\u57df\u76f4\u63a5\u6267\u884cVAR\u3002\u63d0\u51fa\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1aSTeP\uff08\u79d8\u5bc6\u65f6\u7a7a\u4fc3\u8fdb\uff09\u4f7f\u7528\u79d8\u5bc6\u89c6\u9891\u6307\u5bfc\u9690\u5199\u57df\u4e2d\u7684\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\uff1bCroDA\uff08\u8de8\u6ce2\u6bb5\u5dee\u5f02\u6ce8\u610f\u529b\uff09\u901a\u8fc7\u6355\u6349\u8de8\u6ce2\u6bb5\u8bed\u4e49\u5dee\u5f02\u6765\u6291\u5236\u5c01\u9762\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eStegaVAR\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u548c\u9690\u79c1\u4fdd\u62a4\u6027\u80fd\u3002\u8be5\u6846\u67b6\u5bf9\u591a\u79cd\u9690\u5199\u6a21\u578b\u90fd\u6709\u6548\u3002", "conclusion": "StegaVAR\u6210\u529f\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u9690\u853d\u6027\u548c\u65f6\u7a7a\u7279\u5f81\u4fdd\u6301\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u7684VAR\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12595", "abs": "https://arxiv.org/abs/2512.12595", "authors": ["Karthikeya KV"], "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation", "comment": null, "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5148\u8fdbTransformer\u67b6\u6784\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u6d41\u6d41\u673a\u5236\u548c\u53cc\u5411\u6807\u8bb0\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\u548c\u591a\u6a21\u6001\u6570\u636e\u7406\u89e3\uff0c\u76f8\u6bd4\u6269\u6563\u65b9\u6cd5\u63d0\u534725%\u56fe\u50cf\u5206\u8fa8\u7387\u6e05\u6670\u5ea6\u5e76\u51cf\u5c1120%\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u548c\u591a\u6a21\u6001\u6570\u636e\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6574\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u7b49\u591a\u79cd\u6570\u636e\u7c7b\u578b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u6574\u6d41\u6d41\u673a\u5236\u8fde\u63a5\u566a\u58f0\u4e0e\u6570\u636e\uff0c\u4f7f\u7528\u53cc\u5411\u6807\u8bb0\u5316\u7b56\u7565\u878d\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u8f93\u5165\uff0c\u5d4c\u5165\u65f6\u7a7a\u7279\u5f81\u5e76\u91c7\u7528\u6df7\u5408\u6587\u672c-\u56fe\u50cf\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u67b6\u6784\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u56fe\u50cf\u5206\u8fa8\u7387\u6e05\u6670\u5ea6\u63d0\u534725%\uff0c\u8ba1\u7b97\u9700\u6c42\u51cf\u5c1120%\uff0c\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u89c6\u89c9\u4e2d\u5fc3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u65b0\u5b9a\u4e49\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u5177\u6709\u5728\u81ea\u4e3b\u7cfb\u7edf\u3001\u521b\u610f\u5185\u5bb9\u751f\u6210\u548c\u9ad8\u7ea7\u89c6\u9891\u5206\u6790\u7b49\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.12596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12596", "abs": "https://arxiv.org/abs/2512.12596", "authors": ["Kei Yoshitake", "Kento Hosono", "Ken Kobayashi", "Kazuhide Nakata"], "title": "Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models", "comment": null, "summary": "In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based \"placement plan\" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u56fe\u50cf\u5e7f\u544a\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u80cc\u666f\u56fe\u50cf\u5185\u5bb9\u6765\u4f18\u5316\u6587\u672c\u548clogo\u7684\u653e\u7f6e\u4f4d\u7f6e\u3002", "motivation": "\u4f20\u7edf\u5e7f\u544a\u5e03\u5c40\u6280\u672f\u4e3b\u8981\u4f9d\u8d56\u663e\u8457\u6027\u6620\u5c04\u6765\u68c0\u6d4b\u80cc\u666f\u56fe\u50cf\u4e2d\u7684\u663e\u8457\u533a\u57df\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u5145\u5206\u8003\u8651\u56fe\u50cf\u7684\u8be6\u7ec6\u6784\u56fe\u548c\u8bed\u4e49\u5185\u5bb9\uff0c\u5bfc\u81f4\u5e03\u5c40\u8d28\u91cf\u53d7\u9650\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u4f7f\u7528VLM\u5206\u6790\u56fe\u50cf\uff0c\u8bc6\u522b\u7269\u4f53\u7c7b\u578b\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u751f\u6210\u57fa\u4e8e\u6587\u672c\u7684\"\u653e\u7f6e\u8ba1\u5212\"\uff1b\u7136\u540e\u5c06\u8be5\u8ba1\u5212\u6e32\u67d3\u4e3aHTML\u683c\u5f0f\u7684\u6700\u7ec8\u5e03\u5c40\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u660e\u663e\u66f4\u9ad8\u8d28\u91cf\u7684\u5e7f\u544a\u5e03\u5c40\uff0c\u7279\u522b\u662f\u5728\u8003\u8651\u80cc\u666f\u56fe\u50cf\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528VLM\u663e\u5f0f\u8003\u8651\u80cc\u666f\u56fe\u50cf\u7684\u8bed\u4e49\u5185\u5bb9\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5e7f\u544a\u5e03\u5c40\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u663e\u8457\u6027\u6620\u5c04\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.12598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12598", "abs": "https://arxiv.org/abs/2512.12598", "authors": ["Cong Xie", "Che Wang", "Yan Zhang", "Zheng Pan", "Han Zou", "Zhenpeng Zhan"], "title": "Geometry-Aware Scene-Consistent Image Generation", "comment": null, "summary": "We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u573a\u666f\u4e00\u81f4\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u573a\u666f\u4e00\u81f4\u6570\u636e\u6784\u5efa\u548c\u51e0\u4f55\u5f15\u5bfc\u6ce8\u610f\u529b\u635f\u5931\uff0c\u5728\u4fdd\u6301\u53c2\u8003\u573a\u666f\u7269\u7406\u73af\u5883\u7684\u540c\u65f6\uff0c\u6839\u636e\u6587\u672c\u63cf\u8ff0\u7684\u7a7a\u95f4\u5173\u7cfb\u751f\u6210\u65b0\u5b9e\u4f53\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u573a\u666f\u4fdd\u6301\u548c\u63d0\u793a\u9075\u5faa\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u8981\u4e48\u9ad8\u4fdd\u771f\u5730\u590d\u5236\u573a\u666f\u4f46\u5bf9\u63d0\u793a\u54cd\u5e94\u5dee\uff0c\u8981\u4e48\u4f18\u5148\u9075\u5faa\u63d0\u793a\u4f46\u727a\u7272\u573a\u666f\u4e00\u81f4\u6027\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u5173\u952e\u8d21\u732e\uff1a(1) \u573a\u666f\u4e00\u81f4\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u51e0\u4f55\u57fa\u7840\u8bad\u7ec3\u5bf9\uff1b(2) \u65b0\u9896\u7684\u51e0\u4f55\u5f15\u5bfc\u6ce8\u610f\u529b\u635f\u5931\uff0c\u5229\u7528\u8de8\u89c6\u56fe\u7ebf\u7d22\u6765\u6b63\u5219\u5316\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u573a\u666f\u4e00\u81f4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u573a\u666f\u5bf9\u9f50\u548c\u6587\u672c\u56fe\u50cf\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u81ea\u52a8\u6307\u6807\u548c\u4eba\u7c7b\u504f\u597d\u7814\u7a76\u5747\u663e\u793a\u66f4\u597d\u7ed3\u679c\u3002\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u4fdd\u6301\u5bf9\u6587\u672c\u6307\u4ee4\u548c\u5e95\u5c42\u573a\u666f\u7ed3\u6784\u7684\u5fe0\u5b9e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u573a\u666f\u4fdd\u6301\u4e0e\u63d0\u793a\u9075\u5faa\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u573a\u666f\u4e00\u81f4\u7684\u56fe\u50cf\u751f\u6210\uff0c\u5728\u4fdd\u6301\u7269\u7406\u73af\u5883\u7684\u540c\u65f6\u6b63\u786e\u751f\u6210\u7b26\u5408\u7a7a\u95f4\u5173\u7cfb\u63cf\u8ff0\u7684\u5b9e\u4f53\u3002"}}
{"id": "2512.12604", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12604", "abs": "https://arxiv.org/abs/2512.12604", "authors": ["Tingyan Wen", "Haoyu Li", "Yihuang Chen", "Xing Zhou", "Lifei Zhu", "Xueqian Wang"], "title": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching", "comment": "Project page: https://thu-accdiff.github.io/xslim-page/ Code: https://github.com/THU-AccDiff/xslim", "summary": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.", "AI": {"tldr": "X-Slim\u662f\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684\u7f13\u5b58\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u4e09\u7ea7\u7f13\u5b58\uff08\u65f6\u95f4\u6b65\u3001\u7ed3\u6784\u5757\u3001\u7a7a\u95f4\u6807\u8bb0\uff09\u548c\u53cc\u9608\u503c\u63a7\u5236\u5668\uff0c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u52a0\u901f\uff0c\u5e73\u8861\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u4f18\u79c0\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u968f\u6b65\u6570\u3001\u6a21\u578b\u6df1\u5ea6\u548c\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u3002\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u6743\u8861\uff1a\u6fc0\u8fdb\u7684\u65f6\u95f4\u6b65\u91cd\u7528\u80fd\u5927\u5e45\u52a0\u901f\u4f46\u5bb9\u6613\u635f\u5bb3\u4fdd\u771f\u5ea6\uff0c\u800c\u5757\u7ea7\u6216\u6807\u8bb0\u7ea7\u91cd\u7528\u66f4\u5b89\u5168\u4f46\u8ba1\u7b97\u8282\u7701\u6709\u9650\u3002", "method": "\u63d0\u51faX-Slim\u6846\u67b6\uff0c\u9996\u6b21\u7edf\u4e00\u5229\u7528\u65f6\u95f4\u6b65\u3001\u7ed3\u6784\u5757\u548c\u7a7a\u95f4\u6807\u8bb0\u4e09\u4e2a\u7ef4\u5ea6\u7684\u7f13\u5b58\u5197\u4f59\u3002\u91c7\u7528\u53cc\u9608\u503c\u63a7\u5236\u5668\u5b9e\u73b0\"\u63a8\u9001-\u629b\u5149\"\u8fc7\u7a0b\uff1a\u5148\u5c06\u65f6\u95f4\u6b65\u91cd\u7528\u63a8\u81f3\u9884\u8b66\u7ebf\uff0c\u7136\u540e\u5207\u6362\u5230\u8f7b\u91cf\u7ea7\u5757\u7ea7\u548c\u6807\u8bb0\u7ea7\u5237\u65b0\u6765\u629b\u5149\u5269\u4f59\u5197\u4f59\uff0c\u5f53\u8de8\u8fc7\u4e34\u754c\u7ebf\u65f6\u89e6\u53d1\u5b8c\u6574\u63a8\u7406\u4ee5\u91cd\u7f6e\u7d2f\u79ef\u8bef\u5dee\u3002\u6bcf\u4e2a\u5c42\u7ea7\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u6307\u6807\u51b3\u5b9a\u4f55\u65f6\u4f55\u5730\u7f13\u5b58\u3002", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u663e\u8457\u63a8\u8fdb\u4e86\u901f\u5ea6-\u8d28\u91cf\u524d\u6cbf\uff1a\u5728FLUX.1-dev\u4e0a\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe4.97\u500d\uff0c\u5728HunyuanVideo\u4e0a\u964d\u4f4e3.52\u500d\uff0c\u611f\u77e5\u635f\u5931\u6700\u5c0f\uff1b\u5728DiT-XL/2\u4e0a\u8fbe\u52303.13\u500d\u52a0\u901f\uff0cFID\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u53472.42\u3002", "conclusion": "X-Slim\u901a\u8fc7\u7edf\u4e00\u7684\u4e09\u7ea7\u7f13\u5b58\u6846\u67b6\u548c\u667a\u80fd\u7684\u53cc\u9608\u503c\u63a7\u5236\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u52a0\u901f\u4e2d\u7684\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.12623", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12623", "abs": "https://arxiv.org/abs/2512.12623", "authors": ["Chengzhi Liu", "Yuzhe Yang", "Yue Fan", "Qingyue Wei", "Sheng Liu", "Xin Eric Wang"], "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.", "AI": {"tldr": "DMLR\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u6a21\u6001\u6f5c\u5728\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u6f5c\u5728\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u6765\u7cbe\u70bc\u6f5c\u5728\u601d\u8003\u6807\u8bb0\uff0c\u5b9e\u73b0\u63a8\u7406\u4e0e\u611f\u77e5\u7684\u52a8\u6001\u4ea4\u9519\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u7684\u9010\u6b65\u63a8\u7406\uff0c\u5b58\u5728\u611f\u77e5-\u63a8\u7406\u4ea4\u4e92\u4e0d\u7a33\u5b9a\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\uff0c\u8ba4\u4e3a\u601d\u7ef4\u4e0d\u662f\u7ebf\u6027\u5c55\u5f00\uff0c\u800c\u662f\u901a\u8fc7\u63a8\u7406\u548c\u611f\u77e5\u5728\u5934\u8111\u4e2d\u7684\u52a8\u6001\u4ea4\u9519\u8fdb\u884c\u7684\u3002", "method": "\u63d0\u51faDMLR\u6846\u67b6\uff1a1) \u4f7f\u7528\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u6f5c\u5728\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u6765\u7cbe\u70bc\u6f5c\u5728\u601d\u8003\u6807\u8bb0\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\uff1b2) \u5f15\u5165\u52a8\u6001\u89c6\u89c9\u6ce8\u5165\u7b56\u7565\uff0c\u5728\u6bcf\u4e2a\u6f5c\u5728\u601d\u8003\u6807\u8bb0\u5904\u68c0\u7d22\u6700\u76f8\u5173\u7684\u89c6\u89c9\u7279\u5f81\u5e76\u66f4\u65b0\u6700\u4f73\u89c6\u89c9\u8865\u4e01\u96c6\uff0c\u7136\u540e\u5c06\u66f4\u65b0\u7684\u8865\u4e01\u6ce8\u5165\u6f5c\u5728\u601d\u8003\u6807\u8bb0\u4ee5\u5b9e\u73b0\u52a8\u6001\u89c6\u89c9-\u6587\u672c\u4ea4\u9519\u3002", "result": "\u5728\u4e03\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u5404\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDMLR\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u548c\u611f\u77e5\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u63a8\u7406\u6548\u7387\u3002", "conclusion": "DMLR\u901a\u8fc7\u52a8\u6001\u591a\u6a21\u6001\u6f5c\u5728\u63a8\u7406\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u4e0e\u611f\u77e5\u7684\u52a8\u6001\u4ea4\u9519\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u663e\u5f0f\u9010\u6b65\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2512.12657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12657", "abs": "https://arxiv.org/abs/2512.12657", "authors": ["Hongyang Li", "Junyi Tao", "Qijie Wei", "Ningzhi Yang", "Meng Wang", "Weihong Yu", "Xirong Li"], "title": "Cross-modal Fundus Image Registration under Large FoV Disparity", "comment": "Accepted as a regular paper at MMM 2026", "summary": "Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCARe\u65b9\u6cd5\u89e3\u51b3\u5927\u89c6\u573a\u5dee\u5f02\u7684\u8de8\u6a21\u6001\u773c\u5e95\u56fe\u50cf\u914d\u51c6\u95ee\u9898\uff0c\u901a\u8fc7\u88c1\u526a\u548c\u5bf9\u9f50\u64cd\u4f5c\u5b9e\u73b0\u6709\u6548\u914d\u51c6", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u773c\u5e95\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u5047\u8bbe\u5c0f\u89c6\u573a\u5dee\u5f02\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5927\u89c6\u573a\u5dee\u5f02\u7684\u6311\u6218\u573a\u666f\uff0c\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u65b9\u6cd5\u4f1a\u5931\u8d25", "method": "\u63d0\u51faCARe\u65b9\u6cd5\uff1a1)\u88c1\u526a\u64cd\u4f5c\u5229\u7528\u89c6\u7f51\u819c\u751f\u7406\u7ed3\u6784\u4ece\u76ee\u6807\u56fe\u50cf\u88c1\u526a\u4e0e\u6e90\u56fe\u50cf\u89c6\u573a\u5927\u81f4\u5bf9\u9f50\u7684\u5b50\u56fe\u50cf\uff1b2)\u5bf9\u9f50\u6a21\u5757\u91c7\u7528\u53cc\u62df\u5408\u65b9\u6cd5\uff0c\u987a\u5e8f\u4f7f\u7528RANSAC\u7b97\u6cd5\u548c\u591a\u9879\u5f0f\u5750\u6807\u62df\u5408\u6539\u8fdb\u7a7a\u95f4\u53d8\u6362", "result": "\u5728\u5305\u542b60\u5bf9OCTA-wfCFP\u56fe\u50cf\u7684\u65b0\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86CARe\u65b9\u6cd5\u5728\u5927\u89c6\u573a\u5dee\u5f02\u8de8\u6a21\u6001\u773c\u5e95\u56fe\u50cf\u914d\u51c6\u4e2d\u7684\u53ef\u884c\u6027", "conclusion": "CARe\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3\u5177\u6709\u5927\u89c6\u573a\u5dee\u5f02\u7684\u8de8\u6a21\u6001\u773c\u5e95\u56fe\u50cf\u914d\u51c6\u6311\u6218\uff0c\u4e3a\u5148\u524d\u5c0f\u89c6\u573a\u5dee\u5f02\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u65b0\u5229\u7528\u7684\u9014\u5f84"}}
{"id": "2512.12658", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12658", "abs": "https://arxiv.org/abs/2512.12658", "authors": ["Qixin Xu", "Haozhe Wang", "Che Liu", "Fangzhen Lin", "Wenhu Chen"], "title": "CogDoc: Towards Unified thinking in Documents", "comment": null, "summary": "Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution \"Fast Reading\" phase for scalable information localization,followed by a high-resolution \"Focused Thinking\" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the \"policy conflict\" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.", "AI": {"tldr": "CogDoc\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7c97\u5230\u7ec6\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5feb\u901f\u9605\u8bfb\u548c\u4e13\u6ce8\u601d\u8003\u4e24\u9636\u6bb5\u5904\u7406\u6587\u6863\uff0c\u89e3\u51b3\u4e86\u957f\u6587\u6863\u5904\u7406\u4e0e\u7ec6\u7c92\u5ea6\u7ec6\u8282\u6355\u6349\u7684\u6743\u8861\u95ee\u9898\uff0c7B\u53c2\u6570\u6a21\u578b\u5728\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u7b49\u66f4\u5927\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u6587\u6863\u63a8\u7406\u8303\u5f0f\u5b58\u5728\u4e00\u4e2a\u57fa\u672c\u6743\u8861\uff1a\u53ef\u6269\u5c55\u6027\uff08\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u6587\u6863\uff09\u4e0e\u4fdd\u771f\u5ea6\uff08\u6355\u6349\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u7ec6\u8282\uff09\u4e4b\u95f4\u7684\u77db\u76fe\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCogDoc\u7edf\u4e00\u4ece\u7c97\u5230\u7ec6\u601d\u8003\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff1a1\uff09\u4f4e\u5206\u8fa8\u7387\"\u5feb\u901f\u9605\u8bfb\"\u9636\u6bb5\u8fdb\u884c\u53ef\u6269\u5c55\u4fe1\u606f\u5b9a\u4f4d\uff1b2\uff09\u9ad8\u5206\u8fa8\u7387\"\u4e13\u6ce8\u601d\u8003\"\u9636\u6bb5\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\u3002\u7814\u7a76\u4e86\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u53d1\u73b0\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u5e26\u76d1\u7763\u5fae\u8c03\u521d\u59cb\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "7B\u53c2\u6570\u6a21\u578b\u5728\u5176\u53c2\u6570\u7c7b\u522b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86GPT-4o\u7b49\u66f4\u5927\u7684\u4e13\u6709\u6a21\u578b\u3002\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u907f\u514d\u4e86\u76d1\u7763\u5fae\u8c03\u4e2d\u89c2\u5bdf\u5230\u7684\"\u7b56\u7565\u51b2\u7a81\"\u95ee\u9898\u3002", "conclusion": "CogDoc\u6846\u67b6\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u7684\u4e24\u9636\u6bb5\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u6863\u63a8\u7406\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u4fdd\u771f\u5ea6\u6743\u8861\u95ee\u9898\uff0c\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c0f\u6a21\u578b\u4e5f\u80fd\u5728\u590d\u6742\u6587\u6863\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5927\u6a21\u578b\u3002"}}
{"id": "2512.12664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12664", "abs": "https://arxiv.org/abs/2512.12664", "authors": ["Sreehari Rajan", "Kunal Bhosikar", "Charu Sharma"], "title": "InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation", "comment": null, "summary": "Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.", "AI": {"tldr": "InteracTalker\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u751f\u6210\u8bed\u97f3\u9a71\u52a8\u7684\u624b\u52bf\u548c\u7269\u4f53\u4ea4\u4e92\u52a8\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u5355\u72ec\u5904\u7406\u8fd9\u4e24\u7c7b\u4efb\u52a1\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u53ea\u80fd\u72ec\u7acb\u5904\u7406\u8bed\u97f3\u9a71\u52a8\u624b\u52bf\u6216\u7269\u4f53\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u96c6\u6210\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u54cd\u5e94\u8bed\u8a00\u548c\u7269\u7406\u5bf9\u8c61\u7684\u81ea\u7136\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u5b66\u4e60\u7edf\u4e00\u7684\u8fd0\u52a8\u3001\u8bed\u97f3\u548c\u63d0\u793a\u5d4c\u5165\u7a7a\u95f4\uff1b\u6784\u5efa\u4e30\u5bcc\u7684\u4eba-\u7269\u4ea4\u4e92\u6570\u636e\u96c6\uff1b\u4f7f\u7528\u5e7f\u4e49\u8fd0\u52a8\u9002\u5e94\u6a21\u5757\u8fdb\u884c\u72ec\u7acb\u8bad\u7ec3\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u52a8\u6001\u91cd\u65b0\u52a0\u6743\u5f02\u8d28\u6761\u4ef6\u4fe1\u53f7\u3002", "result": "InteracTalker\u5728\u8bed\u97f3\u624b\u52bf\u751f\u6210\u548c\u7269\u4f53\u4ea4\u4e92\u5408\u6210\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u4e13\u6ce8\u4e8e\u624b\u52bf\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u4ea7\u751f\u9ad8\u5ea6\u771f\u5b9e\u3001\u7269\u4f53\u611f\u77e5\u7684\u5168\u8eab\u8fd0\u52a8\uff0c\u5177\u6709\u589e\u5f3a\u7684\u771f\u5b9e\u611f\u3001\u7075\u6d3b\u6027\u548c\u63a7\u5236\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u5148\u524d\u5206\u79bb\u7684\u4efb\u52a1\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u6570\u5b57\u4f53\u9a8c\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u3001\u96c6\u6210\u7684\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12667", "abs": "https://arxiv.org/abs/2512.12667", "authors": ["Haiyang Zheng", "Nan Pu", "Wenjing Li", "Teng Long", "Nicu Sebe", "Zhun Zhong"], "title": "Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning", "comment": "Accepted by AAAI2026", "summary": "The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.", "AI": {"tldr": "\u63d0\u51faCAL\u6846\u67b6\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u95ee\u9898\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u975e\u5bf9\u79f0\u5b66\u4e60\u548c\u52a8\u6001\u539f\u578b\u526a\u679d\u7b56\u7565\uff0c\u5728\u5df2\u77e5\u548c\u672a\u77e5\u4f2a\u9020\u7c7b\u578b\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u5f00\u653e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1) \u7f6e\u4fe1\u5ea6\u504f\u659c\u5bfc\u81f4\u5bf9\u65b0\u578b\u4f2a\u9020\u7684\u4f2a\u6807\u7b7e\u4e0d\u53ef\u9760\uff0c\u4ea7\u751f\u8bad\u7ec3\u504f\u5dee\uff1b2) \u4e0d\u5207\u5b9e\u9645\u5730\u5047\u8bbe\u672a\u77e5\u4f2a\u9020\u7c7b\u578b\u7684\u6570\u91cf\u5df2\u77e5\u3002\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6df7\u5408\u4f2a\u9020\u7c7b\u578b\u3002", "method": "\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u611f\u77e5\u975e\u5bf9\u79f0\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u975e\u5bf9\u79f0\u7f6e\u4fe1\u5ea6\u589e\u5f3a\u3002\u524d\u8005\u901a\u8fc7\u57fa\u4e8e\u5f52\u4e00\u5316\u7f6e\u4fe1\u5ea6\u52a8\u6001\u7f29\u653e\u6837\u672c\u635f\u5931\u6765\u7f13\u89e3\u4f2a\u6807\u7b7e\u504f\u5dee\uff1b\u540e\u8005\u901a\u8fc7\u9009\u62e9\u6027\u5b66\u4e60\u5206\u522b\u6821\u51c6\u5df2\u77e5\u548c\u65b0\u578b\u4f2a\u9020\u7684\u7f6e\u4fe1\u5ea6\u3002\u6b64\u5916\u5f15\u5165\u52a8\u6001\u539f\u578b\u526a\u679d\u7b56\u7565\u81ea\u52a8\u4f30\u8ba1\u65b0\u578b\u4f2a\u9020\u7c7b\u578b\u6570\u91cf\u3002", "result": "\u5728\u6807\u51c6\u5f00\u653e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u57fa\u51c6\u548c\u65b0\u6269\u5c55\u7684\u5305\u542b\u9ad8\u7ea7\u64cd\u4f5c\u7684\u57fa\u51c6\u4e0a\uff0cCAL\u6846\u67b6\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5728\u5df2\u77e5\u548c\u65b0\u578b\u4f2a\u9020\u6eaf\u6e90\u4e0a\u90fd\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "CAL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u504f\u659c\u548c\u5148\u9a8c\u5047\u8bbe\u95ee\u9898\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u5b66\u4e60\u548c\u52a8\u6001\u539f\u578b\u526a\u679d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2512.12675", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12675", "abs": "https://arxiv.org/abs/2512.12675", "authors": ["Yuran Wang", "Bohan Zeng", "Chengzhuo Tong", "Wenxuan Liu", "Yang Shi", "Xiaochen Ma", "Hao Liang", "Yuanxing Zhang", "Wentao Zhang"], "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling", "comment": "Code: https://github.com/Ryann-Ran/Scone", "summary": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.", "AI": {"tldr": "Scone\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u89e3-\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u5b9e\u73b0\u591a\u4e3b\u4f53\u7ec4\u5408\u4e0e\u533a\u5206\uff0c\u5728\u590d\u6742\u89c6\u89c9\u573a\u666f\u4e2d\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4e3b\u9898\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u4ece\u5355\u4e3b\u4f53\u53d1\u5c55\u5230\u591a\u4e3b\u4f53\u7ec4\u5408\uff0c\u4f46\u5ffd\u89c6\u4e86\u533a\u5206\u80fd\u529b\u2014\u2014\u5f53\u8f93\u5165\u5305\u542b\u591a\u4e2a\u5019\u9009\u4e3b\u4f53\u65f6\u8bc6\u522b\u548c\u751f\u6210\u6b63\u786e\u4e3b\u4f53\u7684\u80fd\u529b\u3002\u8fd9\u4e00\u9650\u5236\u5f71\u54cd\u4e86\u5728\u590d\u6742\u73b0\u5b9e\u89c6\u89c9\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faScone\u7edf\u4e00\u7406\u89e3-\u751f\u6210\u65b9\u6cd5\uff0c\u96c6\u6210\u7ec4\u5408\u4e0e\u533a\u5206\u80fd\u529b\u3002\u8ba9\u7406\u89e3\u4e13\u5bb6\u4f5c\u4e3a\u8bed\u4e49\u6865\u6881\uff0c\u4f20\u9012\u8bed\u4e49\u4fe1\u606f\u5e76\u6307\u5bfc\u751f\u6210\u4e13\u5bb6\u5728\u6700\u5c0f\u5316\u5e72\u6270\u7684\u540c\u65f6\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff1a\u5148\u5b66\u4e60\u7ec4\u5408\uff0c\u7136\u540e\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u63a9\u7801\u589e\u5f3a\u533a\u5206\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eScone\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u7ec4\u5408\u548c\u533a\u5206\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u6a21\u578b\u3002\u540c\u65f6\u63d0\u51fa\u4e86SconeEval\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u7ec4\u5408\u548c\u533a\u5206\u80fd\u529b\u3002", "conclusion": "Scone\u901a\u8fc7\u7edf\u4e00\u7684\u7406\u89e3-\u751f\u6210\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4e3b\u4f53\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7ec4\u5408\u4e0e\u533a\u5206\u95ee\u9898\uff0c\u5728\u590d\u6742\u73b0\u5b9e\u89c6\u89c9\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u5173\u6a21\u578b\u3001\u57fa\u51c6\u548c\u8bad\u7ec3\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.12678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12678", "abs": "https://arxiv.org/abs/2512.12678", "authors": ["Fatimah Zohra", "Chen Zhao", "Hani Itani", "Bernard Ghanem"], "title": "$\u03b2$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment", "comment": null, "summary": "CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $\u03b2$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $\u03b2$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $\u03b2$-Contextualized Contrastive Alignment Loss ($\u03b2$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $\u03b2$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $\u03b2$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.", "AI": {"tldr": "\u03b2-CLIP\u901a\u8fc7\u591a\u7c92\u5ea6\u6587\u672c\u6761\u4ef6\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4ece\u5b8c\u6574\u63cf\u8ff0\u5230\u53e5\u5b50\u548c\u77ed\u8bed\u7684\u591a\u5c42\u6b21\u6587\u672c\u4e0e\u5bf9\u5e94\u89c6\u89c9\u533a\u57df\u7684\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "CLIP\u867d\u7136\u5728\u96f6\u6837\u672c\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u4f7f\u7528\u957f\u800c\u8be6\u7ec6\u7684\u63cf\u8ff0\u8fdb\u884c\u5fae\u8c03\u4e5f\u96be\u4ee5\u6539\u5584\u3002\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3aCLIP\u53ea\u5bf9\u9f50\u5168\u5c40\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\uff0c\u7f3a\u4e4f\u5bf9\u591a\u5c42\u6b21\u6587\u672c\u7c92\u5ea6\u4e0e\u5bf9\u5e94\u89c6\u89c9\u533a\u57df\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u03b2-CLIP\u591a\u7c92\u5ea6\u6587\u672c\u6761\u4ef6\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u52a8\u6001\u6c60\u5316\u56fe\u50cf\u5757\uff0c\u4e3a\u6bcf\u4e2a\u7c92\u5ea6\u7ea7\u522b\u751f\u6210\u4e0a\u4e0b\u6587\u89c6\u89c9\u5d4c\u5165\uff1b2\uff09\u5f15\u5165\u03b2-\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u5bf9\u9f50\u635f\u5931\uff08\u03b2-CAL\uff09\uff0c\u53c2\u6570\u5316\u4e25\u683c\u67e5\u8be2\u7279\u5b9a\u5339\u914d\u4e0e\u5bbd\u677e\u56fe\u50cf\u5185\u4e0a\u4e0b\u6587\u5316\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u652f\u6301\u8f6f\u4ea4\u53c9\u71b5\u548c\u786c\u4e8c\u5143\u4ea4\u53c9\u71b5\u516c\u5f0f\u3002", "result": "\u5728Urban1K\u6570\u636e\u96c6\u4e0a\u8fbe\u523091.8% T2I\u548c92.3% I2T\u7684R@1\u51c6\u786e\u7387\uff0c\u5728FG-OVD\uff08Hard\uff09\u4e0a\u8fbe\u523030.9%\uff0c\u5728\u6ca1\u6709\u4f7f\u7528\u786c\u8d1f\u6837\u672c\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u5bf9\u9f50\u6027\u80fd\u3002", "conclusion": "\u03b2-CLIP\u4e3a\u5bc6\u96c6\u89c6\u89c9\u8bed\u8a00\u5bf9\u5e94\u5efa\u7acb\u4e86\u5f3a\u5927\u3001\u81ea\u9002\u5e94\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u5c42\u6b21\u5bf9\u9f50\u6709\u6548\u89e3\u51b3\u4e86CLIP\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.12701", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12701", "abs": "https://arxiv.org/abs/2512.12701", "authors": ["Xue Li", "Xiaonan Song", "Henry Hu"], "title": "Efficient Vision-Language Reasoning via Adaptive Token Pruning", "comment": "10 pages, 3 figures. Expanded version of an extended abstract accepted at NeurIPS 2025 Workshop on VLM4RWD. Presents methodology and preliminary experimental results", "summary": "Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.", "AI": {"tldr": "ATP\u662f\u4e00\u79cd\u52a8\u6001\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7\u4fdd\u7559\u6700\u5177\u4fe1\u606f\u91cf\u7684token\u6765\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5b9e\u73b0\u7ea640%\u7684FLOPs\u964d\u4f4e\u548c1.5\u500d\u52a0\u901f\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e1%\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u5bf9\u6240\u6709token\u8fdb\u884c\u7edf\u4e00\u5904\u7406\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u52a8\u6001\u63a8\u7406\u673a\u5236\u6765\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94token\u526a\u679d(ATP)\uff0c\u5728\u89c6\u89c9-\u8bed\u8a00\u63a5\u53e3\u5904\u57fa\u4e8e\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u4fdd\u7559\u6700\u5177\u4fe1\u606f\u91cf\u7684token\u3002\u4f7f\u7528\u6df7\u5408\u91cd\u8981\u6027\u8bc4\u5206\u7ed3\u5408ViT CLS\u6ce8\u610f\u529b\uff08\u6a21\u6001\u5185\u663e\u8457\u6027\uff09\u548cCLIP\u6587\u672c-\u56fe\u50cf\u76f8\u4f3c\u5ea6\uff08\u6a21\u6001\u95f4\u76f8\u5173\u6027\uff09\uff0c\u4e3aLLM\u4fdd\u7559top-K token\u3002", "result": "\u5728VQAv2\u3001GQA\u548cCOCO\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cATP\u51cf\u5c11\u7ea640%\u7684\u63a8\u7406FLOPs\uff0c\u5b9e\u73b0\u7ea61.5\u500d\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e1%\u3002\u5b9a\u6027\u5206\u6790\u8868\u660eATP\u4fdd\u6301\u4e86\u89c6\u89c9\u57fa\u7840\u5e76\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ATP\u8bc1\u660e\u8d44\u6e90\u53d7\u9650\u7684\u63a8\u7406\u548c\u6a21\u578b\u53ef\u9760\u6027\u4e0d\u662f\u7ade\u4e89\u76ee\u6807\u3002\u81ea\u9002\u5e94\u526a\u679d\u53ef\u4ee5\u6291\u5236\u865a\u5047\u76f8\u5173\u6027\uff0c\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u5728\u9ad8\u6548\u591a\u6a21\u6001\u8fb9\u7f18\u8ba1\u7b97\u7ba1\u9053\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.12751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12751", "abs": "https://arxiv.org/abs/2512.12751", "authors": ["Zhenya Yang", "Zhe Liu", "Yuxiang Lu", "Liping Hou", "Chenxuan Miao", "Siyi Peng", "Bailan Feng", "Xiang Bai", "Hengshuang Zhao"], "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation", "comment": "The project page is available at https://huster-yzy.github.io/geniedrive_project_page/", "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.", "AI": {"tldr": "GenieDrive\uff1a\u4e00\u79cd\u57fa\u4e8e4D\u5360\u636e\u8868\u793a\u7684\u7269\u7406\u611f\u77e5\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u5360\u636e\u5230\u4e09\u5e73\u9762\u6f5c\u5728\u8868\u793a\uff0c\u7ed3\u5408\u4e92\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u7269\u7406\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u6269\u6563\u6a21\u578b\u76f4\u63a5\u5c06\u9a7e\u9a76\u52a8\u4f5c\u6620\u5c04\u5230\u89c6\u9891\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5b66\u4e60\u56f0\u96be\u4e14\u5bb9\u6613\u4ea7\u751f\u7269\u7406\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u7269\u7406\u611f\u77e5\u9a7e\u9a76\u89c6\u9891\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u9a7e\u9a76\u89c4\u5212\u3001\u5206\u5e03\u5916\u6570\u636e\u5408\u6210\u548c\u95ed\u73af\u8bc4\u4f30\u7b49\u4efb\u52a1\u3002", "method": "\u63d0\u51faGenieDrive\u6846\u67b6\uff1a1\uff09\u9996\u5148\u751f\u6210\u5305\u542b\u4e30\u5bcc\u7269\u7406\u4fe1\u606f\uff08\u9ad8\u5206\u8fa8\u73873D\u7ed3\u6784\u548c\u52a8\u6001\uff09\u76844D\u5360\u636e\uff1b2\uff09\u8bbe\u8ba1VAE\u5c06\u5360\u636e\u538b\u7f29\u5230\u4e09\u5e73\u9762\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u6f5c\u5728\u5927\u5c0f\u51cf\u5c11\u5230\u5148\u524d\u65b9\u6cd5\u768458%\uff1b3\uff09\u5f15\u5165\u4e92\u63a7\u6ce8\u610f\u529b\uff08MCA\uff09\u51c6\u786e\u5efa\u6a21\u63a7\u5236\u5bf9\u5360\u636e\u6f14\u5316\u7684\u5f71\u54cd\uff1b4\uff09\u7aef\u5230\u7aef\u8054\u5408\u8bad\u7ec3VAE\u548c\u9884\u6d4b\u6a21\u5757\uff1b5\uff09\u5728\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u5f15\u5165\u5f52\u4e00\u5316\u591a\u89c6\u89d2\u6ce8\u610f\u529b\uff0c\u4ee54D\u5360\u636e\u4e3a\u6307\u5bfc\u751f\u6210\u591a\u89c6\u89d2\u9a7e\u9a76\u89c6\u9891\u3002", "result": "\u5728\u63a8\u7406\u901f\u5ea641 FPS\u4e0b\uff0c\u9884\u6d4bmIoU\u63d0\u53477.2%\uff0c\u4ec5\u4f7f\u75283.47M\u53c2\u6570\u3002\u89c6\u9891\u8d28\u91cf\u663e\u8457\u6539\u5584\uff0cFVD\u964d\u4f4e20.7%\u3002\u5b9e\u9a8c\u8868\u660eGenieDrive\u80fd\u591f\u5b9e\u73b0\u9ad8\u5ea6\u53ef\u63a7\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u4e14\u7269\u7406\u611f\u77e5\u7684\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u3002", "conclusion": "GenieDrive\u901a\u8fc74D\u5360\u636e\u8868\u793a\u548c\u4e09\u5e73\u9762\u538b\u7f29\uff0c\u7ed3\u5408\u4e92\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u7269\u7406\u611f\u77e5\u7684\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2512.12756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12756", "abs": "https://arxiv.org/abs/2512.12756", "authors": ["Yue Jiang", "Dingkang Yang", "Minghao Han", "Jinghang Han", "Zizhi Chen", "Yizhou Liu", "Mingcheng Li", "Peng Zhai", "Lihua Zhang"], "title": "FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning", "comment": "The omni-modal benchmark report from Fysics AI", "summary": "Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.", "AI": {"tldr": "FysicsWorld\u662f\u9996\u4e2a\u7edf\u4e00\u7684\u5168\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u4e4b\u95f4\u7684\u53cc\u5411\u8f93\u5165\u8f93\u51fa\uff0c\u5305\u542b16\u4e2a\u4e3b\u8981\u4efb\u52a1\u548c3,268\u4e2a\u6837\u672c\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7406\u89e3\u3001\u751f\u6210\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6a21\u6001\u8986\u76d6\u4e0d\u5b8c\u6574\u3001\u4ea4\u4e92\u4ec5\u9650\u4e8e\u6587\u672c\u8f93\u51fa\u3001\u6a21\u6001\u95f4\u76f8\u4e92\u4f9d\u8d56\u6027\u548c\u4e92\u8865\u6027\u5f31\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u63a8\u52a8\u5168\u6a21\u6001\u67b6\u6784\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86FysicsWorld\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b16\u4e2a\u4e3b\u8981\u4efb\u52a1\u548c3,268\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6837\u672c\uff0c\u8986\u76d640\u591a\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u6e90\u3002\u91c7\u7528\u8de8\u6a21\u6001\u4e92\u8865\u6027\u7b5b\u9009(CMCS)\u7b56\u7565\u548c\u7cfb\u7edf\u5316\u6570\u636e\u6784\u5efa\u6846\u67b6\uff0c\u751f\u6210\u7528\u4e8e\u53e3\u8bed\u4ea4\u4e92\u548c\u878d\u5408\u4f9d\u8d56\u8de8\u6a21\u6001\u63a8\u7406\u7684\u5168\u6a21\u6001\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5bf930\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ecMLLMs\u3001\u6a21\u6001\u7279\u5b9a\u6a21\u578b\u3001\u7edf\u4e00\u7406\u89e3-\u751f\u6210\u6a21\u578b\u548c\u5168\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff09\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u7406\u89e3\u3001\u751f\u6210\u548c\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\u5dee\u5f02\u548c\u5c40\u9650\u6027\u3002", "conclusion": "FysicsWorld\u4e3a\u8bc4\u4f30\u548c\u63a8\u8fdb\u4e0b\u4e00\u4ee3\u5168\u6a21\u6001\u67b6\u6784\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u57fa\u7840\u548c\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u586b\u8865\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u652f\u6301\u4efb\u610f\u6a21\u6001\u95f4\u7684\u53cc\u5411\u8bc4\u4f30\u3002"}}
{"id": "2512.12768", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12768", "abs": "https://arxiv.org/abs/2512.12768", "authors": ["Tianjiao Yu", "Xinzhuo Li", "Yifan Shen", "Yuanzhe Liu", "Ismini Lourentzou"], "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence", "comment": null, "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.", "AI": {"tldr": "CoRe3D\u63d0\u51fa\u7edf\u4e003D\u7406\u89e3\u4e0e\u751f\u6210\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u7a7a\u95f4\u63a8\u7406\u548c\u7ed3\u6784\u5316\u7a7a\u95f4\u63a8\u7406\u7684\u7d27\u5bc6\u8026\u5408\uff0c\u5b9e\u73b0\u8bed\u8a00\u5f15\u5bfc\u76843D\u5185\u5bb9\u751f\u6210", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u663e\u5f0f\u63a8\u7406\u673a\u5236\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f46\u57283D\u9886\u57df\u5e94\u7528\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u74063D\u7406\u89e3\u548c\u751f\u6210\u7684\u63a8\u7406\u6846\u67b6", "method": "\u5f15\u5165\u7a7a\u95f4\u57fa\u7840\u63a8\u7406\u8868\u793a\uff0c\u5c063D\u6f5c\u5728\u7a7a\u95f4\u5206\u89e3\u4e3a\u5c40\u90e8\u5316\u533a\u57df\uff0c\u901a\u8fc7\u8bed\u4e49\u94fe\u5f0f\u63a8\u7406\u4e0e\u7ed3\u6784\u5316\u7a7a\u95f4\u63a8\u7406\u7684\u7d27\u5bc6\u8026\u5408\uff0c\u5b9e\u73b0\u4ece\u9ad8\u5c42\u8bed\u8a00\u610f\u56fe\u5230\u4f4e\u5c423D\u5185\u5bb9\u5f62\u6210\u7684\u76f4\u63a5\u5f15\u5bfc", "result": "CoRe3D\u751f\u6210\u76843D\u8f93\u51fa\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5c40\u90e8\u4e00\u81f4\u6027\u548c\u4e0e\u8bed\u8a00\u63cf\u8ff0\u7684\u9ad8\u5ea6\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u7ec4\u5408\u5f0f\u548c\u8fc7\u7a0b\u5f0f\u7684\u51e0\u4f55\u63a8\u7406", "conclusion": "CoRe3D\u901a\u8fc7\u7edf\u4e00\u7684\u8bed\u4e49-\u7a7a\u95f4\u63a8\u7406\u6846\u67b6\uff0c\u4e3a3D\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b"}}
{"id": "2512.12799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12799", "abs": "https://arxiv.org/abs/2512.12799", "authors": ["Zhe Liu", "Runhui Huang", "Rui Yang", "Siming Yan", "Zining Wang", "Lu Hou", "Di Lin", "Xiang Bai", "Hengshuang Zhao"], "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning", "comment": null, "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI", "AI": {"tldr": "DrivePI\u662f\u4e00\u4e2a\u7a7a\u95f4\u611f\u77e5\u76844D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f5c\u4e3a\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5e76\u884c\u6267\u884c\u7a7a\u95f4\u7406\u89e3\u30013D\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\uff0c\u4ec5\u4f7f\u75280.5B\u53c2\u6570\u5c31\u80fd\u8d85\u8d8a\u73b0\u67097B\u6a21\u578b\u548c\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u751f\u6210\u7ec6\u7c92\u5ea63D\u611f\u77e5\u548c\u9884\u6d4b\u8f93\u51fa\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u7406\u89e3\u30013D\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u7684\u96c6\u6210\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDrivePI\u6846\u67b6\uff0c\u96c6\u6210\u70b9\u4e91\u3001\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u8bed\u8a00\u6307\u4ee4\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u5e76\u884c\u6267\u884c\u7a7a\u95f4\u7406\u89e3\u30013D\u5360\u7528\u611f\u77e5\u3001\u5360\u7528\u6d41\u9884\u6d4b\u548c\u89c4\u5212\u3002\u5f00\u53d1\u6570\u636e\u5f15\u64ce\u751f\u6210\u6587\u672c-\u5360\u7528\u548c\u6587\u672c-\u6d41\u95ee\u7b54\u5bf9\u7528\u4e8e4D\u7a7a\u95f4\u7406\u89e3\u8bad\u7ec3\u3002", "result": "\u4ec5\u4f7f\u75280.5B\u53c2\u6570\u7684Qwen2.5\u4f5c\u4e3a\u9aa8\u5e72\uff0cDrivePI\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff1a\u5728nuScenes-QA\u4e0a\u6bd4OpenDriveVLA-7B\u9ad82.5%\u5e73\u5747\u51c6\u786e\u7387\uff1b\u78b0\u649e\u7387\u6bd4ORION\u964d\u4f4e70%\uff1b\u57283D\u5360\u7528\u3001\u5360\u7528\u6d41\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "DrivePI\u8bc1\u660e\u4e86\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6709\u6548\u96c6\u6210\u7a7a\u95f4\u7406\u89e3\u30013D\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\uff0c\u4ec5\u9700\u8f83\u5c0f\u6a21\u578b\u5c31\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u5927\u578b\u6a21\u578b\u548c\u4e13\u7528\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2512.12800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12800", "abs": "https://arxiv.org/abs/2512.12800", "authors": ["Yunlong He", "Gwilherm Lesn\u00e9", "Ziqian Liu", "Micha\u00ebl Soumm", "Pietro Gori"], "title": "Learning Common and Salient Generative Factors Between Two Image Datasets", "comment": "This is the author's version of a work submitted to IEEE for possible publication. The final version may differ from this version", "summary": "Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4e24\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u5206\u79bb\u5171\u4eab\u7684\u751f\u6210\u56e0\u5b50\u548c\u7279\u5b9a\u4e8e\u5355\u4e2a\u6570\u636e\u96c6\u7684\u663e\u8457\u56e0\u5b50\uff0c\u9002\u7528\u4e8eGAN\u548c\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6761\u4ef6\u64cd\u4f5c\u6216\u89e3\u8026\u8868\u793a\u5b66\u4e60\uff0c\u800c\u672c\u6587\u5173\u6ce8\u4e00\u4e2a\u8f83\u5c11\u88ab\u7814\u7a76\u7684\u95ee\u9898\u2014\u2014\u5bf9\u6bd4\u5206\u6790\uff08CA\uff09\uff0c\u5373\u7ed9\u5b9a\u4e24\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5206\u79bb\u51fa\u8de8\u6570\u636e\u96c6\u5171\u4eab\u7684\u516c\u5171\u751f\u6210\u56e0\u5b50\u548c\u4ec5\u7279\u5b9a\u4e8e\u4e00\u4e2a\u6570\u636e\u96c6\u7684\u663e\u8457\u56e0\u5b50\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5bf9\u6bd4\u5206\u6790\u6846\u67b6\uff0c\u53ef\u9002\u914d\u4e8eGAN\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5b9a\u4e49\u65b0\u7684\u5b66\u4e60\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fdd\u516c\u5171\u56e0\u5b50\u548c\u663e\u8457\u56e0\u5b50\u7684\u6709\u6548\u5206\u79bb\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\uff08\u5305\u62ec\u4eba\u8138\u3001\u52a8\u7269\u56fe\u50cf\u548c\u533b\u5b66\u626b\u63cf\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u56e0\u5b50\u5206\u79bb\u80fd\u529b\u548c\u56fe\u50cf\u5408\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u5bf9\u6bd4\u5206\u6790\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4ec5\u4f7f\u7528\u6570\u636e\u96c6\u4fe1\u53f7\uff08\u800c\u975e\u5c5e\u6027\u76d1\u7763\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u6210\u529f\u5206\u79bb\u516c\u5171\u548c\u663e\u8457\u751f\u6210\u56e0\u5b50\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.12822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12822", "abs": "https://arxiv.org/abs/2512.12822", "authors": ["Yongyuan Liang", "Xiyao Wang", "Yuanchen Ju", "Jianwei Yang", "Furong Huang"], "title": "Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding", "comment": null, "summary": "Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.", "AI": {"tldr": "Lemon\u662f\u4e00\u4e2a\u7edf\u4e00\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5c063D\u70b9\u4e91\u5757\u548c\u8bed\u8a00\u6807\u8bb0\u4f5c\u4e3a\u5355\u4e00\u5e8f\u5217\u8054\u5408\u5904\u7406\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u57283D\u7406\u89e3\u4e2d\u7684\u6269\u5c55\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u6269\u5c55\u52303D\u7406\u89e3\u65f6\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u70b9\u4e91\u6570\u636e\u7a00\u758f\u4e0d\u89c4\u5219\u3001\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u788e\u7247\u5316\u67b6\u6784\uff08\u5177\u6709\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\uff09\u3001\u8bad\u7ec3\u6d41\u7a0b\u4e0d\u7a33\u5b9a\u4e14\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684Transformer\u67b6\u6784\uff0c\u5c063D\u70b9\u4e91\u5757\u548c\u8bed\u8a00\u6807\u8bb0\u4f5c\u4e3a\u5355\u4e00\u5e8f\u5217\u8054\u5408\u5904\u7406\uff1b\u5f00\u53d1\u7ed3\u6784\u5316\u5206\u5757\u548c\u6807\u8bb0\u5316\u65b9\u6848\u4ee5\u4fdd\u7559\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff1b\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u4ece\u5bf9\u8c61\u7ea7\u8bc6\u522b\u9010\u6b65\u6784\u5efa\u5230\u573a\u666f\u7ea7\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u5168\u9762\u76843D\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\uff08\u4ece\u5bf9\u8c61\u8bc6\u522b\u548c\u63cf\u8ff0\u52303D\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\uff09\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u968f\u7740\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u6570\u636e\u589e\u52a0\u800c\u5177\u6709\u7684\u7a33\u5065\u6269\u5c55\u7279\u6027\u3002", "conclusion": "Lemon\u4e3a\u63a8\u8fdb\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u76843D\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u65e9\u671f\u7a7a\u95f4-\u8bed\u8a00\u878d\u5408\u3001\u6d88\u9664\u5197\u4f59\u7f16\u7801\u5668\u3001\u63d0\u9ad8\u53c2\u6570\u6548\u7387\u548c\u66f4\u6709\u6548\u7684\u6a21\u578b\u6269\u5c55\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.12875", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.12875", "abs": "https://arxiv.org/abs/2512.12875", "authors": ["Weihan Xu", "Kan Jen Cheng", "Koichi Saito", "Muhammad Jehanzeb Mirza", "Tingle Li", "Yisi Liu", "Alexander H. Liu", "Liming Wang", "Masato Ishii", "Takashi Shibuya", "Yuki Mitsufuji", "Gopala Anumanchipalli", "Paul Pu Liang"], "title": "Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal", "comment": null, "summary": "Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.", "AI": {"tldr": "SAVE\u6a21\u578b\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6d41\u5339\u914d\u6a21\u578b\uff0c\u7528\u4e8e\u8054\u5408\u7f16\u8f91\u97f3\u9891\u548c\u89c6\u9891\u5185\u5bb9\uff0c\u901a\u8fc7Schrodinger Bridge\u5b9e\u73b0\u6e90\u5230\u76ee\u6807\u89c6\u542c\u6df7\u5408\u7684\u76f4\u63a5\u4f20\u8f93\uff0c\u5728\u4fdd\u6301\u5185\u5bb9\u5bf9\u9f50\u7684\u540c\u65f6\u79fb\u9664\u76ee\u6807\u5bf9\u8c61\u3002", "motivation": "\u8054\u5408\u7f16\u8f91\u97f3\u9891\u548c\u89c6\u89c9\u5185\u5bb9\u5bf9\u4e8e\u7cbe\u786e\u53ef\u63a7\u7684\u5185\u5bb9\u521b\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u914d\u5bf9\u89c6\u542c\u6570\u636e\u6709\u9650\u548c\u6a21\u6001\u5f02\u8d28\u6027\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faSAVEBench\u914d\u5bf9\u89c6\u542c\u6570\u636e\u96c6\uff0c\u8bad\u7ec3SAVE\u6a21\u578b\uff08Schrodinger Audio-Visual Editor\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6d41\u5339\u914d\u6a21\u578b\uff0c\u4f7f\u7528Schrodinger Bridge\u5b66\u4e60\u4ece\u6e90\u5230\u76ee\u6807\u89c6\u542c\u6df7\u5408\u7684\u76f4\u63a5\u4f20\u8f93\uff0c\u5e76\u884c\u7f16\u8f91\u97f3\u9891\u548c\u89c6\u9891\u5e76\u4fdd\u6301\u5bf9\u9f50\u3002", "result": "SAVE\u6a21\u578b\u80fd\u591f\u79fb\u9664\u97f3\u9891\u548c\u89c6\u89c9\u5185\u5bb9\u4e2d\u7684\u76ee\u6807\u5bf9\u8c61\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4f59\u5185\u5bb9\uff0c\u5728\u65f6\u95f4\u540c\u6b65\u6027\u548c\u89c6\u542c\u8bed\u4e49\u5bf9\u5e94\u65b9\u9762\u4f18\u4e8e\u97f3\u9891\u7f16\u8f91\u5668\u548c\u89c6\u9891\u7f16\u8f91\u5668\u7684\u6210\u5bf9\u7ec4\u5408\u3002", "conclusion": "SAVE\u6a21\u578b\u901a\u8fc7Schrodinger Bridge\u548c\u6d41\u5339\u914d\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8054\u5408\u89c6\u542c\u7f16\u8f91\u7684\u6570\u636e\u548c\u5efa\u6a21\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5185\u5bb9\u5bf9\u9f50\u548c\u7f16\u8f91\u6548\u679c\u3002"}}
{"id": "2512.12906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12906", "abs": "https://arxiv.org/abs/2512.12906", "authors": ["Zhimao Peng", "Enguang Wang", "Xialei Liu", "Ming-Ming Cheng"], "title": "Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection", "comment": "Accepted by TCSVT2024", "summary": "Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u6837\u672c\u5206\u914d\uff08PSA\uff09\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u5206\u5e03\u5916\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9608\u503c\u4e09\u5143\u6837\u672c\u5206\u914d\u7b56\u7565\u63d0\u9ad8ID\u548cOOD\u6837\u672c\u96c6\u7684\u7eaf\u5ea6\uff0c\u5e76\u5f15\u5165\u6982\u5ff5\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u635f\u5931\u6765\u589e\u5f3aID/OOD\u533a\u5206\u80fd\u529b\u3002", "motivation": "\u5f53\u524dSCOOD\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684ID\u6837\u672c\u8fc7\u6ee4\u7b56\u7565\uff0c\u4ece\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u9009\u62e9\u5e72\u51c0\u7684ID\u6837\u672c\uff0c\u5e76\u5c06\u5269\u4f59\u6837\u672c\u4f5c\u4e3a\u8f85\u52a9OOD\u6570\u636e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u53ef\u907f\u514d\u5730\u5f15\u5165\u4e86\u5927\u91cf\u566a\u58f0\u6837\u672c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6837\u672c\u9009\u62e9\u548c\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u9884\u6d4b\u6837\u672c\u5206\u914d\uff08PSA\uff09\u7684SCOOD\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u57fa\u4e8e\u9884\u6d4b\u80fd\u91cf\u5206\u6570\u7684\u53cc\u9608\u503c\u4e09\u5143\u6837\u672c\u5206\u914d\u7b56\u7565\uff0c\u5c06\u4e0d\u786e\u5b9a\u7684\u65e0\u6807\u7b7e\u6570\u636e\u5206\u914d\u5230\u989d\u5916\u7684\u4e22\u5f03\u6837\u672c\u96c6\u4e2d\uff1b2\uff09\u6982\u5ff5\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u635f\u5931\uff0c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u6269\u5927ID\u548cOOD\u6837\u672c\u4e4b\u95f4\u7684\u8ddd\u79bb\uff1b3\uff09\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\uff0c\u5e2e\u52a9\u6a21\u578b\u5145\u5206\u62df\u5408\u9009\u5b9a\u7684\u8f85\u52a9ID/OOD\u6837\u672c\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6SCOOD\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684PSA\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u6837\u672c\u5206\u914d\u7eaf\u5ea6\u548c\u589e\u5f3a\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86SCOOD\u4efb\u52a1\u4e2d\u7684\u566a\u58f0\u6837\u672c\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.12925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12925", "abs": "https://arxiv.org/abs/2512.12925", "authors": ["Zhimao Peng", "Enguang Wang", "Fei Yang", "Xialei Liu", "Ming-Ming Cheng"], "title": "Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery", "comment": "Accepted by TMM2025", "summary": "Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.", "AI": {"tldr": "\u63d0\u51faLSP\u548cDAS\u4e24\u4e2a\u6a21\u5757\u6765\u89e3\u51b3\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u4e2d\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u635f\u5931\u9510\u5ea6\u60e9\u7f5a\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u52a8\u6001\u951a\u70b9\u9009\u62e9\u6539\u5584\u672a\u77e5\u7c7b\u522b\u5b66\u4e60\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u53c2\u6570\u5316\u5206\u7c7b\u7684GCD\u65b9\u6cd5\u91c7\u7528DINO\u5f0f\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u4f46\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u7279\u5b9a\u89c6\u89c9\u6a21\u5f0f\u6709\u504f\u597d\uff0c\u5bfc\u81f4\u5bf9\u672a\u6807\u8bb0\u6570\u636e\u7f16\u7801\u865a\u5047\u76f8\u5173\u6027\u5e76\u4ea7\u751f\u566a\u58f0\u4f2a\u6807\u7b7e\u3002\u9700\u8981\u89e3\u51b3\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u4ee5\u63d0\u9ad8\u805a\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e24\u4e2a\u6a21\u5757\u7684\u65b0\u65b9\u6cd5\uff1a1) \u635f\u5931\u9510\u5ea6\u60e9\u7f5a(LSP)\uff1a\u901a\u8fc7\u6700\u5c0f\u5316\u6a21\u578b\u6700\u574f\u60c5\u51b5\u635f\u5931\u9510\u5ea6\u6765\u589e\u5f3a\u53c2\u6570\u5bf9\u5c0f\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u6291\u5236\u7410\u788e\u7279\u5f81\u7f16\u7801\uff0c\u51cf\u5c11\u566a\u58f0\u6837\u672c\u8fc7\u62df\u5408\uff1b2) \u52a8\u6001\u951a\u70b9\u9009\u62e9(DAS)\uff1a\u57fa\u4e8eKNN\u5bc6\u5ea6\u548c\u7c7b\u522b\u6982\u7387\u9009\u62e9\u672a\u77e5\u7c7b\u522b\u7684\u4ee3\u8868\u6027\u6837\u672c\u5e76\u5206\u914d\u786c\u4f2a\u6807\u7b7e\uff0c\u7f13\u89e3\u5df2\u77e5\u4e0e\u672a\u77e5\u7c7b\u522b\u7684\u7f6e\u4fe1\u5ea6\u5dee\u5f02\uff0c\u5feb\u901f\u5b66\u4e60\u66f4\u51c6\u786e\u7684\u7279\u5f81\u5206\u5e03\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u4f2a\u6807\u7b7e\u566a\u58f0\uff0c\u5728\u591a\u4e2aGCD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684LSP\u548cDAS\u6a21\u5757\u80fd\u6709\u6548\u89e3\u51b3GCD\u4e2d\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0cLSP\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0cDAS\u6539\u5584\u672a\u77e5\u7c7b\u522b\u5b66\u4e60\uff0c\u5171\u540c\u63d0\u9ad8\u4e86\u805a\u7c7b\u51c6\u786e\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.12935", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12935", "abs": "https://arxiv.org/abs/2512.12935", "authors": ["Toan Le Ngo Thanh", "Phat Ha Huu", "Tan Nguyen Dang Duy", "Thong Nguyen Le Minh", "Anh Nguyen Nhu Tinh"], "title": "Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion", "comment": "Accepted at AAAI Workshop 2026", "summary": "The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u591a\u6a21\u6001\u65f6\u523b\u68c0\u7d22\u7cfb\u7edf\uff0c\u89e3\u51b3\u56fa\u5b9a\u6743\u91cd\u878d\u5408\u3001\u65f6\u5e8f\u5efa\u6a21\u548c\u624b\u52a8\u6a21\u6001\u9009\u62e9\u4e09\u5927\u6311\u6218\uff0c\u901a\u8fc7\u7ea7\u8054\u53cc\u5d4c\u5165\u3001\u65f6\u5e8f\u611f\u77e5\u8bc4\u5206\u548c\u667a\u80fd\u4f53\u5f15\u5bfc\u67e5\u8be2\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u3002", "motivation": "\u89c6\u9891\u5185\u5bb9\u7206\u70b8\u5f0f\u589e\u957f\u6025\u9700\u9ad8\u6548\u591a\u6a21\u6001\u65f6\u523b\u68c0\u7d22\u7cfb\u7edf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u56fa\u5b9a\u6743\u91cd\u878d\u5408\u7b56\u7565\u65e0\u6cd5\u5e94\u5bf9\u8de8\u6a21\u6001\u566a\u58f0\u548c\u6a21\u7cca\u67e5\u8be2\uff1b\u65f6\u5e8f\u5efa\u6a21\u96be\u4ee5\u6355\u6349\u8fde\u8d2f\u4e8b\u4ef6\u5e8f\u5217\u5e76\u60e9\u7f5a\u4e0d\u73b0\u5b9e\u7684\u65f6\u95f4\u95f4\u9694\uff1b\u7cfb\u7edf\u9700\u8981\u624b\u52a8\u6a21\u6001\u9009\u62e9\uff0c\u964d\u4f4e\u53ef\u7528\u6027\u3002", "method": "1. \u7ea7\u8054\u53cc\u5d4c\u5165\u7ba1\u9053\uff1a\u7ed3\u5408BEIT-3\u548cSigLIP\u8fdb\u884c\u5e7f\u6cdb\u68c0\u7d22\uff0c\u901a\u8fc7BLIP-2\u91cd\u6392\u5e8f\u5e73\u8861\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u30022. \u65f6\u5e8f\u611f\u77e5\u8bc4\u5206\u673a\u5236\uff1a\u901a\u8fc7\u6ce2\u675f\u641c\u7d22\u5bf9\u5927\u7684\u65f6\u95f4\u95f4\u9694\u5e94\u7528\u6307\u6570\u8870\u51cf\u60e9\u7f5a\uff0c\u6784\u5efa\u8fde\u8d2f\u4e8b\u4ef6\u5e8f\u5217\u800c\u975e\u5b64\u7acb\u5e27\u30023. \u667a\u80fd\u4f53\u5f15\u5bfc\u67e5\u8be2\u5206\u89e3\uff1a\u4f7f\u7528GPT-4o\u81ea\u52a8\u89e3\u91ca\u6a21\u7cca\u67e5\u8be2\uff0c\u5206\u89e3\u4e3a\u6a21\u6001\u7279\u5b9a\u5b50\u67e5\u8be2\uff08\u89c6\u89c9/OCR/ASR\uff09\uff0c\u5e76\u8fdb\u884c\u81ea\u9002\u5e94\u5206\u6570\u878d\u5408\u6d88\u9664\u624b\u52a8\u6a21\u6001\u9009\u62e9\u3002", "result": "\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u5904\u7406\u6a21\u7cca\u67e5\u8be2\uff0c\u68c0\u7d22\u65f6\u5e8f\u8fde\u8d2f\u7684\u5e8f\u5217\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u878d\u5408\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u4ea4\u4e92\u5f0f\u65f6\u523b\u641c\u7d22\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u591a\u6a21\u6001\u65f6\u523b\u68c0\u7d22\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u7ea7\u8054\u68c0\u7d22\u3001\u65f6\u5e8f\u5efa\u6a21\u548c\u667a\u80fd\u67e5\u8be2\u5206\u89e3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u89c6\u9891\u5185\u5bb9\u7684\u9ad8\u6548\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12936", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12936", "abs": "https://arxiv.org/abs/2512.12936", "authors": ["Tiange Zhang", "Xiandong Meng", "Siwei Ma"], "title": "Content Adaptive based Motion Alignment Framework for Learned Video Compression", "comment": "Accepted to Data Compression Conference (DCC) 2026 as a poster paper", "summary": "Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5185\u5bb9\u81ea\u9002\u5e94\u7684\u8fd0\u52a8\u5bf9\u9f50\u6846\u67b6CAMA\uff0c\u901a\u8fc7\u4e09\u65b9\u9762\u6539\u8fdb\u63d0\u5347\u7aef\u5230\u7aef\u89c6\u9891\u538b\u7f29\u6027\u80fd\uff1a\u4e24\u9636\u6bb5\u6d41\u5f15\u5bfc\u53ef\u53d8\u5f62\u626d\u66f2\u673a\u5236\u3001\u591a\u53c2\u8003\u8d28\u91cf\u611f\u77e5\u7b56\u7565\u3001\u65e0\u8bad\u7ec3\u7684\u4e0b\u91c7\u6837\u6a21\u5757\uff0c\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578bDCVC-TCM\u5b9e\u73b024.95% BD-rate\u8282\u7701\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u89c6\u9891\u538b\u7f29\u6846\u67b6\u7f3a\u4e4f\u5185\u5bb9\u7279\u5f02\u6027\u9002\u5e94\u80fd\u529b\uff0c\u5bfc\u81f4\u538b\u7f29\u6027\u80fd\u4e0d\u7406\u60f3\u3002\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u5185\u5bb9\u7279\u6027\u81ea\u9002\u5e94\u8c03\u6574\u7f16\u7801\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u538b\u7f29\u6548\u7387\u3002", "method": "1. \u4e24\u9636\u6bb5\u6d41\u5f15\u5bfc\u53ef\u53d8\u5f62\u626d\u66f2\u673a\u5236\uff1a\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u504f\u79fb\u9884\u6d4b\u548c\u63a9\u7801\u8c03\u5236\uff0c\u5b9e\u73b0\u7cbe\u786e\u7279\u5f81\u5bf9\u9f50\uff1b2. \u591a\u53c2\u8003\u8d28\u91cf\u611f\u77e5\u7b56\u7565\uff1a\u57fa\u4e8e\u53c2\u8003\u8d28\u91cf\u8c03\u6574\u5931\u771f\u6743\u91cd\uff0c\u5e94\u7528\u4e8e\u5206\u5c42\u8bad\u7ec3\u4ee5\u51cf\u5c11\u8bef\u5dee\u4f20\u64ad\uff1b3. \u65e0\u8bad\u7ec3\u6a21\u5757\uff1a\u6839\u636e\u8fd0\u52a8\u5e45\u5ea6\u548c\u5206\u8fa8\u7387\u4e0b\u91c7\u6837\u5e27\uff0c\u83b7\u5f97\u5e73\u6ed1\u8fd0\u52a8\u4f30\u8ba1\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff0cCAMA\u6846\u67b6\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578bDCVC-TCM\u5b9e\u73b024.95% BD-rate\uff08PSNR\uff09\u8282\u7701\uff0c\u540c\u65f6\u4f18\u4e8e\u590d\u73b0\u7684DCVC-DC\u548c\u4f20\u7edf\u7f16\u89e3\u7801\u5668HM-16.25\u3002", "conclusion": "\u63d0\u51fa\u7684\u5185\u5bb9\u81ea\u9002\u5e94\u8fd0\u52a8\u5bf9\u9f50\u6846\u67b6\u901a\u8fc7\u4e09\u65b9\u9762\u6280\u672f\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u89c6\u9891\u538b\u7f29\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5185\u5bb9\u81ea\u9002\u5e94\u7b56\u7565\u5728\u89c6\u9891\u538b\u7f29\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.12963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12963", "abs": "https://arxiv.org/abs/2512.12963", "authors": ["Luan Thanh Trinh", "Kenji Doi", "Atsuki Osanai"], "title": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer", "comment": "Accepted to WACV 2026", "summary": "Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.", "AI": {"tldr": "SCAdapter\u662f\u4e00\u79cd\u57fa\u4e8eCLIP\u56fe\u50cf\u7a7a\u95f4\u7684\u98ce\u683c\u8fc1\u79fb\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u6548\u5206\u79bb\u548c\u6574\u5408\u5185\u5bb9\u4e0e\u98ce\u683c\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u98ce\u683c\u8fc1\u79fb\u4e2d\u867d\u7136\u9886\u5148\uff0c\u4f46\u5728\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u8fc1\u79fb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e38\u5e38\u4ea7\u751f\u7c7b\u4f3c\u7ed8\u753b\u7684\u6548\u679c\u6216\u9057\u6f0f\u7ec6\u8282\u98ce\u683c\u5143\u7d20\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u539f\u59cb\u5185\u5bb9\u98ce\u683c\u548c\u98ce\u683c\u53c2\u8003\u5185\u5bb9\u7279\u5f81\u7684\u4e0d\u5fc5\u8981\u5f71\u54cd\u3002", "method": "SCAdapter\u5229\u7528CLIP\u56fe\u50cf\u7a7a\u95f4\u6709\u6548\u5206\u79bb\u548c\u6574\u5408\u5185\u5bb9\u4e0e\u98ce\u683c\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u53ef\u63a7\u98ce\u683c\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\uff08CSAdaIN\uff09\u7528\u4e8e\u7cbe\u786e\u7684\u591a\u98ce\u683c\u6df7\u5408\uff0cKVS\u6ce8\u5165\u7528\u4e8e\u76ee\u6807\u98ce\u683c\u6574\u5408\uff0c\u4ee5\u53ca\u98ce\u683c\u8fc1\u79fb\u4e00\u81f4\u6027\u76ee\u6807\u4fdd\u6301\u8fc7\u7a0b\u8fde\u8d2f\u6027\u3002", "result": "SCAdapter\u5728\u4f20\u7edf\u548c\u57fa\u4e8e\u6269\u6563\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u901a\u8fc7\u6d88\u9664DDIM\u53cd\u8f6c\u548c\u63a8\u7406\u9636\u6bb5\u4f18\u5316\uff0c\u8be5\u65b9\u6cd5\u63a8\u7406\u901f\u5ea6\u81f3\u5c11\u6bd4\u5176\u4ed6\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5feb2\u500d\u3002", "conclusion": "SCAdapter\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u63d0\u53d6\u7eaf\u5185\u5bb9\u548c\u98ce\u683c\u5143\u7d20\uff0c\u786e\u4fdd\u771f\u5b9e\u7684\u98ce\u683c\u8fc1\u79fb\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2512.12977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12977", "abs": "https://arxiv.org/abs/2512.12977", "authors": ["Shengling Qin", "Hao Yu", "Chenxin Wu", "Zheng Li", "Yizhong Cao", "Zhengyang Zhuge", "Yuxin Zhou", "Wentao Yao", "Yi Zhang", "Zhengheng Wang", "Shuai Bai", "Jianwei Zhang", "Junyang Lin"], "title": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference", "comment": null, "summary": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.", "AI": {"tldr": "VLCache\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7f13\u5b58\u91cd\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u7528KV\u7f13\u5b58\u548c\u7f16\u7801\u5668\u7f13\u5b58\u6765\u907f\u514d\u91cd\u590d\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5f53\u76f8\u540c\u7684\u591a\u6a21\u6001\u8f93\u5165\u91cd\u590d\u51fa\u73b0\u65f6\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u9700\u8981\u8fdb\u884c\u6602\u8d35\u7684\u91cd\u65b0\u8ba1\u7b97\u3002\u4e3a\u4e86\u6d88\u9664\u8fd9\u79cd\u91cd\u590d\u8ba1\u7b97\u7684\u5f00\u9500\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u590d\u7528\u5148\u524d\u8ba1\u7b97\u7ed3\u679c\u7684\u7f13\u5b58\u673a\u5236\u3002", "method": "1. \u5f62\u5f0f\u5316\u8bc6\u522b\u7d2f\u79ef\u91cd\u7528\u8bef\u5dee\u6548\u5e94\u5e76\u6700\u5c0f\u5316\u975e\u524d\u7f00\u7f13\u5b58\u91cd\u7528\u8bef\u5dee\uff1b2. \u5206\u6790\u6a21\u578b\u5404\u5c42\u7684\u91cd\u8981\u6027\u5dee\u5f02\uff0c\u63d0\u51fa\u52a8\u6001\u3001\u5c42\u611f\u77e5\u7684\u91cd\u65b0\u8ba1\u7b97\u7b56\u7565\u6765\u5e73\u8861\u7cbe\u5ea6\u548c\u6548\u7387\uff1b3. \u5c06VLCache\u7ba1\u9053\u96c6\u6210\u5230SGLang\u4e2d\u3002", "result": "VLCache\u5728\u4fdd\u6301\u4e0e\u5b8c\u5168\u91cd\u65b0\u8ba1\u7b97\u76f8\u5f53\u7684\u7cbe\u5ea6\u540c\u65f6\uff0c\u4ec5\u9700\u8ba1\u7b972-5%\u7684token\uff0c\u5b9e\u73b0\u4e861.2\u500d\u523016\u500d\u7684\u9996\u6b21token\u65f6\u95f4(TTFT)\u52a0\u901f\u3002\u8be5\u6846\u67b6\u5df2\u96c6\u6210\u5230SGLang\u4e2d\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u663e\u8457\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "VLCache\u901a\u8fc7\u6709\u6548\u590d\u7528\u591a\u6a21\u6001\u7f13\u5b58\uff0c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u5927\u5e45\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12982", "abs": "https://arxiv.org/abs/2512.12982", "authors": ["Ziheng Qin", "Yuheng Ji", "Renshuai Tao", "Yuxuan Tian", "Yuyang Liu", "Yipu Wang", "Xiaolong Zheng"], "title": "Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes", "comment": null, "summary": "The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0AIGI\u68c0\u6d4b\u5668\u5728\u6570\u636e\u6e90\u591a\u6837\u6027\u589e\u52a0\u65f6\u4f1a\u51fa\u73b0\"\u5148\u53d7\u76ca\u540e\u51b2\u7a81\"\u56f0\u5883\uff0c\u63d0\u51faGAPL\u6846\u67b6\u901a\u8fc7\u539f\u578b\u5b66\u4e60\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u548c\u6a21\u578b\u74f6\u9888\u95ee\u9898", "motivation": "\u73b0\u6709\u901a\u7528AIGI\u68c0\u6d4b\u5668\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u751f\u6210\u5668\u6570\u636e\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u672c\u6587\u53d1\u73b0\u968f\u7740\u6570\u636e\u6e90\u591a\u6837\u6027\u6269\u5927\uff0c\u68c0\u6d4b\u5668\u6027\u80fd\u4f1a\u51fa\u73b0\u505c\u6ede\u751a\u81f3\u4e0b\u964d\u7684\"\u5148\u53d7\u76ca\u540e\u51b2\u7a81\"\u56f0\u5883", "method": "\u63d0\u51faGenerator-Aware Prototype Learning (GAPL)\u6846\u67b6\uff1a1) \u5b66\u4e60\u4e00\u7ec4\u7d27\u51d1\u7684\u5178\u578b\u4f2a\u9020\u539f\u578b\uff0c\u521b\u5efa\u7edf\u4e00\u4f4e\u65b9\u5dee\u7279\u5f81\u7a7a\u95f4\u4ee5\u5e94\u5bf9\u6570\u636e\u5f02\u8d28\u6027\uff1b2) \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94\u6280\u672f\uff0c\u589e\u5f3a\u5224\u522b\u80fd\u529b\u540c\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6", "result": "GAPL\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u591a\u79cdGAN\u548c\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u5668\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u68c0\u6d4b\u51c6\u786e\u7387", "conclusion": "GAPL\u901a\u8fc7\u7ed3\u6784\u5316\u5b66\u4e60\u8303\u5f0f\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6709\u6548\u89e3\u51b3\u4e86AIGI\u68c0\u6d4b\u4e2d\u7684\"\u5148\u53d7\u76ca\u540e\u51b2\u7a81\"\u56f0\u5883\uff0c\u5efa\u7acb\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u51b3\u7b56\u8fb9\u754c"}}
{"id": "2512.12997", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12997", "abs": "https://arxiv.org/abs/2512.12997", "authors": ["Wenjing lu", "Zerui Tao", "Dongping Zhang", "Yuning Qiu", "Yang Yang", "Qibin Zhao"], "title": "Calibrating Uncertainty for Zero-Shot Adversarial CLIP", "comment": null, "summary": "CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9CLIP\u6a21\u578b\u7684\u65b0\u578b\u5bf9\u6297\u6027\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u72c4\u5229\u514b\u96f7\u5206\u5e03\u91cd\u65b0\u53c2\u6570\u5316\u8f93\u51fa\uff0c\u540c\u65f6\u4f18\u5316\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u89e3\u51b3\u5bf9\u6297\u653b\u51fb\u4e0b\u4e0d\u786e\u5b9a\u6027\u88ab\u6291\u5236\u7684\u95ee\u9898\u3002", "motivation": "CLIP\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u5bf9\u6297\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\u3002\u73b0\u6709\u5bf9\u6297\u5fae\u8c03\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e72\u51c0\u6837\u672c\u548c\u5bf9\u6297\u6837\u672c\u9884\u6d4blogits\u7684\u5339\u914d\uff0c\u5ffd\u7565\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u53ef\u80fd\u635f\u5bb3\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u5bf9\u6297\u6270\u52a8\u4e0d\u4ec5\u964d\u4f4e\u51c6\u786e\u7387\uff0c\u8fd8\u4f1a\u6291\u5236\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u6821\u51c6\u9519\u8bef\u548c\u4e0d\u53ef\u9760\u7684\u8fc7\u5ea6\u81ea\u4fe1\u3002", "method": "\u901a\u8fc7\u5c06CLIP\u8f93\u51fa\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u72c4\u5229\u514b\u96f7\u5206\u5e03\u7684\u6d53\u5ea6\u53c2\u6570\uff0c\u63d0\u51fa\u7edf\u4e00\u8868\u793a\u65b9\u6cd5\uff0c\u6355\u6349\u76f8\u5bf9\u8bed\u4e49\u7ed3\u6784\u548c\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u5927\u5c0f\u3002\u8bbe\u8ba1\u65b0\u7684\u5bf9\u6297\u5fae\u8c03\u76ee\u6807\uff0c\u5728\u6270\u52a8\u4e0b\u6574\u4f53\u5bf9\u9f50\u8fd9\u4e9b\u5206\u5e03\uff0c\u8d85\u8d8a\u5355\u4e00logits\u951a\u5b9a\uff0c\u6062\u590d\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u6062\u590d\u4e86\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5e72\u51c0\u6837\u672c\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86CLIP\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u95ee\u9898\uff0c\u586b\u8865\u4e86\u53ef\u9760\u6027\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u96f6\u6837\u672c\u5206\u7c7b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.13007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13007", "abs": "https://arxiv.org/abs/2512.13007", "authors": ["Nikolai Goncharov", "James L. Gray", "Donald G. Dansereau"], "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects", "comment": null, "summary": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5149\u573a\u56fe\u50cf\u7684\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5bf9\u590d\u6742\u89c6\u89c9\u884c\u4e3a\uff08\u5982\u53cd\u5c04\uff09\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f7f\u7528\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u59ff\u6001\u4f18\u5316\u5b9e\u73b06DoF\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u9ad8\u6027\u80fd\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9884\u6355\u83b7\u7684\u5bf9\u8c61\u89c6\u56fe\u6784\u5efa\u663e\u5f0f\u53c2\u8003\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u53ea\u80fd\u5904\u7406\u5df2\u77e5\u5bf9\u8c61\u96c6\u5408\uff0c\u4e14\u5bf9\u89c6\u89c9\u590d\u6742\u5916\u89c2\uff08\u5982\u53cd\u5c04\uff09\u5904\u7406\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u80fd\u5904\u7406\u590d\u6742\u89c6\u89c9\u884c\u4e3a\u7684\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u4ece\u5149\u573a\u8f93\u5165\u4e2d\u63d0\u53d6\u8bed\u4e49\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u4f7f\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5904\u7406\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u89c6\u56fe\u76f8\u5173\u7684\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff0c\u4f5c\u4e3a\u7edf\u4e00\u7684\u5bf9\u8c61\u8868\u793a\u652f\u6301\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u59ff\u6001\u4f18\u5316\u3002", "result": "\u5728\u5305\u542b\u6311\u6218\u6027\u53cd\u5c04\u5bf9\u8c61\u7684\u5149\u573a\u76ee\u6807\u8ddf\u8e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56f0\u96be\u60c5\u51b5\u4e0b\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u8ddf\u8e2a\u5668\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4e0d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5bf9\u590d\u6742\u89c6\u89c9\u884c\u4e3a\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.13008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13008", "abs": "https://arxiv.org/abs/2512.13008", "authors": ["Xi Luo", "Shixin Xu", "Ying Xie", "JianZhong Hu", "Yuwei He", "Yuhui Deng", "Huaxiong Huang"], "title": "TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading", "comment": null, "summary": "Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.", "AI": {"tldr": "TWLR\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u53ef\u89e3\u91ca\u6027\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6574\u5408\u773c\u79d1\u77e5\u8bc6\u8fdb\u884c\u5206\u7ea7\u548c\u75c5\u53d8\u5206\u7c7b\uff0c\u7136\u540e\u901a\u8fc7\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u8fed\u4ee3\u4e25\u91cd\u6027\u56de\u5f52\u6846\u67b6\u5b9e\u73b0\u75c5\u53d8\u5b9a\u4f4d\u548c\u75be\u75c5\u5230\u5065\u5eb7\u8f6c\u6362\u7684\u53ef\u89c6\u5316\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9700\u8981\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6807\u6ce8\uff0c\u4f46\u83b7\u53d6\u50cf\u7d20\u7ea7\u6807\u7b7e\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u540c\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u91c7\u7528\u3002\u9700\u8981\u89e3\u51b3\u6807\u6ce8\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u53cc\u91cd\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6TWLR\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6574\u5408\u773c\u79d1\u9886\u57df\u77e5\u8bc6\uff0c\u8054\u5408\u6267\u884cDR\u5206\u7ea7\u548c\u75c5\u53d8\u5206\u7c7b\uff0c\u5c06\u8bed\u4e49\u533b\u5b66\u6982\u5ff5\u4e0e\u89c6\u89c9\u7279\u5f81\u5173\u8054\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u8fed\u4ee3\u4e25\u91cd\u6027\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7cbe\u70bc\u751f\u6210\u75c5\u53d8\u663e\u8457\u56fe\uff0c\u6307\u5bfc\u6e10\u8fdb\u4fee\u590d\u673a\u5236\u7cfb\u7edf\u6d88\u9664\u75c5\u7406\u7279\u5f81\uff0c\u6709\u6548\u964d\u4f4e\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u3002", "result": "\u5728FGADR\u3001DDR\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTWLR\u5728DR\u5206\u7c7b\u548c\u75c5\u53d8\u5206\u5272\u65b9\u9762\u5747\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u89c6\u7f51\u819c\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u4e14\u6807\u6ce8\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "TWLR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8fed\u4ee3\u4e25\u91cd\u6027\u56de\u5f52\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u50cf\u7d20\u7ea7\u76d1\u7763\u7684\u51c6\u786e\u75c5\u53d8\u5b9a\u4f4d\uff0c\u5e76\u63d0\u4f9b\u4e86\u75be\u75c5\u5230\u5065\u5eb7\u8f6c\u6362\u7684\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u4e3a\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u6807\u6ce8\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13015", "abs": "https://arxiv.org/abs/2512.13015", "authors": ["Xinjie Li", "Zhimin Chen", "Rui Zhao", "Florian Schiffers", "Zhenyu Liao", "Vimal Bhat"], "title": "What Happens Next? Next Scene Prediction with a Unified Video Model", "comment": null, "summary": "Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.", "AI": {"tldr": "\u63d0\u51faNext Scene Prediction\uff08NSP\uff09\u65b0\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u57fa\u4e8e\u524d\u5e8f\u4e0a\u4e0b\u6587\u9884\u6d4b\u5408\u7406\u672a\u6765\u573a\u666f\uff0c\u63a8\u52a8\u7edf\u4e00\u89c6\u9891\u6a21\u578b\u5411\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u53d1\u5c55\u3002", "motivation": "\u5f53\u524d\u7edf\u4e00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7b49\u4f20\u7edf\u4efb\u52a1\uff0c\u5bf9\u65f6\u5e8f\u63a8\u7406\u6f5c\u529b\u63a2\u7d22\u4e0d\u8db3\u3002\u9700\u8981\u65b0\u4efb\u52a1\u6765\u63a8\u52a8\u6a21\u578b\u8fdb\u884c\u66f4\u6df1\u5c42\u6b21\u7684\u7406\u89e3\u548c\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff1a\u7ed3\u5408Qwen-VL\u8fdb\u884c\u7406\u89e3\uff0cLTX\u8fdb\u884c\u5408\u6210\uff0c\u901a\u8fc7\u6f5c\u5728\u67e5\u8be2\u5d4c\u5165\u548c\u8fde\u63a5\u6a21\u5757\u6865\u63a5\u3002\u4f7f\u7528\u65b0\u6784\u5efa\u7684\u5927\u89c4\u6a21NSP\u6570\u636e\u96c6\u8fdb\u884c\u4e09\u9636\u6bb5\u8bad\u7ec3\uff1a\u6587\u672c\u5230\u89c6\u9891\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u3001\u4ee5\u53ca\u91c7\u7528\u56e0\u679c\u4e00\u81f4\u6027\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u901a\u7528\u591a\u6a21\u6001\u7cfb\u7edf\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u7684\u80fd\u529b\u3002", "conclusion": "NSP\u4efb\u52a1\u6210\u529f\u63a8\u52a8\u4e86\u7edf\u4e00\u89c6\u9891\u6a21\u578b\u5411\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u65b9\u5411\u53d1\u5c55\uff0c\u4e3a\u901a\u7528\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9884\u6d4b\u672a\u6765\u573a\u666f\u7684\u65b0\u80fd\u529b\u3002"}}
{"id": "2512.13018", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13018", "abs": "https://arxiv.org/abs/2512.13018", "authors": ["Tomoya Tanaka", "Tomonori Ikeda", "Ryo Yonemoto"], "title": "Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing", "comment": "8 pages, 6 figures. Comprehensive evaluation of preprocessing, data augmentation, and transfer learning for cross-environment generalization in deep learning-based mmWave radar sensing", "summary": "This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u7a7a\u95f4\u6cdb\u5316\u6280\u672f\uff0c\u9488\u5bf9\u5ba4\u5185\u4eba\u5458\u8ba1\u6570\u5e94\u7528\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u5e45\u5ea6\u7684\u9884\u5904\u7406\u548c\u8fc1\u79fb\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347\u96f7\u8fbe\u4f20\u611f\u7cfb\u7edf\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5c04\u9891\u4f20\u611f\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u7a7a\u95f4\u6cdb\u5316\u6311\u6218\uff0c\u5f53\u90e8\u7f72\u73af\u5883\u53d8\u5316\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u5ba4\u5185\u4eba\u5458\u8ba1\u6570\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u6709\u6548\u7684\u7a7a\u95f4\u6cdb\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528FMCW MIMO\u96f7\u8fbe\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u7a7a\u95f4\u6cdb\u5316\u6280\u672f\uff1a\u57fa\u4e8e\u5e45\u5ea6\u7684\u7edf\u8ba1\u9884\u5904\u7406\uff08sigmoid\u52a0\u6743\u548c\u9608\u503c\u5f52\u96f6\uff09\u3001\u9891\u57df\u6ee4\u6ce2\u3001\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u80cc\u666f\u6291\u5236\u3001\u6570\u636e\u589e\u5f3a\u7b56\u7565\u548c\u8fc1\u79fb\u5b66\u4e60\u3002\u5728\u4e24\u4e2a\u4e0d\u540c\u5e03\u5c40\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "sigmoid\u5e45\u5ea6\u52a0\u6743\u5728\u8de8\u73af\u5883\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5206\u522b\u964d\u4f4eRMSE 50.1%\u548cMAE 55.2%\u3002\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u989d\u5916\u4f46\u6709\u9650\u7684\u6539\u8fdb\uff08MAE\u63d0\u5347\u8fbe8.8%\uff09\u3002\u8fc1\u79fb\u5b66\u4e60\u5bf9\u4e8e\u5927\u7a7a\u95f4\u53d8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f7f\u7528540\u4e2a\u76ee\u6807\u57df\u6837\u672c\u65f6\u5206\u522b\u964d\u4f4eRMSE 82.1%\u548cMAE 91.3%\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3001\u57fa\u4e8e\u5e45\u5ea6\u7684\u9884\u5904\u7406\u548c\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u5728\u7a7a\u95f4\u53d8\u5316\u4e0b\u4fdd\u6301\u9c81\u68d2\u7cbe\u5ea6\u7684\u96f7\u8fbe\u4f20\u611f\u7cfb\u7edf\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u5411\u3002"}}
{"id": "2512.13019", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13019", "abs": "https://arxiv.org/abs/2512.13019", "authors": ["Cheeun Hong", "German Barquero", "Fadime Sener", "Markos Georgopoulos", "Edgar Sch\u00f6nfeld", "Stefan Popov", "Yuming Du", "Oscar Ma\u00f1as", "Albert Pumarola"], "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation", "comment": null, "summary": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.", "AI": {"tldr": "SneakPeek\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7cbe\u786e\u7684\u5206\u6b65\u6559\u5b66\u89c6\u9891\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u56e0\u679c\u9002\u5e94\u3001\u672a\u6765\u5f15\u5bfc\u81ea\u5f3a\u5236\u548c\u591a\u63d0\u793a\u6761\u4ef6\u63a7\u5236\u6765\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u95ee\u9898\u3002", "motivation": "\u6559\u5b66\u89c6\u9891\u751f\u6210\u5728\u5185\u5bb9\u521b\u4f5c\u3001\u6559\u80b2\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u5177\u6709\u5e7f\u6cdb\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u957f\u5e8f\u5217\u591a\u52a8\u4f5c\u6b65\u9aa4\u4e2d\u96be\u4ee5\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\uff0c\u5b58\u5728\u65f6\u95f4\u6f02\u79fb\u548c\u8fd0\u52a8\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSneakPeek\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u9884\u6d4b\u6027\u56e0\u679c\u9002\u5e94\uff0c\u901a\u8fc7\u56e0\u679c\u6a21\u578b\u5b66\u4e60\u4e0b\u4e00\u5e27\u9884\u6d4b\u548c\u672a\u6765\u5173\u952e\u5e27\u9884\u6d4b\uff1b2) \u672a\u6765\u5f15\u5bfc\u81ea\u5f3a\u5236\uff0c\u91c7\u7528\u53cc\u533a\u57dfKV\u7f13\u5b58\u65b9\u6848\u89e3\u51b3\u63a8\u7406\u65f6\u7684\u66b4\u9732\u504f\u5dee\u95ee\u9898\uff1b3) \u591a\u63d0\u793a\u6761\u4ef6\u63a7\u5236\uff0c\u63d0\u4f9b\u5bf9\u591a\u6b65\u9aa4\u6307\u4ee4\u7684\u7ec6\u7c92\u5ea6\u7a0b\u5e8f\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u3001\u8bed\u4e49\u5fe0\u5b9e\u4e14\u51c6\u786e\u9075\u5faa\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u63cf\u8ff0\u7684\u6559\u5b66\u89c6\u9891\uff0c\u6709\u6548\u7f13\u89e3\u65f6\u95f4\u6f02\u79fb\uff0c\u4fdd\u6301\u8fd0\u52a8\u4e00\u81f4\u6027\uff0c\u5e76\u652f\u6301\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u3002", "conclusion": "SneakPeek\u901a\u8fc7\u521b\u65b0\u7684\u9884\u6d4b\u6027\u56e0\u679c\u9002\u5e94\u3001\u672a\u6765\u5f15\u5bfc\u81ea\u5f3a\u5236\u548c\u591a\u63d0\u793a\u6761\u4ef6\u63a7\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6559\u5b66\u89c6\u9891\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u95ee\u9898\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u6d41\u5a92\u4f53\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13039", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.13039", "abs": "https://arxiv.org/abs/2512.13039", "authors": ["Hao Chen", "Yiwei Wang", "Songze Li"], "title": "Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models", "comment": "Under Review", "summary": "Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u56fe\u50cf\u5f15\u5bfc\u7684\u6982\u5ff5\u64e6\u9664\u6846\u67b6\uff08Bi-Erasing\uff09\uff0c\u901a\u8fc7\u540c\u65f6\u6291\u5236\u6709\u5bb3\u6982\u5ff5\u548c\u589e\u5f3a\u5b89\u5168\u66ff\u4ee3\u6982\u5ff5\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u5b9e\u73b0\u6982\u5ff5\u79fb\u9664\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5355\u5411\u7b56\u7565\uff08\u8981\u4e48\u6291\u5236\u76ee\u6807\u6982\u5ff5\uff0c\u8981\u4e48\u5f3a\u5316\u5b89\u5168\u66ff\u4ee3\uff09\uff0c\u96be\u4ee5\u5728\u6982\u5ff5\u79fb\u9664\u6548\u679c\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5e73\u8861\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBi-Erasing\u6846\u67b6\uff0c\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u548c\u5bf9\u5e94\u56fe\u50cf\u7684\u8054\u5408\u8868\u793a\uff0c\u5f15\u5165\u4e24\u4e2a\u89e3\u8026\u7684\u56fe\u50cf\u5206\u652f\uff1a\u8d1f\u5206\u652f\u8d1f\u8d23\u6291\u5236\u6709\u5bb3\u8bed\u4e49\uff0c\u6b63\u5206\u652f\u4e3a\u5b89\u5168\u66ff\u4ee3\u63d0\u4f9b\u89c6\u89c9\u6307\u5bfc\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8fd9\u4e24\u4e2a\u4e92\u8865\u65b9\u5411\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u63a9\u7801\u7684\u8fc7\u6ee4\u6765\u9632\u6b62\u65e0\u5173\u5185\u5bb9\u5e72\u6270\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\uff0cBi-Erasing\u5728\u5e73\u8861\u6982\u5ff5\u79fb\u9664\u6548\u679c\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Bi-Erasing\u6846\u67b6\u901a\u8fc7\u53cc\u5411\u4f18\u5316\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6982\u5ff5\u64e6\u9664\u4e2d\u79fb\u9664\u6548\u679c\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13043", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13043", "abs": "https://arxiv.org/abs/2512.13043", "authors": ["Tong Wei", "Yijun Yang", "Changhao Zhang", "Junliang Xing", "Yuanchun Shi", "Zongqing Lu", "Deheng Ye"], "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "comment": null, "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "AI": {"tldr": "GTR-Turbo\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u6602\u8d35\u6559\u5e08\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u5e76\u8bad\u7ec3\u4e2d\u7684\u68c0\u67e5\u70b9\u6743\u91cd\u4f5c\u4e3a\"\u514d\u8d39\"\u6559\u5e08\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7a00\u758f\u5956\u52b1\u548c\u957f\u89c6\u91ce\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u901a\u5e38\u5177\u6709\u7279\u6743\u7684\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u6b65\u9aa4\u7ea7\u53cd\u9988\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "method": "GTR-Turbo\u5728\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5408\u5e76\u68c0\u67e5\u70b9\u6743\u91cd\uff0c\u4f7f\u7528\u8fd9\u4e2a\u5408\u5e76\u6a21\u578b\u4f5c\u4e3a\"\u514d\u8d39\"\u6559\u5e08\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6216\u8f6f\u5bf9\u6570\u84b8\u998f\u6307\u5bfc\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\uff0c\u6d88\u9664\u4e86\u5bf9\u7279\u6743VLM\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u5404\u79cd\u89c6\u89c9\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\uff0cGTR-Turbo\u5c06\u57fa\u7ebf\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u534710-30%\uff0c\u540c\u65f6\u76f8\u6bd4GTR\u51cf\u5c1150%\u7684\u5899\u4e0a\u8bad\u7ec3\u65f6\u95f4\u548c60%\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "GTR-Turbo\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u6559\u5e08\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\"\u71b5\u5d29\u6e83\"\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.13072", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13072", "abs": "https://arxiv.org/abs/2512.13072", "authors": ["Zizhi Chen", "Yizhen Gao", "Minghao Han", "Yizhou Liu", "Zhaoyu Chen", "Dingkang Yang", "Lihua Zhang"], "title": "Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models", "comment": null, "summary": "Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \\textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u7684\u7efc\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u52a8\u6001\u77e5\u8bc6\u84b8\u998f\u6765\u89e3\u51b3\u6a21\u6001\u95f4\u9886\u57df\u5dee\u8ddd\u4e0e\u7ec6\u7c92\u5ea6\u7279\u5f81\u4fdd\u7559\u7684\u6838\u5fc3\u77db\u76fe\u3002", "motivation": "\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9762\u4e34\u6838\u5fc3\u56f0\u5883\uff1a\u5982\u4f55\u5728\u5f25\u5408\u4e0d\u540c\u6a21\u6001\u95f4\u663e\u8457\u9886\u57df\u5dee\u8ddd\u7684\u540c\u65f6\uff0c\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7684\u6a21\u6001\u5185\u7279\u5f81\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8fd9\u4e24\u4e2a\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\u3002", "method": "1. \u57fa\u4e8e1800\u4e07PubMed\u79d1\u5b66\u8bba\u6587\u6784\u5efa\u591a\u6a21\u6001\u533b\u5b66\u68c0\u7d22\u6570\u636e\u5e93\uff1b2. \u9996\u6b21\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u5f15\u5165\u6301\u7eed\u5b66\u4e60\uff0c\u91c7\u7528\u591a\u6a21\u6001\u591a\u5c42RAG\u7cfb\u7edf\u4e3a\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u5b9e\u65f6\u6307\u5bfc\uff1b3. \u63d0\u51fa\u52a8\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u6839\u636e\u6240\u9700\u7ec6\u8282\u6c34\u5e73\u52a8\u6001\u8c03\u8282\u53c2\u6570\u7a7a\u95f4\u91cd\u8981\u6027\u3001\u84b8\u998f\u77e5\u8bc6\u7c92\u5ea6\u4ee5\u53ca\u53c2\u8003\u6570\u636e\u96c6\u5206\u5e03\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8bbe\u8ba1\u7684\u533b\u5b66\u901a\u7528\u4efb\u52a1\u589e\u91cf\u5b66\u4e60\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5168\u9762\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u9002\u5e94\u663e\u8457\u9886\u57df\u8f6c\u79fb\u3001\u4fdd\u7559\u7ec6\u5fae\u9886\u57df\u5185\u7279\u5f81\u4ee5\u53ca\u5b9e\u65f6\u5b66\u4e60\u65b0\u9896\u590d\u6742\u533b\u5b66\u4efb\u52a1\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7efc\u5408\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u52a8\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5728\u4fdd\u6301\u7ec6\u7c92\u5ea6\u7279\u5f81\u7684\u540c\u65f6\u6210\u529f\u5f25\u5408\u4e86\u6a21\u6001\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002"}}
{"id": "2512.13083", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13083", "abs": "https://arxiv.org/abs/2512.13083", "authors": ["Saumyaranjan Mohanty", "Aravind Reddy", "Konda Reddy Mopuri"], "title": "DiRe: Diversity-promoting Regularization for Dataset Condensation", "comment": "Accepted to WACV 2026", "summary": "In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6837\u6027\u6b63\u5219\u5316\u5668DiRe\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6b27\u6c0f\u8ddd\u79bb\u51cf\u5c11\u6570\u636e\u96c6\u538b\u7f29\u4e2d\u7684\u5197\u4f59\uff0c\u63d0\u9ad8\u5408\u6210\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\uff0c\u63d0\u5347\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u5408\u6210\u7684\u6570\u636e\u96c6\u5b58\u5728\u663e\u8457\u5197\u4f59\uff0c\u9700\u8981\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u5408\u6210\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u76f4\u89c2\u7684\u591a\u6837\u6027\u6b63\u5219\u5316\u5668DiRe\uff0c\u7ed3\u5408\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6b27\u6c0f\u8ddd\u79bb\uff0c\u53ef\u4ee5\u5373\u63d2\u5373\u7528\u5730\u5e94\u7528\u4e8e\u5404\u79cd\u6700\u5148\u8fdb\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6dfb\u52a0\u8be5\u6b63\u5219\u5316\u5668\u80fd\u591f\u6539\u8fdb\u6700\u5148\u8fdb\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u5728\u4eceCIFAR-10\u5230ImageNet-1K\u7684\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u6cdb\u5316\u6027\u548c\u591a\u6837\u6027\u6307\u6807\u65b9\u9762\u90fd\u6709\u63d0\u5347\u3002", "conclusion": "DiRe\u6b63\u5219\u5316\u5668\u80fd\u6709\u6548\u51cf\u5c11\u6570\u636e\u96c6\u538b\u7f29\u4e2d\u7684\u5197\u4f59\uff0c\u63d0\u9ad8\u5408\u6210\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\uff0c\u63d0\u5347\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2512.13089", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13089", "abs": "https://arxiv.org/abs/2512.13089", "authors": ["Ziqiang Zhu", "Bowei Yang"], "title": "UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era", "comment": "10 pages, 6 figures", "summary": "Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.", "AI": {"tldr": "\u63d0\u51faUniVCD\u65b9\u6cd5\uff0c\u57fa\u4e8e\u51bb\u7ed3\u7684SAM2\u548cCLIP\u6a21\u578b\u5b9e\u73b0\u65e0\u76d1\u7763\u3001\u5f00\u653e\u8bcd\u6c47\u7684\u53d8\u5316\u68c0\u6d4b\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u6216\u914d\u5bf9\u56fe\u50cf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u548c\u591a\u6a21\u6001\u878d\u5408\u5b9e\u73b0\u9ad8\u6027\u80fd\u53d8\u5316\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u6027\u80fd\u53d7\u6570\u636e\u96c6\u9650\u5236\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u901a\u5e38\u53ea\u5173\u6ce8\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u5316\u573a\u666f\u3002\u968f\u7740\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982SAM2\u548cCLIP\uff09\u7684\u53d1\u5c55\uff0c\u4e3a\u653e\u677e\u8fd9\u4e9b\u9650\u5236\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u57fa\u4e8e\u51bb\u7ed3\u7684SAM2\u548cCLIP\u6a21\u578b\u6784\u5efa\u65e0\u76d1\u7763\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\u3002\u5f15\u5165\u8f7b\u91cf\u7ea7\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff0c\u5c06SAM2\u7684\u7a7a\u95f4\u7ec6\u8282\u8868\u5f81\u4e0eCLIP\u7684\u8bed\u4e49\u5148\u9a8c\u8fdb\u884c\u6865\u63a5\uff0c\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u3001\u8bed\u4e49\u611f\u77e5\u7684\u53d8\u5316\u4f30\u8ba1\u3002\u91c7\u7528\u7b80\u5316\u7684\u540e\u5904\u7406\u6d41\u7a0b\u6291\u5236\u566a\u58f0\u548c\u4f2a\u53d8\u5316\uff0c\u63d0\u5347\u8fb9\u754c\u6e05\u6670\u5bf9\u8c61\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u7684BCD\uff08\u4e8c\u503c\u53d8\u5316\u68c0\u6d4b\uff09\u548cSCD\uff08\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniVCD\u5c55\u73b0\u51fa\u7a33\u5b9a\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5728F1\u548cIoU\u7b49\u5173\u952e\u6307\u6807\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u51bb\u7ed3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u65e0\u76d1\u7763\u53d8\u5316\u68c0\u6d4b\u662f\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u7684\u5b9e\u7528\u6709\u6548\u8303\u5f0f\uff0c\u4e3a\u573a\u666f\u53d8\u5316\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13095", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13095", "abs": "https://arxiv.org/abs/2512.13095", "authors": ["Feng Zhang", "Zezhong Tan", "Xinhong Ma", "Ziqiang Dong", "Xi Leng", "Jianfei Zhao", "Xin Sun", "Yang Yang"], "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning", "comment": null, "summary": "To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.", "AI": {"tldr": "ADHint\u662f\u4e00\u79cd\u7ed3\u5408SFT\u548cRL\u4f18\u52bf\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u6837\u672c\u96be\u5ea6\u6765\u8c03\u5ea6\u63d0\u793a\u6bd4\u4f8b\u548c\u4f30\u8ba1\u76f8\u5bf9\u4f18\u52bf\uff0c\u5b9e\u73b0\u63a2\u7d22\u4e0e\u6a21\u4eff\u7684\u66f4\u597d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684RL\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u96be\u5ea6\u56e0\u7d20\uff0c\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u8fc7\u5ea6\u6a21\u4eff\u79bb\u7b56\u7565\u63d0\u793a\uff0c\u9700\u8981\u66f4\u597d\u7684\u63a2\u7d22\u4e0e\u6a21\u4eff\u5e73\u8861\u673a\u5236\u3002", "method": "1. \u57fa\u4e8e\u6837\u672c\u96be\u5ea6\u5148\u9a8c\u7684\u81ea\u9002\u5e94\u63d0\u793a\uff1a\u8bc4\u4f30\u6837\u672c\u96be\u5ea6\u5e76\u8c03\u5ea6\u9002\u5f53\u63d0\u793a\u6bd4\u4f8b\uff1b2. \u4e00\u81f4\u6027\u68af\u5ea6\u8c03\u5236\u548c\u9009\u62e9\u6027\u63a9\u7801\uff1a\u8c03\u5236\u63d0\u793a\u5185\u7684\u4ee4\u724c\u7ea7\u68af\u5ea6\uff1b3. \u57fa\u4e8e\u63a8\u51fa\u96be\u5ea6\u540e\u9a8c\u7684\u4f18\u52bf\u4f30\u8ba1\uff1a\u5229\u7528\u6709/\u65e0\u63d0\u793a\u63a8\u51fa\u7684\u76f8\u5bf9\u96be\u5ea6\u4f30\u8ba1\u4f18\u52bf\u3002", "result": "\u5728\u591a\u79cd\u6a21\u6001\u3001\u6a21\u578b\u89c4\u6a21\u548c\u9886\u57df\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cADHint\u5728\u63a8\u7406\u80fd\u529b\u548c\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728pass@1\u548cavg@8\u6307\u6807\u4e0a\u6301\u7eed\u9886\u5148\u3002", "conclusion": "ADHint\u901a\u8fc7\u5c06\u96be\u5ea6\u4f5c\u4e3a\u5173\u952e\u56e0\u7d20\u7eb3\u5165\u63d0\u793a\u6bd4\u4f8b\u8c03\u5ea6\u548c\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u63a2\u7d22\u4e0e\u6a21\u4eff\u7684\u66f4\u597d\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.13101", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13101", "abs": "https://arxiv.org/abs/2512.13101", "authors": ["Wenjing Lu", "Yi Hong", "Yang Yang"], "title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation", "comment": "This work has been submitted to the IEEE TMI for possible publication", "summary": "Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.", "AI": {"tldr": "UnCoL\u662f\u4e00\u4e2a\u53cc\u6559\u5e08\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u534f\u540c\u5b66\u4e60\uff0c\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u5e73\u8861\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4efb\u52a1\u7279\u5b9a\u4e13\u4e1a\u5316\uff0c\u5b9e\u73b0\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u6709\u9650\u6807\u6ce8\u6216\u7f55\u89c1\u75c5\u7406\u53d8\u5f02\u4e0b\uff0c\u7531\u4e8e\u901a\u7528\u5148\u9a8c\u4e0e\u4efb\u52a1\u7279\u5b9a\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u4e13\u4e1a\u4e34\u5e8a\u4efb\u52a1\u3002", "method": "\u63d0\u51faUnCoL\u53cc\u6559\u5e08\u6846\u67b6\uff1a\u4ece\u51bb\u7ed3\u7684\u57fa\u7840\u6a21\u578b\u4e2d\u84b8\u998f\u89c6\u89c9\u548c\u8bed\u4e49\u8868\u793a\u4ee5\u4f20\u9012\u901a\u7528\u77e5\u8bc6\uff0c\u540c\u65f6\u7ef4\u62a4\u9010\u6b65\u9002\u5e94\u7684\u6559\u5e08\u6a21\u578b\u6355\u83b7\u7ec6\u7c92\u5ea6\u548c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u3002\u901a\u8fc7\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u81ea\u9002\u5e94\u8c03\u8282\u4f2a\u6807\u7b7e\u5b66\u4e60\uff0c\u6291\u5236\u4e0d\u53ef\u9760\u76d1\u7763\u5e76\u7a33\u5b9a\u6a21\u7cca\u533a\u57df\u7684\u5b66\u4e60\u3002", "result": "\u5728\u591a\u6837\u5316\u76842D\u548c3D\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUnCoL\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u548c\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\u3002\u6a21\u578b\u5728\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5168\u76d1\u7763\u7684\u6027\u80fd\u3002", "conclusion": "UnCoL\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u534f\u540c\u5b66\u4e60\uff0c\u6709\u6548\u5e73\u8861\u4e86\u6cdb\u5316\u4e0e\u4e13\u4e1a\u5316\uff0c\u4e3a\u6709\u9650\u6807\u6ce8\u4e0b\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13104", "abs": "https://arxiv.org/abs/2512.13104", "authors": ["Yan Zhang", "Baoxin Li", "Han Sun", "Yuhang Gao", "Mingtai Zhang", "Pei Wang"], "title": "FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection", "comment": null, "summary": "Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.", "AI": {"tldr": "\u63d0\u51faFID-Net\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u65e0\u4eba\u673a\u53ef\u89c1\u5149\u56fe\u50cf\u68c0\u6d4b\u68ee\u6797\u75c5\u866b\u5bb3\u6811\u6728\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u7a7a\u95f4\u6307\u6807\u8fdb\u884c\u866b\u5bb3\u5206\u6790\uff0c\u5728\u65b0\u7586\u5929\u5c71\u5730\u533a32\u4e2a\u6797\u533a\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e3b\u6d41YOLO\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u68ee\u6797\u75c5\u866b\u5bb3\u76d1\u6d4b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u76d1\u6d4b\u6280\u672f\u6765\u51c6\u786e\u8bc6\u522b\u611f\u67d3\u6811\u6728\u5e76\u5206\u6790\u866b\u5bb3\u6a21\u5f0f\uff0c\u4ee5\u652f\u6301\u751f\u6001\u7cfb\u7edf\u4fdd\u62a4\u3002", "method": "\u57fa\u4e8eYOLOv8n\u6784\u5efaFID-Net\u6a21\u578b\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u63d0\u53d6\u75c5\u5bb3\u654f\u611f\u7279\u5f81\uff0c\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\u5bf9\u9f50\u878d\u5408RGB\u548cFEM\u589e\u5f3a\u7279\u5f81\uff0c\u4ee5\u53ca\u9ad8\u6548\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u5224\u522b\u4fe1\u606f\u3002\u4ece\u68c0\u6d4b\u7ed3\u679c\u6784\u5efa\u866b\u5bb3\u5206\u6790\u6846\u67b6\uff0c\u5305\u62ec\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5b9a\u4f4d\u611f\u67d3\u70ed\u70b9\u3001\u90bb\u57df\u8bc4\u4f30\u5065\u5eb7\u6811\u611f\u67d3\u98ce\u9669\u3001DBSCAN\u805a\u7c7b\u8bc6\u522b\u9ad8\u5bc6\u5ea6\u5065\u5eb7\u96c6\u7fa4\u4f5c\u4e3a\u4f18\u5148\u4fdd\u62a4\u533a\u57df\u3002", "result": "\u5728\u65b0\u7586\u5929\u5c71\u5730\u533a32\u4e2a\u6797\u533a\u7684\u65e0\u4eba\u673a\u56fe\u50cf\u5b9e\u9a8c\u4e2d\uff0cFID-Net\u8fbe\u523086.10%\u7cbe\u786e\u7387\u300175.44%\u53ec\u56de\u7387\u300182.29% mAP@0.5\u548c64.30% mAP@0.5:0.95\uff0c\u4f18\u4e8e\u4e3b\u6d41YOLO\u6a21\u578b\u3002\u5206\u6790\u786e\u8ba4\u611f\u67d3\u6811\u6728\u5448\u73b0\u660e\u663e\u805a\u7c7b\u6a21\u5f0f\u3002", "conclusion": "FID-Net\u80fd\u591f\u51c6\u786e\u533a\u5206\u6811\u6728\u5065\u5eb7\u72b6\u51b5\uff0c\u7ed3\u5408\u7a7a\u95f4\u6307\u6807\u4e3a\u667a\u80fd\u75c5\u866b\u5bb3\u76d1\u6d4b\u3001\u65e9\u671f\u9884\u8b66\u548c\u7cbe\u51c6\u7ba1\u7406\u63d0\u4f9b\u53ef\u9760\u6570\u636e\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u9488\u5bf9\u6027\u68ee\u6797\u4fdd\u62a4\u3002"}}
{"id": "2512.13107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13107", "abs": "https://arxiv.org/abs/2512.13107", "authors": ["Zhijian He", "Feifei Liu", "Yuwei Li", "Zhanpeng Liu", "Jintao Cheng", "Xieyuanli Chen", "Xiaoyu Tang"], "title": "Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather", "comment": null, "summary": "Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.", "AI": {"tldr": "DiffFusion\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u548c\u70b9\u4e91\u4fee\u590d\u4ee5\u53ca\u53cc\u5411\u81ea\u9002\u5e94\u878d\u5408\uff0c\u63d0\u5347\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u6548\u679c\u53d7\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5929\u6c14\u5bfc\u81f4\u7684\u56fe\u50cf\u5931\u771f\u548c\u4e0d\u540c\u6570\u636e\u6a21\u6001\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51faDiffFusion\u6846\u67b6\uff1a1\uff09Diffusion-IR\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u4fee\u590d\u5929\u6c14\u9000\u5316\u7684\u56fe\u50cf\uff1b2\uff09Point Cloud Restoration (PCR)\uff1a\u5229\u7528\u56fe\u50cf\u76ee\u6807\u7ebf\u7d22\u8865\u507f\u53d7\u635f\u7684LiDAR\u6570\u636e\uff1b3\uff09BAFAM\uff1a\u53cc\u5411\u81ea\u9002\u5e94\u878d\u5408\u548c\u5bf9\u9f50\u6a21\u5757\uff0c\u5b9e\u73b0\u52a8\u6001\u591a\u6a21\u6001\u878d\u5408\u548cBEV\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cDiffFusion\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u5e72\u51c0\u6570\u636e\u6027\u80fd\u3002\u5728DENSE\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u7ed3\u679c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiffFusion\u901a\u8fc7\u6269\u6563\u6a21\u578b\u9a71\u52a8\u7684\u4fee\u590d\u548c\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.13157", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13157", "abs": "https://arxiv.org/abs/2512.13157", "authors": ["Peter Kocsis", "Lukas H\u00f6llein", "Matthias Nie\u00dfner"], "title": "Intrinsic Image Fusion for Multi-View 3D Material Reconstruction", "comment": "Project page: https://peter-kocsis.github.io/IntrinsicImageFusion/ Video: https://www.youtube.com/watch?v=-Vs3tR1Xl7k", "summary": "We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.", "AI": {"tldr": "\u63d0\u51faIntrinsic Image Fusion\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u9ad8\u8d28\u91cf\u7269\u7406\u6750\u8d28\uff0c\u7ed3\u5408\u5355\u89c6\u89d2\u5148\u9a8c\u548c\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u9c81\u68d2\u4f18\u5316\u6846\u67b6\u6574\u5408\u4e0d\u4e00\u81f4\u7684\u9884\u6d4b\uff0c\u6700\u7ec8\u901a\u8fc7\u9006\u8def\u5f84\u8ffd\u8e2a\u4f18\u5316\u4f4e\u7ef4\u53c2\u6570\u3002", "motivation": "\u6750\u8d28\u91cd\u5efa\u662f\u4e00\u4e2a\u9ad8\u5ea6\u6b20\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u901a\u5e38\u4f9d\u8d56\u4e8e\u5206\u6790\u5408\u6210\u65b9\u6cd5\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u4e14\u566a\u58f0\u5927\u7684\u8def\u5f84\u8ffd\u8e2a\u3002\u4e3a\u4e86\u66f4\u597d\u7ea6\u675f\u4f18\u5316\u8fc7\u7a0b\uff0c\u9700\u8981\u5c06\u5355\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u878d\u5165\u91cd\u5efa\u8fc7\u7a0b\u3002", "method": "1. \u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6750\u8d28\u4f30\u8ba1\u5668\u4e3a\u6bcf\u4e2a\u89c6\u89d2\u751f\u6210\u591a\u4e2a\u5019\u9009\u5206\u89e3\uff1b2. \u62df\u5408\u663e\u5f0f\u4f4e\u7ef4\u53c2\u6570\u51fd\u6570\u4ee5\u51cf\u5c11\u4e0d\u4e00\u81f4\u6027\uff1b3. \u63d0\u51fa\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u8f6f\u6bcf\u89c6\u89d2\u9884\u6d4b\u9009\u62e9\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u8f6f\u591a\u89c6\u89d2\u5185\u70b9\u96c6\uff0c\u5c06\u6700\u4e00\u81f4\u9884\u6d4b\u878d\u5408\u5230\u4e00\u81f4\u7684\u53c2\u6570\u6750\u8d28\u7a7a\u95f4\u4e2d\uff1b4. \u4f7f\u7528\u9006\u8def\u5f84\u8ffd\u8e2a\u4f18\u5316\u4f4e\u7ef4\u53c2\u6570\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u7684\u6750\u8d28\u89e3\u7f20\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u4ea7\u751f\u6e05\u6670\u5e72\u51c0\u7684\u91cd\u5efa\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u9ad8\u8d28\u91cf\u91cd\u5149\u7167\u5e94\u7528\u3002", "conclusion": "Intrinsic Image Fusion\u65b9\u6cd5\u901a\u8fc7\u878d\u5408\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u5355\u89c6\u89d2\u5148\u9a8c\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6750\u8d28\u91cd\u5efa\u7684\u6b20\u7ea6\u675f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7269\u7406\u6750\u8d28\u7684\u91cd\u5efa\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13164", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13164", "abs": "https://arxiv.org/abs/2512.13164", "authors": ["Xianchao Guan", "Zhiyuan Fan", "Yifeng Wang", "Fuqiang Chen", "Yanjiang Zhou", "Zengyang Che", "Hongxue Meng", "Xin Li", "Yaowei Wang", "Hongpeng Wang", "Min Zhang", "Heng Tao Shen", "Zheng Zhang", "Yongbing Zhang"], "title": "A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis", "comment": "67 pages, 9 figures, 16 tables", "summary": "The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.", "AI": {"tldr": "CRAFTS\u662f\u4e00\u4e2a\u9488\u5bf9\u75c5\u7406\u5b66\u7684\u9996\u4e2a\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u8c03\u8282\u5bf9\u9f50\u6846\u67b6\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u80fd\u591f\u751f\u621030\u79cd\u764c\u75c7\u7c7b\u578b\u7684\u591a\u6837\u5316\u75c5\u7406\u56fe\u50cf\uff0c\u5e76\u589e\u5f3a\u4e34\u5e8a\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u75c5\u7406\u5b66\u4e2d\u4e34\u5e8a\u7ea7\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u53d7\u5230\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\u3002\u73b0\u6709\u751f\u6210\u6a21\u578b\u5b58\u5728\u8bed\u4e49\u4e0d\u7a33\u5b9a\u548c\u5f62\u6001\u5e7b\u89c9\u95ee\u9898\uff0c\u5f71\u54cd\u8bca\u65ad\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faCRAFTS\uff08\u76f8\u5173\u6027\u8c03\u8282\u5bf9\u9f50\u6846\u67b6\uff09\uff0c\u91c7\u7528\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u7ea6280\u4e07\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5f15\u5165\u65b0\u9896\u7684\u5bf9\u9f50\u673a\u5236\u6291\u5236\u8bed\u4e49\u6f02\u79fb\uff0c\u786e\u4fdd\u751f\u7269\u5b66\u51c6\u786e\u6027\u3002\u6a21\u578b\u8fd8\u80fd\u4e0eControlNet\u7ed3\u5408\uff0c\u901a\u8fc7\u6838\u5206\u5272\u63a9\u7801\u548c\u8367\u5149\u56fe\u50cf\u7b49\u8f93\u5165\u7cbe\u786e\u63a7\u5236\u7ec4\u7ec7\u67b6\u6784\u3002", "result": "\u6a21\u578b\u751f\u6210\u6db5\u76d630\u79cd\u764c\u75c7\u7c7b\u578b\u7684\u591a\u6837\u5316\u75c5\u7406\u56fe\u50cf\uff0c\u8d28\u91cf\u901a\u8fc7\u5ba2\u89c2\u6307\u6807\u548c\u75c5\u7406\u5b66\u5bb6\u8bc4\u4f30\u5f97\u5230\u9a8c\u8bc1\u3002CRAFTS\u589e\u5f3a\u7684\u6570\u636e\u96c6\u5728\u5206\u7c7b\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "CRAFTS\u901a\u8fc7\u514b\u670d\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\u7684\u5173\u952e\u969c\u788d\uff0c\u63d0\u4f9b\u4e86\u65e0\u9650\u591a\u6837\u7684\u6807\u6ce8\u7ec4\u7ec7\u5b66\u6570\u636e\u6765\u6e90\uff0c\u6709\u6548\u89e3\u9501\u4e86\u9488\u5bf9\u7f55\u89c1\u548c\u590d\u6742\u764c\u75c7\u8868\u578b\u7684\u7a33\u5065\u8bca\u65ad\u5de5\u5177\u7684\u521b\u5efa\u3002"}}
{"id": "2512.13175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13175", "abs": "https://arxiv.org/abs/2512.13175", "authors": ["Hongxuan Sun", "Tao Wu"], "title": "Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation", "comment": null, "summary": "Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.", "AI": {"tldr": "DFSS\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u8bed\u4e49\u5206\u5272\u8bbe\u8ba1\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u6279\u5f52\u4e00\u5316\u7edf\u8ba1\u4fe1\u606f\u6765\u6307\u5bfc\u8fd1\u4f3c\u5206\u5e03\u91c7\u6837\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u5206\u5e03\u6e10\u8fdb\u84b8\u998f\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5206\u7c7b\u4efb\u52a1\u8bbe\u8ba1\uff0c\u5ffd\u89c6\u4e86\u8bed\u4e49\u5206\u5272\u4e2d\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u76f4\u63a5\u5e94\u7528\u4e8e\u5206\u5272\u4efb\u52a1\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u8bed\u4e49\u5206\u5272\u7279\u6027\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u3002", "method": "DFSS\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u6279\u5f52\u4e00\u5316\u7edf\u8ba1\u4fe1\u606f\u6307\u5bfc\u8fd1\u4f3c\u5206\u5e03\u91c7\u6837\uff0c\u9009\u62e9\u66f4\u63a5\u8fd1\u539f\u59cb\u8bad\u7ec3\u5206\u5e03\u7684\u6570\u636e\uff1b2\uff09\u63d0\u51fa\u52a0\u6743\u5206\u5e03\u6e10\u8fdb\u84b8\u998f\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u65e9\u671f\u4f18\u5148\u5904\u7406\u53ef\u9760\u6837\u672c\uff0c\u9010\u6b65\u5f15\u5165\u66f4\u5177\u6311\u6218\u6027\u7684\u6837\u672c\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDFSS\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u8f85\u52a9\u6570\u636e\u7684\u4f9d\u8d56\u3002", "conclusion": "DFSS\u901a\u8fc7\u5c0a\u91cd\u771f\u5b9e\u573a\u666f\u7684\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u8fde\u7eed\u6027\uff0c\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7a7a\u95f4\u8fde\u7eed\u6027\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.13191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13191", "abs": "https://arxiv.org/abs/2512.13191", "authors": ["Gong Chen", "Chaokun Zhang", "Pengcheng Lv", "Xiaohui Xie"], "title": "CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception", "comment": "Accepted by AAAI2026", "summary": "Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.", "AI": {"tldr": "CoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u534f\u4f5c\u611f\u77e5\u67b6\u6784\uff0c\u901a\u8fc7\u6df7\u5408\u878d\u5408\u65b9\u6cd5\u89e3\u8026\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\uff0c\u5728\u4fdd\u6301\u4f4e\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u5bf9\u6297\u901a\u4fe1\u6761\u4ef6\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709SOTA\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u4e2d\u95f4\u878d\u5408\u5b9e\u73b0\u4e86\u901a\u4fe1\u6548\u7387\u548c\u6027\u80fd\uff0c\u4f46\u5728\u6076\u52a3\u901a\u4fe1\u6761\u4ef6\u4e0b\u6027\u80fd\u4f1a\u56e0\u6570\u636e\u4f20\u8f93\u5f15\u8d77\u7684\u9519\u4f4d\u800c\u4e0b\u964d\uff0c\u8fd9\u4e25\u91cd\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u89e3\u51b3\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u4e0d\u540c\u878d\u5408\u8303\u5f0f\uff0c\u53d1\u73b0\u4e2d\u95f4\u878d\u5408\u548c\u540e\u671f\u878d\u5408\u7684\u4f18\u52bf\u4e0d\u662f\u6743\u8861\u800c\u662f\u4e92\u8865\u3002\u63d0\u51faCoRA\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u5206\u652f\uff1a\u7279\u5f81\u7ea7\u878d\u5408\u5206\u652f\uff08\u9009\u62e9\u5173\u952e\u7279\u5f81\u5e76\u9ad8\u6548\u878d\u5408\u4ee5\u786e\u4fdd\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff09\u548c\u5bf9\u8c61\u7ea7\u6821\u6b63\u5206\u652f\uff08\u5229\u7528\u8bed\u4e49\u76f8\u5173\u6027\u6821\u6b63\u7a7a\u95f4\u4f4d\u79fb\uff0c\u4fdd\u8bc1\u5bf9\u59ff\u6001\u9519\u8bef\u7684\u9c81\u68d2\u6027\uff09\u3002", "result": "\u5728\u6781\u7aef\u573a\u666f\u4e0b\uff0cCoRA\u76f8\u6bd4\u57fa\u7ebf\u5728AP@0.7\u4e0a\u63d0\u5347\u4e86\u7ea619%\uff0c\u540c\u65f6\u901a\u4fe1\u91cf\u51cf\u5c11\u4e865\u500d\u4ee5\u4e0a\uff0c\u6210\u4e3a\u9c81\u68d2\u534f\u4f5c\u611f\u77e5\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "CoRA\u901a\u8fc7\u6df7\u5408\u878d\u5408\u65b9\u6cd5\u6210\u529f\u89e3\u8026\u4e86\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\uff0c\u5728\u4fdd\u6301\u4f4e\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u901a\u4fe1\u6761\u4ef6\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u534f\u4f5c\u611f\u77e5\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.13192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13192", "abs": "https://arxiv.org/abs/2512.13192", "authors": ["Zhuo Chen", "Chengqun Yang", "Zhuo Su", "Zheng Lv", "Jingnan Gao", "Xiaoyuan Zhang", "Xiaokang Yang", "Yichao Yan"], "title": "POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling", "comment": "19 pages, 19 figures", "summary": "Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining \"chicken-and-egg\" cycle for scalable and reproducible portrait illumination.", "AI": {"tldr": "POLAR\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7269\u7406\u6821\u51c6\u7684\u5355\u5149\u6e90\u6570\u636e\u96c6\uff0cPOLARNet\u662f\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u8096\u50cf\u9884\u6d4b\u6bcf\u4e2a\u5149\u6e90\u7684\u54cd\u5e94\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u7684\u4eba\u8138\u91cd\u5149\u7167\u3002", "motivation": "\u4eba\u8138\u91cd\u5149\u7167\u7814\u7a76\u53d7\u9650\u4e8e\u5927\u89c4\u6a21\u3001\u7269\u7406\u4e00\u81f4\u7684\u5149\u7167\u6570\u636e\u53ef\u7528\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u6765\u5408\u6210\u771f\u5b9e\u8096\u50cf\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u548c\u51e0\u4f55\u7279\u5f81\u3002", "method": "1. \u6784\u5efaPOLAR\u6570\u636e\u96c6\uff1a\u5305\u542b200\u591a\u4e2a\u4e3b\u4f53\uff0c156\u4e2a\u5149\u7167\u65b9\u5411\uff0c\u591a\u89c6\u89d2\u548c\u591a\u6837\u5316\u8868\u60c5\u7684OLAT\u6570\u636e\uff1b2. \u5f00\u53d1POLARNet\uff1a\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\uff0c\u4ece\u5355\u5f20\u8096\u50cf\u9884\u6d4b\u6bcf\u4e2a\u5149\u6e90\u7684OLAT\u54cd\u5e94\uff0c\u5c06\u5149\u7167\u5efa\u6a21\u4e3a\u8fde\u7eed\u3001\u7269\u7406\u53ef\u89e3\u91ca\u7684\u53d8\u6362\u3002", "result": "POLAR\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u7269\u7406\u6821\u51c6\u7684\u5149\u7167\u6570\u636e\uff0cPOLARNet\u80fd\u591f\u6355\u6349\u7ec6\u7c92\u5ea6\u3001\u65b9\u5411\u611f\u77e5\u7684\u5149\u7167\u6548\u679c\u5e76\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\uff0c\u76f8\u6bd4\u6269\u6563\u6216\u80cc\u666f\u6761\u4ef6\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "POLAR\u548cPOLARNet\u6784\u6210\u4e86\u7edf\u4e00\u7684\u5149\u7167\u5b66\u4e60\u6846\u67b6\uff0c\u8fde\u63a5\u771f\u5b9e\u6570\u636e\u3001\u751f\u6210\u5408\u6210\u548c\u7269\u7406\u57fa\u7840\u91cd\u5149\u7167\uff0c\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u8096\u50cf\u5149\u7167\"\u9e21\u4e0e\u86cb\"\u5faa\u73af\u3002"}}
{"id": "2512.13238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13238", "abs": "https://arxiv.org/abs/2512.13238", "authors": ["Francesco Ragusa", "Michele Mazzamuto", "Rosario Forte", "Irene D'Ambra", "James Fort", "Jakob Engel", "Antonino Furnari", "Giovanni Maria Farinella"], "title": "Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance", "comment": null, "summary": "We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.", "AI": {"tldr": "Ego-EXTRA\u662f\u4e00\u4e2a\u7528\u4e8e\u4e13\u5bb6-\u5b66\u5458\u8f85\u52a9\u768450\u5c0f\u65f6\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u4e13\u5bb6\u6307\u5bfc\u5b66\u5458\u6267\u884c\u7a0b\u5e8f\u6027\u6d3b\u52a8\u7684\u5bf9\u8bdd\uff0c\u521b\u5efa\u4e86\u8d85\u8fc715k\u4e2a\u89c6\u89c9\u95ee\u7b54\u5bf9\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u8bed\u8a00\u6570\u636e\u96c6\u6765\u652f\u6301\u4e13\u5bb6\u7ea7\u8f85\u52a9\u7cfb\u7edf\u7684\u5f00\u53d1\u4e0e\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u7a0b\u5e8f\u6027\u6d3b\u52a8\u6307\u5bfc\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\"\u7eff\u91ce\u4ed9\u8e2a\"\u6570\u636e\u6536\u96c6\u8303\u5f0f\uff0c\u4e13\u5bb6\u901a\u8fc7\u7a7f\u6234\u5f0f\u667a\u80fd\u52a9\u624b\u4ece\u5b66\u5458\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c2\u5bdf\u6d3b\u52a8\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u4f9b\u6307\u5bfc\u3001\u56de\u7b54\u95ee\u9898\u6216\u4e3b\u52a8\u63d0\u51fa\u5efa\u8bae\uff0c\u8bb0\u5f55\u53cc\u5411\u5bf9\u8bdd\u5e76\u8f6c\u5f55\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u8d85\u8fc715,000\u4e2a\u9ad8\u8d28\u91cf\u89c6\u89c9\u95ee\u7b54\u5bf9\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u4f9b\u4e13\u5bb6\u7ea7\u8f85\u52a9\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0cEgo-EXTRA\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "Ego-EXTRA\u662f\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u9ad8\u8d28\u91cf\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u4e3a\u8bc4\u4f30\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u8bed\u8a00\u52a9\u624b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u4e13\u5bb6\u7ea7\u8f85\u52a9\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2512.13247", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13247", "abs": "https://arxiv.org/abs/2512.13247", "authors": ["Foivos Paraperas Papantoniou", "Stathis Galanakis", "Rolandos Alexandros Potamias", "Bernhard Kainz", "Stefanos Zafeiriou"], "title": "STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits", "comment": "Project page: https://foivospar.github.io/STARCaster/", "summary": "This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.", "AI": {"tldr": "STARCaster\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8eab\u4efd\u611f\u77e5\u65f6\u7a7a\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u8bed\u97f3\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u548c\u81ea\u7531\u89c6\u89d2\u8bf4\u8bdd\u8096\u50cf\u5408\u6210\uff0c\u901a\u8fc7\u8f6f\u8eab\u4efd\u7ea6\u675f\u548c\u9690\u5f0f3D\u611f\u77e5\u57282D\u89c6\u9891\u57df\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u8fd0\u52a8\u591a\u6837\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u3002", "motivation": "\u73b0\u67092D\u8bed\u97f3\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u8003\u6307\u5bfc\uff0c\u5bfc\u81f4\u8fd0\u52a8\u591a\u6837\u6027\u6709\u9650\uff1b\u800c3D\u611f\u77e5\u52a8\u753b\u901a\u5e38\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u4e09\u5e73\u9762\u751f\u6210\u5668\u53cd\u6f14\uff0c\u5bfc\u81f4\u91cd\u5efa\u4e0d\u5b8c\u7f8e\u548c\u8eab\u4efd\u6f02\u79fb\u3002\u9700\u8981\u91cd\u65b0\u601d\u8003\u57fa\u4e8e\u53c2\u8003\u548c\u51e0\u4f55\u7684\u8303\u5f0f\u3002", "method": "1. \u91c7\u7528\u8f6f\u8eab\u4efd\u7ea6\u675f\u800c\u975e\u4e25\u683c\u7684\u53c2\u8003\u6761\u4ef6\u5316\uff1b2. \u5229\u7528\u89c6\u9891\u6570\u636e\u56fa\u6709\u7684\u591a\u89c6\u89d2\u7279\u6027\uff0c\u57282D\u89c6\u9891\u57df\u4e2d\u9690\u5f0f\u5b9e\u73b03D\u611f\u77e5\uff1b3. \u91c7\u7528\u7ec4\u5408\u5f0f\u65b9\u6cd5\uff1aID\u611f\u77e5\u8fd0\u52a8\u5efa\u6a21\u2192\u57fa\u4e8e\u5507\u8bfb\u7684\u89c6\u542c\u540c\u6b65\u2192\u901a\u8fc7\u65f6\u7a7a\u9002\u5e94\u7684\u65b0\u89c6\u89d2\u52a8\u753b\uff1b4. \u63d0\u51fa\u89e3\u8026\u5b66\u4e60\u65b9\u6cd5\uff0c\u72ec\u7acb\u8bad\u7ec3\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\uff1b5. \u4f7f\u7528\u81ea\u5f3a\u5236\u8bad\u7ec3\u65b9\u6848\u5b66\u4e60\u6bd4\u63a8\u7406\u65f6\u66f4\u957f\u7684\u65f6\u5e8f\u4e0a\u4e0b\u6587\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cSTARCaster\u5728\u4efb\u52a1\u548c\u8eab\u4efd\u4e0a\u90fd\u80fd\u6709\u6548\u6cdb\u5316\uff0c\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u8fc7\u5ea6\u9759\u6001\u52a8\u753b\u95ee\u9898\u3002", "conclusion": "STARCaster\u901a\u8fc7\u91cd\u65b0\u601d\u8003\u53c2\u8003\u548c\u51e0\u4f55\u8303\u5f0f\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bed\u97f3\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u548c\u81ea\u7531\u89c6\u89d2\u8bf4\u8bdd\u8096\u50cf\u5408\u6210\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8eab\u4efd\u611f\u77e5\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13250", "abs": "https://arxiv.org/abs/2512.13250", "authors": ["Juil Koo", "Daehyeon Choi", "Sangwoo Youn", "Phillip Y. Lee", "Minhyuk Sung"], "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection", "comment": "Project page: https://active-view-selection.github.io/", "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.", "AI": {"tldr": "VG-AVS\u4efb\u52a1\u8ba9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u9759\u6001\u56fe\u50cf\u63a8\u7406\u8f6c\u5411\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u5fae\u8c03\u6846\u67b6\u5b9e\u73b0\u57fa\u4e8e\u89c6\u89c9\u4fe1\u606f\u7684\u667a\u80fd\u89c6\u89d2\u9009\u62e9\uff0c\u63d0\u5347\u4e0b\u6e38\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ec5\u9650\u4e8e\u9759\u6001\u56fe\u50cf\u63a8\u7406\uff08\u5feb\u7167\u89c6\u89c9\uff09\uff0c\u800c\u5177\u8eab\u667a\u80fd\u4f53\u9700\u8981\u4e3b\u52a8\u79fb\u52a8\u83b7\u53d6\u66f4\u4e30\u5bcc\u89c6\u89d2\uff08\u884c\u8d70\u89c6\u89c9\uff09\u3002VG-AVS\u4efb\u52a1\u65e8\u5728\u8ba9\u6a21\u578b\u4ec5\u57fa\u4e8e\u5f53\u524d\u89c6\u89c9\u4fe1\u606f\u9009\u62e9\u6700\u6709\u4fe1\u606f\u91cf\u7684\u4e0b\u4e00\u4e2a\u89c6\u89d2\uff0c\u4e0d\u4f9d\u8d56\u573a\u666f\u8bb0\u5fc6\u6216\u5916\u90e8\u77e5\u8bc6\u3002", "method": "1) \u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff0c\u81ea\u52a8\u751f\u6210\u914d\u5bf9\u67e5\u8be2-\u76ee\u6807\u89c6\u89d2\u548c\u95ee\u7b54\u63d0\u793a\uff1b2) \u63d0\u51fa\u5fae\u8c03\u6846\u67b6\uff1a\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5fae\u8c03\u9884\u8bad\u7ec3VLM\uff0c\u518d\u8fdb\u884c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b56\u7565\u4f18\u5316\u3002", "result": "\u65b9\u6cd5\u5728\u89c6\u89d2\u9009\u62e9\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u95ee\u7b54\u6027\u80fd\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5c06VG-AVS\u6846\u67b6\u96c6\u6210\u5230\u73b0\u6709\u57fa\u4e8e\u573a\u666f\u63a2\u7d22\u7684EQA\u7cfb\u7edf\u4e2d\uff0c\u63d0\u9ad8\u4e86\u4e0b\u6e38\u95ee\u7b54\u51c6\u786e\u6027\u3002", "conclusion": "VG-AVS\u4efb\u52a1\u6210\u529f\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u9759\u6001\u56fe\u50cf\u63a8\u7406\u6269\u5c55\u5230\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u8bad\u7ec3\u548c\u5fae\u8c03\u6846\u67b6\u5b9e\u73b0\u4e86\u4ec5\u57fa\u4e8e\u89c6\u89c9\u4fe1\u606f\u7684\u667a\u80fd\u89c6\u89d2\u51b3\u7b56\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.13276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13276", "abs": "https://arxiv.org/abs/2512.13276", "authors": ["Yan Li", "Lin Liu", "Xiaopeng Zhang", "Wei Xue", "Wenhan Luo", "Yike Guo", "Qi Tian"], "title": "CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing", "comment": null, "summary": "Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods strug- gle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across con- secutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation", "AI": {"tldr": "CogniEdit\uff1a\u4e00\u4e2a\u7ed3\u5408\u591a\u6a21\u6001\u63a8\u7406\u4e0e\u5bc6\u96c6\u5956\u52b1\u4f18\u5316\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u9075\u5faa\u7ec6\u7c92\u5ea6\u6307\u4ee4\uff08\u5982\u989c\u8272\u3001\u4f4d\u7f6e\u3001\u6570\u91cf\uff09\u8fdb\u884c\u56fe\u50cf\u7f16\u8f91\u65f6\u7684\u56f0\u96be", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u6307\u4ee4\uff08\u5982\u7cbe\u786e\u7684\u989c\u8272\u3001\u4f4d\u7f6e\u3001\u6570\u91cf\u5c5e\u6027\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u867d\u7136\u6700\u8fd1\u7684\u65b9\u6cd5\u4f7f\u7528GRPO\u8fdb\u884c\u5bf9\u9f50\uff0c\u4f46\u53ea\u5728\u5355\u4e2a\u91c7\u6837\u6b65\u9aa4\u4f18\u5316\uff0c\u53cd\u9988\u7a00\u758f\uff0c\u9650\u5236\u4e86\u8f68\u8ff9\u7ea7\u522b\u7684\u63a7\u5236\u80fd\u529b\u3002", "method": "CogniEdit\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u590d\u6742\u6307\u4ee4\u5206\u89e3\u4e3a\u53ef\u64cd\u4f5c\u7684\u6307\u4ee4\uff1b2) \u52a8\u6001\u4ee4\u724c\u7126\u70b9\u91cd\u5b9a\u4f4d\u81ea\u9002\u5e94\u5730\u5f3a\u8c03\u7ec6\u7c92\u5ea6\u5c5e\u6027\uff1b3) \u57fa\u4e8e\u5bc6\u96c6GRPO\u7684\u4f18\u5316\uff0c\u5728\u8fde\u7eed\u53bb\u566a\u6b65\u9aa4\u95f4\u4f20\u64ad\u68af\u5ea6\uff0c\u5b9e\u73b0\u8f68\u8ff9\u7ea7\u522b\u7684\u76d1\u7763\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCogniEdit\u5728\u5e73\u8861\u7ec6\u7c92\u5ea6\u6307\u4ee4\u9075\u5faa\u4e0e\u89c6\u89c9\u8d28\u91cf\u53ca\u53ef\u7f16\u8f91\u6027\u4fdd\u6301\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u63a8\u7406\u4e0e\u5bc6\u96c6\u5956\u52b1\u4f18\u5316\uff0cCogniEdit\u5b9e\u73b0\u4e86\u8f68\u8ff9\u7ea7\u522b\u7684\u68af\u5ea6\u6d41\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u9075\u5faa\u7ec6\u7c92\u5ea6\u6307\u4ee4\u8fdb\u884c\u56fe\u50cf\u7f16\u8f91\u65f6\u7684\u80fd\u529b\u3002"}}
{"id": "2512.13285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13285", "abs": "https://arxiv.org/abs/2512.13285", "authors": ["Bo Liu", "Qiao Qin", "Qinghui He"], "title": "CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images", "comment": "9 pages Accepted to AAAI 2026", "summary": "The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.", "AI": {"tldr": "CausalCLIP\uff1a\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u56fe\u50cf\u751f\u6210\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u56e0\u679c\u7279\u5f81\u4e0e\u975e\u56e0\u679c\u7279\u5f81\u63d0\u5347\u8de8\u751f\u6210\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6837\u5316\u548c\u4e0d\u65ad\u6f14\u8fdb\u7684\u751f\u6210\u6280\u672f\u65f6\uff0c\u5176\u8868\u5f81\u5f80\u5f80\u9ad8\u5ea6\u7ea0\u7f20\uff0c\u6df7\u5408\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u53d6\u8bc1\u7ebf\u7d22\uff08\u56e0\u679c\u7279\u5f81\uff09\u4e0e\u865a\u5047\u6216\u65e0\u5173\u6a21\u5f0f\uff08\u975e\u56e0\u679c\u7279\u5f81\uff09\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faCausalCLIP\u6846\u67b6\uff0c\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u539f\u7406\u8fdb\u884c\u9488\u5bf9\u6027\u8fc7\u6ee4\uff0c\u901a\u8fc7\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u5efa\u6a21\u751f\u6210\u8fc7\u7a0b\uff0c\u5229\u7528Gumbel-Softmax\u7279\u5f81\u63a9\u7801\u548cHilbert-Schmidt\u72ec\u7acb\u6027\u51c6\u5219\u7ea6\u675f\u6765\u5f3a\u5236\u7edf\u8ba1\u72ec\u7acb\u6027\uff0c\u4ece\u800c\u89e3\u8026\u56e0\u679c\u7279\u5f81\u4e0e\u975e\u56e0\u679c\u7279\u5f81\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u4e0d\u540c\u7cfb\u5217\u751f\u6210\u6a21\u578b\u4e0a\u6d4b\u8bd5\u65f6\uff0cCausalCLIP\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u51c6\u786e\u7387\u4e0a\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u53476.83%\uff0c\u5e73\u5747\u7cbe\u5ea6\u63d0\u53474.06%\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u56e0\u679c\u7279\u5f81\u4e0e\u975e\u56e0\u679c\u7279\u5f81\u5e76\u4fdd\u7559\u6700\u5177\u53ef\u8fc1\u79fb\u6027\u548c\u5224\u522b\u6027\u7684\u53d6\u8bc1\u7ebf\u7d22\uff0cCausalCLIP\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5206\u5e03\u504f\u79fb\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.13290", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13290", "abs": "https://arxiv.org/abs/2512.13290", "authors": ["Shu Yu", "Chaochao Lu"], "title": "LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models", "comment": null, "summary": "Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLINA\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u7269\u7406\u5bf9\u9f50\u548c\u5206\u5e03\u5916\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u7684\u56f0\u96be\uff0c\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u7269\u7406\u5bf9\u9f50\u548c\u5206\u5e03\u5916\u6307\u4ee4\u9075\u5faa\u7684\u56f0\u96be\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u6a21\u578b\u672a\u80fd\u5b66\u4e60\u56e0\u679c\u65b9\u5411\u548c\u89e3\u8026\u56e0\u679c\u56e0\u5b50\u4ee5\u5b9e\u73b0\u65b0\u9896\u91cd\u7ec4\u3002", "method": "\u5f15\u5165\u56e0\u679c\u573a\u666f\u56fe\u548c\u7269\u7406\u5bf9\u9f50\u63a2\u6d4b\u6570\u636e\u96c6\u8fdb\u884c\u8bca\u65ad\u5e72\u9884\u5206\u6790\uff0c\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u63d0\u51faLINA\u6846\u67b6\uff1a\u5b66\u4e60\u9884\u6d4b\u7279\u5b9a\u63d0\u793a\u7684\u5e72\u9884\uff0c\u91c7\u7528\u63d0\u793a\u548c\u89c6\u89c9\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u9488\u5bf9\u6027\u5f15\u5bfc\uff0c\u4ee5\u53ca\u91cd\u65b0\u5206\u914d\u3001\u56e0\u679c\u611f\u77e5\u7684\u53bb\u566a\u8c03\u5ea6\u3002", "result": "LINA\u6846\u67b6\u5728\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u540c\u65f6\u5b9e\u73b0\u4e86\u7269\u7406\u5bf9\u9f50\u548c\u5206\u5e03\u5916\u6307\u4ee4\u9075\u5faa\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u56e0\u679c\u751f\u6210\u4efb\u52a1\u548cWinoground\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u548c\u91cd\u65b0\u5206\u914d\u7684\u53bb\u566a\u8c03\u5ea6\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u7269\u7406\u5bf9\u9f50\u548c\u5206\u5e03\u5916\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u7684\u6839\u672c\u95ee\u9898\uff0c\u4e3a\u66f4\u53ef\u63a7\u548c\u53ef\u9760\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.13303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13303", "abs": "https://arxiv.org/abs/2512.13303", "authors": ["Zhihang Liu", "Xiaoyi Bao", "Pandeng Li", "Junjie Zhou", "Zhaohe Liao", "Yefei He", "Kaixun Jiang", "Chen-Wei Xie", "Yun Zheng", "Hongtao Xie"], "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement", "comment": "project page: https://lntzm.github.io/showtable-page/", "summary": "While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faShowTable\u6846\u67b6\uff0c\u7ed3\u5408MLLM\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u81ea\u6821\u6b63\u8fc7\u7a0b\u751f\u6210\u521b\u610f\u8868\u683c\u53ef\u89c6\u5316\uff0c\u5e76\u6784\u5efaTableVisBench\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u548c\u7edf\u4e00\u6a21\u578b\u5728\u9700\u8981\u6df1\u5ea6\u63a8\u7406\u3001\u89c4\u5212\u548c\u7cbe\u786e\u6570\u636e\u5230\u89c6\u89c9\u6620\u5c04\u80fd\u529b\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u8d85\u8d8a\u4e00\u822c\u573a\u666f\u7684\u521b\u610f\u8868\u683c\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d", "method": "\u63d0\u51faShowTable\u6846\u67b6\uff0c\u5c06MLLM\u4f5c\u4e3a\u4e2d\u5fc3\u534f\u8c03\u5668\u8fdb\u884c\u89c6\u89c9\u89c4\u5212\u63a8\u7406\u548c\u89c6\u89c9\u9519\u8bef\u5224\u65ad\uff0c\u6269\u6563\u6a21\u578b\u6267\u884cMLLM\u6307\u4ee4\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u81ea\u6821\u6b63\u8fc7\u7a0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7ed3\u679c", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u4e0d\u540c\u6a21\u578b\u5b9e\u4f8b\u5316\u7684ShowTable\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u7684\u591a\u6a21\u6001\u63a8\u7406\u3001\u751f\u6210\u548c\u9519\u8bef\u6821\u6b63\u80fd\u529b", "conclusion": "\u63d0\u51fa\u7684\u521b\u610f\u8868\u683c\u53ef\u89c6\u5316\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0cShowTable\u6846\u67b6\u901a\u8fc7MLLM\u548c\u6269\u6563\u6a21\u578b\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u7ed3\u5408\u6e10\u8fdb\u5f0f\u81ea\u6821\u6b63\u8fc7\u7a0b\uff0c\u80fd\u591f\u751f\u6210\u5fe0\u5b9e\u4e14\u7f8e\u89c2\u7684\u6570\u636e\u53ef\u89c6\u5316\u56fe\u8868"}}
{"id": "2512.13313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13313", "abs": "https://arxiv.org/abs/2512.13313", "authors": ["Kling Team", "Jialu Chen", "Yikang Ding", "Zhixue Fang", "Kun Gai", "Yuan Gao", "Kang He", "Jingyun Hua", "Boyuan Jiang", "Mingming Lao", "Xiaohan Li", "Hui Liu", "Jiwen Liu", "Xiaoqiang Liu", "Yuan Liu", "Shun Lu", "Yongsen Mao", "Yingchao Shao", "Huafeng Shi", "Xiaoyu Shi", "Peiqin Sun", "Songlin Tang", "Pengfei Wan", "Chao Wang", "Xuebo Wang", "Haoxian Zhang", "Yuanxing Zhang", "Yan Zhou"], "title": "KlingAvatar 2.0 Technical Report", "comment": "14 pages, 7 figures", "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.", "AI": {"tldr": "KlingAvatar 2.0\u662f\u4e00\u4e2a\u65f6\u7a7a\u7ea7\u8054\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u5206\u8fa8\u7387\u63d0\u5347\u548c\u65f6\u95f4\u7ef4\u5ea6\u6269\u5c55\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6548\u7387\u4f4e\u4e0b\u3001\u65f6\u95f4\u6f02\u79fb\u3001\u8d28\u91cf\u4e0b\u964d\u548c\u63d0\u793a\u8ddf\u968f\u5f31\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5934\u50cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u957f\u65f6\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u65f6\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u3001\u65f6\u95f4\u6f02\u79fb\u3001\u8d28\u91cf\u4e0b\u964d\u548c\u63d0\u793a\u8ddf\u968f\u5f31\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u957f\u89c6\u9891\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u65f6\u7a7a\u7ea7\u8054\u6846\u67b6\uff1a\u9996\u5148\u751f\u6210\u4f4e\u5206\u8fa8\u7387\u84dd\u56fe\u89c6\u9891\u5173\u952e\u5e27\u6355\u6349\u5168\u5c40\u8bed\u4e49\u548c\u8fd0\u52a8\uff0c\u7136\u540e\u4f7f\u7528\u9996\u5c3e\u5e27\u7b56\u7565\u5c06\u5176\u7ec6\u5316\u4e3a\u9ad8\u5206\u8fa8\u7387\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u5b50\u7247\u6bb5\u3002\u5f15\u5165\u534f\u540c\u63a8\u7406\u5bfc\u6f14\uff08\u4e09\u4e2a\u6a21\u6001\u7279\u5b9aLLM\u4e13\u5bb6\uff09\u589e\u5f3a\u8de8\u6a21\u6001\u6307\u4ee4\u878d\u5408\u548c\u5bf9\u9f50\uff0c\u4ee5\u53ca\u8d1f\u5411\u5bfc\u6f14\u4f18\u5316\u8d1f\u5411\u63d0\u793a\u3002\u6269\u5c55\u6846\u67b6\u652f\u6301ID\u7279\u5b9a\u7684\u591a\u89d2\u8272\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u6548\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u957f\u65f6\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u6311\u6218\uff0c\u63d0\u4f9b\u589e\u5f3a\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u3001\u903c\u771f\u7684\u5507\u9f7f\u6e32\u67d3\u4e0e\u51c6\u786e\u7684\u53e3\u578b\u540c\u6b65\u3001\u5f3a\u8eab\u4efd\u4fdd\u6301\u548c\u8fde\u8d2f\u7684\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u3002", "conclusion": "KlingAvatar 2.0\u901a\u8fc7\u65f6\u7a7a\u7ea7\u8054\u6846\u67b6\u548c\u534f\u540c\u63a8\u7406\u5bfc\u6f14\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u65f6\u9ad8\u5206\u8fa8\u7387\u5934\u50cf\u89c6\u9891\u751f\u6210\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7387\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2512.13317", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13317", "abs": "https://arxiv.org/abs/2512.13317", "authors": ["Mikhail Zakharov"], "title": "Face Identity Unlearning for Retrieval via Embedding Dispersion", "comment": "12 pages, 1 figure, 5 tables, 10 equations. Preprint", "summary": "Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4eba\u8138\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u8eab\u4efd\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6563\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u4f7f\u7279\u5b9a\u8eab\u4efd\u65e0\u6cd5\u88ab\u68c0\u7d22\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u8eab\u4efd\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u867d\u7136\u80fd\u6709\u6548\u8fdb\u884c\u8eab\u4efd\u8bc6\u522b\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u95ee\u9898\uff0c\u53ef\u80fd\u88ab\u7528\u4e8e\u672a\u7ecf\u6388\u6743\u7684\u8eab\u4efd\u8ffd\u8e2a\u3002\u73b0\u6709\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5728\u4eba\u8138\u68c0\u7d22\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u9488\u5bf9\u73b0\u4ee3\u57fa\u4e8e\u5d4c\u5165\u7684\u8bc6\u522b\u6a21\u578b\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u73b0\u6709\u7684\u8fd1\u4f3c\u7c7b\u522b\u9057\u5fd8\u65b9\u6cd5\uff08\u5982\u968f\u673a\u6807\u8bb0\u3001\u68af\u5ea6\u4e0a\u5347\u3001\u8fb9\u754c\u9057\u5fd8\u7b49\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u57fa\u4e8e\u5206\u6563\u7684\u9057\u5fd8\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u8d85\u7403\u9762\u4e0a\u5206\u6563\u76ee\u6807\u8eab\u4efd\u7684\u5d4c\u5165\uff0c\u9632\u6b62\u5f62\u6210\u7d27\u51d1\u7684\u8eab\u4efd\u805a\u7c7b\uff0c\u4ece\u800c\u5b9e\u73b0\u9057\u5fd8\u6548\u679c\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\uff08VGGFace2\u3001CelebA\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u9057\u5fd8\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u68c0\u7d22\u6548\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4eba\u8138\u68c0\u7d22\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8eab\u4efd\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u7279\u5b9a\u8eab\u4efd\u9690\u79c1\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u7cfb\u7edf\u5bf9\u5176\u4ed6\u8eab\u4efd\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u7cfb\u7edf\u6548\u7528\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2512.13376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13376", "abs": "https://arxiv.org/abs/2512.13376", "authors": ["Carla Monteiro", "Valentina Corbetta", "Regina Beets-Tan", "Lu\u00eds F. Teixeira", "Wilson Silva"], "title": "Unlocking Generalization in Polyp Segmentation with DINO Self-Attention \"keys\"", "comment": "29 pages, 10 figures, 8 tables, under review at MIDL 2026", "summary": "Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention \"key\" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528DINO\u81ea\u6ce8\u610f\u529b\"key\"\u7279\u5f81\u8fdb\u884c\u606f\u8089\u5206\u5272\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u5377\u79ef\u89e3\u7801\u5668\u5b9e\u73b0\uff0c\u5728\u6570\u636e\u53d7\u9650\u548c\u6311\u6218\u6027\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u606f\u8089\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u53d7\u9650\u6216\u6311\u6218\u6027\u573a\u666f\u4e0b\uff0c\u4e14\u8bb8\u591a\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u3001\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528DINO\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u7684\"key\"\u7279\u5f81\uff0c\u800c\u975e\u4f20\u7edf\u65b9\u6cd5\u4eceVision Transformer\u6700\u6df1\u5c42\u63d0\u53d6token\uff0c\u7ed3\u5408\u7b80\u5355\u7684\u5377\u79ef\u89e3\u7801\u5668\u9884\u6d4b\u606f\u8089\u63a9\u7801\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u590d\u6742\u7684\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u3002", "result": "\u5728\u57df\u6cdb\u5316(DG)\u548c\u6781\u7aef\u5355\u57df\u6cdb\u5316(ESDG)\u4e24\u79cd\u4e25\u683c\u534f\u8bae\u4e0b\uff0c\u4f7f\u7528\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u548c\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0c\u8d85\u8d8a\u4e86nnU-Net\u548cUM-Net\u7b49\u6210\u719f\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528DINO\u81ea\u6ce8\u610f\u529b\u5173\u952e\u7279\u5f81\u7684\u56fa\u6709\u9c81\u68d2\u6027\uff0c\u7ed3\u5408\u7b80\u5355\u89e3\u7801\u5668\uff0c\u53ef\u4ee5\u6784\u5efa\u51fa\u6cdb\u5316\u80fd\u529b\u5f3a\u3001\u6027\u80fd\u4f18\u8d8a\u7684\u606f\u8089\u5206\u5272\u6846\u67b6\uff0c\u907f\u514d\u4e86\u590d\u6742\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u7684\u9700\u6c42\uff0c\u4e3a\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13416", "abs": "https://arxiv.org/abs/2512.13416", "authors": ["Haoxuan Qu", "Qiuchi Xiang", "Yujun Cai", "Yirui Wu", "Majid Mirmehdi", "Hossein Rahmani", "Jun Liu"], "title": "Learning to Generate Cross-Task Unexploitable Examples", "comment": null, "summary": "Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.", "AI": {"tldr": "\u63d0\u51faMCT-UEG\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u8de8\u4efb\u52a1\u8bad\u7ec3\u751f\u6210\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u7684\u4e2a\u4eba\u56fe\u50cf\uff0c\u9632\u6b62\u5728\u7ebf\u56fe\u50cf\u88ab\u672a\u7ecf\u6388\u6743\u5229\u7528", "motivation": "\u73b0\u6709\u4e0d\u53ef\u5229\u7528\u793a\u4f8b\u751f\u6210\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u751f\u6210\u5728\u4e0d\u540c\u771f\u5b9e\u4e16\u754c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u90fd\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u7684\u793a\u4f8b", "method": "\u63d0\u51faMCT-UEG\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u8bbe\u8ba1\u9762\u5411\u5e73\u5766\u6700\u5c0f\u503c\u7684\u5143\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65b9\u6848\uff0c\u4f18\u5316\u4e0d\u53ef\u5229\u7528\u793a\u4f8b\u751f\u6210\u5668", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684MCT-UEG\u6846\u67b6\u80fd\u591f\u751f\u6210\u8de8\u4efb\u52a1\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u7684\u793a\u4f8b\uff0c\u63d0\u9ad8\u4e86\u4e2a\u4eba\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u7684\u5b9e\u9645\u9002\u7528\u6027"}}
{"id": "2512.13421", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13421", "abs": "https://arxiv.org/abs/2512.13421", "authors": ["Qingyu Shi", "Size Wu", "Jinbin Bai", "Kaidong Yu", "Yujing Wang", "Yunhai Tong", "Xiangtai Li", "Xuelong Li"], "title": "RecTok: Reconstruction Distillation along Rectified Flow", "comment": null, "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.", "AI": {"tldr": "RecTok\u901a\u8fc7\u6d41\u8bed\u4e49\u84b8\u998f\u548c\u91cd\u5efa\u5bf9\u9f50\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u89c6\u89c9\u5206\u8bcd\u5668\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u56fe\u50cf\u91cd\u5efa\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5206\u8bcd\u5668\u5b58\u5728\u7ef4\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6839\u672c\u6743\u8861\uff0c\u9ad8\u7ef4\u5206\u8bcd\u5668\u6027\u80fd\u4ecd\u4e0d\u5982\u4f4e\u7ef4\u7248\u672c\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5173\u952e\u521b\u65b0\uff1a1\uff09\u6d41\u8bed\u4e49\u84b8\u998f - \u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u4fe1\u606f\u84b8\u998f\u5230\u6d41\u5339\u914d\u7684\u524d\u5411\u6d41\u8f68\u8ff9\u4e2d\uff1b2\uff09\u91cd\u5efa\u5bf9\u9f50\u84b8\u998f - \u5f15\u5165\u63a9\u7801\u7279\u5f81\u91cd\u5efa\u635f\u5931\u8fdb\u4e00\u6b65\u589e\u5f3a\u8bed\u4e49\u3002", "result": "\u5728gFID-50K\u57fa\u51c6\u4e0a\uff0c\u65e0\u8bba\u662f\u5426\u4f7f\u7528\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff0c\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4e30\u5bcc\u7684\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\uff0c\u4e14\u968f\u7740\u6f5c\u5728\u7ef4\u5ea6\u589e\u52a0\u6027\u80fd\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "RecTok\u6210\u529f\u514b\u670d\u4e86\u9ad8\u7ef4\u89c6\u89c9\u5206\u8bcd\u5668\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u56fe\u50cf\u91cd\u5efa\u3001\u751f\u6210\u8d28\u91cf\u548c\u5224\u522b\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u5206\u8bcd\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.13427", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13427", "abs": "https://arxiv.org/abs/2512.13427", "authors": ["Noa Cohen", "Nurit Spingarn-Eliezer", "Inbar Huberman-Spiegelglas", "Tomer Michaeli"], "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models", "comment": "Code and examples are available on the project's webpage at https://noa-cohen.github.io/MineTheGap/", "summary": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.", "AI": {"tldr": "MineTheGap\u662f\u4e00\u79cd\u81ea\u52a8\u6316\u6398\u5bfc\u81f4\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u504f\u89c1\u8f93\u51fa\u7684\u63d0\u793a\u8bcd\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u8bcd\u6c60\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u504f\u89c1\u8bc4\u5206\u6765\u8bc6\u522b\u548c\u91cf\u5316\u504f\u89c1\u4e25\u91cd\u7a0b\u5ea6\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\uff0c\u6587\u672c\u63d0\u793a\u5f80\u5f80\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u6a21\u578b\u4f1a\u8868\u73b0\u51fa\u89e3\u91ca\u504f\u89c1\u3002\u8fd9\u4e9b\u504f\u89c1\u53ef\u80fd\u4ea7\u751f\u793e\u4f1a\u5f71\u54cd\uff08\u5982\u804c\u4e1a\u4e0e\u79cd\u65cf\u7684\u523b\u677f\u5370\u8c61\uff09\uff0c\u4e5f\u4f1a\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff08\u751f\u6210\u5197\u4f59\u56fe\u50cf\u800c\u975e\u591a\u6837\u5316\u53ef\u80fd\u6027\uff09\u3002", "method": "\u63d0\u51faMineTheGap\u65b9\u6cd5\uff0c\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u8bcd\u6c60\uff0c\u5bfb\u627e\u80fd\u66b4\u9732\u504f\u89c1\u7684\u63d0\u793a\u8bcd\u3002\u4f18\u5316\u8fc7\u7a0b\u7531\u65b0\u9896\u7684\u504f\u89c1\u8bc4\u5206\u9a71\u52a8\uff0c\u8be5\u8bc4\u5206\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u56fe\u50cf\u7684\u5206\u5e03\u4e0eLLM\u751f\u6210\u7684\u6587\u672c\u53d8\u4f53\u5206\u5e03\u6765\u8ba1\u7b97\uff0c\u4ece\u800c\u6839\u636e\u504f\u89c1\u4e25\u91cd\u7a0b\u5ea6\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u4ec5\u68c0\u6d4b\u7ed9\u5b9a\u63d0\u793a\u8bcd\u7684\u504f\u89c1\uff0c\u80fd\u591f\u81ea\u52a8\u6316\u6398\u5bfc\u81f4\u504f\u89c1\u7684\u63d0\u793a\u8bcd\u3002\u5728\u5df2\u77e5\u504f\u89c1\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u504f\u89c1\u8bc4\u5206\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u548c\u793a\u4f8b\u5df2\u5728\u9879\u76ee\u7f51\u9875\u4e0a\u63d0\u4f9b\u3002", "conclusion": "MineTheGap\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u504f\u89c1\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u504f\u89c1\u673a\u5236\u5e76\u4fc3\u8fdb\u66f4\u516c\u5e73\u3001\u591a\u6837\u5316\u7684\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2512.13428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13428", "abs": "https://arxiv.org/abs/2512.13428", "authors": ["Anika Islam", "Tasfia Tahsin", "Zaarin Anjum", "Md. Bakhtiar Hasan", "Md. Hasanul Kabir"], "title": "A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification", "comment": null, "summary": "Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u690d\u7269\u53f6\u7247\u75c5\u5bb3\u8bc6\u522b\uff0c\u7ed3\u5408MobileNet\u7279\u5f81\u63d0\u53d6\u3001\u7279\u5f81\u878d\u5408\u548c\u6ce8\u610f\u529b\u589e\u5f3a\u7684Bi-LSTM\u5206\u7c7b\u5668\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u75c5\u5bb3\u8bca\u65ad\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u690d\u7269\u75c5\u5bb3\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u6a21\u578b\uff0c\u4e0d\u9002\u5408\u6570\u636e\u7a00\u7f3a\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u5c11\u6837\u672c\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u9886\u57df\u9002\u5e94\u7684MobileNetV2\u548cMobileNetV3\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u7ed3\u5408\u7279\u5f81\u878d\u5408\u6280\u672f\u751f\u6210\u9c81\u68d2\u7279\u5f81\u8868\u793a\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u7684Bi-LSTM\u5206\u7c7b\u5668\u6355\u83b7\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb\u5e76\u805a\u7126\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728PlantVillage\u6570\u636e\u96c6\u4e0a15-shot\u8fbe\u523098.23%\u51c6\u786e\u7387\uff0c\u63a5\u8fd1SOTA\u768499.98%\uff1b\u5728\u771f\u5b9e\u73af\u5883Dhan Shomadhan\u6570\u636e\u96c6\u4e0a15-shot\u8fbe\u523069.28%\uff1b\u5728PlantVillage\u516d\u79cd\u75c5\u5bb3\u4e0a\u4ee515-shot\u5b66\u4e60\u8fbe\u523099.72%\uff0c\u8d85\u8d8a\u4e4b\u524d96.0%\u7684SOTA\u3002", "conclusion": "\u8be5\u6846\u67b6\u6a21\u578b\u5927\u5c0f\u7ea640MB\uff0c\u63a8\u7406\u590d\u6742\u5ea6\u7ea61.12GFLOPs\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u5730\u533a\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u79fb\u52a8\u5c31\u7eea\u7684\u7cbe\u786e\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u57fa\u7840\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2512.13440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13440", "abs": "https://arxiv.org/abs/2512.13440", "authors": ["Thalyssa Baiocco-Rodrigues", "Antoine Olivier", "Reda Belbahri", "Thomas Duboudin", "Pierre-Antoine Bannier", "Benjamin Adjadj", "Katharina Von Loga", "Nathan Noiry", "Maxime Touzot", "Hector Roux de Bezieux"], "title": "IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images", "comment": null, "summary": "As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.", "AI": {"tldr": "IMILIA\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u793a\u4f8b\u5b66\u4e60\u9884\u6d4bIBD\u7ec4\u7ec7\u5207\u7247\u4e2d\u7684\u708e\u75c7\u5b58\u5728\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u6a21\u5757\u81ea\u52a8\u8ba1\u7b97\u9a71\u52a8\u9884\u6d4b\u7684\u7ec4\u7ec7\u533a\u57df\u6807\u8bb0\u7269\u3002", "motivation": "\u968f\u7740IBD\u6cbb\u7597\u76ee\u6807\u8f6c\u5411\u7ec4\u7ec7\u5b66\u7f13\u89e3\uff0c\u51c6\u786e\u8bc4\u4f30\u5fae\u89c2\u708e\u75c7\u5bf9\u4e8e\u8bc4\u4f30\u75be\u75c5\u6d3b\u52a8\u6027\u548c\u6cbb\u7597\u53cd\u5e94\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u9884\u6d4b\u708e\u75c7\u5b58\u5728\u5e76\u89e3\u91ca\u9884\u6d4b\u7ed3\u679c\u3002", "method": "IMILIA\u5305\u542b\u708e\u75c7\u9884\u6d4b\u6a21\u5757\uff08\u57fa\u4e8e\u591a\u793a\u4f8b\u5b66\u4e60\u7684\u6a21\u578b\uff09\u548c\u53ef\u89e3\u91ca\u6027\u6a21\u5757\uff08HistoPLUS\u7528\u4e8e\u7ec6\u80de\u68c0\u6d4b\u3001\u5206\u5272\u548c\u5206\u7c7b\uff0cEpiSeg\u7528\u4e8e\u4e0a\u76ae\u5206\u5272\uff09\u3002", "result": "\u5728\u53d1\u73b0\u961f\u5217\u4e2d\u4ea4\u53c9\u9a8c\u8bc1ROC-AUC\u4e3a0.83\uff0c\u5728\u4e24\u4e2a\u5916\u90e8\u9a8c\u8bc1\u961f\u5217\u4e2d\u5206\u522b\u4e3a0.99\u548c0.84\u3002\u53ef\u89e3\u91ca\u6027\u6a21\u5757\u663e\u793a\u9ad8\u5206\u56fe\u5757\u514d\u75ab\u7ec6\u80de\u5bc6\u5ea6\u589e\u52a0\uff0c\u4f4e\u5206\u56fe\u5757\u4e3b\u8981\u4e3a\u6b63\u5e38\u4e0a\u76ae\u7ec6\u80de\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u5728\u6240\u6709\u6570\u636e\u96c6\u4e2d\u4e00\u81f4\u3002", "conclusion": "IMILIA\u80fd\u591f\u51c6\u786e\u9884\u6d4bIBD\u7ec4\u7ec7\u5207\u7247\u4e2d\u7684\u708e\u75c7\u5b58\u5728\uff0c\u5e76\u63d0\u4f9b\u751f\u7269\u5b66\u4e00\u81f4\u7684\u53ef\u89e3\u91ca\u6027\u7ed3\u679c\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u8bc4\u4f30\u548c\u51b3\u7b56\u3002"}}
{"id": "2512.13454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13454", "abs": "https://arxiv.org/abs/2512.13454", "authors": ["Arpit Jadon", "Joshua Niemeijer", "Yuki M. Asano"], "title": "Test-Time Modification: Inverse Domain Transformation for Robust Perception", "comment": "Preprint", "summary": "Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u5c06\u76ee\u6807\u57df\u56fe\u50cf\u6620\u5c04\u56de\u6e90\u57df\u5206\u5e03\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e0a\u5b9e\u73b0\u663e\u8457\u7684\u57df\u6cdb\u5316\u6027\u80fd\u63d0\u5347", "motivation": "\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\u5305\u542b\u5e7f\u6cdb\u7684\u89c6\u89c9\u77e5\u8bc6\u5e76\u80fd\u4ea7\u751f\u591a\u6837\u7684\u56fe\u50cf\u53d8\u4f53\uff0c\u4f46\u7528\u4e8e\u8bad\u7ec3\u6570\u636e\u589e\u5f3a\u65f6\u5408\u6210\u5168\u9762\u7684\u76ee\u6807\u57df\u53d8\u4f53\u4ecd\u7136\u7f13\u6162\u3001\u6602\u8d35\u4e14\u4e0d\u5b8c\u6574\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u57df\u6cdb\u5316\u65b9\u6cd5\u3002", "method": "\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5c06\u76ee\u6807\u57df\u56fe\u50cf\u6620\u5c04\u56de\u6e90\u57df\u5206\u5e03\uff0c\u4ec5\u9700\u6e90\u57df\u63cf\u8ff0\uff0c\u4fdd\u7559\u4efb\u52a1\u6a21\u578b\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u751f\u6210\u3002\u652f\u6301\u591a\u79cd\u751f\u6210\u6a21\u578b\u548c\u4e0b\u6e38\u6a21\u578b\uff0c\u5305\u62ec\u589e\u5f3a\u9c81\u68d2\u6027\u7684\u96c6\u6210\u53d8\u4f53\u3002", "result": "\u5728\u5206\u5272\u3001\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u9762\u5bf9\u771f\u5b9e\u5230\u771f\u5b9e\u7684\u57df\u6cdb\u5316\u573a\u666f\u548c\u672a\u77e5\u76ee\u6807\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff1aBDD100K-Night\u4e0a137%\uff0cImageNet-R\u4e0a68%\uff0cDarkZurich\u4e0a62%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d4b\u8bd5\u65f6\u57df\u6620\u5c04\u65b9\u6cd5\u4e3a\u57df\u6cdb\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.13465", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13465", "abs": "https://arxiv.org/abs/2512.13465", "authors": ["Ruiyan Wang", "Teng Hu", "Kaihui Huang", "Zihan Su", "Ran Yi", "Lizhuang Ma"], "title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence", "comment": null, "summary": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.", "AI": {"tldr": "PoseAnything\u662f\u4e00\u4e2a\u901a\u7528\u7684\u59ff\u6001\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4eba\u7c7b\u548c\u975e\u4eba\u7c7b\u89d2\u8272\uff0c\u652f\u6301\u4efb\u610f\u9aa8\u9abc\u8f93\u5165\uff0c\u5e76\u5b9e\u73b0\u4e86\u72ec\u7acb\u76f8\u673a\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u5f53\u524d\u59ff\u6001\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4ec5\u63a5\u53d7\u4eba\u4f53\u59ff\u6001\u4f5c\u4e3a\u8f93\u5165\uff0c\u5bf9\u5176\u4ed6\u4e3b\u4f53\u7684\u59ff\u6001\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9650\u5236\u4e86\u5728\u52a8\u753b\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "1) \u63d0\u51faPoseAnything\u901a\u7528\u6846\u67b6\uff1b2) \u5f15\u5165\u90e8\u4ef6\u611f\u77e5\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u5757\uff0c\u901a\u8fc7\u90e8\u4ef6\u5206\u5272\u3001\u5bf9\u5e94\u5173\u7cfb\u5efa\u7acb\u548c\u8de8\u5e27\u6ce8\u610f\u529b\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e00\u81f4\u6027\uff1b3) \u63d0\u51fa\u4e3b\u4f53\u4e0e\u76f8\u673a\u8fd0\u52a8\u89e3\u8026CFG\uff0c\u901a\u8fc7\u5206\u522b\u6ce8\u5165\u63a7\u5236\u4fe1\u606f\u5230CFG\u7684\u6b63\u8d1f\u951a\u70b9\u5b9e\u73b0\u72ec\u7acb\u76f8\u673a\u63a7\u5236\uff1b4) \u6784\u5efaXPose\u6570\u636e\u96c6\uff085\u4e07\u975e\u4eba\u7c7b\u59ff\u6001-\u89c6\u9891\u5bf9\uff09\u53ca\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\u3002", "result": "PoseAnything\u5728\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b\u548c\u975e\u4eba\u7c7b\u89d2\u8272\u89c6\u9891\uff0c\u5e76\u5b9e\u73b0\u72ec\u7acb\u7684\u76f8\u673a\u8fd0\u52a8\u63a7\u5236\u3002", "conclusion": "PoseAnything\u662f\u9996\u4e2a\u901a\u7528\u7684\u59ff\u6001\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u7a81\u7834\u4e86\u4ec5\u9650\u4eba\u4f53\u59ff\u6001\u7684\u9650\u5236\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u90e8\u4ef6\u4e00\u81f4\u6027\u6a21\u5757\u548c\u8fd0\u52a8\u89e3\u8026CFG\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610f\u9aa8\u9abc\u8f93\u5165\u7684\u652f\u6301\u548c\u72ec\u7acb\u76f8\u673a\u63a7\u5236\uff0c\u4e3a\u52a8\u753b\u5236\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2512.13492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13492", "abs": "https://arxiv.org/abs/2512.13492", "authors": ["Jiangning Zhang", "Junwei Zhu", "Teng Hu", "Yabiao Wang", "Donghao Luo", "Weijian Cao", "Zhenye Gan", "Xiaobin Hu", "Zhucun Xue", "Chengjie Wang"], "title": "Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\\times$", "comment": "Project page: https://zhangzjn.github.io/projects/T3-Video", "summary": "Native 4K (2160$\\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\\textbf{T3}$ ($\\textbf{T}$ransform $\\textbf{T}$rained $\\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an \"attention pattern\" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\\uparrow$ VQA and +0.08$\\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\\times$. Project page at https://zhangzjn.github.io/projects/T3-Video", "AI": {"tldr": "T3-Video\u662f\u4e00\u79cdTransformer\u6539\u9020\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6743\u91cd\u5171\u4eab\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u548c\u5206\u5c42\u5206\u5757\u8bbe\u8ba1\uff0c\u5728\u4e0d\u6539\u53d8\u9884\u8bad\u7ec3\u6a21\u578b\u6838\u5fc3\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u539f\u751f4K\u89c6\u9891\u751f\u6210\u901f\u5ea6\u63d0\u534710\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u539f\u751f4K\u89c6\u9891\u751f\u6210\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u7206\u70b8\u5f0f\u589e\u957f\u7684\u95ee\u9898\uff0c\u5168\u6ce8\u610f\u529b\u673a\u5236\u5728\u65f6\u7a7a\u5206\u8fa8\u7387\u589e\u52a0\u65f6\u8ba1\u7b97\u91cf\u5448\u4e8c\u6b21\u65b9\u589e\u957f\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51faT3\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6743\u91cd\u5171\u4eab\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u3001\u5206\u5c42\u5206\u5757\u548c\u8f74\u4fdd\u6301\u5168\u6ce8\u610f\u529b\u8bbe\u8ba1\uff0c\u4ec5\u7528\u9002\u5ea6\u8ba1\u7b97\u548c\u6570\u636e\u5c31\u80fd\u5b9e\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u7684\"\u6ce8\u610f\u529b\u6a21\u5f0f\"\u8f6c\u6362\uff0c\u800c\u4e0d\u6539\u53d8\u5176\u6838\u5fc3\u67b6\u6784\u3002", "result": "\u57284K-VBench\u6d4b\u8bd5\u4e2d\uff0cT3-Video\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1a\u5728\u6027\u80fd\u63d0\u5347\u65b9\u9762\uff08VQA\u63d0\u53474.29\uff0cVTC\u63d0\u53470.08\uff09\uff0c\u540c\u65f6\u5c06\u539f\u751f4K\u89c6\u9891\u751f\u6210\u901f\u5ea6\u52a0\u901f\u8d85\u8fc710\u500d\u3002", "conclusion": "T3-Video\u901a\u8fc7\u521b\u65b0\u7684Transformer\u6539\u9020\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e864K\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13507", "abs": "https://arxiv.org/abs/2512.13507", "authors": ["Siyan Chen", "Yanfei Chen", "Ying Chen", "Zhuo Chen", "Feng Cheng", "Xuyan Chi", "Jian Cong", "Qinpeng Cui", "Qide Dong", "Junliang Fan", "Jing Fang", "Zetao Fang", "Chengjian Feng", "Han Feng", "Mingyuan Gao", "Yu Gao", "Qiushan Guo", "Boyang Hao", "Qingkai Hao", "Bibo He", "Qian He", "Tuyen Hoang", "Ruoqing Hu", "Xi Hu", "Weilin Huang", "Zhaoyang Huang", "Zhongyi Huang", "Siqi Jiang", "Wei Jiang", "Yunpu Jiang", "Zhuo Jiang", "Ashley Kim", "Jianan Kong", "Zhichao Lai", "Shanshan Lao", "Ai Li", "Feiya Li", "Gen Li", "Huixia Li", "JiaShi Li", "Liang Li", "Ming Li", "Tao Li", "Xian Li", "Xiaojie Li", "Xiaoyang Li", "Xingxing Li", "Yameng Li", "Yifu Li", "Yiying Li", "Chao Liang", "Ying Liang", "Zhiqiang Liang", "Wang Liao", "Yalin Liao", "Heng Lin", "Kengyu Lin", "Shanchuan Lin", "Xi Lin", "Zhijie Lin", "Feng Ling", "Fangfang Liu", "Gaohong Liu", "Jiawei Liu", "Jie Liu", "Shouda Liu", "Shu Liu", "Sichao Liu", "Songwei Liu", "Xin Liu", "Xue Liu", "Yibo Liu", "Zikun Liu", "Zuxi Liu", "Junlin Lyu", "Lecheng Lyu", "Qian Lyu", "Han Mu", "Xiaonan Nie", "Jingzhe Ning", "Xitong Pan", "Yanghua Peng", "Lianke Qin", "Xueqiong Qu", "Yuxi Ren", "Yuchen Shen", "Guang Shi", "Lei Shi", "Yan Song", "Yinglong Song", "Fan Sun", "Li Sun", "Renfei Sun", "Zeyu Sun", "Wenjing Tang", "Zirui Tao", "Feng Wang", "Furui Wang", "Jinran Wang", "Junkai Wang", "Ke Wang", "Kexin Wang", "Qingyi Wang", "Rui Wang", "Sen Wang", "Shuai Wang", "Tingru Wang", "Weichen Wang", "Xin Wang", "Yanhui Wang", "Yue Wang", "Yuping Wang", "Yuxuan Wang", "Ziyu Wang", "Guoqiang Wei", "Wanru Wei", "Di Wu", "Guohong Wu", "Hanjie Wu", "Jian Wu", "Jie Wu", "Ruolan Wu", "Xinglong Wu", "Yonghui Wu", "Ruiqi Xia", "Liang Xiang", "Fei Xiao", "XueFeng Xiao", "Pan Xie", "Shuangyi Xie", "Shuang Xu", "Jinlan Xue", "Bangbang Yang", "Ceyuan Yang", "Jiaqi Yang", "Runkai Yang", "Tao Yang", "Yang Yang", "Yihang Yang", "ZhiXian Yang", "Ziyan Yang", "Yifan Yao", "Zilyu Ye", "Bowen Yu", "Chujie Yuan", "Linxiao Yuan", "Sichun Zeng", "Weihong Zeng", "Xuejiao Zeng", "Yan Zeng", "Chuntao Zhang", "Heng Zhang", "Jingjie Zhang", "Kuo Zhang", "Liang Zhang", "Liying Zhang", "Manlin Zhang", "Ting Zhang", "Weida Zhang", "Xiaohe Zhang", "Xinyan Zhang", "Yan Zhang", "Yuan Zhang", "Zixiang Zhang", "Fengxuan Zhao", "Huating Zhao", "Yang Zhao", "Hao Zheng", "Jianbin Zheng", "Xiaozheng Zheng", "Yangyang Zheng", "Yijie Zheng", "Jiexin Zhou", "Kuan Zhu", "Shenhan Zhu", "Wenjia Zhu", "Benhui Zou", "Feilong Zuo"], "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model", "comment": "Seedance 1.5 pro Technical Report", "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.", "AI": {"tldr": "Seedance 1.5 pro\u662f\u4e00\u4e2a\u7528\u4e8e\u539f\u751f\u8054\u5408\u97f3\u9891-\u89c6\u9891\u751f\u6210\u7684\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u5206\u652fDiffusion Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8054\u5408\u6a21\u5757\u548c\u591a\u9636\u6bb5\u6570\u636e\u7ba1\u9053\u5b9e\u73b0\u5353\u8d8a\u7684\u89c6\u542c\u540c\u6b65\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u7edf\u4e00\u7684\u89c6\u542c\u751f\u6210\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u539f\u751f\u3001\u8054\u5408\u97f3\u9891-\u89c6\u9891\u751f\u6210\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u540c\u6b65\u6548\u679c\u548c\u751f\u6210\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652fDiffusion Transformer\u67b6\u6784\uff0c\u96c6\u6210\u8de8\u6a21\u6001\u8054\u5408\u6a21\u5757\u548c\u4e13\u95e8\u7684\u591a\u9636\u6bb5\u6570\u636e\u7ba1\u9053\uff1b\u5b9e\u65bd\u7ec6\u81f4\u7684\u8bad\u7ec3\u540e\u4f18\u5316\uff0c\u5305\u62ec\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u57fa\u4e8e\u591a\u7ef4\u5956\u52b1\u6a21\u578b\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\uff1b\u5f15\u5165\u52a0\u901f\u6846\u67b6\u63d0\u5347\u63a8\u7406\u901f\u5ea610\u500d\u4ee5\u4e0a\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u89c6\u542c\u540c\u6b65\u548c\u751f\u6210\u8d28\u91cf\uff0c\u5177\u5907\u7cbe\u786e\u7684\u591a\u8bed\u8a00\u548c\u65b9\u8a00\u5507\u5f62\u540c\u6b65\u3001\u52a8\u6001\u7535\u5f71\u7ea7\u6444\u50cf\u673a\u63a7\u5236\u3001\u589e\u5f3a\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u6210\u4e3a\u4e13\u4e1a\u7ea7\u5185\u5bb9\u521b\u4f5c\u7684\u5f3a\u5927\u5f15\u64ce\u3002", "conclusion": "Seedance 1.5 pro\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u539f\u751f\u8054\u5408\u97f3\u9891-\u89c6\u9891\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5728\u89c6\u542c\u540c\u6b65\u3001\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u4e13\u4e1a\u5185\u5bb9\u521b\u4f5c\u3002"}}
{"id": "2512.13511", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13511", "abs": "https://arxiv.org/abs/2512.13511", "authors": ["Piyush Bagad", "Andrew Zisserman"], "title": "TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding", "comment": "18 Pages. Project page at http://bpiyush.github.io/tara-website", "summary": "Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.", "AI": {"tldr": "TARA\u662f\u4e00\u79cd\u65e0\u9700\u89c6\u9891\u6570\u636e\u3001\u901a\u8fc7\u9002\u914d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u65f6\u95f4\u611f\u77e5\u89c6\u9891-\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5728\u65f6\u95f4\u611f\u77e5\u68c0\u7d22\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728\u5426\u5b9a\u7406\u89e3\u3001\u52a8\u8bcd\u526f\u8bcd\u7406\u89e3\u7b49\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u6784\u5efa\u901a\u7528\u7684\u65f6\u95f4\u611f\u77e5\u89c6\u9891-\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u65f6\u95f4\u611f\u77e5\u68c0\u7d22\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u65f6\u95f4\u987a\u5e8f\u654f\u611f\u7684\u52a8\u4f5c\u8bc6\u522b\u3002", "method": "\u63d0\u51faTARA\uff08\u65f6\u95f4\u611f\u77e5\u68c0\u7d22\u9002\u914d\uff09\u65b9\u6cd5\uff0c\u5728\u4e0d\u4f7f\u7528\u4efb\u4f55\u89c6\u9891\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9002\u914d\u4e3a\u65f6\u95f4\u611f\u77e5\u89c6\u9891-\u6587\u672c\u5d4c\u5165\u6a21\u578b\u3002\u901a\u8fc7\u65f6\u95f4\u5bf9\u7acb\uff08\u624b\u6027\uff09\u52a8\u4f5c\u4f5c\u4e3a\u56f0\u96be\u8d1f\u6837\u672c\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "TARA\u5728\u65f6\u95f4\u611f\u77e5\u68c0\u7d22\u57fa\u51c6\u4e0a\u8d85\u8d8a\u6240\u6709\u73b0\u6709\u89c6\u9891-\u6587\u672c\u6a21\u578b\uff0c\u540c\u65f6\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u53d6\u5f97\u5f3a\u52b2\u7ed3\u679c\u3002\u6a21\u578b\u8fd8\u5c55\u73b0\u51fa\u5426\u5b9a\u611f\u77e5\u80fd\u529b\uff0c\u5728\u52a8\u8bcd\u548c\u526f\u8bcd\u7406\u89e3\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TARA\u4ea7\u751f\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u901a\u7528\u3001\u65f6\u95f4\u611f\u77e5\u7684\u89c6\u9891-\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u5177\u6709\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5728\u65f6\u95f4\u611f\u77e5\u3001\u5426\u5b9a\u7406\u89e3\u548c\u52a8\u4f5c\u7406\u89e3\u65b9\u9762\u5747\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2512.13534", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13534", "abs": "https://arxiv.org/abs/2512.13534", "authors": ["Marianne Rakic", "Siyu Gai", "Etienne Chollet", "John V. Guttag", "Adrian V. Dalca"], "title": "Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains", "comment": "Accepted at NeurIPS 2025. Code available at: https://github.com/mariannerakic/Pancakes", "summary": "A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.", "AI": {"tldr": "Pancakes\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u4e3a\u533b\u5b66\u56fe\u50cf\u751f\u6210\u591a\u79cd\u8bed\u4e49\u4e00\u81f4\u7684\u5206\u5272\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u53ea\u80fd\u652f\u6301\u5355\u4e00\u534f\u8bae\u6216\u9700\u8981\u624b\u52a8\u63d0\u793a\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u53ef\u4ee5\u6839\u636e\u4e0d\u540c\u5e94\u7528\u9700\u6c42\u8fdb\u884c\u591a\u79cd\u6709\u610f\u4e49\u7684\u5206\u5272\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u5206\u5272\u6a21\u578b\u8981\u4e48\u53ea\u652f\u6301\u5355\u4e00\u8bad\u7ec3\u534f\u8bae\uff0c\u8981\u4e48\u9700\u8981\u4eba\u5de5\u624b\u52a8\u6307\u5b9a\u5206\u5272\u65b9\u6848\uff0c\u7f3a\u4e4f\u81ea\u52a8\u751f\u6210\u591a\u79cd\u8bed\u4e49\u4e00\u81f4\u5206\u5272\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faPancakes\u6846\u67b6\uff0c\u5f15\u5165\u65b0\u7684\u95ee\u9898\u8868\u8ff0\uff0c\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u533b\u5b66\u56fe\u50cf\u57df\u4e2d\u81ea\u52a8\u751f\u6210\u591a\u6807\u7b7e\u5206\u5272\u56fe\uff0c\u652f\u6301\u591a\u79cd\u53ef\u80fd\u7684\u5206\u5272\u534f\u8bae\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5173\u56fe\u50cf\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u57287\u4e2a\u4fdd\u7559\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPancakes\u5728\u751f\u6210\u591a\u79cd\u8bed\u4e49\u4e00\u81f4\u7684\u6574\u56fe\u5206\u5272\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "Pancakes\u6846\u67b6\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u591a\u534f\u8bae\u81ea\u52a8\u5206\u5272\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u73b0\u6709\u57fa\u7840\u6a21\u578b\u65e0\u6cd5\u8fbe\u5230\u7684\u529f\u80fd\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13573", "abs": "https://arxiv.org/abs/2512.13573", "authors": ["Tao Zhang", "Ziqi Zhang", "Zongyang Ma", "Yuxin Chen", "Bing Li", "Chunfeng Yuan", "Guangting Wang", "Fengyun Rao", "Ying Shan", "Weiming Hu"], "title": "MMhops-R1: Multimodal Multi-hop Reasoning", "comment": "Acceped by AAAI 2026", "summary": "The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.", "AI": {"tldr": "MMhops\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u591a\u8df3\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542bBridging\u548cComparison\u4e24\u79cd\u4efb\u52a1\u683c\u5f0f\uff0c\u9700\u8981\u6a21\u578b\u901a\u8fc7\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u6784\u5efa\u590d\u6742\u63a8\u7406\u94fe\u3002\u4f5c\u8005\u63d0\u51fa\u4e86MMhops-R1\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u52a8\u6001\u63a8\u7406\u8def\u5f84\u89c4\u5212\uff0c\u5728MMhops\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u6b65\u63a8\u7406\uff0c\u7f3a\u4e4f\u80fd\u591f\u8bc4\u4f30\u548c\u9a71\u52a8\u591a\u8df3\u63a8\u7406\u80fd\u529b\u7684\u590d\u6742\u57fa\u51c6\u3002\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u6311\u6218\u9700\u8981\u6a21\u578b\u80fd\u591f\u8de8\u591a\u79cd\u6a21\u6001\u548c\u5916\u90e8\u77e5\u8bc6\u8fdb\u884c\u8fed\u4ee3\u5f0f\u4fe1\u606f\u6574\u5408\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86MMhops-R1\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u63a8\u7406\u8def\u5f84\u3001\u5236\u5b9a\u9488\u5bf9\u6027\u67e5\u8be2\u5e76\u6574\u5408\u591a\u5c42\u7ea7\u4fe1\u606f\u3002\u6846\u67b6\u5305\u542b\u52a8\u6001\u63a8\u7406\u8def\u5f84\u89c4\u5212\u548c\u591a\u6a21\u6001\u77e5\u8bc6\u6574\u5408\u673a\u5236\u3002", "result": "MMhops-R1\u5728MMhops\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u89c4\u5212\u548c\u591a\u6a21\u6001\u77e5\u8bc6\u6574\u5408\u5bf9\u590d\u6742\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002\u540c\u65f6\uff0c\u8be5\u6846\u67b6\u5728\u9700\u8981\u56fa\u5b9a\u8df3\u6570\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8d21\u732e\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u65b0\u57fa\u51c6MMhops\u548c\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578bMMhops-R1\uff0c\u5c06\u53d1\u5e03\u76f8\u5173\u4ee3\u7801\u3001\u6570\u636e\u548c\u6743\u91cd\u4ee5\u63a8\u52a8\u8be5\u5173\u952e\u9886\u57df\u7684\u7814\u7a76\u3002\u52a8\u6001\u89c4\u5212\u548c\u591a\u6a21\u6001\u77e5\u8bc6\u6574\u5408\u662f\u590d\u6742\u63a8\u7406\u7684\u5173\u952e\u8981\u7d20\u3002"}}
{"id": "2512.13597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13597", "abs": "https://arxiv.org/abs/2512.13597", "authors": ["Christophe Bolduc", "Julien Philip", "Li Ma", "Mingming He", "Paul Debevec", "Jean-Fran\u00e7ois Lalonde"], "title": "Lighting in Motion: Spatiotemporal HDR Lighting Estimation", "comment": null, "summary": "We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.", "AI": {"tldr": "LiMo\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65f6\u7a7a\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e0d\u540c\u66dd\u5149\u4e0b\u7684\u955c\u9762\u548c\u6f2b\u53cd\u5c04\u7403\u4f53\uff0c\u7ed3\u5408\u6df1\u5ea6\u548c\u51e0\u4f55\u6761\u4ef6\uff0c\u5b9e\u73b0\u9ad8\u7ec6\u8282\u9884\u6d4b\u548c\u51c6\u786e\u7167\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u540c\u65f6\u5b9e\u73b0\u771f\u5b9e\u9ad8\u9891\u7ec6\u8282\u9884\u6d4b\u548c\u51c6\u786e\u7167\u5ea6\u4f30\u8ba1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u8fd9\u4e24\u4e2a\u65b9\u9762\u7684\u65f6\u7a7a\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "1. \u57fa\u4e8e\u8f93\u51653D\u4f4d\u7f6e\u751f\u6210\u4e0d\u540c\u66dd\u5149\u4e0b\u7684\u955c\u9762\u548c\u6f2b\u53cd\u5c04\u7403\u4f53\u96c6\u5408\uff1b2. \u5728\u5927\u89c4\u6a21\u5b9a\u5236\u5ba4\u5185\u5916\u573a\u666f\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u73b0\u6709\u6269\u6563\u6a21\u578b\uff1b3. \u5f15\u5165\u65b0\u7684\u51e0\u4f55\u6761\u4ef6\u63d0\u4f9b\u573a\u666f\u5230\u76ee\u68073D\u4f4d\u7f6e\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\uff1b4. \u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u5c06\u4e0d\u540c\u66dd\u5149\u7684\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u9884\u6d4b\u5408\u5e76\u4e3a\u5355\u4e2aHDRI\u56fe\u3002", "result": "LiMo\u5728\u7a7a\u95f4\u63a7\u5236\u548c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u9ad8\u9891\u7ec6\u8282\u9884\u6d4b\u548c\u51c6\u786e\u7684\u7167\u5ea6\u4f30\u8ba1\u3002", "conclusion": "LiMo\u901a\u8fc7\u521b\u65b0\u7684\u51e0\u4f55\u6761\u4ef6\u548c\u591a\u66dd\u5149\u7403\u4f53\u751f\u6210\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u7a7a\u5149\u7167\u4f30\u8ba1\u4e2d\u540c\u65f6\u5b9e\u73b0\u7ec6\u8282\u548c\u51c6\u786e\u6027\u5e73\u8861\u7684\u6311\u6218\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13604", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13604", "abs": "https://arxiv.org/abs/2512.13604", "authors": ["Jianxiong Gao", "Zhaoxi Chen", "Xian Liu", "Junhao Zhuang", "Chengming Xu", "Jianfeng Feng", "Yu Qiao", "Yanwei Fu", "Chenyang Si", "Ziwei Liu"], "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "comment": "Project Page: https://vchitect.github.io/LongVie2-project/", "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "AI": {"tldr": "LongVie 2\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5f15\u5bfc\u3001\u9000\u5316\u611f\u77e5\u8bad\u7ec3\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u3001\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u4e16\u754c\u5efa\u6a21\uff0c\u652f\u6301\u957f\u8fbe5\u5206\u949f\u7684\u8fde\u7eed\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u6784\u5efa\u89c6\u9891\u4e16\u754c\u6a21\u578b\u662f\u5b9e\u73b0\u901a\u7528\u65f6\u7a7a\u667a\u80fd\u7684\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u6b65\u9aa4\u3002\u4e16\u754c\u6a21\u578b\u9700\u8981\u5177\u5907\u4e09\u4e2a\u5173\u952e\u5c5e\u6027\uff1a\u53ef\u63a7\u6027\u3001\u957f\u671f\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u65b9\u6cd5\uff1a1) \u591a\u6a21\u6001\u5f15\u5bfc\uff1a\u6574\u5408\u5bc6\u96c6\u548c\u7a00\u758f\u63a7\u5236\u4fe1\u53f7\u63d0\u4f9b\u9690\u5f0f\u4e16\u754c\u7ea7\u76d1\u7763\uff1b2) \u8f93\u5165\u5e27\u7684\u9000\u5316\u611f\u77e5\u8bad\u7ec3\uff1a\u5f25\u5408\u8bad\u7ec3\u4e0e\u957f\u671f\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff1b3) \u5386\u53f2\u4e0a\u4e0b\u6587\u5f15\u5bfc\uff1a\u5bf9\u9f50\u76f8\u90bb\u7247\u6bb5\u95f4\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "LongVie 2\u5728\u957f\u8ddd\u79bb\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u957f\u8fbe5\u5206\u949f\u7684\u8fde\u7eed\u89c6\u9891\u751f\u6210\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u5305\u542b100\u4e2a\u9ad8\u5206\u8fa8\u7387\u4e00\u5206\u949f\u89c6\u9891\u7684LongVGenBench\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "LongVie 2\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u80fd\u529b\uff0c\u5728\u53ef\u63a7\u6027\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u5411\u7edf\u4e00\u7684\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.13609", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13609", "abs": "https://arxiv.org/abs/2512.13609", "authors": ["Shweta Mahajan", "Shreya Kadambi", "Hoang Le", "Munawar Hayat", "Fatih Porikli"], "title": "Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models", "comment": null, "summary": "We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.", "AI": {"tldr": "Do-Undo\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u7531\u771f\u5b9e\u4e16\u754c\u52a8\u4f5c\u9a71\u52a8\u7684\u7269\u7406\u5408\u7406\u573a\u666f\u53d8\u6362\u65b9\u9762\u7684\u5173\u952e\u7f3a\u9677\uff0c\u8981\u6c42\u6a21\u578b\u6a21\u62df\u7269\u7406\u52a8\u4f5c\u7684\u7ed3\u679c\u5e76\u51c6\u786e\u53cd\u8f6c\u5b83\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u53ef\u9006\u6027\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u586b\u8865\u4e86\u6a21\u578b\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u56e0\u679c\u5173\u7cfb\u548c\u7269\u7406\u5408\u7406\u573a\u666f\u53d8\u6362\u7684\u7a7a\u767d\uff0c\u8fd9\u5bf9\u5177\u8eabAI\u3001\u673a\u5668\u4eba\u548c\u7269\u7406\u611f\u77e5\u751f\u6210\u5efa\u6a21\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4ece\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u6784\u5efa\u5927\u89c4\u6a21\u53ef\u9006\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u8bad\u7ec3\u7b56\u7565\u4ee5\u589e\u5f3a\u52a8\u4f5c\u5b9a\u4f4d\u7684\u4e00\u81f4\u6027\uff0c\u901a\u8fc7Do-Undo\u4efb\u52a1\u8981\u6c42\u6a21\u578b\u6a21\u62df\u7269\u7406\u52a8\u4f5c\u7ed3\u679c\u5e76\u51c6\u786e\u53cd\u8f6c\u5b83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u7269\u7406\u53ef\u9006\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u8be5\u4efb\u52a1\u7684\u91cd\u8981\u6027\uff0cDo-Undo\u4e3a\u8bc4\u4f30\u548c\u63a8\u8fdb\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u7684\u7269\u7406\u63a8\u7406\u63d0\u4f9b\u4e86\u76f4\u89c2\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "Do-Undo\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u5408\u7406\u573a\u666f\u53d8\u6362\u7406\u89e3\u65b9\u9762\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2512.13639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13639", "abs": "https://arxiv.org/abs/2512.13639", "authors": ["Michal Nazarczuk", "Thomas Tanay", "Arthur Moreau", "Zhensong Zhang", "Eduardo P\u00e9rez-Pellitero"], "title": "Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All", "comment": "Project page: https://charge-benchmark.github.io/", "summary": "This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u65b0\u9896\u89c6\u89d2\u5408\u6210\u7684\u5168\u65b0\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u52a8\u753b\u7535\u5f71\u5236\u4f5c\uff0c\u5305\u542b\u52a8\u6001\u573a\u666f\u3001\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u591a\u6a21\u6001\u3001\u5305\u542b\u52a8\u6001\u573a\u666f\u7684\u65b0\u9896\u89c6\u89d2\u5408\u6210\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e864D\u573a\u666f\u91cd\u5efa\u548c\u89c6\u89d2\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "method": "\u4ece\u9ad8\u8d28\u91cf\u52a8\u753b\u7535\u5f71\u4e2d\u751f\u6210\u6570\u636e\u96c6\uff0c\u63d0\u4f9bRGB\u56fe\u50cf\u3001\u6df1\u5ea6\u3001\u8868\u9762\u6cd5\u7ebf\u3001\u7269\u4f53\u5206\u5272\u548c\u5149\u6d41\u7b49\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u5e76\u7ec4\u7ec7\u6210\u5bc6\u96c6\u591a\u89c6\u89d2\u3001\u7a00\u758f\u76f8\u673a\u548c\u5355\u76ee\u89c6\u9891\u4e09\u79cd\u57fa\u51c6\u6d4b\u8bd5\u573a\u666f\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u89c6\u89c9\u4e30\u5bcc\u3001\u6807\u6ce8\u8d28\u91cf\u9ad8\u3001\u5b9e\u9a8c\u8bbe\u7f6e\u591a\u6837\u7684\u6570\u636e\u96c6\uff0c\u80fd\u591f\u652f\u6301\u4ece\u5bc6\u96c6\u5230\u7a00\u758f\u4e0d\u540c\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u89c6\u89d2\u5408\u6210\u7814\u7a76\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u63a8\u8fdb\u89c6\u89d2\u5408\u6210\u548c3D\u89c6\u89c9\u7814\u7a76\u63d0\u4f9b\u4e86\u72ec\u7279\u8d44\u6e90\uff0c\u80fd\u591f\u652f\u6301\u591a\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u6a21\u578b\u8bc4\u4f30\u3002"}}
{"id": "2512.13665", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13665", "abs": "https://arxiv.org/abs/2512.13665", "authors": ["Wenhan Chen", "Sezer Karaoglu", "Theo Gevers"], "title": "Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency", "comment": null, "summary": "Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.", "AI": {"tldr": "\u63d0\u51faGrab-3D\u6846\u67b6\uff0c\u5229\u75283D\u51e0\u4f55\u4e00\u81f4\u6027\u68c0\u6d4bAI\u751f\u6210\u89c6\u9891\uff0c\u901a\u8fc7\u6d88\u5931\u70b9\u5206\u6790\u51e0\u4f55\u6a21\u5f0f\u5dee\u5f02\uff0c\u5728\u9759\u6001\u573a\u666f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u6269\u6563\u751f\u6210\u6280\u672f\u80fd\u4ea7\u751f\u9ad8\u5ea6\u903c\u771f\u7684\u89c6\u9891\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5bf93D\u51e0\u4f55\u6a21\u5f0f\u63a2\u7d22\u6709\u9650\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u68c0\u6d4b\u673a\u5236", "method": "\u4f7f\u7528\u6d88\u5931\u70b9\u4f5c\u4e3a3D\u51e0\u4f55\u6a21\u5f0f\u7684\u663e\u5f0f\u8868\u793a\uff1b\u63d0\u51faGrab-3D\u6846\u67b6\uff0c\u5305\u542b\u51e0\u4f55\u4f4d\u7f6e\u7f16\u7801\u3001\u65f6\u7a7a\u51e0\u4f55\u6ce8\u610f\u529b\u3001EMA\u51e0\u4f55\u5206\u7c7b\u5668\u5934\uff1b\u6784\u5efa\u9759\u6001\u573a\u666fAI\u751f\u6210\u89c6\u9891\u6570\u636e\u96c6", "result": "Grab-3D\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u68c0\u6d4b\u5668\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u751f\u6210\u5668\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc73D\u51e0\u4f55\u65f6\u95f4\u4e00\u81f4\u6027\u5206\u6790\uff0cGrab-3D\u4e3aAI\u751f\u6210\u89c6\u9891\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u51e0\u4f55\u611f\u77e5\u6846\u67b6\uff0c\u5728\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02"}}
{"id": "2512.13671", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13671", "abs": "https://arxiv.org/abs/2512.13671", "authors": ["Junwen Miao", "Penghui Du", "Yi Liu", "Yu Wang", "Yan Wang"], "title": "AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection", "comment": null, "summary": "Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.", "AI": {"tldr": "AgentIAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u5de5\u5177\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u89c6\u89c9\u68c0\u67e5\u3001\u611f\u77e5\u7f29\u653e\u548c\u6bd4\u8f83\u68c0\u7d22\u6765\u63d0\u5347\u5bf9\u5c0f\u7f3a\u9677\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5728MMAD\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.62%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6b63\u5e38\u53c2\u8003\u6837\u672c\u7a00\u7f3a\u548c\u7f3a\u9677\u5fae\u5c0f\u3001\u5c40\u90e8\u5316\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u5355\u901a\u9053\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f80\u5f80\u5ffd\u7565\u5c0f\u5f02\u5e38\uff0c\u7f3a\u4e4f\u4e0e\u6807\u51c6\u6b63\u5e38\u6a21\u5f0f\u8fdb\u884c\u663e\u5f0f\u6bd4\u8f83\u7684\u673a\u5236\u3002", "method": "\u63d0\u51faAgentIAD\u6846\u67b6\uff0c\u5305\u542b\u611f\u77e5\u7f29\u653e\u5668\uff08PZ\uff09\u8fdb\u884c\u5c40\u90e8\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u6bd4\u8f83\u68c0\u7d22\u5668\uff08CR\uff09\u5728\u8bc1\u636e\u6a21\u7cca\u65f6\u67e5\u8be2\u6b63\u5e38\u6837\u672c\u3002\u4f7f\u7528MMAD\u6570\u636e\u96c6\u6784\u5efa\u7ed3\u6784\u5316\u7684\u611f\u77e5\u548c\u6bd4\u8f83\u8f68\u8ff9\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u540e\u63a5\u5f3a\u5316\u5b66\u4e60\uff0c\u91c7\u7528\u611f\u77e5\u5956\u52b1\u548c\u884c\u4e3a\u5956\u52b1\u7684\u53cc\u91cd\u5956\u52b1\u8bbe\u8ba1\u3002", "result": "\u5728MMAD\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.62%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u521b\u4e0b\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u57fa\u4e8eMLLM\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4ea7\u751f\u900f\u660e\u53ef\u89e3\u91ca\u7684\u68c0\u67e5\u8f68\u8ff9\u3002", "conclusion": "AgentIAD\u901a\u8fc7\u5de5\u5177\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u6846\u67b6\u5b9e\u73b0\u4e86\u591a\u9636\u6bb5\u89c6\u89c9\u68c0\u67e5\uff0c\u80fd\u591f\u901a\u8fc7\u9010\u6b65\u89c2\u5bdf\u3001\u7f29\u653e\u548c\u9a8c\u8bc1\u6765\u5b8c\u5584\u5224\u65ad\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5c0f\u7f3a\u9677\u68c0\u6d4b\u548c\u6b63\u5e38\u6a21\u5f0f\u6bd4\u8f83\u95ee\u9898\u3002"}}
{"id": "2512.13677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13677", "abs": "https://arxiv.org/abs/2512.13677", "authors": ["Xiaohu Huang", "Hao Zhou", "Qiangpeng Yang", "Shilei Wen", "Kai Han"], "title": "JoVA: Unified Multimodal Learning for Joint Video-Audio Generation", "comment": "Project page: \\url{https://visual-ai.github.io/jova}", "summary": "In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.", "AI": {"tldr": "JoVA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u9891-\u97f3\u9891\u8054\u5408\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u65e0\u9700\u989d\u5916\u5bf9\u9f50\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u5634\u90e8\u533a\u57df\u635f\u5931\u6765\u63d0\u5347\u5507\u8bed\u540c\u6b65\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1) \u5927\u591a\u53ea\u80fd\u751f\u6210\u73af\u5883\u97f3\uff0c\u7f3a\u4e4f\u751f\u6210\u4e0e\u5507\u90e8\u52a8\u4f5c\u540c\u6b65\u7684\u4eba\u7c7b\u8bed\u97f3\u7684\u80fd\u529b\uff1b2) \u73b0\u6709\u7edf\u4e00\u89c6\u9891-\u97f3\u9891\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u663e\u5f0f\u878d\u5408\u6216\u6a21\u6001\u7279\u5b9a\u5bf9\u9f50\u6a21\u5757\uff0c\u589e\u52a0\u4e86\u67b6\u6784\u590d\u6742\u6027\u5e76\u524a\u5f31\u4e86\u539f\u59cbTransformer\u7684\u7b80\u6d01\u6027\u3002", "method": "JoVA\u91c7\u7528\u8054\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728Transformer\u5c42\u5185\u5bf9\u89c6\u9891\u548c\u97f3\u9891token\u8fdb\u884c\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u65e0\u9700\u989d\u5916\u5bf9\u9f50\u6a21\u5757\u3002\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u5634\u90e8\u533a\u57df\u635f\u5931\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u589e\u5f3a\u5bf9\u5173\u952e\u5634\u90e8\u533a\u57df\u7684\u76d1\u7763\uff0c\u800c\u4e0d\u5f71\u54cd\u67b6\u6784\u7b80\u6d01\u6027\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cJoVA\u5728\u5507\u8bed\u540c\u6b65\u51c6\u786e\u6027\u3001\u8bed\u97f3\u8d28\u91cf\u548c\u6574\u4f53\u89c6\u9891-\u97f3\u9891\u751f\u6210\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u6216\u4e0e\u6700\u5148\u8fdb\u7684\u7edf\u4e00\u65b9\u6cd5\u548c\u97f3\u9891\u9a71\u52a8\u65b9\u6cd5\u76f8\u7ade\u4e89\u3002", "conclusion": "JoVA\u4f5c\u4e3a\u4e00\u4e2a\u4f18\u96c5\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5507\u8bed\u540c\u6b65\u548c\u67b6\u6784\u7b80\u6d01\u6027\u65b9\u9762\u7684\u9650\u5236\u3002"}}
{"id": "2512.13678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13678", "abs": "https://arxiv.org/abs/2512.13678", "authors": ["Ziqi Ma", "Hongqiao Chen", "Yisong Yue", "Georgia Gkioxari"], "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D", "comment": "https://glab-caltech.github.io/steer3d/", "summary": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/", "AI": {"tldr": "Steer3D\uff1a\u4e00\u79cd\u4e3a\u56fe\u50cf\u52303D\u6a21\u578b\u6dfb\u52a0\u6587\u672c\u53ef\u64cd\u63a7\u6027\u7684\u524d\u9988\u65b9\u6cd5\uff0c\u652f\u6301\u7528\u8bed\u8a00\u7f16\u8f91\u751f\u6210\u76843D\u8d44\u4ea7", "motivation": "AI\u751f\u6210\u76843D\u8d44\u4ea7\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u6613\u4e8e\u7f16\u8f91\u7684\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u7684\u6587\u672c\u64cd\u63a7\u6027", "method": "\u53d7ControlNet\u542f\u53d1\uff0c\u5c06\u6587\u672c\u64cd\u63a7\u6027\u9002\u914d\u5230\u56fe\u50cf\u52303D\u751f\u6210\uff1b\u6784\u5efa\u81ea\u52a8\u6570\u636e\u751f\u6210\u5f15\u64ce\uff0c\u91c7\u7528\u57fa\u4e8e\u6d41\u5339\u914d\u8bad\u7ec3\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848", "result": "\u76f8\u6bd4\u7ade\u4e89\u65b9\u6cd5\uff0cSteer3D\u66f4\u5fe0\u5b9e\u9075\u5faa\u8bed\u8a00\u6307\u4ee4\uff0c\u4e0e\u539f\u59cb3D\u8d44\u4ea7\u4fdd\u6301\u66f4\u597d\u4e00\u81f4\u6027\uff0c\u901f\u5ea6\u63d0\u53472.4\u500d\u523028.5\u500d", "conclusion": "Steer3D\u8bc1\u660e\u53ef\u4ee5\u752810\u4e07\u6570\u636e\u4e3a\u9884\u8bad\u7ec3\u56fe\u50cf\u52303D\u751f\u6210\u6a21\u578b\u6dfb\u52a0\u65b0\u6a21\u6001\uff08\u6587\u672c\uff09\u7684\u64cd\u63a7\u6027"}}
{"id": "2512.13680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13680", "abs": "https://arxiv.org/abs/2512.13680", "authors": ["Tianye Ding", "Yiming Xie", "Yiqing Liang", "Moitreya Chatterjee", "Pedro Miraldo", "Huaizu Jiang"], "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction", "comment": "16 pages", "summary": "Recent feed-forward reconstruction models like VGGT and $\u03c0^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$", "AI": {"tldr": "LASER\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6d41\u5f0f\u89c6\u9891\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u95f4\u5c3a\u5ea6\u5bf9\u9f50\u5c06\u79bb\u7ebf\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u5f0f\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u4e8c\u6b21\u5185\u5b58\u590d\u6742\u5ea6\u65e0\u6cd5\u5904\u7406\u6d41\u5f0f\u89c6\u9891\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u524d\u9988\u91cd\u5efa\u6a21\u578b\uff08\u5982VGGT\u548c\u03c0\u00b3\uff09\u867d\u7136\u91cd\u5efa\u8d28\u91cf\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u4e8c\u6b21\u5185\u5b58\u590d\u6742\u5ea6\u65e0\u6cd5\u5904\u7406\u6d41\u5f0f\u89c6\u9891\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u7684\u6d41\u5f0f\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\uff0c\u4e14\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6700\u5148\u8fdb\u79bb\u7ebf\u6a21\u578b\u7684\u51e0\u4f55\u5148\u9a8c\u3002", "method": "\u63d0\u51faLASER\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u8fde\u7eed\u65f6\u95f4\u7a97\u53e3\u7684\u5bf9\u9f50\u9884\u6d4b\u5c06\u79bb\u7ebf\u91cd\u5efa\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u5f0f\u7cfb\u7edf\u3002\u9488\u5bf9\u7b80\u5355\u76f8\u4f3c\u53d8\u6362\u5bf9\u9f50\u56e0\u5355\u76ee\u5c3a\u5ea6\u6b67\u4e49\u5bfc\u81f4\u4e0d\u540c\u573a\u666f\u5c42\u6df1\u5ea6\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5f15\u5165\u5c42\u95f4\u5c3a\u5ea6\u5bf9\u9f50\u65b9\u6cd5\uff1a\u5c06\u6df1\u5ea6\u9884\u6d4b\u5206\u5272\u4e3a\u79bb\u6563\u5c42\uff0c\u8ba1\u7b97\u6bcf\u5c42\u5c3a\u5ea6\u56e0\u5b50\uff0c\u5e76\u5728\u76f8\u90bb\u7a97\u53e3\u548c\u65f6\u95f4\u6233\u95f4\u4f20\u64ad\u3002", "result": "LASER\u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u70b9\u4e91\u56fe\u91cd\u5efa\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728RTX A6000 GPU\u4e0a\u4ee514 FPS\u8fd0\u884c\uff0c\u5cf0\u503c\u5185\u5b58\u4ec56 GB\uff0c\u80fd\u591f\u5904\u7406\u5343\u7c73\u7ea7\u6d41\u5f0f\u89c6\u9891\u3002", "conclusion": "LASER\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5c06\u9ad8\u8d28\u91cf\u79bb\u7ebf\u91cd\u5efa\u6a21\u578b\u8f6c\u6362\u4e3a\u5b9e\u7528\u7684\u6d41\u5f0f\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5185\u5b58\u548c\u8bad\u7ec3\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u6d41\u5f0f\u89c6\u9891\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.13683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13683", "abs": "https://arxiv.org/abs/2512.13683", "authors": ["Lu Ling", "Yunhao Ge", "Yichen Sheng", "Aniket Bera"], "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners", "comment": null, "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cd\u65b0\u7f16\u7a0b\u9884\u8bad\u7ec3\u76843D\u5b9e\u4f8b\u751f\u6210\u5668\uff0c\u4f7f\u5176\u6210\u4e3a\u573a\u666f\u7ea7\u5b66\u4e60\u5668\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u7a7a\u95f4\u77e5\u8bc6\u800c\u975e\u6570\u636e\u96c6\u76d1\u7763\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u5e03\u5c40\u548c\u65b0\u7269\u4f53\u7ec4\u5408\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6709\u9650\u573a\u666f\u6570\u636e\u96c6\u7684\u7a7a\u95f4\u7406\u89e3\uff0c\u9650\u5236\u4e86\u5411\u65b0\u5e03\u5c40\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5229\u7528\u9884\u8bad\u7ec33D\u5b9e\u4f8b\u751f\u6210\u5668\u7684\u53ef\u8fc1\u79fb\u7a7a\u95f4\u77e5\u8bc6\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u3002", "method": "\u91cd\u65b0\u7f16\u7a0b\u9884\u8bad\u7ec3\u76843D\u5b9e\u4f8b\u751f\u6210\u5668\u4f5c\u4e3a\u573a\u666f\u7ea7\u5b66\u4e60\u5668\uff0c\u7528\u6a21\u578b\u4e2d\u5fc3\u7684\u7a7a\u95f4\u76d1\u7763\u66ff\u4ee3\u6570\u636e\u96c6\u76d1\u7763\u3002\u91c7\u7528\u89c6\u70b9\u4e2d\u5fc3\u7684\u573a\u666f\u7a7a\u95f4\u8868\u793a\uff0c\u800c\u975e\u4f20\u7edf\u7684\u89c4\u8303\u7a7a\u95f4\uff0c\u6784\u5efa\u5b8c\u5168\u524d\u9988\u3001\u53ef\u6cdb\u5316\u7684\u573a\u666f\u751f\u6210\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u5e03\u5c40\u548c\u65b0\u7269\u4f53\u7ec4\u5408\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u573a\u666f\u7531\u968f\u673a\u7ec4\u5408\u7269\u4f53\u6784\u6210\u65f6\uff0c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u663e\u73b0\u3002\u91cf\u5316\u4e0e\u5b9a\u6027\u7ed3\u679c\u8868\u660e3D\u5b9e\u4f8b\u751f\u6210\u5668\u662f\u9690\u5f0f\u7684\u7a7a\u95f4\u5b66\u4e60\u4e0e\u63a8\u7406\u5668\u3002", "conclusion": "3D\u5b9e\u4f8b\u751f\u6210\u5668\u5177\u6709\u53ef\u8fc1\u79fb\u7684\u573a\u666f\u5148\u9a8c\uff0c\u80fd\u591f\u4ece\u7eaf\u51e0\u4f55\u7ebf\u7d22\u4e2d\u63a8\u65ad\u90bb\u8fd1\u6027\u3001\u652f\u6491\u548c\u5bf9\u79f0\u5173\u7cfb\uff0c\u4e3a\u4ea4\u4e92\u5f0f3D\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u7684\u57fa\u7840\u6a21\u578b\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.13687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13687", "abs": "https://arxiv.org/abs/2512.13687", "authors": ["Jingfeng Yao", "Yuda Song", "Yucong Zhou", "Xinggang Wang"], "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "comment": "Our pre-trained models are available at https://github.com/MiniMax-AI/VTP", "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "AI": {"tldr": "VTP\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u5206\u8bcd\u5668\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u3001\u81ea\u76d1\u7763\u548c\u91cd\u5efa\u635f\u5931\u6765\u89e3\u51b3\u4f20\u7edf\u91cd\u5efa\u8bad\u7ec3\u5bfc\u81f4\u7684\u6f5c\u5728\u7a7a\u95f4\u504f\u5411\u4f4e\u7ea7\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u663e\u8457\u6539\u5584\u4e86\u751f\u6210\u6a21\u578b\u7684\u7f29\u653e\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91cd\u5efa\u7684\u89c6\u89c9\u5206\u8bcd\u5668\u8bad\u7ec3\u8303\u5f0f\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u504f\u5411\u4f4e\u7ea7\u4fe1\u606f\uff0c\u9020\u6210\"\u9884\u8bad\u7ec3\u7f29\u653e\u95ee\u9898\"\uff1a\u66f4\u597d\u7684\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u5e76\u4e0d\u5e26\u6765\u66f4\u9ad8\u8d28\u91cf\u7684\u751f\u6210\uff0c\u5927\u91cf\u8ba1\u7b97\u6295\u5165\u65e0\u6cd5\u6709\u6548\u8f6c\u5316\u4e3a\u751f\u6210\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faVTP\u7edf\u4e00\u89c6\u89c9\u5206\u8bcd\u5668\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u9996\u6b21\u8054\u5408\u4f18\u5316\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u635f\u5931\u3001\u81ea\u76d1\u7763\u635f\u5931\u548c\u91cd\u5efa\u635f\u5931\uff0c\u4f7f\u6f5c\u5728\u7a7a\u95f4\u80fd\u591f\u7b80\u6d01\u5730\u8868\u793a\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u3002", "result": "VTP\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u7f29\u653e\u7279\u6027\uff1a\u751f\u6210\u6027\u80fd\u968f\u8ba1\u7b97\u3001\u53c2\u6570\u548c\u6570\u636e\u6295\u5165\u6709\u6548\u63d0\u5347\uff1b\u5728ImageNet\u4e0a\u8fbe\u523078.2%\u96f6\u6837\u672c\u51c6\u786e\u7387\u548c0.36 rFID\uff1b\u751f\u6210\u6536\u655b\u901f\u5ea6\u6bd4\u5148\u8fdb\u84b8\u998f\u65b9\u6cd5\u5feb4.1\u500d\uff1b\u4ec5\u901a\u8fc7\u589e\u52a0VTP\u9884\u8bad\u7ec3\u8ba1\u7b97\u91cf\u5c31\u5b9e\u73b0\u4e0b\u6e38\u751f\u621065.8%\u7684FID\u6539\u8fdb\u3002", "conclusion": "\u7406\u89e3\u662f\u9a71\u52a8\u7684\u5173\u952e\u56e0\u7d20\uff0cVTP\u6846\u67b6\u901a\u8fc7\u8054\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u89e3\u51b3\u4e86\u89c6\u89c9\u5206\u8bcd\u5668\u7684\u9884\u8bad\u7ec3\u7f29\u653e\u95ee\u9898\uff0c\u4f7f\u751f\u6210\u6027\u80fd\u80fd\u591f\u6709\u6548\u968f\u9884\u8bad\u7ec3\u6295\u5165\u800c\u63d0\u5347\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89c6\u89c9\u8868\u793a\u3002"}}
{"id": "2512.13690", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13690", "abs": "https://arxiv.org/abs/2512.13690", "authors": ["Susung Hong", "Chongjian Ge", "Zhifei Zhang", "Jui-Hsien Wang"], "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders", "comment": "Project page: https://susunghong.github.io/DiffusionBrowser", "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.", "AI": {"tldr": "DiffusionBrowser\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u6846\u67b6\uff0c\u53ef\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u751f\u6210\u89c6\u9891\u9884\u89c8\uff0c\u652f\u6301RGB\u548c\u573a\u666f\u5185\u5728\u8868\u793a\uff0c\u901f\u5ea6\u8d85\u8fc7\u5b9e\u65f64\u500d\uff0c\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u751f\u6210\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b58\u5728\u751f\u6210\u4e0d\u7cbe\u786e\u3001\u901f\u5ea6\u6162\u3001\u751f\u6210\u8fc7\u7a0b\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u7528\u6237\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u957f\u65f6\u95f4\u5904\u4e8e\"\u9ed1\u6697\"\u72b6\u6001\uff0c\u65e0\u6cd5\u4e86\u89e3\u751f\u6210\u8fdb\u5ea6\u548c\u8fdb\u884c\u4ea4\u4e92\u63a7\u5236\u3002", "method": "\u63d0\u51faDiffusionBrowser\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\uff0c\u53ef\u5728\u53bb\u566a\u8fc7\u7a0b\u7684\u4efb\u4f55\u65f6\u95f4\u6b65\u6216transformer\u5757\u5904\u751f\u6210\u591a\u6a21\u6001\u9884\u89c8\u8868\u793a\uff08\u5305\u62ecRGB\u548c\u573a\u666f\u5185\u5728\u7279\u5f81\uff09\u3002\u901a\u8fc7\u8bad\u7ec3\u7684\u89e3\u7801\u5668\uff0c\u652f\u6301\u901a\u8fc7\u968f\u673a\u6027\u91cd\u6ce8\u5165\u548c\u6a21\u6001\u5f15\u5bfc\u8fdb\u884c\u4ea4\u4e92\u5f0f\u751f\u6210\u63a7\u5236\u3002", "result": "\u6a21\u578b\u80fd\u4ee5\u8d85\u8fc74\u500d\u5b9e\u65f6\u901f\u5ea6\u751f\u6210\u9884\u89c8\uff084\u79d2\u89c6\u9891\u4e0d\u52301\u79d2\uff09\uff0c\u9884\u89c8\u4e0e\u6700\u7ec8\u89c6\u9891\u5177\u6709\u4e00\u81f4\u7684\u5916\u89c2\u548c\u8fd0\u52a8\u3002\u89e3\u7801\u5668\u8fd8\u652f\u6301\u4ea4\u4e92\u5f0f\u751f\u6210\u63a7\u5236\uff0c\u5e76\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u63a2\u6d4b\u6a21\u578b\uff0c\u63ed\u793a\u53bb\u566a\u8fc7\u7a0b\u4e2d\u573a\u666f\u3001\u7269\u4f53\u7b49\u7ec6\u8282\u7684\u7ec4\u6210\u548c\u7ec4\u88c5\u8fc7\u7a0b\u3002", "conclusion": "DiffusionBrowser\u89e3\u51b3\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u4ea4\u4e92\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5b9e\u65f6\u9884\u89c8\u548c\u4ea4\u4e92\u63a7\u5236\u80fd\u529b\uff0c\u540c\u65f6\u4e3a\u7406\u89e3\u9ed1\u76d2\u53bb\u566a\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u3002"}}
