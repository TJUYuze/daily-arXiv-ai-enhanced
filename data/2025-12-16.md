<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 155]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.RO](#cs.RO) [Total: 54]
- [cs.HC](#cs.HC) [Total: 15]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 提出基于OpenVLA-OFT框架的可解释对抗鲁棒视觉-语言-动作模型，通过Evidence-3模块检测光度扰动并生成自然语言解释，在对抗条件下提升动作预测准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 智能农业中依赖RGB相机感知和机器人操纵器的系统容易受到色调、光照和噪声等光度扰动的攻击，导致在对抗攻击下功能失效，需要提高系统的鲁棒性和可解释性

Method: 基于OpenVLA-OFT框架构建可解释对抗鲁棒视觉-语言-动作模型，集成Evidence-3模块检测光度扰动并生成自然语言解释，说明扰动原因和影响

Result: 相比基线模型，当前动作L1损失降低21.7%，下一动作L1损失降低18.4%，在对抗条件下显著提升动作预测准确性和可解释性

Conclusion: 提出的可解释对抗鲁棒视觉-语言-动作模型能有效应对智能农业中的光度扰动问题，提高系统在对抗条件下的鲁棒性和可解释性

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [2] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

TL;DR: Temporal-Anchor3DLane通过改进损失函数、添加轻量级LSTM时间融合模块和ESCOP训练优化，显著提升了单目3D车道线检测的精度和时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的Anchor3DLane方法虽然性能不错，但仍存在回归异常值敏感、全局曲线几何监督弱、多损失项平衡困难以及时间连续性利用有限等问题，需要改进以提升3D车道线检测的鲁棒性。

Method: 提出了Temporal-Anchor3DLane框架，包含三个关键改进：(1) 多任务损失改进：平衡L1回归、Chamfer点集距离、不确定性损失加权，以及分类和可见性的focal和Dice损失；(2) 轻量级时间LSTM融合模块：跨帧聚合每个锚点的特征，替代较重的Transformer式时间融合；(3) ESCOP式训练优化：将曲线级监督与时间一致性相结合。

Result: 在OpenLane数据集上，Temporal-Anchor3DLane将F1分数提升了+6.2，并产生了更平滑的时间轨迹，表明小的架构和损失改进可以显著增强3D车道线检测的鲁棒性，无需额外传感器或扩展规模。

Conclusion: 通过针对性的损失函数改进、轻量级时间融合和训练优化，Temporal-Anchor3DLane有效解决了Anchor3DLane的局限性，显著提升了单目3D车道线检测的性能和时序稳定性，证明了小规模改进也能带来显著性能提升。

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

</details>


### [3] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: 研究人员为埃塞俄比亚提格雷地区开发了离线作物病害检测系统，重点关注仙人掌无花果病害，使用三种移动高效架构进行性能比较，最终部署到支持本地语言的离线应用中。


<details>
  <summary>Details</summary>
Motivation: 提格雷地区80%以上人口依赖农业，但基础设施中断限制了作物病害专家诊断的获取。需要开发离线检测系统来支持该地区的粮食安全关键诊断。

Method: 创建了包含3,587张田间图像的本土仙人掌无花果数据集，在部署受限的后冲突边缘环境中，对三种移动高效架构进行基准测试：定制轻量级CNN、EfficientNet-Lite1和CNN-Transformer混合架构MobileViT-XS。

Result: EfficientNet-Lite1达到90.7%测试准确率，轻量级CNN达到89.5%且具有最佳部署特性（42ms推理延迟，4.8MB模型大小），MobileViT-XS达到97.3%平均交叉验证准确率，显示基于MHSA的全局推理比局部纹理CNN核更可靠地区分害虫集群和真菌病变。

Conclusion: ARM兼容模型已部署到支持提格里尼亚语和阿姆哈拉语的Flutter应用中，可在Cortex-A53类设备上完全离线推理，增强了粮食安全关键诊断的包容性。MobileViT-XS的全局注意力机制在区分病害形态方面表现最佳。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [4] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

TL;DR: 提出了一种用于小麦全语义分割竞赛的自训练框架，结合两阶段混合训练策略和大量数据增强，使用SegFormer模型，通过迭代师生循环提升精度，在开发和测试数据集上取得竞争性表现


<details>
  <summary>Details</summary>
Motivation: 解决Global Wheat Full Semantic Segmentation Competition中的小麦全语义分割问题，需要高效利用有限标注数据并提升模型性能

Method: 开发了系统化的自训练框架，采用两阶段混合训练策略和大量数据增强，核心模型为基于Mix Transformer (MiT-B4)骨干的SegFormer，通过迭代师生循环逐步优化模型精度并最大化数据利用率

Result: 在Development和Testing Phase数据集上都取得了竞争性的性能表现

Conclusion: 提出的自训练框架结合两阶段混合训练策略和大量数据增强，能够有效提升小麦全语义分割任务的性能，在竞赛中表现出色

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

</details>


### [5] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

TL;DR: 该研究系统比较了零样本分割模型SAM3与微调YOLO11在密集苹果实例分割任务上的性能，发现IoU阈值选择对性能评估影响显著，SAM3在边界稳定性方面优势明显，而YOLO在检测完整性方面更优。


<details>
  <summary>Details</summary>
Motivation: 深度学习实例分割存在两种范式：任务特定微调的专业模型和零样本分割的通用基础模型。本研究旨在全面比较这两种范式在密集实例分割任务中的表现，为实际应用提供指导。

Method: 使用MinneApple数据集（670张果园图像，28,179个苹果实例）评估SAM3零样本模式与三种YOLO11变体（nano、medium、large）的微调模型。分析不同IoU阈值对性能评估的影响，比较F1分数和边界稳定性。

Result: 在IoU=0.15时，YOLO模型F1分数为68.9%、72.2%、71.9%，SAM3为59.8%。但IoU选择可导致30%的性能差距夸大。YOLO在IoU范围内性能下降48-50点，而SAM3仅下降4点，边界稳定性是YOLO的12倍。

Conclusion: SAM3在掩码精度和边界稳定性方面优势明显，而YOLO在检测完整性方面更优。研究提供了开源代码和评估流程，帮助理解何时选择专业微调模型或通用基础模型进行密集实例分割任务。

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [6] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 提出一种无需训练的解释方法，用平滑可调轮廓替代密集扰动掩码，通过星凸区域参数化和傅里叶级数优化，生成单连通、紧凑的解释区域


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型解释方法使用密集扰动掩码存在碎片化、过拟合问题，需要复杂的后处理，缺乏紧凑性和稳定性

Method: 使用星凸区域参数化，通过截断傅里叶级数表示轮廓，在保持/删除目标下利用分类器梯度优化，生成单连通平滑掩码

Result: 在ImageNet分类器上匹配密集掩码的极值保真度，同时产生更紧凑、可解释的区域，运行一致性更好，在DINO模型上相关性质量提升超过15%

Conclusion: 该方法通过低维平滑轮廓表示解决了密集掩码的碎片化问题，提供更稳定、紧凑的解释，并可扩展到多轮廓定位多个对象

Abstract: Faithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve/delete objective using the classifier gradients. The approach guarantees a single, simply connected mask, cuts the number of free parameters by orders of magnitude, and yields stable boundary updates without cleanup. Restricting solutions to low-dimensional, smooth contours makes the method robust to adversarial masking artifacts. On ImageNet classifiers, it matches the extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency. Explicit area control also enables importance contour maps, yielding a transparent fidelity-area profiles. Finally, we extend the approach to multi-contour and show how it can localize multiple objects within the same framework. Across benchmarks, the method achieves higher relevance mass and lower complexity than gradient and perturbation based baselines, with especially strong gains on self-supervised DINO models where it improves relevance mass by over 15% and maintains positive faithfulness correlations.

</details>


### [7] [Automatic Wire-Harness Color Sequence Detector](https://arxiv.org/abs/2512.12590)
*Indiwara Nanayakkara,Dehan Jayawickrama,Mervyn Parakrama B. Ekanayake*

Main category: cs.CV

TL;DR: 本文提出了一种用于线束检测的半自动化机器视觉系统，能够验证线束位置、连接器极性和颜色序列的正确性，在工业应用中实现了100%检测准确率和44%的时间节省。


<details>
  <summary>Details</summary>
Motivation: 现代电子制造服务行业中，线束检测过程仍然依赖人工操作，劳动密集且容易出错，需要更高效可靠的自动化解决方案。

Method: 系统集成了五个工业标准CMOS摄像头到模块化机械框架中，使用基于HSV和RGB颜色域值比较的颜色序列分类器。用户可通过至少五个参考样本训练系统，训练文件可存储并重复用于类似线束类型。

Result: 系统在GPV Lanka Pvt. Ltd.部署后，实现了100%的检测准确率，相比人工方法减少了44%的检测时间。系统还包含用户管理、可调照明、会话数据存储和安全登录等附加功能。

Conclusion: 该半自动化机器视觉系统为线束检测提供了可靠高效的解决方案，在实际工业应用中证明了其有效性，显著提升了检测质量和效率。

Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.

</details>


### [8] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

TL;DR: 该论文提供了基于无人机采集的开放微观车辆轨迹数据集，用于研究异质化、区域型城市交通环境。


<details>
  <summary>Details</summary>
Motivation: 传统路边视频采集在密集混合交通中因遮挡、视角有限和车辆不规则运动而效果不佳，需要无人机俯拍视角来获取更全面的时空动态数据。

Method: 使用无人机和Data from Sky平台在印度首都地区六个路段采集数据，包含时间戳、车辆位置、速度、加速度和车辆分类信息，帧率为30fps，并通过人工计数、空间平均速度和探测轨迹进行验证。

Result: 创建了包含异质交通组成和密度水平的开放数据集，探索性分析揭示了车道保持偏好、速度分布和横向机动等关键行为模式。

Conclusion: 这些开放数据集为全球研究社区提供了独特资源，支持区域型交通条件下的仿真建模、安全评估和行为研究，有助于开发更准确代表复杂城市交通环境的模型。

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

</details>


### [9] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

TL;DR: 论文提出了一种名为权重空间相关性分析的方法，用于量化深度学习模型是否真正利用了临床相关特征，而不是依赖于图像中编码的元数据（如扫描仪型号）等捷径特征。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的深度学习模型容易受到捷径学习的影响，会依赖图像嵌入中编码的元数据（如扫描仪型号）等混杂因素。关键问题是模型是否主动利用这些编码信息进行最终预测，需要验证模型的可信度。

Method: 提出了权重空间相关性分析方法，通过测量主要临床任务分类头与辅助元数据任务分类头之间的对齐程度，量化特征利用情况。首先通过检测人工诱导的捷径学习验证方法有效性，然后应用于SA-SonoNet模型的自发性早产预测任务。

Result: 方法成功检测到人工诱导的捷径学习。在sPTB预测模型中，虽然嵌入包含大量元数据，但sPTB分类器的权重向量与临床相关因素（如出生体重）高度相关，而与临床无关的采集因素（如扫描仪）解耦。

Conclusion: 该方法为验证模型可信度提供了工具，表明在没有诱导偏差的情况下，临床模型会选择性地利用与真实临床信号相关的特征，而不是依赖捷径特征。

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [10] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 本文提出RIO-VQA任务和RIO-Bench基准，用于评估大视觉语言模型在需要选择性读取或忽略图像中文本时的能力，解决现有评估方法过度强调忽略文本而忽略实际场景中需要联合推理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型容易受到排版攻击，而现有评估协议和防御方法主要关注物体识别，隐含鼓励模型忽略文本以获得鲁棒性。然而，真实世界场景通常需要同时对物体和文本进行联合推理（如识别行人同时读取交通标志）。

Method: 提出Read-or-Ignore VQA任务，形式化视觉问答中的选择性文本使用；构建RIO-Bench基准数据集和评估协议，为每个真实图像提供相同场景的反事实（读取/忽略），仅改变文本内容和问题类型；并基于此基准开发数据驱动的自适应防御方法。

Result: 使用RIO-Bench评估发现，现有强大大视觉语言模型和防御方法无法平衡排版鲁棒性和文本读取能力；RIO-Bench支持的新型数据驱动防御方法能够学习自适应选择性文本使用，超越先前非自适应、忽略文本的防御方法。

Conclusion: 这项工作揭示了现有评估范围与真实世界需求之间的根本性不匹配，为构建可靠的大视觉语言模型提供了原则性路径。RIO-Bench为评估和改进模型在需要选择性文本使用场景中的能力提供了标准化框架。

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

</details>


### [11] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

TL;DR: 该论文提出使用神经细胞自动机（NCA）作为分割掩码的细化机制，通过局部迭代更新修复拓扑错误，无需手工规则或专用架构。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型经常产生碎片化或不连续的掩码输出，修复这些拓扑错误通常需要手工设计的细化规则或针对特定任务的专用架构，缺乏通用解决方案。

Method: 提出细化NCA（rNCA）方法，使用局部、迭代的更新机制，在图像上下文指导下修复分割掩码。通过在不完美掩码和真实标签上训练，自动机学习目标形状的结构特性，仅依赖局部信息。当应用于粗糙的全局预测掩码时，学习到的动态过程逐步重新连接断裂区域、修剪松散碎片，并收敛到稳定、拓扑一致的结果。

Result: 在视网膜血管分割中，rNCA使Dice/clDice指标提升2-3%，改善Betti误差，β₀误差减少60%，β₁误差减少20%。在心肌分割中，零样本设置下修复了61.5%的断裂案例，ASSD和HD分别降低19%和16%。

Conclusion: 神经细胞自动机（NCA）可作为有效且广泛适用的分割掩码细化器，能够修复不同基础分割模型和任务产生的常见拓扑错误，无需手工规则或专用架构。

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [12] [Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life](https://arxiv.org/abs/2512.11905)
*Ming-Zher Poh,Shun Liao,Marco Andreetto,Daniel McDuff,Jonathan Wang,Paolo Di Achille,Jiang Wu,Yun Liu,Lawrence Cai,Eric Teasley,Mark Malhotra,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: 通过智能手机被动捕捉的自然微笑强度可作为主观幸福感的客观行为指标，与全国幸福感调查数据高度相关


<details>
  <summary>Details</summary>
Motivation: 传统的主观幸福感测量方法依赖自我报告，存在回忆偏差和参与者负担重的问题，需要更客观、可扩展的日常幸福感测量方法

Method: 分析233名参与者一周内被动记录的405,448个视频片段，使用深度学习模型量化微笑强度，研究其昼夜和日常模式，并与身体活动、光照暴露等变量关联

Result: 微笑强度的日常模式与全国幸福感调查数据高度相关(r=0.92)，昼夜节律与日重建方法结果高度一致(r=0.80)。微笑强度与身体活动和光照暴露显著正相关，与智能手机使用无显著关联

Conclusion: 被动智能手机传感可作为研究情感行为动态的强大生态效度方法，为大规模人群幸福感研究开辟新途径

Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.

</details>


### [13] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

TL;DR: MPath是一个轻量级多模态框架，通过视觉前缀提示机制将WSI视觉嵌入注入预训练的生物医学语言模型，用于从全切片图像自动生成病理诊断报告。


<details>
  <summary>Details</summary>
Motivation: 从全切片图像自动生成病理诊断报告是计算病理学的新兴方向，但由于组织形态变异大和病理叙述结构复杂，将高分辨率组织模式转化为临床连贯文本仍然困难。

Method: MPath采用轻量级多模态框架，通过学习的视觉前缀提示机制，将基础模型WSI特征（CONCH + Titan）注入到预训练的生物医学语言模型BioBART中，保持语言主干冻结以提高稳定性和数据效率。

Result: 在RED 2025 Grand Challenge数据集上开发和评估，在Test Phase 2中排名第4，尽管提交机会有限。结果表明基于提示的多模态条件化是病理报告生成的可扩展和可解释策略。

Conclusion: MPath展示了基于提示的多模态条件化作为病理报告生成的可扩展和可解释策略的潜力，为计算病理学中的自动化报告生成提供了有效解决方案。

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [14] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: FloraForge是一个LLM辅助框架，让领域专家通过自然语言交互生成生物准确的参数化3D植物模型，无需编程专业知识。


<details>
  <summary>Details</summary>
Motivation: 当前3D植物建模方法存在局限性：基于学习的方法需要大量物种特定训练数据且缺乏可编辑性；程序化建模需要专业几何建模知识和复杂程序规则，领域科学家难以使用。

Method: 利用LLM辅助协同设计，通过迭代自然语言植物精炼(PR)生成Python脚本，创建参数化植物几何体作为分层B样条曲面表示，具有植物学约束、显式控制点和参数变形函数。

Result: 在玉米、大豆和绿豆上演示了框架，通过手动精炼植物描述符(PD)将程序化模型拟合到经验点云数据，生成用于可视化的三角网格和用于定量分析的带参数元数据的三角网格。

Conclusion: 该框架独特地结合了LLM辅助模板创建、支持表型分析和渲染的数学连续表示，以及通过PD的直接参数控制，为植物科学民主化了复杂的几何建模，同时保持数学严谨性。

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [15] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

TL;DR: 研究人员开发了MONET扩散模型，可从明场图像预测细胞染色图像，解决了传统细胞染色技术劳动密集且无法研究细胞动态的问题。


<details>
  <summary>Details</summary>
Motivation: 传统细胞染色技术存在两大问题：1) 劳动密集型，需要大量人工操作；2) 需要化学固定，无法研究细胞动态变化。研究人员希望通过AI方法解决这些问题。

Method: 训练了一个名为MONET的扩散模型，使用大规模数据集从明场图像预测细胞染色通道。模型采用一致性架构，能够生成时间序列视频，并支持上下文学习以适应不同细胞系和成像协议。

Result: 模型质量随规模扩大而提升；一致性架构能够生成时间序列视频（尽管没有视频训练数据）；模型具备上下文学习能力，可部分适应分布外的细胞系和成像协议。

Conclusion: 虚拟细胞染色不是要完全取代物理细胞染色，而是作为补充工具，为生物学研究提供新的工作流程，特别是能够研究细胞动态变化。

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [16] [Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains](https://arxiv.org/abs/2512.11939)
*Clément Fernandes,Wojciech Pieczynski*

Main category: cs.CV

TL;DR: 提出了一种新的HEMC-CPS模型，将上下文Peano扫描与证据隐马尔可夫链结合，用于无监督图像分割，相比传统方法效果更好且计算更快。


<details>
  <summary>Details</summary>
Motivation: 传统的Peano扫描将二维图像像素转换为一维序列，使得隐马尔可夫链可用于图像分割。虽然基于HMC的方法比隐马尔可夫场方法更快，但仍有改进空间。上下文Peano扫描和证据隐马尔可夫链分别被证明能提升分割性能，因此研究将两者结合的新模型。

Method: 提出了HEMC-CPS模型，同时考虑上下文Peano扫描和证据隐马尔可夫链。采用无监督分割方式，使用随机期望最大化方法进行参数估计，通过贝叶斯最大后验概率模式进行分割。

Result: 在合成图像和真实图像上验证了HEMC-CPS模型的有效性。新模型在贝叶斯MPM分割中表现出色，相比传统方法有更好的性能。

Conclusion: HEMC-CPS模型为复杂图像（如三维或多传感器多分辨率图像）的建模和分割提供了潜力。该模型不仅限于图像分割，还可用于任何类型的空间相关数据分析。

Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.

</details>


### [17] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

TL;DR: DynaPURLS是一个用于零样本骨架动作识别的统一框架，通过建立多尺度视觉-语义对应关系并在推理时动态优化，显著提升了未见类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本骨架动作识别方法依赖于骨架特征与静态类别级语义的对齐，这种粗粒度对齐无法弥合可见类别和未见类别之间的领域偏移，阻碍了细粒度视觉知识的有效迁移。

Method: 1. 使用大语言模型生成包含全局运动和局部身体部位动态的分层文本描述；2. 自适应分区模块通过语义分组骨架关节产生细粒度视觉表示；3. 动态优化模块在推理时通过轻量级可学习投影将文本特征适配到输入视觉流；4. 置信感知的类别平衡记忆库稳定优化过程，减少噪声伪标签的错误传播。

Result: 在NTU RGB+D 60/120和PKU-MMD三个大规模基准数据集上的广泛实验表明，DynaPURLS显著优于现有方法，创造了新的最先进记录。

Conclusion: DynaPURLS通过建立鲁棒的多尺度视觉-语义对应关系并在推理时动态优化，有效解决了零样本骨架动作识别中的领域偏移问题，实现了对未见类别的更好泛化。

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [18] [A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer](https://arxiv.org/abs/2512.11977)
*Sushmita Nath*

Main category: cs.CV

TL;DR: 本研究探讨了在数据受限条件下使用DeiT（数据高效图像变换器）进行晶圆缺陷分类，相比传统CNN模型（VGG-19、Xception、Squeeze-Net）在准确率、F1分数和训练收敛速度方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 预测性维护在现代工业中至关重要，特别是在半导体制造这种敏感领域。虽然CNN模型在晶圆缺陷检测中表现出色，但在数据有限且不平衡的情况下效果会下降，因此需要探索更高效的方法。

Method: 使用数据高效图像变换器（DeiT）模型对晶圆缺陷图进行分类，在数据受限条件下进行实验，并与VGG-19、Xception、Squeeze-Net等CNN模型以及混合模型进行对比。

Result: DeiT模型取得了90.83%的最高分类准确率，显著优于VGG-19（65%）、SqueezeNet（82%）、Xception（66%）和混合模型（67%）。同时，DeiT的F1分数达到90.78%，训练收敛更快，在检测少数缺陷类别方面表现出更强的鲁棒性。

Conclusion: DeiT等基于变换器的模型在半导体晶圆缺陷检测中具有巨大潜力，能够支持半导体制造过程中的预测性维护策略，特别是在数据受限和不平衡的情况下表现出色。

Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.

</details>


### [19] [CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction](https://arxiv.org/abs/2512.11988)
*Xianghui Xie,Bowen Wen,Yan Chang,Hesam Rabeti,Jiefeng Li,Ye Yuan,Gerard Pons-Moll,Stan Birchfield*

Main category: cs.CV

TL;DR: CARI4D：首个从单目RGB视频中重建4D人-物交互的类别无关方法，通过姿态假设选择算法和渲染-比较范式实现空间、时间和像素对齐，在未见数据集上重建误差降低36%


<details>
  <summary>Details</summary>
Motivation: 从单目RGB视频准确捕捉人-物交互对于人类理解、游戏和机器人学习应用很重要，但由于未知的物体和人体信息、深度模糊、遮挡和复杂运动，从单视图推断4D交互极具挑战性。先前方法通过假设真实物体模板或限制到有限物体类别来简化设置。

Method: 提出CARI4D方法：1）姿态假设选择算法，鲁棒地整合基础模型的个体预测；2）通过学习的渲染-比较范式联合优化，确保空间、时间和像素对齐；3）推理复杂接触点进行进一步细化，满足物理约束。

Result: 在分布内数据集上重建误差比先前方法降低38%，在未见数据集上降低36%。模型能够泛化到训练类别之外，可以零样本应用于野外互联网视频。

Conclusion: CARI4D是首个从单目RGB视频重建空间和时间一致的4D人-物交互的类别无关方法，通过整合基础模型预测和渲染-比较优化，实现了超越训练类别的泛化能力。

Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

</details>


### [20] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: V-REX是一个评估视觉语言模型多步探索推理能力的评测套件，包含挑战性视觉推理任务和评估协议，将多步探索分解为问题链，评估模型的规划和执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理需要多轮探索和推理的复杂开放任务时表现不佳，而这类任务在实际应用中很常见。目前缺乏对这些模型多步探索推理能力的系统评估方法。

Method: 开发V-REX评测套件，将多步探索推理转化为问题链（Chain-of-Questions），通过有限选项设计实现可靠的定量分析，评估模型的规划能力（分解任务选择问题链）和执行能力（按顺序回答问题收集信息）。

Result: 通过评估最先进的专有和开源视觉语言模型，发现了模型性能的扩展趋势、规划能力和执行能力之间的显著差异，以及多步探索推理方面仍有很大改进空间。

Conclusion: V-REX提供了一个系统评估视觉语言模型多步探索推理能力的框架，揭示了当前模型的局限性，为未来模型开发提供了重要指导。

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [21] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: Semantic-Drive是一个本地优先的神经符号框架，用于从自动驾驶车辆的视频日志中挖掘罕见的安全关键事件，通过解耦感知和认知分析实现高精度语义搜索，在消费级硬件上运行。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆开发面临"长尾"训练数据稀缺的瓶颈，现有解决方案要么精度不足（粗粒度元数据搜索），要么存在隐私侵犯和成本高昂问题（基于云的视觉语言模型）。

Method: 采用神经符号框架，将感知解耦为两个阶段：1）符号接地：通过实时开放词汇检测器（YOLOE）锚定注意力；2）认知分析：通过推理视觉语言模型进行法医场景分析。使用"系统2"推理时间对齐策略和多模型"法官-侦察员"共识机制来减少幻觉。

Result: 在nuScenes数据集上，相比Waymo Open Dataset分类法，Semantic-Drive实现了0.966的召回率（CLIP为0.475），风险评估错误减少了40%。系统完全在消费级硬件（NVIDIA RTX 3090）上运行。

Conclusion: Semantic-Drive提供了一个隐私保护的本地替代方案，能够高效地从海量视频日志中挖掘罕见的安全关键事件，解决了自动驾驶开发中的数据稀缺问题。

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [22] [Towards Interactive Intelligence for Digital Humans](https://arxiv.org/abs/2512.13674)
*Yiyi Cai,Xuangeng Chu,Xiwei Gao,Sitong Gong,Yifei Huang,Caixin Kang,Kunhang Li,Haiyang Liu,Ruicong Liu,Yun Liu,Dianwen Ng,Zixiong Su,Erwin Wu,Yuhan Wu,Dingkun Yan,Tianyu Yan,Chang Zeng,Bo Zheng,You Zhou*

Main category: cs.CV

TL;DR: Mio是一个端到端的数字人框架，通过五个专门模块实现个性表达、自适应交互和自我进化的交互智能


<details>
  <summary>Details</summary>
Motivation: 推动数字人从表面模仿向智能交互发展，实现个性对齐表达、自适应交互和自我进化的交互智能

Method: 提出Mio框架，包含五个专门模块：Thinker（思考者）、Talker（说话者）、Face Animator（面部动画）、Body Animator（身体动画）和Renderer（渲染器），统一架构整合认知推理与实时多模态体现

Result: 在所有评估维度上都优于现有最先进方法，建立了新的交互智能评估基准

Conclusion: Mio框架将数字人从表面模仿推向智能交互，实现了交互智能的新范式

Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.

</details>


### [23] [Adaptive federated learning for ship detection across diverse satellite imagery sources](https://arxiv.org/abs/2512.12053)
*Tran-Vu La,Minh-Tan Pham,Yu Li,Patrick Matgen,Marco Chini*

Main category: cs.CV

TL;DR: 该研究探索了联邦学习在卫星船舶检测中的应用，通过四种FL模型（FedAvg、FedProx、FedOpt、FedMedian）与本地训练基线对比，证明FL能在保护隐私的同时显著提升检测精度，接近使用全部数据的全局训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决卫星船舶检测中数据隐私保护问题，特别是商业卫星图像和敏感船舶标注数据不能共享或集中收集的挑战，同时提升小数据集上的检测性能。

Method: 采用联邦学习框架，使用YOLOv8作为船舶检测模型，评估四种FL算法（FedAvg、FedProx、FedOpt、FedMedian），并与仅在各数据集上独立训练的本地基线模型对比。研究还探讨了通信轮数和本地训练轮数等配置参数的影响。

Result: FL模型相比小数据集上的本地训练显著提高了检测精度，性能接近使用所有数据集的全局训练。同时发现选择合适的FL配置（如通信轮数和本地训练轮数）对优化检测精度和计算效率至关重要。

Conclusion: 联邦学习为卫星船舶检测提供了一种有效的隐私保护解决方案，能够在保护数据隐私的同时实现接近全局训练的性能，但需要仔细选择FL配置参数以达到最佳效果。

Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.

</details>


### [24] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

TL;DR: 本文提出了一种监督语义分割工作流，旨在提升火灾后烧毁区域（BA）的绘制性能和效率，针对SPOT-6/7高分辨率影像，通过实验比较U-Net和SegFormer模型，并探索了土地覆盖数据辅助任务和测试时增强技术。


<details>
  <summary>Details</summary>
Motivation: 当前烧毁区域绘制方法主要依赖计算机视觉模型处理灾后遥感影像，但往往忽视了其在时间紧迫的应急管理场景中的适用性。需要开发既能保证性能又能提高效率的解决方案。

Method: 提出了监督语义分割工作流，针对SPOT-6/7高分辨率影像。比较了U-Net和SegFormer模型在有限训练数据下的表现。引入了土地覆盖数据作为辅助任务来增强模型鲁棒性。测试了测试时增强技术，并采用混合精度等优化方法减少推理时间。

Result: U-Net和SegFormer在有限训练数据下表现相似，但SegFormer需要更多计算资源，在应急场景中实用性受限。加入土地覆盖辅助任务能增强模型鲁棒性且不增加推理时间。测试时增强能提升绘制性能但会增加推理时间，可通过混合精度等优化方法缓解。

Conclusion: 该研究为应急管理场景下的烧毁区域绘制提供了平衡性能与效率的解决方案，U-Net模型在资源受限情况下更具实用性，辅助任务和优化技术能进一步提升系统性能。

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [25] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

TL;DR: CreativeVR是一个针对AI生成视频和真实视频中严重结构/时序伪影的修复框架，通过单一精度控制旋钮在精确修复与结构/运动校正之间平滑权衡，在AIGC54基准测试中达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频扩散模型在精细结构上存在缺陷（扭曲的面部/手部、变形背景、时序不一致），而传统视频修复方法主要针对合成退化（模糊、下采样），扩散先验修复器通常针对光度噪声训练，缺乏对感知质量与保真度权衡的控制。

Method: 提出CreativeVR框架，采用深度适配器方法，通过单一精度控制旋钮调节模型对输入的跟随程度；关键创新是训练时使用的时序一致退化模块，应用精心设计的变换来产生真实的结构失效。

Result: 在AIGC54基准测试（包含FIQA、语义和感知指标）中达到最先进结果；在标准视频修复基准上表现有竞争力；在单张80GB A100上以720p分辨率实现约13FPS的实用吞吐量。

Conclusion: CreativeVR为AI生成内容和真实视频中的严重结构及时序伪影提供了有效的修复解决方案，通过单一控制参数实现了修复精度与创造性校正之间的灵活平衡。

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [26] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

TL;DR: BAgger是一种自监督训练方案，通过从模型自身生成的轨迹中构建纠正路径，解决自回归视频模型中的曝光偏差问题，提高长期生成稳定性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型通过下一帧预测进行世界建模，但存在曝光偏差问题：训练时使用干净上下文，推理时使用自生成帧，导致误差累积和质量漂移。

Method: 提出Backwards Aggregation (BAgger)方案，从模型自身生成的轨迹中构建纠正路径，教导模型从错误中恢复。使用标准分数匹配或流匹配目标训练，避免大型教师模型和长时间链的反向传播。

Result: 在因果扩散变换器上实现BAgger，在文本到视频、视频扩展和多提示生成任务中评估，观察到更稳定的长期运动和更好的视觉一致性，减少了漂移现象。

Conclusion: BAgger通过自监督的纠正轨迹训练，有效解决了自回归视频模型的曝光偏差问题，提高了长期生成的质量和稳定性。

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [27] [RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer](https://arxiv.org/abs/2512.12083)
*Guanfang Dong,Luke Schultz,Negar Hassanpour,Chao Gao*

Main category: cs.CV

TL;DR: RePack是一个简单有效的框架，通过将高维视觉基础模型表示投影到低维流形，解决信息过载问题，加速扩散变换器收敛并提升图像重建性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉基础模型的高维表示虽然能增强潜在扩散模型，但可能导致信息过载，特别是当VFM特征尺寸超过原始图像解码需求时，影响模型效率和性能。

Method: 提出RePack框架，将高维VFM表示投影到低维流形，转化为更紧凑、解码器友好的表示，过滤非语义噪声同时保留核心结构信息。

Result: 在DiT-XL/2上，RePack仅用64个epoch就达到FID 3.66，比最先进方法快35%收敛，显著优于直接注入原始VFM特征的方法。

Conclusion: RePack成功提取了VFM表示的核心语义，同时避免了高维度的副作用，为有效利用预训练视觉基础模型提供了新思路。

Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.

</details>


### [28] [VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering](https://arxiv.org/abs/2512.12089)
*Zihu Wang,Boxun Xu,Yuxuan Xia,Peng Li*

Main category: cs.CV

TL;DR: VEGAS：通过将视觉编码器的注意力图注入语言模型中间层来减少LVLM幻觉的推理时方法


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）虽然能联合推理视觉和文本输入，但经常产生与视觉证据事实不一致的幻觉输出。现有方法未能有效解决解码过程中的视觉注意力问题。

Method: 提出VEGAS方法：1）发现视觉编码器的注意力图比最终视觉注意力图更集中；2）分析解码过程中的视觉-文本冲突，发现冲突在语言模型中间层达到峰值；3）将视觉编码器的注意力图注入语言模型中间层；4）自适应地引导未能聚焦关键图像对象的token。

Result: 在多个基准测试上的广泛实验表明，VEGAS在减少幻觉方面始终达到最先进的性能。

Conclusion: 视觉编码器的注意力图能有效抑制LVLM的幻觉，VEGAS作为一种简单有效的推理时方法，通过注意力注入机制显著改善了模型的视觉一致性。

Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

</details>


### [29] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

TL;DR: SPDMark是一种基于选择性参数位移的视频扩散模型水印框架，通过修改生成模型参数子集嵌入水印，使用低秩适配实现参数效率，能够生成不可感知的水印并抵抗多种视频修改。


<details>
  <summary>Details</summary>
Motivation: 高质量视频生成模型的兴起增加了对可靠水印方案的需求，用于检测和追踪生成视频的来源。现有视频水印方法无法同时实现不可感知性、鲁棒性和计算效率。

Method: 基于选择性参数位移的视频扩散模型水印框架，通过修改生成模型参数子集嵌入水印。使用低秩适配实现参数效率，将位移建模为层间基础位移的加性组合。训练阶段联合学习基础位移和水印提取器，最小化消息恢复、感知相似性和时间一致性损失。使用密码哈希函数从基础水印密钥派生帧特定水印消息，通过最大二分图匹配恢复正确帧顺序。

Result: 在文本到视频和图像到视频生成模型上的评估表明，SPDMark能够生成不可感知的水印，并以高准确率恢复水印，同时对各种常见视频修改具有鲁棒性。

Conclusion: SPDMark框架成功解决了现有视频水印方法在不可感知性、鲁棒性和计算效率方面的平衡问题，为生成视频的溯源提供了有效的解决方案。

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [30] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 该研究比较了传统光学显微镜和数字同轴全息显微镜(DIHM)在花粉自动识别中的性能差异，使用GAN生成合成DIHM图像来改善检测性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决DIHM图像中花粉识别困难的问题，包括散斑噪声、孪生像伪影以及与明场图像的显著差异，推动全自动DIHM工作流程在兽医成像中的应用。

Method: 使用YOLOv8s进行目标检测和MobileNetV3L进行分类，在双模态数据集上训练；采用Wasserstein GAN with spectral normalization (WGAN-SN)生成合成DIHM图像；通过混合真实和合成数据来改善检测性能。

Result: 光学数据检测mAP50达91.3%，分类准确率97%；DIHM数据检测mAP50仅8.15%，分类准确率50%；使用GAN合成数据混合训练后，DIHM检测mAP50提升至15.4%。

Conclusion: GAN数据增强能缩小光学和DIHM图像识别性能差距，为全自动DIHM工作流程在兽医成像中的应用迈出了重要一步。

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


### [31] [EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography](https://arxiv.org/abs/2512.12107)
*Yuheng Li,Yue Zhang,Abdoul Aziz Amadou,Yuxiang Lai,Jike Zhong,Tiziano Passerini,Dorin Comaniciu,Puneet Sharma*

Main category: cs.CV

TL;DR: 提出了首个基于测量的多模态超声心动图数据集EchoGround-MIMIC和相应的视觉语言模型EchoVLM，通过创新的预训练目标在多种临床任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 超声心动图是心脏病学中最广泛使用的成像方式，但其解读仍然劳动密集且本质上是多模态的，需要视图识别、定量测量、定性评估和基于指南的推理。现有的视觉语言模型在超声心动图领域的应用受到缺乏大规模临床基础图像-文本数据集以及缺少测量基础推理的限制。

Method: 1. 创建EchoGround-MIMIC数据集：包含19,065个图像-文本对，来自1,572名患者，具有标准化视图、结构化测量、基于测量的描述和指南衍生的疾病标签。2. 开发EchoVLM模型：采用两种新颖的预训练目标：(i) 视图感知对比损失，编码超声心动图成像的视图依赖结构；(ii) 否定感知对比损失，区分临床关键的阴性发现与阳性发现。

Result: 在涵盖多模态疾病分类、图像-文本检索、视图分类、腔室分割和标志点检测的36个任务中，EchoVLM实现了最先进的性能：零样本疾病分类的AUC达到86.5%，视图分类准确率达到95.1%。临床基础的多模态预训练产生了可迁移的视觉表示。

Conclusion: EchoVLM被确立为端到端超声心动图解读的基础模型，EchoGround-MIMIC数据集和数据处理代码的发布将促进多模态超声心动图解读的可重复性和进一步研究。

Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.

</details>


### [32] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

TL;DR: 该研究提出了一种针对3D CT图像的基于patch的持久同调构造方法，相比传统的3D立方体复形方法，在分类性能和计算效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于3D立方体复形过滤的持久同调方法在处理高分辨率CT图像时存在性能不足和计算复杂度高的问题，需要更有效的拓扑特征提取方法。

Method: 提出了一种新颖的基于patch的持久同调构造方法，专门针对体积医学成像数据（特别是CT模态）设计，并开发了相应的Python包Patch-TDA。

Result: 在多个3D CT数据集上的实验表明，patch-based TDA方法在准确率、AUC、敏感性、特异性和F1分数上平均分别提升了10.38%、6.94%、2.06%、11.58%和8.51%，同时在时间效率上也优于传统方法。

Conclusion: 基于patch的持久同调方法在CT图像分析中比传统的3D立方体复形方法更优越，提供了更好的分类性能和计算效率，有望促进拓扑数据分析在医学影像领域的应用。

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [33] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

TL;DR: 本文提出了目前最大的道路损坏评估与道路对齐基准数据集，包含10次联邦宣布灾害后的无人机影像，提供了18个基线模型，解决了先前数据集的三个主要挑战。


<details>
  <summary>Details</summary>
Motivation: 现有灾害道路损坏评估数据集规模小或依赖低分辨率影像，无法检测应急管理者关心的现象，且先前开发的机器学习系统缺乏实际运营验证。此外，实践中观察到道路线错位问题严重影响模型性能。

Method: 标注了657.25公里道路，采用10类标注方案；在2024年飓风Debby和Helene的应急响应中训练并部署了18个机器学习模型；提供了9,184个道路线调整用于空间对齐。

Result: 当18个基线模型部署到实际错位的道路线时，模型性能平均下降5.596% Macro IoU；如果不考虑空间对齐，约8%（11公里）的道路不良条件会被错误标注，约9%（59公里）的道路线会偏离实际道路。

Conclusion: 空间对齐是灾害响应中道路损坏评估的关键问题，ML、CV和机器人社区需要解决这一差距，以支持更有效和明智的灾害决策。

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [34] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: 开发深度学习模型融合多源遥感数据，生成格陵兰冰盖每日100米分辨率的地表融水分布图，相比现有方法精度显著提升


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰盖加速融化过程尚未完全理解且难以测量，现有融水分布图在时间和空间分辨率上存在权衡，无法同时实现高时空分辨率

Method: 开发深度学习模型融合区域气候模型输出、合成孔径雷达、被动微波和数字高程模型数据，对Helheim冰川区域进行时空降尺度处理

Result: 深度学习融合方法精度达95%，显著优于仅依赖区域气候模型的方法（83%）和仅用被动微波的方法（72%）；同时发布基准数据集MeltwaterBench

Conclusion: 深度学习融合多源数据能有效生成高时空分辨率的融水分布图，为理解冰盖融化过程提供重要工具，并建立了基准数据集促进方法比较

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [35] [Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video](https://arxiv.org/abs/2512.12165)
*Daniel Adebi,Sagnik Majumder,Kristen Grauman*

Main category: cs.CV

TL;DR: 本文提出首个利用音频辅助视觉进行相对相机姿态估计的方法，通过整合声源方向谱和双耳嵌入到视觉模型中，在真实世界视频中实现更鲁棒的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 视觉方法在运动模糊、遮挡等视觉退化条件下表现不佳，而被动场景声音提供了互补线索，可以增强相对相机姿态估计的鲁棒性。

Method: 提出一个简单有效的音频-视觉框架，将声源方向（DOA）谱和双耳化嵌入整合到最先进的纯视觉姿态估计模型中。

Result: 在两个大型数据集上的结果显示，相比强大的视觉基线方法有持续提升，且在视觉信息受损时表现出鲁棒性。

Conclusion: 这是首个成功利用音频进行真实世界视频相对相机姿态估计的工作，确立了日常音频作为经典空间挑战的一个意外但有前景的信号。

Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.

</details>


### [36] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

TL;DR: 提出了一种用于微型无人机群在有限带宽条件下对野火环境进行轻量级周界跟踪的方法，结合热成像和RGB图像处理，通过自适应阈值、形态学优化和梯度滤波实现边界检测，在嵌入式SoC平台上实现低延迟跟踪。


<details>
  <summary>Details</summary>
Motivation: 针对野火应急侦察场景中微型无人机群在有限带宽条件下的周界跟踪需求，需要一种轻量级、低延迟的跟踪方法，能够在GPS信号退化时保持轨迹稳定性，同时减少计算资源消耗。

Method: 1) 热成像帧通过自适应阈值和形态学细化生成粗略的热区域掩码；2) RGB帧提供边缘线索，使用基于梯度的滤波抑制纹理相关的误检测；3) 规则级合并策略选择边界候选点，通过Ramer-Douglas-Peucker算法简化；4) 系统包含周期性信标和惯性反馈回路，在GPS退化时保持轨迹稳定性；5) 在嵌入式SoC平台上通过限制每帧像素操作和预计算梯度表实现低于50ms的延迟。

Result: 小规模仿真显示：与纯边缘跟踪基线相比，平均路径长度和边界抖动减少，同时通过交集合并分析保持环境覆盖；电池消耗和计算利用率证实了在标准微型平台上实现10-15m/s前向运动的可行性。

Conclusion: 该方法实现了快速现场部署，仅需鲁棒的传感和最小化的通信，适用于应急侦察应用，为有限带宽条件下微型无人机群的野火周界跟踪提供了可行的轻量级解决方案。

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [37] [A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection](https://arxiv.org/abs/2512.12205)
*Peizheng Li,Ioannis Mavromatis,Ajith Sahadevan,Tim Farnham,Adnan Aijaz,Aftab Khan*

Main category: cs.CV

TL;DR: 英国布里斯托尔城市路灯大规模视觉数据集，包含22个固定摄像头在2021-2025年间每小时采集的52.6万张图像，涵盖不同光照、天气和季节条件，用于研究视觉漂移和智能城市MLOps策略。


<details>
  <summary>Details</summary>
Motivation: 为智能城市部署中的视觉漂移、异常检测和MLOps策略研究提供真实世界、细粒度的基准数据集，解决长期模型稳定性和部署就绪视觉系统的评估需求。

Method: 使用22个固定角度摄像头每小时采集图像，提供包含时间戳、GPS坐标和设备标识的丰富元数据。采用基于卷积变分自编码器（CNN-VAEs）的自监督框架，为每个摄像头节点和日/夜图像集单独训练模型，定义相对质心漂移和相对重建误差两种漂移度量。

Result: 创建了包含超过526,000张图像的大规模纵向视觉数据集，以JPEG和CSV格式公开，支持街道照明监控、天气推断和城市场景理解等下游应用。

Conclusion: 该数据集为评估长期模型稳定性、漂移感知学习和部署就绪视觉系统提供了现实、细粒度的基准，促进了智能城市视觉系统的可重复研究和实际应用。

Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.

</details>


### [38] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: 提出ALERT数据集和ISA-ViT框架，解决UWB雷达在分心驾驶检测中的数据集不足和ViT输入尺寸固定问题，显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 分心驾驶导致全球致命事故，UWB雷达具有抗干扰、低功耗和隐私保护优势，但面临两大挑战：缺乏大规模真实世界UWB数据集，以及固定输入尺寸的Vision Transformers难以适应非标准维度的UWB雷达数据。

Method: 1) 提出ALERT数据集：包含10,220个真实驾驶条件下的雷达样本，涵盖7种分心驾驶行为；2) 提出ISA-ViT框架：通过调整补丁配置和利用预训练位置嵌入向量，在调整UWB数据尺寸时保留多普勒频移和相位特征等雷达特定信息；3) 采用域融合策略：结合距离域和频域特征提升分类性能。

Result: ISA-ViT相比现有基于ViT的UWB-DAR方法，准确率提升22.68%。ALERT数据集公开可用，输入尺寸无关策略详细公开。

Conclusion: 通过提供ALERT数据集和ISA-ViT框架，促进了更鲁棒、可扩展的分心驾驶检测系统的开发，有助于实际部署应用。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [39] [CineLOG: A Training Free Approach for Cinematic Long Video Generation](https://arxiv.org/abs/2512.12209)
*Zahra Dehghanian,Morteza Abolghasemi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: CineLOG：一个包含5000个高质量视频片段的数据集，用于可控视频合成，每个片段都标注了详细场景描述、基于标准电影分类的相机指令和类型标签，解决了现有数据集的数据不平衡、噪声标签和模拟到真实差距问题。


<details>
  <summary>Details</summary>
Motivation: 当前可控视频合成模型难以实现超越文本提示的细粒度控制，特别是在相机轨迹和电影类型等电影属性方面。现有数据集存在严重的数据不平衡、噪声标签或显著的模拟到真实差距。

Method: 提出了CineLOG数据集，包含5000个高质量、平衡、未剪辑的视频片段，每个片段都有详细场景描述、基于标准电影分类的相机指令和类型标签。开发了新的流水线，将复杂的文本到视频生成任务解耦为四个更简单的阶段，并引入了轨迹引导过渡模块来生成平滑的时空插值。

Result: 广泛的人类评估表明，该流水线在遵循特定相机和剧本指令方面显著优于最先进的端到端文本到视频模型，同时保持专业的视觉质量。

Conclusion: CineLOG数据集和新流水线为可控视频合成提供了高质量、平衡的数据资源和有效的生成方法，解决了现有方法在细粒度控制方面的局限性。

Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.

</details>


### [40] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

TL;DR: 该论文提出了一种评估视觉语言模型推理链视觉忠实性的方法，并设计了轻量级自反思程序来检测和修复不忠实的感知步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的推理增强视觉语言模型虽然能生成显式的思维链，但存在新的失败模式：模型可能通过视觉上不忠实的中间步骤得出正确答案，或者推理忠实但最终预测失败。仅评估最终答案准确性的标准方法无法区分这些行为。

Method: 提出了一个无需训练和参考的框架：1) 将推理链分解为感知步骤和推理步骤；2) 使用现成的VLM作为评判者进行步骤级忠实性评估；3) 通过人类元评估验证该方法；4) 基于该指标设计了轻量级自反思程序，检测并局部重新生成不忠实的感知步骤。

Result: 在多个经过推理训练的VLM和感知密集型基准测试中，该方法降低了不忠实感知率，同时保持了最终答案准确性，提高了多模态推理的可靠性。

Conclusion: 视觉推理链的忠实性是一个独立的评估维度，提出的训练和参考无关的评估框架以及自反思程序能够有效提高多模态推理的可靠性。

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [41] [Fine-Grained Zero-Shot Learning with Attribute-Centric Representations](https://arxiv.org/abs/2512.12219)
*Zhi Chen,Jingcai Guo,Taotao Cai,Yuxiang Cai*

Main category: cs.CV

TL;DR: 提出ACR框架，通过属性解缠学习来解决零样本细粒度识别中的属性纠缠问题，在多个基准数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 细粒度类别识别需要区分细微视觉差异，传统方法将颜色、形状、纹理等不同属性压缩到单一视觉嵌入中，导致属性纠缠和干扰，现有后处理方法无法解决已混合的表征问题。

Method: 提出属性中心表示(ACR)框架，包含两个混合专家组件：MoPE(补丁专家混合)和MoAE(属性专家混合)。MoPE通过双级路由机制将图像补丁分派给专门专家处理连贯属性族，MoAE头将专家精炼特征投影为稀疏、部分感知的属性映射。

Result: 在CUB、AwA2和SUN等零样本学习基准数据集上，ACR框架取得了持续的最先进结果。

Conclusion: 通过在表示学习过程中施加属性解缠，ACR框架有效解决了属性纠缠问题，为细粒度零样本识别提供了更鲁棒的解决方案。

Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.

</details>


### [42] [ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation](https://arxiv.org/abs/2512.12220)
*Minheng Ni,Zhengyuan Yang,Yaowen Zhang,Linjie Li,Chung-Ching Lin,Kevin Lin,Zhendong Wang,Xiaofei Wang,Shujie Liu,Lei Zhang,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

TL;DR: 研究者开发了ProImage-Bench基准测试，用于评估专业图像生成模型在科学插图方面的表现，发现现有模型在科学准确性上存在显著差距，并通过反馈机制显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 专业图像生成需要从技术描述中合成信息密集、科学精确的插图，而不仅仅是视觉上合理的图片。现有模型在科学准确性方面表现不足，需要量化评估工具来推动该领域发展。

Method: 1. 构建ProImage-Bench基准：收集654个真实教科书和技术报告中的图表，创建详细图像指令；2. 建立分层评分标准：将正确性分解为6,076个标准和44,131个二元检查；3. 使用大型多模态模型从文本和参考图中推导评分标准；4. 开发自动化LMM评估器，采用原则性惩罚方案将子问题结果聚合成可解释的标准分数。

Result: 1. 基准测试显示，尽管在开放域表现良好，但最佳基础模型仅达到0.791的评分准确率和0.553的标准分数；2. 揭示了在细粒度科学保真度方面存在显著差距；3. 反馈机制实验表明，将失败的检查反馈给编辑模型进行迭代优化，可将评分准确率从0.653提升到0.865，标准分数从0.388提升到0.697。

Conclusion: ProImage-Bench为专业图像生成提供了严格的诊断工具，并为改进规范忠实度的科学插图提供了可扩展的信号。该基准不仅能够评估模型性能，还能通过反馈机制指导模型改进，推动科学插图生成领域的发展。

Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.

</details>


### [43] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

TL;DR: 该研究比较了SynthSeg和SamSeg两种方法在婴儿脑MRI分割中的准确性，发现SynthSeg在所有质量指标上均优于SamSeg，能提供更可靠的体积和分形维度估计。


<details>
  <summary>Details</summary>
Motivation: 婴儿脑MRI的准确分割对于量化结构发育变化至关重要，但由于髓鞘化过程中组织对比度降低，自动分割面临挑战。需要评估不同分割方法对体积和分形维度估计的影响。

Method: 使用Baby Open Brains数据集（71个扫描，1-9个月），比较SynthSeg和SamSeg两种分割方法。通过Dice系数、IoU、95% Hausdorff距离和归一化互信息等指标评估分割准确性，并分析对体积和分形维度估计的影响。

Result: SynthSeg在所有质量指标上均优于SamSeg（主要区域平均Dice > 0.8），体积估计与手动参考接近（平均+4%）。SamSeg系统性地高估脑室和全脑体积（平均+76%）。分割准确性随年龄增长而提高，与髓鞘化过程中组织对比度增加一致。分形维度分析显示SynthSeg与专家分割存在显著区域差异，分割相关的FD变异性超过了大多数发育队列报告的组间差异。

Conclusion: SynthSeg为儿科MRI提供了最可靠的体积和分形维度结果，但由于分割相关的不确定性，对体积和分形维度的微小形态学差异应谨慎解释。分割偏差直接影响FD估计，分割准确性随婴儿年龄增长而改善。

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [44] [Moment and Highlight Detection via MLLM Frame Segmentation](https://arxiv.org/abs/2512.12246)
*I Putu Andika Bagas Jiwanta,Ayu Purwarianti*

Main category: cs.CV

TL;DR: 提出了一种新颖的视频时刻检测方法，通过让多模态大语言模型输出二进制字符序列（0/1）来同时实现时刻定位和高亮检测，避免了传统文本生成方法无法提供帧级梯度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型的方法虽然有效，但文本生成方式无法为帧级预测提供直接梯度。尽管最近有强化学习方法尝试解决此问题，但作者提出了更直接的解决方案。

Method: 让LLM接收固定数量的帧，并通过提示使其输出连续的"0"和/或"1"字符序列（每个字符对应一帧）。这些字符既利用了LLM的语言能力，又作为背景和前景概率。训练时结合分割损失和因果语言模型损失。

Result: 在QVHighlights数据集上取得了56.74 HIT@1的高亮检测性能，仅采样25帧（少于同类方法的一半）。在时刻检索任务上也超过了基线（35.28 MAP）。

Conclusion: 该方法通过分割损失为LLM提供了稳定的补充学习信号，即使在因果语言模型损失平台期也能有效训练。二进制字符表示既利用了LLM的语言能力，又实现了帧级预测。

Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.

</details>


### [45] [MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2512.12268)
*Yuqing Lei,Yingjun Du,Yawen Huang,Xiantong Zhen,Ling Shao*

Main category: cs.CV

TL;DR: MetaTPT提出了一种元学习框架，通过自监督辅助任务动态学习参数化增强来指导测试时提示调优，提升视觉语言模型在域偏移下的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时提示调优方法使用固定增强策略，在更具挑战性的域偏移场景中可能失效。需要更灵活、更具表达力的增强方法来捕获目标域的关键特征。

Method: 采用元学习双循环优化范式：内循环学习自监督任务生成信息丰富的视图，外循环通过强制这些视图间的一致性进行提示调优。通过将增强学习与提示调优耦合，动态学习每个样本的参数化增强。

Result: 在域泛化和跨数据集基准测试中，MetaTPT取得了最先进的性能表现。

Conclusion: MetaTPT通过元学习自监督辅助任务动态生成参数化增强，有效提升了视觉语言模型在测试时面对域偏移的适应能力。

Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.

</details>


### [46] [Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions](https://arxiv.org/abs/2512.12277)
*Thibault Geoffroy,Myriam Maumy,Lionel Prevost*

Main category: cs.CV

TL;DR: 提出一种用于连续学习面部表情识别的混合框架，结合深度卷积特征和面部动作单元，通过贝叶斯高斯混合模型缓解灾难性遗忘，在CFEE数据集上验证了从基本表情到复合表情的渐进学习能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益融入日常生活，识别和适应人类情感对于有效的人机交互至关重要。面部表情识别是推断情感状态的主要渠道，但情感具有动态性和文化差异性，需要能够持续学习而不遗忘先前知识的模型。

Method: 提出混合框架，整合两种互补模态：深度卷积特征和面部动作单元。通过贝叶斯高斯混合模型对组合表示进行建模，提供轻量级概率解决方案，避免重新训练同时保持强判别能力。

Result: 在CFEE数据集上，模型能够先学习基本表情，然后逐步识别复合表情。实验显示提高了准确性、增强了知识保留能力并减少了遗忘。

Conclusion: 该框架有助于开发具有情感智能的AI系统，可应用于教育、医疗保健和自适应用户界面等领域。

Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.

</details>


### [47] [RealDrag: The First Dragging Benchmark with Real Target Image](https://arxiv.org/abs/2512.12287)
*Ahmad Zafarani,Zahra Dehghanian,Mohammadreza Davoodi,Mohsen Shadroo,MohammadAmin Fazli,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: RealDrag是首个包含真实目标图像的基于点拖拽图像编辑基准，包含400+人工标注样本和四个新指标，用于系统评估17个SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于拖拽的图像编辑模型评估不可靠，缺乏标准化基准和指标，没有包含真实目标图像的数据集，导致方法间难以客观比较。

Method: 创建RealDrag基准数据集，包含400+人工标注样本（来源/目标图像、控制点/目标点、可编辑区域掩码、描述性标注）；提出四个新指标：语义距离(SeD)、外部掩码保持分数(OMPS)、内部补丁保持分数(IPPS)、方向相似性(DiS)。

Result: 使用该基准对17个SOTA模型进行了首次大规模系统分析，揭示了当前方法间的明确权衡，建立了稳健、可复现的基线。

Conclusion: RealDrag为基于点拖拽的图像编辑提供了首个包含真实目标图像的标准化评估基准，提出的指标能全面量化编辑质量，为未来研究提供了可靠基础。

Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.

</details>


### [48] [From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving](https://arxiv.org/abs/2512.12302)
*Huan Zheng,Yucheng Zhou,Tianyi Yan,Jiayi Su,Hongjun Chen,Dubing Chen,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出了Intention-Drive基准测试，用于评估自动驾驶系统从高级人类意图到精确驾驶动作的转换能力，揭示了现有模型在意图理解方面的显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统仅能执行低级转向指令，缺乏理解并实现高级人类抽象意图的能力。实现真正智能自动驾驶需要从"指令跟随者"向"意图实现者"的范式转变，但目前缺乏标准化基准来衡量和推动这一复杂任务的进展。

Method: 提出Intention-Drive基准，包含两个核心贡献：1)包含复杂场景和对应自然语言意图的新数据集；2)以意图成功率(ISR)为中心的新评估协议，评估人类目标的语义实现程度，超越简单的几何精度。

Result: 通过对一系列基线模型在Intention-Drive上的广泛评估，揭示了显著的性能缺陷，表明基线模型难以达到这一高级任务所需的全面场景和意图理解能力。

Conclusion: Intention-Drive填补了自动驾驶领域的关键空白，为评估从高级意图到驾驶动作的转换能力提供了首个全面基准，揭示了当前系统在意图理解方面的局限性，为未来研究指明了方向。

Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

</details>


### [49] [OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2512.12303)
*Yang Ou,Xiongwei Zhao,Xinye Yang,Yihan Wang,Yicheng Di,Rong Yuan,Xieyuanli Chen,Xu Zhu*

Main category: cs.CV

TL;DR: OMUDA提出了一种用于无监督域自适应语义分割的统一框架，通过分层掩码策略解决跨域上下文模糊、特征表示不一致和类别伪标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应方法在语义分割中仍难以弥合域间差距，主要面临三个挑战：跨域上下文模糊性、不一致的特征表示以及类别级别的伪标签噪声。

Method: 提出OMUDA框架，包含三个分层掩码策略：1) 上下文感知掩码(CAM)自适应区分前景与背景以平衡全局上下文和局部细节；2) 特征蒸馏掩码(FDM)通过预训练模型的知识转移增强鲁棒且一致的特征学习；3) 类别解耦掩码(CDM)通过显式建模类别不确定性来减轻噪声伪标签的影响。

Result: 在多个具有挑战性的跨域语义分割基准测试中验证了OMUDA的有效性。在SYNTHIA->Cityscapes和GTA5->Cityscapes任务中，OMUDA可以无缝集成到现有UDA方法中，平均提升7%，达到最先进的结果。

Conclusion: OMUDA通过分层掩码范式在上下文、表示和类别三个层面有效减少域偏移，为无监督域自适应语义分割提供了超越现有方法的统一解决方案。

Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.

</details>


### [50] [MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding](https://arxiv.org/abs/2512.12307)
*Benjamin Beilharz,Thomas S. A. Wallis*

Main category: cs.CV

TL;DR: MRD方法使用基于物理的可微分渲染来探索视觉模型对3D场景属性的隐式理解，通过寻找产生相同模型激活的不同3D场景参数（模型同构体）。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在视觉任务上取得显著成功，但理解这些模型的表示和决策仍然困难。视觉模型通常基于2D输入训练，但被认为发展了对底层3D场景的隐式理解。需要一种方法来探测模型对生成性3D场景属性的理解。

Method: MRD（可微分渲染的同构体）方法使用基于物理的可微分渲染，通过寻找在物理上不同但产生相同模型激活的3D场景参数来探测视觉模型。与之前基于像素的方法不同，这些重建结果始终基于物理场景描述，可以独立探测模型对特定场景属性（如形状、材质）的敏感性。

Result: 作为原理验证，评估了多个模型在恢复场景几何（形状）和双向反射分布函数（材质）参数方面的能力。结果显示目标场景和优化场景之间的模型激活高度相似，但视觉结果各异。这些重建有助于定性研究模型对哪些物理场景属性敏感或不敏感。

Conclusion: MRD方法通过分析物理场景参数如何驱动模型响应变化，有望增进对计算机视觉和人类视觉的理解，为模型解释性提供新的分析工具。

Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

</details>


### [51] [WeDetect: Fast Open-Vocabulary Object Detection as Retrieval](https://arxiv.org/abs/2512.12309)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: WeDetect是一个基于检索哲学的开集目标检测模型家族，包含三个变体：基础版实现实时检测，Uni版支持历史数据回溯，Ref版结合大语言模型处理复杂指代表达。该系列在15个基准测试中达到SOTA，具有高效推理优势。


<details>
  <summary>Details</summary>
Motivation: 探索无跨模态融合层的开集目标检测方法，利用检索哲学实现高效推理和多功能应用，包括实时检测、历史数据回溯和复杂指代表达理解。

Method: 采用双塔架构，将目标检测视为检索问题：匹配区域与文本查询在共享嵌入空间。WeDetect为基础检测器，WeDetect-Uni冻结检测器并微调目标性提示生成通用建议，WeDetect-Ref结合大语言模型处理复杂指代表达。

Result: 在15个基准测试中达到最先进性能，实现实时检测，支持历史数据中的对象检索，能够处理复杂指代表达，同时保持高效推理速度。

Conclusion: WeDetect家族在统一的检索框架下集成了检测、建议生成、对象检索和指代表达理解，证明了检索哲学在开集目标检测中的高效性和多功能性优势。

Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.

</details>


### [52] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: UniCoDe提出了一种统一的算法，将采样方法和梯度引导方法结合，通过整合局部梯度信号提高采样效率，在奖励对齐和扩散模型先验之间实现更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型与下游目标的对齐对于提高任务特定性能至关重要。现有的推理时无训练对齐方法主要分为采样方法和梯度引导方法，但各自存在局限性，需要一种统一框架来结合两者的优势。

Method: 提出UniCoDe统一算法，将采样方法和梯度引导方法结合到一个框架中。在采样过程中整合局部梯度信号，解决复杂奖励采样方法的效率问题，实现更高效的采样。

Result: 实验结果表明，UniCoDe在一系列任务中与最先进的基线方法保持竞争力，在奖励对齐和扩散模型无条件先验偏离之间实现了更好的权衡。

Conclusion: UniCoDe成功地将采样和梯度引导两种范式统一起来，通过整合局部梯度信号提高了采样效率，为扩散模型的对齐问题提供了一个有效的解决方案。

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [53] [TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection](https://arxiv.org/abs/2512.12357)
*Zishen Song,Yongjian Zhu,Dong Wang,Hongzhan Liu,Lingyu Jiang,Yongxing Duan,Zehua Zhang,Sihan Li,Jiarui Li*

Main category: cs.CV

TL;DR: 提出TCLeaf-Net，一种针对田间叶片病害检测的Transformer-CNN混合检测器，配合新发布的Daylily-Leaf数据集，在复杂背景、域偏移和有限病变数据下实现鲁棒检测。


<details>
  <summary>Details</summary>
Motivation: 田间叶片病害检测面临三大挑战：复杂背景干扰、域偏移问题（理想条件与田间条件差异）、以及病变级数据集有限。现有方法在真实田间环境下检测效果不佳。

Method: 1) 提出TCLeaf-Net混合检测器；2) Transformer-卷积模块(TCM)结合全局上下文和局部卷积抑制非叶片区域；3) 原始尺度特征召回与采样(RSFRS)块通过双线性重采样和卷积保留空间细节；4) 可变形对齐FPN(DFPN)使用偏移对齐和多感受野感知增强多尺度融合。

Result: 在Daylily-Leaf数据集田间分割上，mAP@50提升5.4个百分点至78.2%，计算量减少7.5 GFLOPs，GPU内存使用降低8.7%。优于YOLO和RT-DETR系列，在PlantDoc、Tomato-Leaf和Rice-Leaf数据集上表现良好。

Conclusion: TCLeaf-Net能有效应对田间叶片病害检测的三大挑战，在精度、效率和泛化性方面均表现优异，为植物病害检测提供了鲁棒解决方案。

Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.

</details>


### [54] [STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative](https://arxiv.org/abs/2512.12372)
*Peixuan Zhang,Zijian Jia,Kaiqi Liu,Shuchen Weng,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: STAGE提出了一种基于故事板的多镜头视频生成工作流，通过预测结构化故事板而非稀疏关键帧，结合多镜头记忆包和双编码策略，解决了多镜头叙事中跨镜头一致性和电影语言捕捉的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在视频合成方面取得了显著进展，但创建连贯的多镜头叙事仍然是一个重大挑战。现有的关键帧方法虽然提供了细粒度控制和更高效率，但往往无法保持跨镜头一致性并捕捉电影语言。

Method: 提出了STAGE工作流，使用STEP2预测每个镜头的结构化故事板（起始-结束帧对）。引入了多镜头记忆包确保长距离实体一致性，双编码策略保证镜头内连贯性，两阶段训练方案学习电影化的镜头间过渡。同时贡献了ConStoryBoard数据集。

Result: 大量实验表明，STAGE在结构化叙事控制和跨镜头连贯性方面实现了优越性能。

Conclusion: STAGE通过故事板锚定的生成方法，有效解决了多镜头视频生成中的跨镜头一致性和电影语言捕捉问题，为连贯叙事视频生成提供了新思路。

Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.

</details>


### [55] [V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping](https://arxiv.org/abs/2512.12375)
*Hyunkoo Lee,Wooseok Jang,Jini Yang,Taehwan Kim,Sangoh Kim,Sangwon Jung,Seungryong Kim*

Main category: cs.CV

TL;DR: V-Warper是一个无需训练的视频个性化框架，通过粗粒度外观适应和细粒度外观注入两阶段方法，在保持文本对齐和运动动态的同时显著提升外观保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频个性化方法通常需要大量视频微调或大规模视频数据集，计算成本高且难以扩展，同时在跨帧保持细粒度外观一致性方面存在困难。

Method: 提出两阶段训练免费框架：1) 轻量级粗粒度外观适应阶段，仅使用少量参考图像，通过图像级LoRA和主体嵌入适应编码全局主体身份；2) 推理时细粒度外观注入阶段，通过计算RoPE-free中间层查询-键特征的语义对应关系，引导外观丰富的值表示到生成过程的语义对齐区域。

Result: V-Warper显著提高了外观保真度，同时保持了提示对齐和运动动态，且无需大规模视频微调即可高效实现这些改进。

Conclusion: V-Warper为基于transformer的视频扩散模型提供了一个有效的训练免费个性化框架，解决了现有方法在计算成本和外观一致性方面的局限性。

Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.

</details>


### [56] [M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction](https://arxiv.org/abs/2512.12378)
*Junqiao Fan,Yunjiao Zhou,Yizhuo Yang,Xinyuan Cui,Jiarui Zhang,Lihua Xie,Jianfei Yang,Chris Xiaoxuan Lu,Fangqiang Ding*

Main category: cs.CV

TL;DR: M4Human是目前最大规模的多模态人体网格重建基准数据集，包含66.1万帧高分辨率毫米波雷达、RGB和深度数据，提供原始雷达张量和处理后的雷达点云，支持不同粒度RF信号研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模人体网格重建数据集主要依赖视线RGB输入，但视觉传感受遮挡、光照变化和隐私问题限制。毫米波雷达能实现隐私保护的室内人体感知，但现有雷达数据集存在骨架标签稀疏、规模有限、动作简单等问题。

Method: 构建了包含20名受试者和50种多样化动作的M4Human数据集，提供高分辨率毫米波雷达、RGB和深度数据，包括原始雷达张量和处理后的雷达点云，并配有高质量动作捕捉标注（3D网格和全局轨迹）。

Result: M4Human是目前最大规模的雷达人体建模数据集（比之前最大数据集大9倍），建立了雷达张量和雷达点云两种模态的基准，并探索了与RGB-D模态的多模态融合。实验结果显示该数据集对雷达人体建模具有重要意义，但也揭示了在快速、无约束运动下的持续挑战。

Conclusion: M4Human为人体网格重建研究社区提供了重要的多模态基准，克服了现有雷达数据集的局限性，支持不同粒度RF信号研究，将促进隐私保护人体感知技术的发展。

Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.

</details>


### [57] [Speedrunning ImageNet Diffusion](https://arxiv.org/abs/2512.12386)
*Swayam Bhanded*

Main category: cs.CV

TL;DR: SR-DiT框架通过系统整合多种技术（token路由、架构改进、训练修改和表示对齐），在140M参数模型上实现了与更大模型相当的性能，为扩散变换器提供了高效训练基准。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散变换器的训练效率已有显著提升，但现有技术多被孤立研究，缺乏对多种方法组合协同效应的探索。本研究旨在系统整合不同技术，发掘其潜在协同作用，为扩散变换器提供更高效的训练框架。

Method: 提出SR-DiT（Speedrun Diffusion Transformer）框架，在表示对齐基础上系统整合：1）token路由技术；2）架构改进；3）训练修改。通过大量消融实验验证不同技术组合的有效性，识别协同作用和兼容性问题。

Result: 在ImageNet-256上仅使用140M参数模型、400K迭代、无分类器引导的情况下，达到FID 3.49和KDD 0.319，性能与训练时间更长、参数更多（685M）的模型相当。这是该模型规模下的最先进结果。

Conclusion: SR-DiT框架成功展示了多种技术整合的协同效应，为扩散变换器提供了计算可访问的基准。研究不仅实现了小模型的高性能，还通过消融实验为未来研究提供了技术组合的有效性指导。

Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.

</details>


### [58] [ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States](https://arxiv.org/abs/2512.12395)
*Haowen Wang,Xiaoping Yuan,Fugang Zhang,Rui Jian,Yuanwei Zhu,Xiuquan Qiao,Yakun Huang*

Main category: cs.CV

TL;DR: ArtGen：基于扩散模型的框架，能从单视图图像或文本描述生成具有准确几何结构和连贯运动学的铰接式3D物体，解决了现有方法中几何形状与关节动力学纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型通常依赖表示闭合状态的单视图输入，导致几何形状与关节动力学纠缠，产生模糊或不现实的运动学结构。铰接式资产生成对机器人、数字孪生和具身智能至关重要。

Method: 1. 使用跨状态蒙特卡洛采样明确强制执行全局运动学一致性；2. 集成思维链推理模块推断结构先验（部件语义、关节类型、连接性）；3. 稀疏专家扩散变换器专门处理多样运动学交互；4. 局部-全局注意力增强的组合式3D-VAE潜在先验。

Result: 在PartNet-Mobility基准测试上的广泛实验表明，ArtGen显著优于现有最先进方法。

Conclusion: ArtGen通过解耦几何形状与运动学，能够从单视图输入生成具有准确几何和连贯运动学的铰接式3D物体，为机器人、数字孪生和具身智能应用提供了有效解决方案。

Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.

</details>


### [59] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

TL;DR: ViInfographicVQA是首个越南语信息图表视觉问答基准，包含6747个真实信息图表和20409个人工验证问答对，涵盖单图和多图推理任务，揭示了当前多模态模型在低资源语言中的局限性。


<details>
  <summary>Details</summary>
Motivation: 信息图表视觉问答需要模型理解数据丰富、布局复杂的视觉内容，结合文本、图表、图标和设计元素。相比场景文本或自然图像VQA，信息图表需要更强的OCR、布局理解以及数值和语义推理能力。目前缺乏越南语的信息图表VQA基准，特别是在跨图像推理方面。

Method: 创建ViInfographicVQA基准，包含6747个真实世界信息图表和20409个人工验证的问答对，涵盖经济、医疗、教育等多个领域。基准包含两种评估设置：单图任务（传统设置）和多图任务（需要跨多个语义相关信息图表合成证据）。评估了一系列最新的视觉语言模型在该基准上的表现。

Result: 评估结果显示显著的性能差异，最严重的错误发生在多图问题上，特别是涉及跨图像整合和非跨度推理的任务。当前多模态模型在低资源语言环境中存在明显局限性。

Conclusion: ViInfographicVQA为越南语信息图表VQA提供了基准结果，揭示了当前多模态模型在低资源环境中的局限性，鼓励未来探索布局感知和跨图像推理方法。

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [60] [D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation](https://arxiv.org/abs/2512.12622)
*Zihan Wang,Seungjun Lee,Guangzhao Dai,Gim Hee Lee*

Main category: cs.CV

TL;DR: D3D-VLP模型通过动态3D思维链和协同学习策略，统一了具身智能中的规划、导航、问答等任务，在多个基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能系统存在两个问题：端到端模型缺乏可解释性和显式3D推理能力，而模块化系统忽略了跨组件间的相互依赖和协同效应。需要一种既能保持可解释性又能实现跨任务协同的解决方案。

Method: 提出动态3D视觉-语言-规划模型(D3D-VLP)，包含两个关键创新：1) 动态3D思维链(3D CoT)，将规划、定位、导航和问答统一到单个3D-VLM和CoT流程中；2) 碎片化监督协同学习(SLFS)策略，使用掩码自回归损失从大规模部分标注的混合数据中学习，让不同CoT组件相互增强和隐式监督。

Result: 构建了包含10M混合样本的大规模数据集，涵盖5K真实扫描和20K合成场景。在多个基准测试中取得SOTA结果：视觉语言导航(R2R-CE, REVERIE-CE, NavRAG-CE)、目标导航(HM3D-OVON)、任务导向顺序定位和导航(SG3D)。真实世界移动操作实验进一步验证了有效性。

Conclusion: D3D-VLP成功解决了具身智能中可解释性与协同学习的矛盾，通过统一的3D思维链框架实现了跨任务的协同增强，在多个导航和定位任务中表现出色，为具身智能系统提供了新的解决方案。

Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.

</details>


### [61] [BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation](https://arxiv.org/abs/2512.12425)
*Hangwei Zhang,Armando Teles Fortes,Tianyi Wei,Xingang Pan*

Main category: cs.CV

TL;DR: BokehDepth：一个两阶段框架，将散景合成与深度预测解耦，利用散焦作为无监督的几何线索，提升散景渲染质量和单目深度估计精度。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能充分利用散景与单目深度估计之间的紧密联系。高质量的散景渲染依赖噪声深度图，而现代单目深度模型在弱纹理、远距离和几何模糊区域表现不佳，这些区域恰恰是散焦线索最丰富的地方。

Method: 两阶段框架：第一阶段使用基于预训练图像编辑骨干的物理引导可控散景生成器，从单张清晰输入生成无深度散景堆栈；第二阶段通过轻量级散焦感知聚合模块，将散焦维度特征融合到现有单目深度编码器中，暴露稳定的深度敏感变化。

Result: 在多个挑战性基准测试中，BokehDepth相比基于深度图的散景基线提升了视觉保真度，并持续增强了强单目深度基础模型的度量精度和鲁棒性。

Conclusion: 通过将散景合成与深度预测解耦，并将散焦作为辅助的无监督几何线索，BokehDepth框架能够同时提升散景渲染质量和单目深度估计性能，充分利用了镜头成像几何的内在联系。

Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.

</details>


### [62] [Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection](https://arxiv.org/abs/2512.12884)
*Xiangzhong Liu,Jiajie Zhang,Hao Shen*

Main category: cs.CV

TL;DR: 提出一种用于自动驾驶传感器融合的端到端跨层级Transformer融合方法，将高度抽象的目标列表信息与原始相机图像结合进行3D目标检测，在nuScenes数据集上显著优于基于视觉的基线方法。


<details>
  <summary>Details</summary>
Motivation: 在汽车传感器融合系统中，智能传感器和V2X模块通常只提供处理后的目标列表而非原始数据。传统方法分别处理原始数据后在目标层级融合，存在效率问题。需要一种能够直接整合高度抽象目标列表信息与原始相机图像的融合方法。

Method: 提出端到端跨层级融合概念：1) 将目标列表作为去噪查询输入Transformer，与可学习查询一起传播；2) 从目标列表的位置和尺寸先验中导出可变形高斯掩码，集成到Transformer解码器中，引导注意力到感兴趣区域并加速训练收敛；3) 由于缺乏包含目标列表作为独立模态的公开数据集，提出从真实边界框生成伪目标列表的方法，模拟状态噪声和误报/漏报。

Result: 在nuScenes数据集上，该方法相比基于视觉的基线方法取得了显著的性能提升。展示了该方法对不同噪声水平的模拟目标列表和真实检测器的泛化能力。

Conclusion: 这是首个进行跨层级融合的工作，成功将高度抽象的目标列表信息与原始相机图像融合，为自动驾驶传感器融合提供了新的有效方法，具有实际应用价值。

Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.

</details>


### [63] [Endless World: Real-Time 3D-Aware Long Video Generation](https://arxiv.org/abs/2512.12430)
*Ke Zhang,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: Endless World是一个实时无限3D一致视频生成框架，通过条件自回归训练和全局3D感知注意力机制，实现长序列、几何一致的视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在生成长、连贯视频序列时面临的3D结构不稳定问题，特别是在流式场景中，需要实现无限、3D一致的实时视频生成。

Method: 1. 条件自回归训练策略：对齐新生成内容与现有视频帧，保持长程依赖且计算高效；2. 全局3D感知注意力：提供跨时间的连续几何指导；3. 3D注入机制：确保物理合理性和几何一致性。

Result: 实验表明Endless World能够生成长、稳定、视觉连贯的视频，在视觉保真度和空间一致性方面达到或超越现有方法的性能，支持单GPU实时推理。

Conclusion: Endless World成功解决了长序列视频生成的3D一致性问题，实现了无限、实时、几何一致的视频生成，为动态场景合成提供了有效解决方案。

Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.

</details>


### [64] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

TL;DR: 本文提出了一种基于检索增强生成(RAG)的零样本路标识别框架，使用视觉语言模型生成文本描述，检索相关候选标志，再由大语言模型进行细粒度识别，在303个监管标志上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法面临路标类别繁多和标注数据不足的挑战，需要一种无需任务特定训练就能准确识别大量路标类别的可扩展解决方案。

Method: 采用RAG范式：1) 使用视觉语言模型从输入图像生成文本描述；2) 从参考设计的向量数据库中检索最相关的候选标志；3) 使用大语言模型对检索到的候选进行推理，做出最终细粒度识别。

Result: 在俄亥俄州MUTCD的303个监管标志上进行验证，在理想参考图像上达到95.58%的准确率，在具有挑战性的真实道路数据上达到82.45%的准确率。

Conclusion: 该工作证明了基于RAG的架构在无需任务特定训练的情况下，创建可扩展且准确的路标识别系统的可行性。

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [65] [From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields](https://arxiv.org/abs/2512.12459)
*Jiachen Tao,Benjamin Planche,Van Nguyen Nguyen,Junyi Wu,Yuchun Liu,Haoxuan Wang,Zhongpai Gao,Gengyu Zhang,Meng Zheng,Feiran Wang,Anwesa Choudhuri,Zhenghao Zhao,Weitai Kang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: 本文提出高斯光子场(GPF)，一种可学习的表示方法，将光子映射重新表述为连续可重用的辐射函数，用于加速多视角渲染。


<details>
  <summary>Details</summary>
Motivation: 光子映射虽然能提供物理上准确的光线传输模拟，但在渲染同一场景的多个视角时效率低下，因为每个视角都需要独立的光子追踪和随机核估计，导致大量冗余计算。

Method: 引入高斯光子场(GPF)，将光子分布编码为各向异性的3D高斯基元，参数包括位置、旋转、尺度和光谱。GPF从第一个SPPM迭代的物理追踪光子初始化，并使用最终辐射度的多视角监督进行优化，将基于光子的光线传输提炼为连续场。

Result: 在具有复杂光线传输的场景（如焦散和镜面-漫反射交互）上进行广泛实验，证明GPF在保持光子级精度的同时，将计算量减少了数个数量级。

Conclusion: GPF将基于光子的渲染的物理严谨性与神经场景表示的效率相结合，实现了可微分的辐射度评估，无需重复光子追踪或迭代优化。

Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.

</details>


### [66] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: Motus是一个统一潜在动作世界模型，通过混合Transformer架构整合理解、视频生成和动作三个专家模块，利用光流学习潜在动作，在仿真和真实场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能方法采用孤立的理解、世界建模和控制模型，这种碎片化阻碍了多模态生成能力的统一，也妨碍了从大规模异构数据中学习。需要构建统一的系统来整合这些功能。

Method: 提出Motus统一潜在动作世界模型：1）采用混合Transformer架构整合三个专家模块（理解、视频生成、动作）；2）使用UniDiffuser风格调度器实现不同建模模式的灵活切换；3）利用光流学习潜在动作，提取像素级"delta动作"；4）采用三阶段训练流程和六层数据金字塔进行大规模动作预训练。

Result: 在仿真场景中：比X-VLA提升15%，比Pi0.5提升45%；在真实场景中：提升11-48%。证明了统一建模所有功能和先验对下游机器人任务有显著益处。

Conclusion: Motus通过统一潜在动作世界模型成功整合了理解、世界建模和控制功能，利用现有预训练模型和丰富的可共享运动信息，显著提升了具身智能系统的性能，证明了统一建模方法的有效性。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [67] [More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models](https://arxiv.org/abs/2512.12487)
*Hoang Anh Just,Yifei Fan,Handong Zhao,Jiuxiang Gu,Ruiyi Zhang,Simon Jenni,Kushal Kafle,Ruoxi Jia,Jing Shi*

Main category: cs.CV

TL;DR: PeRL-VL提出了一种解耦的视觉语言模型训练框架，通过分别改进视觉感知和文本推理来提升RLVR训练效果，在多个多模态基准测试中显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR训练方法虽然从纯文本扩展到视觉语言模型，但仍然存在两个主要问题：视觉提取不准确（遗漏或幻觉细节）和思维链逻辑不一致，因为可验证信号只监督最终答案。

Method: PeRL-VL采用解耦框架，分别改进视觉感知和文本推理。对于感知，引入基于VLM的描述奖励，评估模型自生成图像描述的忠实性和充分性；对于推理，增加纯文本推理SFT阶段，在逻辑丰富的思维链数据上训练，独立于视觉提升逻辑一致性。

Result: 在多样化多模态基准测试中，PeRL-VL将平均Pass@1准确率从63.3%（基础Qwen2.5-VL-7B）提升到68.8%，优于标准RLVR、纯文本推理SFT和从GPT-4o的朴素多模态蒸馏。

Conclusion: PeRL-VL通过解耦视觉感知和文本推理的改进，有效解决了RLVR训练中视觉提取不准确和逻辑不一致的问题，在多模态任务中取得了显著性能提升。

Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

</details>


### [68] [MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion](https://arxiv.org/abs/2512.13177)
*Minghui Hou,Wei-Hsing Huang,Shaofeng Liang,Daizong Liu,Tai-Hao Wen,Gang Wang,Runwei Guan,Weiping Ding*

Main category: cs.CV

TL;DR: MMDrive是一个用于自动驾驶的多模态视觉语言模型框架，将传统2D图像理解扩展到3D场景理解，融合占据图、LiDAR点云和文本描述三种模态，通过自适应跨模态融合和关键信息提取提升复杂驾驶环境下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型受限于2D平面图像理解范式，难以感知3D空间信息并进行深度语义融合，导致在复杂自动驾驶环境中表现不佳。需要突破传统图像理解限制，实现更强大的多模态推理能力。

Method: 提出MMDrive框架，包含三种互补模态：占据图、LiDAR点云和文本场景描述。引入两个核心组件：1) 面向文本的多模态调制器，根据问题语义动态加权各模态贡献；2) 跨模态抽象器，使用可学习的抽象令牌生成紧凑的跨模态摘要，突出关键区域和语义信息。

Result: 在DriveLM和NuScenes-QA基准测试中表现优异：DriveLM上BLEU-4得分54.56，METEOR得分41.78；NuScenes-QA上准确率62.7%，显著优于现有自动驾驶视觉语言模型。

Conclusion: MMDrive有效突破了传统仅图像理解的限制，实现了复杂驾驶环境下的鲁棒多模态推理，为可解释的自动驾驶场景理解提供了新基础。

Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

</details>


### [69] [Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings](https://arxiv.org/abs/2512.12492)
*Shengkai Xu,Hsiang Lun Kao,Tianxiang Xu,Honghui Zhang,Junqiao Wang,Runmeng Ding,Guanyu Liu,Tianyu Shi,Zhenyu Yu,Guofeng Pan,Ziqian Bi,Yuqi Ouyang*

Main category: cs.CV

TL;DR: 提出AdaptiveDetector框架，通过YOLOv11检测器和VLM验证器的两阶段设计，结合自适应阈值调整和成本敏感强化学习，显著提升内镜息肉检测在真实恶劣条件下的召回率。


<details>
  <summary>Details</summary>
Motivation: 现有息肉检测器在干净数据集上训练，但在真实内镜环境中（光照变化、运动模糊、遮挡等恶劣成像条件）表现不佳，存在临床实践与实验室条件之间的领域差距问题。

Method: 提出两阶段检测器-验证器框架：1）YOLOv11检测器在VLM指导下自适应调整每帧置信度阈值；2）VLM验证器使用非对称成本敏感奖励函数通过GRPO进行微调，专门设计来减少漏检。构建合成测试平台系统性地模拟临床恶劣条件。

Result: 在合成退化的CVC-ClinicDB和Kvasir-SEG图像上进行零样本评估，召回率比单独使用YOLO提高14-22个百分点，精度保持在基线以下0.7点到以上1.7点范围内。显著减少假阴性，降低漏检癌前息肉风险。

Conclusion: 自适应阈值调整与成本敏感强化学习的结合实现了临床对齐的开放世界息肉检测，通过大幅减少假阴性改善了患者预后，为恶劣成像条件下的可靠息肉检测提供了有效解决方案。

Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

</details>


### [70] [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
*Haoyu Fu,Diankun Zhang,Zongchuang Zhao,Jianfeng Cui,Hongwei Xie,Bing Wang,Guang Chen,Dingkang Liang,Xiang Bai*

Main category: cs.CV

TL;DR: MindDrive提出了一种基于在线强化学习的VLA框架，通过将连续动作空间映射到有限的语言决策空间来解决自动驾驶中的探索效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的视觉-语言-动作范式主要依赖模仿学习，存在分布偏移和因果混淆问题。在线强化学习能通过试错学习解决这些问题，但在连续动作空间中探索效率低下。

Method: MindDrive采用具有两组不同LoRA参数的大语言模型：一个作为决策专家进行场景推理和驾驶决策，另一个作为动作专家将语言决策动态映射为可行轨迹。通过将轨迹级奖励反馈到推理空间，在有限的语言决策空间而非连续动作空间中进行试错学习。

Result: 在Bench2Drive基准测试中，MindDrive取得了78.04的驾驶分数和55.09%的成功率，首次证明了在线强化学习在自动驾驶VLA模型中的有效性。

Conclusion: MindDrive通过将连续动作空间映射到离散语言决策空间，有效平衡了复杂场景中的最优决策、类人驾驶行为和在线强化学习的高效探索，为解决自动驾驶VLA模型的探索效率问题提供了新思路。

Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

</details>


### [71] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

TL;DR: 利用视频基础模型进行时空数据增强，通过视频扩散模型生成真实的3D空间和时间变化，在低数据场景下提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法主要基于简单的几何变换或外观扰动，无法有效生成3D空间和时间变化。在无人机图像等标注稀缺的低数据场景中，需要更有效的增强方法来扩展数据分布

Method: 使用现成的视频扩散模型从给定图像数据集中生成真实的3D空间和时间变化，将合成的视频片段作为补充训练数据。提供实用指南：选择适当的时空生成设置、将标注转移到合成帧、处理新暴露区域的遮挡问题

Result: 在COCO子集和无人机捕获数据集上的实验表明，时空增强能够沿着传统和先前生成方法未能充分代表的轴扩展数据分布，在数据稀缺情况下持续提升模型性能

Conclusion: 时空数据增强通过视频基础模型能够有效扩展数据分布，特别是在低数据场景下，为提升模型性能提供了有效手段。需要谨慎应用，并考虑遮挡等实际问题

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [72] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

TL;DR: Animus3D：基于文本驱动的3D动画框架，通过Motion Score Distillation（MSD）替代传统SDS，结合LoRA增强的视频扩散模型和时空正则化，为静态3D资产生成高质量运动场


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用vanilla Score Distillation Sampling（SDS）从预训练文本到视频扩散模型中提取运动，导致动画运动幅度小或存在明显抖动问题，需要更有效的运动生成方案

Method: 提出Motion Score Distillation（MSD）替代SDS，包含：1）LoRA增强的视频扩散模型，以静态源分布而非纯噪声作为起点；2）基于反转的噪声估计技术保持外观一致性；3）时空正则化项减少几何失真；4）运动细化模块提升时间分辨率和细节

Result: 实验表明Animus3D能成功为多样文本提示的静态3D资产生成动画，相比现有基线方法产生更显著、更详细的运动，同时保持高视觉完整性

Conclusion: Animus3D通过创新的MSD方法和综合优化策略，有效解决了文本驱动3D动画中的运动幅度不足和抖动问题，为静态3D资产生成高质量动画提供了新方案

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [73] [Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling](https://arxiv.org/abs/2512.12539)
*Huan Huang,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 提出一种冠状动脉分割框架，整合心肌解剖先验、结构感知特征编码和三维小波变换，在复杂几何条件下实现更稳定一致的冠状动脉分割。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉CT血管成像的准确分割对定量分析和临床决策至关重要，但由于血管管径小、分支复杂、边界模糊和心肌干扰等因素，可靠分割仍然具有挑战性。

Method: 提出冠状动脉分割框架，整合心肌解剖先验、结构感知特征编码和三维小波-逆小波变换。编码阶段加入心肌先验和残差注意力特征增强，小波-逆小波下采样和上采样实现联合空间频率建模，解码阶段多尺度特征融合模块整合语义和几何信息。

Result: 在公开ImageCAS数据集上评估，采用3D重叠补丁策略，训练/验证/测试比例为7:1:2。Dice系数0.8082，敏感性0.7946，精确度0.8471，HD95为9.77mm，优于多个主流分割模型。消融研究证实各组件互补贡献。

Conclusion: 该方法在复杂几何条件下实现更稳定一致的冠状动脉分割，为后续冠状动脉结构分析任务提供可靠分割结果。

Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.

</details>


### [74] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对多模态大语言模型的视频token剪枝方法，通过最大空间相邻视频token相似度（MSSAVT）指标和掩码剪枝策略，在减少计算开销的同时保持关键信息，显著提升了在线视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 在线视频理解在公共监控和AI眼镜等应用中至关重要，但多模态大语言模型处理视频时面临帧数过多导致的GPU内存占用高和计算延迟大的挑战，需要有效的token剪枝方法来减少上下文长度同时保留关键信息。

Method: 提出基于最大空间相邻视频token相似度（MSSAVT）的冗余度量方法，考虑token相似度和空间位置；设计掩码剪枝策略解决剪枝与冗余之间的双向依赖问题；结合现有的时间冗余剪枝方法消除视频模态的时间冗余。

Result: 在多个在线和离线视频理解基准测试中，该方法显著提高了准确率（最多提升4%），同时剪枝延迟可忽略不计（小于1毫秒）。

Conclusion: 提出的token剪枝方法有效解决了多模态大语言模型在视频理解中的计算效率问题，通过空间和时间冗余的联合剪枝，在保持性能的同时大幅降低了计算开销。

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [75] [StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis](https://arxiv.org/abs/2512.12586)
*Lixin Chen,Chaomeng Chen,Jiale Zhou,Zhijian Wu,Xun Lin*

Main category: cs.CV

TL;DR: StegaVAR是一个新颖的隐私保护视频动作识别框架，首次在隐写域中直接执行视频动作识别，将动作视频嵌入到普通封面视频中，既保护隐私又保持时空特征完整性。


<details>
  <summary>Details</summary>
Motivation: 当前隐私保护方法存在两个主要问题：(1)低隐蔽性，产生视觉扭曲的视频在传输过程中吸引攻击者注意；(2)时空破坏，降低准确VAR所需的基本时空特征。需要解决视频动作识别中的隐私泄露问题。

Method: 提出StegaVAR框架，将动作视频嵌入普通封面视频中，首次在隐写域直接执行VAR。提出两个关键技术：STeP（秘密时空促进）使用秘密视频指导隐写域中的时空特征提取；CroDA（跨波段差异注意力）通过捕捉跨波段语义差异来抑制封面干扰。

Result: 实验证明StegaVAR在广泛使用的数据集上实现了优越的视频动作识别和隐私保护性能。该框架对多种隐写模型都有效。

Conclusion: StegaVAR成功解决了隐私保护视频动作识别中的隐蔽性和时空特征保持问题，为隐私敏感的VAR应用提供了有效的解决方案。

Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.

</details>


### [76] [Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation](https://arxiv.org/abs/2512.12595)
*Karthikeya KV*

Main category: cs.CV

TL;DR: 提出了一种结合视觉增强大语言模型与先进Transformer架构的框架，通过整流流机制和双向标记化策略，实现高质量图像合成和多模态数据理解，相比扩散方法提升25%图像分辨率清晰度并减少20%计算需求。


<details>
  <summary>Details</summary>
Motivation: 当前在高分辨率图像合成和多模态数据解释方面存在挑战，需要一种能够有效整合文本、图像和视频等多种数据类型的统一框架，以提升生成质量和计算效率。

Method: 采用整流流机制连接噪声与数据，使用双向标记化策略融合文本、图像和视频输入，嵌入时空特征并采用混合文本-图像序列建模方法，通过噪声感知学习算法优化架构。

Result: 在基准数据集上评估显示，相比基于扩散的方法，图像分辨率清晰度提升25%，计算需求减少20%，模型展现出强大的可扩展性和适应性。

Conclusion: 该研究证明了视觉中心大语言模型在重新定义计算机视觉和多模态人工智能能力方面的作用，具有在自主系统、创意内容生成和高级视频分析等应用中的潜力。

Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

</details>


### [77] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型（VLM）的图像广告布局生成方法，通过分析背景图像内容来优化文本和logo的放置位置。


<details>
  <summary>Details</summary>
Motivation: 传统广告布局技术主要依赖显著性映射来检测背景图像中的显著区域，但这种方法往往无法充分考虑图像的详细构图和语义内容，导致布局质量受限。

Method: 采用两阶段流程：首先使用VLM分析图像，识别物体类型和空间关系，生成基于文本的"放置计划"；然后将该计划渲染为HTML格式的最终布局。

Result: 通过定量和定性实验验证，与现有方法相比，该方法能生成明显更高质量的广告布局，特别是在考虑背景图像内容方面表现优异。

Conclusion: 通过利用VLM显式考虑背景图像的语义内容，提出的方法能够生成更高质量的广告布局，克服了传统显著性映射方法的局限性。

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [78] [Geometry-Aware Scene-Consistent Image Generation](https://arxiv.org/abs/2512.12598)
*Cong Xie,Che Wang,Yan Zhang,Zheng Pan,Han Zou,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 提出了一种几何感知的场景一致图像生成方法，通过场景一致数据构建和几何引导注意力损失，在保持参考场景物理环境的同时，根据文本描述的空间关系生成新实体。


<details>
  <summary>Details</summary>
Motivation: 现有方法在场景保持和提示遵循之间存在权衡：要么高保真地复制场景但对提示响应差，要么优先遵循提示但牺牲场景一致性。需要解决这一平衡问题。

Method: 提出两个关键贡献：(1) 场景一致数据构建流程，生成多样化的几何基础训练对；(2) 新颖的几何引导注意力损失，利用跨视图线索来正则化模型的空间推理能力。

Result: 在场景一致基准测试中，该方法在场景对齐和文本图像一致性方面优于现有最先进基线，自动指标和人类偏好研究均显示更好结果。生成几何一致的图像，保持对文本指令和底层场景结构的忠实性。

Conclusion: 该方法成功解决了场景保持与提示遵循之间的权衡问题，通过几何感知方法实现了场景一致的图像生成，在保持物理环境的同时正确生成符合空间关系描述的实体。

Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.

</details>


### [79] [No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching](https://arxiv.org/abs/2512.12604)
*Tingyan Wen,Haoyu Li,Yihuang Chen,Xing Zhou,Lifei Zhu,Xueqian Wang*

Main category: cs.CV

TL;DR: X-Slim是一种训练免费的缓存加速器，通过三级缓存（时间步、结构块、空间标记）和双阈值控制器，在扩散模型中实现高效加速，平衡速度与生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量优秀，但计算开销随步数、模型深度和序列长度线性增长。现有缓存方法存在固有权衡：激进的时间步重用能大幅加速但容易损害保真度，而块级或标记级重用更安全但计算节省有限。

Method: 提出X-Slim框架，首次统一利用时间步、结构块和空间标记三个维度的缓存冗余。采用双阈值控制器实现"推送-抛光"过程：先将时间步重用推至预警线，然后切换到轻量级块级和标记级刷新来抛光剩余冗余，当跨过临界线时触发完整推理以重置累积误差。每个层级使用上下文感知指标决定何时何地缓存。

Result: 在多样化任务中显著推进了速度-质量前沿：在FLUX.1-dev上延迟降低高达4.97倍，在HunyuanVideo上降低3.52倍，感知损失最小；在DiT-XL/2上达到3.13倍加速，FID比先前方法提升2.42。

Conclusion: X-Slim通过统一的三级缓存框架和智能的双阈值控制机制，有效解决了扩散模型加速中的速度-质量权衡问题，实现了显著的性能提升。

Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

</details>


### [80] [Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space](https://arxiv.org/abs/2512.12623)
*Chengzhi Liu,Yuzhe Yang,Yue Fan,Qingyue Wei,Sheng Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: DMLR提出了一种动态多模态潜在推理框架，通过置信度引导的潜在策略梯度优化来精炼潜在思考标记，实现推理与感知的动态交错，显著提升多模态推理性能同时保持高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式的逐步推理，存在感知-推理交互不稳定和计算开销大的问题。受人类认知启发，认为思维不是线性展开，而是通过推理和感知在头脑中的动态交错进行的。

Method: 提出DMLR框架：1) 使用置信度引导的潜在策略梯度优化来精炼潜在思考标记进行深度推理；2) 引入动态视觉注入策略，在每个潜在思考标记处检索最相关的视觉特征并更新最佳视觉补丁集，然后将更新的补丁注入潜在思考标记以实现动态视觉-文本交错。

Result: 在七个多模态推理基准测试和各种模型架构上的实验表明，DMLR显著提高了推理和感知性能，同时保持了高推理效率。

Conclusion: DMLR通过动态多模态潜在推理框架，实现了推理与感知的动态交错，有效解决了现有方法中显式逐步推理的局限性，在提升性能的同时保持了高效推理。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

</details>


### [81] [Cross-modal Fundus Image Registration under Large FoV Disparity](https://arxiv.org/abs/2512.12657)
*Hongyang Li,Junyi Tao,Qijie Wei,Ningzhi Yang,Meng Wang,Weihong Yu,Xirong Li*

Main category: cs.CV

TL;DR: 本文提出CARe方法解决大视场差异的跨模态眼底图像配准问题，通过裁剪和对齐操作实现有效配准


<details>
  <summary>Details</summary>
Motivation: 现有跨模态眼底图像配准方法假设小视场差异，但实际应用中存在大视场差异的挑战场景，直接应用现有方法会失败

Method: 提出CARe方法：1)裁剪操作利用视网膜生理结构从目标图像裁剪与源图像视场大致对齐的子图像；2)对齐模块采用双拟合方法，顺序使用RANSAC算法和多项式坐标拟合改进空间变换

Result: 在包含60对OCTA-wfCFP图像的新测试集上进行广泛实验，验证了CARe方法在大视场差异跨模态眼底图像配准中的可行性

Conclusion: CARe是一种简单而有效的方法，能够解决具有大视场差异的跨模态眼底图像配准挑战，为先前小视场差异方法提供了重新利用的途径

Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.

</details>


### [82] [CogDoc: Towards Unified thinking in Documents](https://arxiv.org/abs/2512.12658)
*Qixin Xu,Haozhe Wang,Che Liu,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: CogDoc提出了一种从粗到细的认知框架，通过快速阅读和专注思考两阶段处理文档，解决了长文档处理与细粒度细节捕捉的权衡问题，7B参数模型在视觉丰富文档基准上超越了GPT-4o等更大模型。


<details>
  <summary>Details</summary>
Motivation: 当前文档推理范式存在一个基本权衡：可扩展性（处理长上下文文档）与保真度（捕捉细粒度多模态细节）之间的矛盾。为了弥合这一差距，需要新的方法。

Method: 提出CogDoc统一从粗到细思考框架，模仿人类认知过程：1）低分辨率"快速阅读"阶段进行可扩展信息定位；2）高分辨率"专注思考"阶段进行深度推理。研究了后训练策略，发现直接强化学习方法优于带监督微调初始化的强化学习。

Result: 7B参数模型在其参数类别中实现了最先进的性能，在具有挑战性的视觉丰富文档基准上显著超越了GPT-4o等更大的专有模型。直接强化学习避免了监督微调中观察到的"策略冲突"问题。

Conclusion: CogDoc框架通过模仿人类认知的两阶段处理，有效解决了文档推理中的可扩展性与保真度权衡问题，直接强化学习策略优于传统方法，小模型也能在复杂文档任务上超越大模型。

Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

</details>


### [83] [InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation](https://arxiv.org/abs/2512.12664)
*Sreehari Rajan,Kunal Bhosikar,Charu Sharma*

Main category: cs.CV

TL;DR: InteracTalker是一个统一框架，能够同时生成语音驱动的手势和物体交互动作，解决了现有方法只能单独处理这两类任务的问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法只能独立处理语音驱动手势或物体交互，缺乏集成数据集，限制了真实世界应用。需要一种能够同时响应语言和物理对象的自然人体运动生成方法。

Method: 采用多阶段训练学习统一的运动、语音和提示嵌入空间；构建丰富的人-物交互数据集；使用广义运动适应模块进行独立训练；提出自适应融合策略动态重新加权异质条件信号。

Result: InteracTalker在语音手势生成和物体交互合成方面均优于先前方法，超越了专注于手势的扩散方法，产生高度真实、物体感知的全身运动，具有增强的真实感、灵活性和控制性。

Conclusion: 该框架成功统一了先前分离的任务，为交互式数字体验提供了更自然、集成的人体运动生成解决方案。

Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

</details>


### [84] [Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning](https://arxiv.org/abs/2512.12667)
*Haiyang Zheng,Nan Pu,Wenjing Li,Teng Long,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出CAL框架解决开放世界深度伪造溯源问题，通过置信度感知非对称学习和动态原型剪枝策略，在已知和未知伪造类型上实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有开放世界深度伪造溯源方法存在两个关键限制：1) 置信度偏斜导致对新型伪造的伪标签不可靠，产生训练偏差；2) 不切实际地假设未知伪造类型的数量已知。需要更稳健的方法来处理现实世界中的混合伪造类型。

Method: 提出置信度感知非对称学习框架，包含两个核心组件：置信度感知一致性正则化和非对称置信度增强。前者通过基于归一化置信度动态缩放样本损失来缓解伪标签偏差；后者通过选择性学习分别校准已知和新型伪造的置信度。此外引入动态原型剪枝策略自动估计新型伪造类型数量。

Result: 在标准开放世界深度伪造溯源基准和新扩展的包含高级操作的基准上，CAL框架持续优于先前方法，在已知和新型伪造溯源上都达到了新的最先进性能。

Conclusion: CAL框架有效解决了开放世界深度伪造溯源中的置信度偏斜和先验假设问题，通过置信度感知学习和动态原型剪枝策略，显著提升了模型在现实世界场景中的可扩展性和性能。

Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

</details>


### [85] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: Scone是一个统一的理解-生成方法，通过两阶段训练方案实现多主体组合与区分，在复杂视觉场景中保持主体身份识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于主题的图像生成方法从单主体发展到多主体组合，但忽视了区分能力——当输入包含多个候选主体时识别和生成正确主体的能力。这一限制影响了在复杂现实视觉场景中的有效性。

Method: 提出Scone统一理解-生成方法，集成组合与区分能力。让理解专家作为语义桥梁，传递语义信息并指导生成专家在最小化干扰的同时保持主体身份。采用两阶段训练方案：先学习组合，然后通过语义对齐和基于注意力的掩码增强区分能力。

Result: 实验表明Scone在两个基准测试中，在组合和区分任务上优于现有的开源模型。同时提出了SconeEval基准，用于评估不同场景下的组合和区分能力。

Conclusion: Scone通过统一的理解-生成框架有效解决了多主体图像生成中的组合与区分问题，在复杂现实视觉场景中表现出色，相关模型、基准和训练数据已开源。

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [86] [$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment](https://arxiv.org/abs/2512.12678)
*Fatimah Zohra,Chen Zhao,Hani Itani,Bernard Ghanem*

Main category: cs.CV

TL;DR: β-CLIP通过多粒度文本条件对比学习框架，实现从完整描述到句子和短语的多层次文本与对应视觉区域的对齐，显著提升了细粒度视觉语言对齐性能。


<details>
  <summary>Details</summary>
Motivation: CLIP虽然在零样本图像-文本检索方面表现良好，但在细粒度任务上表现不佳，即使使用长而详细的描述进行微调也难以改善。这主要是因为CLIP只对齐全局视觉和文本表示，缺乏对多层次文本粒度与对应视觉区域的细粒度对齐能力。

Method: 提出β-CLIP多粒度文本条件对比学习框架：1）使用交叉注意力动态池化图像块，为每个粒度级别生成上下文视觉嵌入；2）引入β-上下文对比对齐损失（β-CAL），参数化严格查询特定匹配与宽松图像内上下文化之间的权衡，支持软交叉熵和硬二元交叉熵公式。

Result: 在Urban1K数据集上达到91.8% T2I和92.3% I2T的R@1准确率，在FG-OVD（Hard）上达到30.9%，在没有使用硬负样本训练的方法中达到最先进水平，显著提升了密集对齐性能。

Conclusion: β-CLIP为密集视觉语言对应建立了强大、自适应的基准，通过多粒度层次对齐有效解决了CLIP在细粒度任务上的局限性，代码和模型已开源。

Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.

</details>


### [87] [Efficient Vision-Language Reasoning via Adaptive Token Pruning](https://arxiv.org/abs/2512.12701)
*Xue Li,Xiaonan Song,Henry Hu*

Main category: cs.CV

TL;DR: ATP是一种动态推理机制，通过保留最具信息量的token来减少视觉语言模型的计算需求，实现约40%的FLOPs降低和1.5倍加速，精度损失小于1%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型计算效率低下，对所有token进行统一处理，阻碍了实际部署。需要一种动态推理机制来减少计算需求。

Method: 提出自适应token剪枝(ATP)，在视觉-语言接口处基于上下文相关性保留最具信息量的token。使用混合重要性评分结合ViT CLS注意力（模态内显著性）和CLIP文本-图像相似度（模态间相关性），为LLM保留top-K token。

Result: 在VQAv2、GQA和COCO上的评估显示，ATP减少约40%的推理FLOPs，实现约1.5倍的端到端延迟加速，精度损失小于1%。定性分析表明ATP保持了视觉基础并增强了可解释性。

Conclusion: ATP证明资源受限的推理和模型可靠性不是竞争目标。自适应剪枝可以抑制虚假相关性，提高稳定性，在高效多模态边缘计算管道中具有应用潜力。

Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

</details>


### [88] [GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation](https://arxiv.org/abs/2512.12751)
*Zhenya Yang,Zhe Liu,Yuxiang Lu,Liping Hou,Chenxuan Miao,Siyi Peng,Bailan Feng,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: GenieDrive：一种基于4D占据表示的物理感知驾驶视频生成框架，通过压缩占据到三平面潜在表示，结合互控注意力机制，实现高质量、物理一致的多视角驾驶视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型通常使用单一扩散模型直接将驾驶动作映射到视频，这种方法学习困难且容易产生物理不一致的输出。需要一种能够生成物理感知驾驶视频的框架，以支持驾驶规划、分布外数据合成和闭环评估等任务。

Method: 提出GenieDrive框架：1）首先生成包含丰富物理信息（高分辨率3D结构和动态）的4D占据；2）设计VAE将占据压缩到三平面潜在表示，将潜在大小减少到先前方法的58%；3）引入互控注意力（MCA）准确建模控制对占据演化的影响；4）端到端联合训练VAE和预测模块；5）在视频生成模型中引入归一化多视角注意力，以4D占据为指导生成多视角驾驶视频。

Result: 在推理速度41 FPS下，预测mIoU提升7.2%，仅使用3.47M参数。视频质量显著改善，FVD降低20.7%。实验表明GenieDrive能够实现高度可控、多视角一致且物理感知的驾驶视频生成。

Conclusion: GenieDrive通过4D占据表示和三平面压缩，结合互控注意力机制，成功构建了一个物理感知的驾驶视频生成框架，在保持高效推理的同时显著提升了生成质量和物理一致性。

Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.

</details>


### [89] [FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning](https://arxiv.org/abs/2512.12756)
*Yue Jiang,Dingkang Yang,Minghao Han,Jinghang Han,Zizhi Chen,Yizhou Liu,Mingcheng Li,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: FysicsWorld是首个统一的全模态基准测试，支持图像、视频、音频和文本之间的双向输入输出，包含16个主要任务和3,268个样本，用于评估多模态大语言模型的理解、生成和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基准测试存在模态覆盖不完整、交互仅限于文本输出、模态间相互依赖性和互补性弱等问题，需要建立一个更全面的评估框架来推动全模态架构的发展。

Method: 提出了FysicsWorld基准测试，包含16个主要任务和3,268个精心策划的样本，覆盖40多个高质量数据源。采用跨模态互补性筛选(CMCS)策略和系统化数据构建框架，生成用于口语交互和融合依赖跨模态推理的全模态数据。

Result: 通过对30多个最先进的基线模型（包括MLLMs、模态特定模型、统一理解-生成模型和全模态语言模型）进行全面评估，揭示了不同模型在理解、生成和推理方面的性能差异和局限性。

Conclusion: FysicsWorld为评估和推进下一代全模态架构建立了统一的基础和强大的基线，填补了当前基准测试的空白，支持任意模态间的双向评估。

Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

</details>


### [90] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

TL;DR: CoRe3D提出统一3D理解与生成推理框架，通过语义空间推理和结构化空间推理的紧密耦合，实现语言引导的3D内容生成


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型的显式推理机制在语言和视觉任务中有效，但在3D领域应用不足，需要开发能够同时处理3D理解和生成的推理框架

Method: 引入空间基础推理表示，将3D潜在空间分解为局部化区域，通过语义链式推理与结构化空间推理的紧密耦合，实现从高层语言意图到低层3D内容形成的直接引导

Result: CoRe3D生成的3D输出展现出强大的局部一致性和与语言描述的高度对齐，实现了组合式和过程式的几何推理

Conclusion: CoRe3D通过统一的语义-空间推理框架，为3D理解和生成任务提供了可靠的解决方案，提高了模型的可靠性、可解释性和跨模态对齐能力

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [91] [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
*Zhe Liu,Runhui Huang,Rui Yang,Siming Yan,Zining Wang,Lu Hou,Di Lin,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DrivePI是一个空间感知的4D多模态大语言模型，作为统一的视觉-语言-动作框架，在自动驾驶中并行执行空间理解、3D感知、预测和规划任务，仅使用0.5B参数就能超越现有7B模型和专用模型。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在多个领域表现出色，但在自动驾驶中生成细粒度3D感知和预测输出的应用仍未被充分探索。现有方法在空间理解、3D感知、预测和规划的集成方面存在不足。

Method: 提出DrivePI框架，集成点云、多视角图像和语言指令，通过端到端优化并行执行空间理解、3D占用感知、占用流预测和规划。开发数据引擎生成文本-占用和文本-流问答对用于4D空间理解训练。

Result: 仅使用0.5B参数的Qwen2.5作为骨干，DrivePI在多个基准测试中超越现有模型：在nuScenes-QA上比OpenDriveVLA-7B高2.5%平均准确率；碰撞率比ORION降低70%；在3D占用、占用流预测和规划任务上均优于专用模型。

Conclusion: DrivePI证明了通过统一的多模态大语言模型框架，可以在自动驾驶中有效集成空间理解、3D感知、预测和规划任务，仅需较小模型就能达到或超越现有大型模型和专用模型的性能。

Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI

</details>


### [92] [Learning Common and Salient Generative Factors Between Two Image Datasets](https://arxiv.org/abs/2512.12800)
*Yunlong He,Gwilherm Lesné,Ziqian Liu,Michaël Soumm,Pietro Gori*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的对比分析框架，用于从两个图像数据集中分离共享的生成因子和特定于单个数据集的显著因子，适用于GAN和扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注条件操作或解耦表示学习，而本文关注一个较少被研究的问题——对比分析（CA），即给定两个图像数据集，分离出跨数据集共享的公共生成因子和仅特定于一个数据集的显著因子。

Method: 提出了一个新颖的对比分析框架，可适配于GAN和扩散模型，通过定义新的学习策略和损失函数，确保公共因子和显著因子的有效分离，同时保持高质量的图像生成。

Result: 在多样化数据集（包括人脸、动物图像和医学扫描）上进行评估，相比现有方法，该框架展现出更优的因子分离能力和图像合成质量。

Conclusion: 该研究为解决对比分析问题提供了一个有效的框架，能够在仅使用数据集信号（而非属性监督）的情况下，成功分离公共和显著生成因子，并在多个领域验证了其优越性能。

Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.

</details>


### [93] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

TL;DR: Lemon是一个统一的Transformer架构，通过将3D点云块和语言标记作为单一序列联合处理，解决了大型多模态模型在3D理解中的扩展挑战。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在扩展到3D理解时面临独特挑战：点云数据稀疏不规则、现有模型依赖碎片化架构（具有模态特定编码器）、训练流程不稳定且扩展性差。

Method: 提出统一的Transformer架构，将3D点云块和语言标记作为单一序列联合处理；开发结构化分块和标记化方案以保留空间上下文；采用三阶段训练课程，从对象级识别逐步构建到场景级空间推理能力。

Result: 在全面的3D理解和推理任务中（从对象识别和描述到3D场景中的空间推理）建立了新的最先进性能，同时展示了随着模型规模和训练数据增加而具有的稳健扩展特性。

Conclusion: Lemon为推进现实世界应用中的3D空间智能提供了统一的基础，通过早期空间-语言融合、消除冗余编码器、提高参数效率和更有效的模型扩展，解决了现有方法的局限性。

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [94] [Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875)
*Weihan Xu,Kan Jen Cheng,Koichi Saito,Muhammad Jehanzeb Mirza,Tingle Li,Yisi Liu,Alexander H. Liu,Liming Wang,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji,Gopala Anumanchipalli,Paul Pu Liang*

Main category: cs.CV

TL;DR: SAVE模型是一个端到端的流匹配模型，用于联合编辑音频和视频内容，通过Schrodinger Bridge实现源到目标视听混合的直接传输，在保持内容对齐的同时移除目标对象。


<details>
  <summary>Details</summary>
Motivation: 联合编辑音频和视觉内容对于精确可控的内容创作至关重要，但面临配对视听数据有限和模态异质性等挑战。

Method: 提出SAVEBench配对视听数据集，训练SAVE模型（Schrodinger Audio-Visual Editor），这是一个端到端的流匹配模型，使用Schrodinger Bridge学习从源到目标视听混合的直接传输，并行编辑音频和视频并保持对齐。

Result: SAVE模型能够移除音频和视觉内容中的目标对象，同时保留其余内容，在时间同步性和视听语义对应方面优于音频编辑器和视频编辑器的成对组合。

Conclusion: SAVE模型通过Schrodinger Bridge和流匹配方法有效解决了联合视听编辑的数据和建模挑战，实现了更好的内容对齐和编辑效果。

Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.

</details>


### [95] [Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2512.12906)
*Zhimao Peng,Enguang Wang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于预测样本分配（PSA）的语义一致性分布外检测框架，通过双阈值三元样本分配策略提高ID和OOD样本集的纯度，并引入概念对比表示学习损失来增强ID/OOD区分能力。


<details>
  <summary>Details</summary>
Motivation: 当前SCOOD方法主要采用基于聚类的ID样本过滤策略，从无标签数据中选择干净的ID样本，并将剩余样本作为辅助OOD数据，这种方法不可避免地引入了大量噪声样本。为了解决这个问题，需要一种更有效的样本选择和训练策略。

Method: 提出了基于预测样本分配（PSA）的SCOOD框架，包括：1）基于预测能量分数的双阈值三元样本分配策略，将不确定的无标签数据分配到额外的丢弃样本集中；2）概念对比表示学习损失，在表示空间中扩大ID和OOD样本之间的距离；3）重新训练策略，帮助模型充分拟合选定的辅助ID/OOD样本。

Result: 在两个标准SCOOD基准测试上的实验表明，该方法显著优于现有最先进的方法。

Conclusion: 提出的PSA框架通过改进样本分配纯度和增强表示学习，有效解决了SCOOD任务中的噪声样本问题，取得了显著的性能提升。

Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.

</details>


### [96] [Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery](https://arxiv.org/abs/2512.12925)
*Zhimao Peng,Enguang Wang,Fei Yang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出LSP和DAS两个模块来解决广义类别发现中伪标签噪声问题，通过损失锐度惩罚增强模型鲁棒性，动态锚点选择改善未知类别学习，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前基于参数化分类的GCD方法采用DINO式伪标签策略，但大型预训练模型对特定视觉模式有偏好，导致对未标记数据编码虚假相关性并产生噪声伪标签。需要解决伪标签噪声问题以提高聚类性能。

Method: 提出包含两个模块的新方法：1) 损失锐度惩罚(LSP)：通过最小化模型最坏情况损失锐度来增强参数对小扰动的鲁棒性，抑制琐碎特征编码，减少噪声样本过拟合；2) 动态锚点选择(DAS)：基于KNN密度和类别概率选择未知类别的代表性样本并分配硬伪标签，缓解已知与未知类别的置信度差异，快速学习更准确的特征分布。

Result: 大量实验表明，该方法能有效缓解伪标签噪声，在多个GCD基准测试中实现了最先进的结果。

Conclusion: 提出的LSP和DAS模块能有效解决GCD中的伪标签噪声问题，LSP增强模型鲁棒性，DAS改善未知类别学习，共同提高了聚类准确性，在多个基准测试中表现优异。

Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.

</details>


### [97] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

TL;DR: 提出统一多模态时刻检索系统，解决固定权重融合、时序建模和手动模态选择三大挑战，通过级联双嵌入、时序感知评分和智能体引导查询分解实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 视频内容爆炸式增长急需高效多模态时刻检索系统，但现有方法面临三大挑战：固定权重融合策略无法应对跨模态噪声和模糊查询；时序建模难以捕捉连贯事件序列并惩罚不现实的时间间隔；系统需要手动模态选择，降低可用性。

Method: 1. 级联双嵌入管道：结合BEIT-3和SigLIP进行广泛检索，通过BLIP-2重排序平衡召回率和精确度。2. 时序感知评分机制：通过波束搜索对大的时间间隔应用指数衰减惩罚，构建连贯事件序列而非孤立帧。3. 智能体引导查询分解：使用GPT-4o自动解释模糊查询，分解为模态特定子查询（视觉/OCR/ASR），并进行自适应分数融合消除手动模态选择。

Result: 定性分析表明，该系统能有效处理模糊查询，检索时序连贯的序列，并动态调整融合策略，提升了交互式时刻搜索能力。

Conclusion: 提出的统一多模态时刻检索系统通过创新的级联检索、时序建模和智能查询分解方法，解决了现有系统的关键限制，为视频内容的高效检索提供了更智能、自适应的解决方案。

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [98] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

TL;DR: 提出基于内容自适应的运动对齐框架CAMA，通过三方面改进提升端到端视频压缩性能：两阶段流引导可变形扭曲机制、多参考质量感知策略、无训练的下采样模块，在标准数据集上相比基线模型DCVC-TCM实现24.95% BD-rate节省。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视频压缩框架缺乏内容特异性适应能力，导致压缩性能不理想。需要针对不同内容特性自适应调整编码策略，以提升压缩效率。

Method: 1. 两阶段流引导可变形扭曲机制：通过粗到细的偏移预测和掩码调制，实现精确特征对齐；2. 多参考质量感知策略：基于参考质量调整失真权重，应用于分层训练以减少误差传播；3. 无训练模块：根据运动幅度和分辨率下采样帧，获得平滑运动估计。

Result: 在标准测试数据集上，CAMA框架相比基线模型DCVC-TCM实现24.95% BD-rate（PSNR）节省，同时优于复现的DCVC-DC和传统编解码器HM-16.25。

Conclusion: 提出的内容自适应运动对齐框架通过三方面技术创新，显著提升了端到端视频压缩性能，证明了内容自适应策略在视频压缩中的有效性。

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [99] [SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer](https://arxiv.org/abs/2512.12963)
*Luan Thanh Trinh,Kenji Doi,Atsuki Osanai*

Main category: cs.CV

TL;DR: SCAdapter是一种基于CLIP图像空间的风格迁移新方法，通过有效分离和整合内容与风格特征，解决了扩散模型在照片级真实感风格迁移中的问题，实现了更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在风格迁移中虽然领先，但在照片级真实感迁移方面存在不足，常常产生类似绘画的效果或遗漏细节风格元素。现有方法未能有效处理原始内容风格和风格参考内容特征的不必要影响。

Method: SCAdapter利用CLIP图像空间有效分离和整合内容与风格特征。该方法包含三个核心组件：可控风格自适应实例归一化（CSAdaIN）用于精确的多风格混合，KVS注入用于目标风格整合，以及风格迁移一致性目标保持过程连贯性。

Result: SCAdapter在传统和基于扩散的基准测试中显著优于现有最先进方法。通过消除DDIM反转和推理阶段优化，该方法推理速度至少比其他基于扩散的方法快2倍。

Conclusion: SCAdapter提供了一种更有效和高效的风格迁移方法，通过系统提取纯内容和风格元素，确保真实的风格迁移，同时大幅提升推理速度，适合实际应用。

Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

</details>


### [100] [VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference](https://arxiv.org/abs/2512.12977)
*Shengling Qin,Hao Yu,Chenxin Wu,Zheng Li,Yizhong Cao,Zhengyang Zhuge,Yuxin Zhou,Wentao Yao,Yi Zhang,Zhengheng Wang,Shuai Bai,Jianwei Zhang,Junyang Lin*

Main category: cs.CV

TL;DR: VLCache是一个多模态缓存重用框架，通过复用KV缓存和编码器缓存来避免重复计算，在保持精度的同时大幅提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 当相同的多模态输入重复出现时，现有的方法需要进行昂贵的重新计算。为了消除这种重复计算的开销，需要一种能够有效复用先前计算结果的缓存机制。

Method: 1. 形式化识别累积重用误差效应并最小化非前缀缓存重用误差；2. 分析模型各层的重要性差异，提出动态、层感知的重新计算策略来平衡精度和效率；3. 将VLCache管道集成到SGLang中。

Result: VLCache在保持与完全重新计算相当的精度同时，仅需计算2-5%的token，实现了1.2倍到16倍的首次token时间(TTFT)加速。该框架已集成到SGLang中，在实际部署中显著加快了推理速度。

Conclusion: VLCache通过有效复用多模态缓存，在保证精度的前提下大幅减少了计算开销，为实际应用中的多模态推理提供了高效的解决方案。

Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

</details>


### [101] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 论文发现AIGI检测器在数据源多样性增加时会出现"先受益后冲突"困境，提出GAPL框架通过原型学习和两阶段训练解决数据异质性和模型瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 现有通用AIGI检测器通过聚合多个生成器数据来提升泛化能力，但本文发现随着数据源多样性扩大，检测器性能会出现停滞甚至下降的"先受益后冲突"困境

Method: 提出Generator-Aware Prototype Learning (GAPL)框架：1) 学习一组紧凑的典型伪造原型，创建统一低方差特征空间以应对数据异质性；2) 采用两阶段训练方案结合低秩适应技术，增强判别能力同时保留预训练知识

Result: GAPL在广泛实验中实现了最先进的性能，在多种GAN和基于扩散的生成器上展现出优越的检测准确率

Conclusion: GAPL通过结构化学习范式和两阶段训练有效解决了AIGI检测中的"先受益后冲突"困境，建立了更鲁棒和可泛化的决策边界

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [102] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

TL;DR: 本文提出一种针对CLIP模型的新型对抗性微调方法，通过狄利克雷分布重新参数化输出，同时优化预测准确性和不确定性校准，解决对抗攻击下不确定性被抑制的问题。


<details>
  <summary>Details</summary>
Motivation: CLIP在零样本分类中表现优异，但对对抗攻击高度脆弱。现有对抗微调方法主要关注干净样本和对抗样本预测logits的匹配，忽略了不确定性校准，可能损害零样本泛化能力。研究发现对抗扰动不仅降低准确率，还会抑制不确定性，导致严重的校准错误和不可靠的过度自信。

Method: 通过将CLIP输出重新参数化为狄利克雷分布的浓度参数，提出统一表示方法，捕捉相对语义结构和预测置信度大小。设计新的对抗微调目标，在扰动下整体对齐这些分布，超越单一logits锚定，恢复校准的不确定性。

Result: 在多个零样本分类基准测试中，该方法有效恢复了校准的不确定性，实现了有竞争力的对抗鲁棒性，同时保持了干净样本的准确率。

Conclusion: 提出的方法成功解决了CLIP在对抗攻击下的不确定性校准问题，填补了可靠性的关键空白，为构建更可靠的零样本分类系统提供了有效方案。

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [103] [Light Field Based 6DoF Tracking of Previously Unobserved Objects](https://arxiv.org/abs/2512.13007)
*Nikolai Goncharov,James L. Gray,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 提出基于光场图像的目标跟踪方法，无需预训练模型，对复杂视觉行为（如反射）具有鲁棒性，使用可微分渲染和姿态优化实现6DoF跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有高性能目标跟踪方法通常依赖预捕获的对象视图构建显式参考模型，这限制了它们只能处理已知对象集合，且对视觉复杂外观（如反射）处理效果不佳。需要一种不依赖预训练模型、能处理复杂视觉行为的目标跟踪方法。

Method: 从光场输入中提取语义和几何特征，使用视觉基础模型处理，将其转换为视图相关的高斯泼溅表示，作为统一的对象表示支持可微分渲染和姿态优化。

Result: 在包含挑战性反射对象的光场目标跟踪数据集上实验表明，该方法在困难情况下与最先进的基于模型的跟踪器具有竞争力。

Conclusion: 该方法为机器人系统中的通用目标跟踪铺平了道路，不依赖预训练模型，对复杂视觉行为具有鲁棒性，代码和数据已开源。

Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.

</details>


### [104] [TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading](https://arxiv.org/abs/2512.13008)
*Xi Luo,Shixin Xu,Ying Xie,JianZhong Hu,Yuwei He,Yuhui Deng,Huaxiong Huang*

Main category: cs.CV

TL;DR: TWLR是一个两阶段可解释性糖尿病视网膜病变评估框架，通过视觉语言模型整合眼科知识进行分级和病变分类，然后通过弱监督语义分割的迭代严重性回归框架实现病变定位和疾病到健康转换的可视化。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析需要高质量专家标注，但获取像素级标签成本高且耗时。同时，深度学习在医学影像中缺乏可解释性，限制了临床采用。需要解决标注效率和可解释性双重挑战。

Method: 提出两阶段框架TWLR：第一阶段使用视觉语言模型整合眼科领域知识，联合执行DR分级和病变分类，将语义医学概念与视觉特征关联；第二阶段引入基于弱监督语义分割的迭代严重性回归框架，通过迭代精炼生成病变显著图，指导渐进修复机制系统消除病理特征，有效降低疾病严重程度。

Result: 在FGADR、DDR和私有数据集上的实验结果表明，TWLR在DR分类和病变分割方面均取得竞争性性能，为自动视网膜图像分析提供了更可解释且标注高效的解决方案。

Conclusion: TWLR框架通过结合视觉语言模型和迭代严重性回归，实现了无需像素级监督的准确病变定位，并提供了疾病到健康转换的可视化解释，为糖尿病视网膜病变评估提供了可解释且标注高效的自动化解决方案。

Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.

</details>


### [105] [What Happens Next? Next Scene Prediction with a Unified Video Model](https://arxiv.org/abs/2512.13015)
*Xinjie Li,Zhimin Chen,Rui Zhao,Florian Schiffers,Zhenyu Liao,Vimal Bhat*

Main category: cs.CV

TL;DR: 提出Next Scene Prediction（NSP）新任务，要求模型基于前序上下文预测合理未来场景，推动统一视频模型向时序和因果推理发展。


<details>
  <summary>Details</summary>
Motivation: 当前统一模型主要关注文本到视频生成等传统任务，对时序推理潜力探索不足。需要新任务来推动模型进行更深层次的理解和推理。

Method: 提出统一框架：结合Qwen-VL进行理解，LTX进行合成，通过潜在查询嵌入和连接模块桥接。使用新构建的大规模NSP数据集进行三阶段训练：文本到视频预训练、监督微调、以及采用因果一致性奖励的强化学习（GRPO）。

Result: 实验表明该模型在基准测试中达到最先进性能，提升了通用多模态系统预测未来事件的能力。

Conclusion: NSP任务成功推动了统一视频模型向时序和因果推理方向发展，为通用多模态系统提供了预测未来场景的新能力。

Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

</details>


### [106] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 该研究首次全面评估了空间泛化技术，针对室内人员计数应用，系统比较了多种方法，发现基于幅度的预处理和迁移学习能显著提升雷达传感系统在不同环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在射频传感中的应用面临空间泛化挑战，当部署环境变化时性能会下降。本研究旨在为室内人员计数的实际部署提供有效的空间泛化解决方案。

Method: 使用FMCW MIMO雷达，系统评估了多种空间泛化技术：基于幅度的统计预处理（sigmoid加权和阈值归零）、频域滤波、基于自动编码器的背景抑制、数据增强策略和迁移学习。在两个不同布局的环境中进行实验验证。

Result: sigmoid幅度加权在跨环境性能上表现最佳，相比基线方法分别降低RMSE 50.1%和MAE 55.2%。数据增强提供额外但有限的改进（MAE提升达8.8%）。迁移学习对于大空间变化至关重要，使用540个目标域样本时分别降低RMSE 82.1%和MAE 91.3%。

Conclusion: 通过结合深度学习模型、基于幅度的预处理和高效的迁移学习，可以开发出在空间变化下保持鲁棒精度的雷达传感系统，为实际部署提供了实用方向。

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [107] [SneakPeek: Future-Guided Instructional Streaming Video Generation](https://arxiv.org/abs/2512.13019)
*Cheeun Hong,German Barquero,Fadime Sener,Markos Georgopoulos,Edgar Schönfeld,Stefan Popov,Yuming Du,Oscar Mañas,Albert Pumarola*

Main category: cs.CV

TL;DR: SneakPeek是一个基于扩散的自回归框架，用于生成精确的分步教学视频，通过预测性因果适应、未来引导自强制和多提示条件控制来解决现有模型在长序列中的时间一致性和可控性问题。


<details>
  <summary>Details</summary>
Motivation: 教学视频生成在内容创作、教育和人机交互中具有广泛意义，但现有视频扩散模型在长序列多动作步骤中难以保持时间一致性和可控性，存在时间漂移和运动不一致的问题。

Method: 提出SneakPeek框架，包含三个关键创新：1) 预测性因果适应，通过因果模型学习下一帧预测和未来关键帧预测；2) 未来引导自强制，采用双区域KV缓存方案解决推理时的暴露偏差问题；3) 多提示条件控制，提供对多步骤指令的细粒度程序控制。

Result: 实验结果表明，该方法能够生成时间一致、语义忠实且准确遵循复杂多步骤任务描述的教学视频，有效缓解时间漂移，保持运动一致性，并支持交互式视频生成。

Conclusion: SneakPeek通过创新的预测性因果适应、未来引导自强制和多提示条件控制，成功解决了教学视频生成中的时间一致性和可控性问题，为交互式流媒体视频生成提供了有效解决方案。

Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

</details>


### [108] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

TL;DR: 本文提出了一种双向图像引导的概念擦除框架（Bi-Erasing），通过同时抑制有害概念和增强安全替代概念，在文本到图像模型中实现概念移除与生成质量之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法通常采用单向策略（要么抑制目标概念，要么强化安全替代），难以在概念移除效果和生成质量之间取得平衡。为了解决这一局限性，需要一种更平衡的方法。

Method: 提出Bi-Erasing框架，基于文本提示和对应图像的联合表示，引入两个解耦的图像分支：负分支负责抑制有害语义，正分支为安全替代提供视觉指导。通过联合优化这两个互补方向，并应用基于掩码的过滤来防止无关内容干扰。

Result: 在广泛的实验评估中，Bi-Erasing在平衡概念移除效果和视觉保真度方面优于基线方法。

Conclusion: Bi-Erasing框架通过双向优化策略，成功解决了概念擦除中移除效果与生成质量之间的平衡问题，为文本到图像模型的安全应用提供了有效解决方案。

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [109] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

TL;DR: GTR-Turbo：一种高效的多模态智能体强化学习方法，无需昂贵教师模型，通过合并训练中的检查点权重作为"免费"教师，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的多模态智能体多轮强化学习面临稀疏奖励和长视野信用分配问题。现有方法依赖昂贵且通常具有特权的教师模型提供步骤级反馈，限制了实用性和可复现性。

Method: GTR-Turbo在持续强化学习训练过程中合并检查点权重，使用这个合并模型作为"免费"教师，通过监督微调或软对数蒸馏指导后续强化学习，消除了对特权VLM的依赖。

Result: 在各种视觉智能体任务中，GTR-Turbo将基线模型的准确率提升10-30%，同时相比GTR减少50%的墙上训练时间和60%的计算成本。

Conclusion: GTR-Turbo提供了一种高效实用的多模态智能体强化学习方案，无需依赖昂贵教师模型，解决了先前工作中的"熵崩溃"问题，保持了训练稳定性。

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [110] [Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models](https://arxiv.org/abs/2512.13072)
*Zizhi Chen,Yizhen Gao,Minghao Han,Yizhou Liu,Zhaoyu Chen,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种用于医学视觉语言模型持续学习的综合框架，通过多模态检索增强生成和动态知识蒸馏来解决模态间领域差距与细粒度特征保留的核心矛盾。


<details>
  <summary>Details</summary>
Motivation: 多模态生物医学视觉语言模型在持续学习中面临核心困境：如何在弥合不同模态间显著领域差距的同时，保留细粒度的模态内特征。现有方法难以平衡这两个相互冲突的目标。

Method: 1. 基于1800万PubMed科学论文构建多模态医学检索数据库；2. 首次将检索增强生成技术引入持续学习，采用多模态多层RAG系统为模型微调提供实时指导；3. 提出动态知识蒸馏框架，根据所需细节水平动态调节参数空间重要性、蒸馏知识粒度以及参考数据集分布。

Result: 该方法在设计的医学通用任务增量学习基准上实现了最先进的性能，全面验证了模型在适应显著领域转移、保留细微领域内特征以及实时学习新颖复杂医学任务方面的能力。

Conclusion: 提出的综合框架有效解决了医学视觉语言模型持续学习中的核心挑战，通过检索增强生成和动态知识蒸馏的协同作用，在保持细粒度特征的同时成功弥合了模态间的领域差距。

Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.

</details>


### [111] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

TL;DR: 提出多样性正则化器DiRe，通过余弦相似度和欧氏距离减少数据集压缩中的冗余，提高合成数据集的多样性，提升现有压缩方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法合成的数据集存在显著冗余，需要减少冗余并提高合成数据集的多样性。

Method: 提出直观的多样性正则化器DiRe，结合余弦相似度和欧氏距离，可以即插即用地应用于各种最先进的压缩方法。

Result: 通过大量实验证明，添加该正则化器能够改进最先进的压缩方法，在从CIFAR-10到ImageNet-1K的各种基准数据集上，在泛化性和多样性指标方面都有提升。

Conclusion: DiRe正则化器能有效减少数据集压缩中的冗余，提高合成数据集的多样性，提升现有压缩方法的性能。

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [112] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

TL;DR: 提出UniVCD方法，基于冻结的SAM2和CLIP模型实现无监督、开放词汇的变化检测，无需标注数据或配对图像，通过轻量级特征对齐模块和多模态融合实现高性能变化检测。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法主要依赖监督学习，性能受数据集限制且标注成本高，通常只关注预定义类别，难以泛化到多样化场景。随着视觉基础模型（如SAM2和CLIP）的发展，为放松这些限制提供了新机会。

Method: 基于冻结的SAM2和CLIP模型构建无监督开放词汇变化检测框架。引入轻量级特征对齐模块，将SAM2的空间细节表征与CLIP的语义先验进行桥接，实现高分辨率、语义感知的变化估计。采用简化的后处理流程抑制噪声和伪变化，提升边界清晰对象的检测精度。

Result: 在多个公开的BCD（二值变化检测）和SCD（语义变化检测）基准测试中，UniVCD展现出稳定强大的性能，在F1和IoU等关键指标上匹配或超越了现有开放词汇变化检测方法。

Conclusion: 基于冻结视觉基础模型和轻量级多模态对齐的无监督变化检测是开放词汇变化检测的实用有效范式，为场景变化识别提供了新的解决方案。

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [113] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: ADHint是一种结合SFT和RL优势的后训练方法，通过考虑样本难度来调度提示比例和估计相对优势，实现探索与模仿的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的RL方法通常忽略难度因素，导致学习不稳定和过度模仿离策略提示，需要更好的探索与模仿平衡机制。

Method: 1. 基于样本难度先验的自适应提示：评估样本难度并调度适当提示比例；2. 一致性梯度调制和选择性掩码：调制提示内的令牌级梯度；3. 基于推出难度后验的优势估计：利用有/无提示推出的相对难度估计优势。

Result: 在多种模态、模型规模和领域上的实验表明，ADHint在推理能力和分布外泛化方面优于现有方法，在pass@1和avg@8指标上持续领先。

Conclusion: ADHint通过将难度作为关键因素纳入提示比例调度和相对优势估计，实现了探索与模仿的更好平衡，显著提升了推理能力和泛化性能。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [114] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

TL;DR: UnCoL是一个双教师框架，通过不确定性引导的协同学习，在有限标注下平衡视觉基础模型的泛化能力和任务特定专业化，实现半监督医学图像分割。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在医学图像分割中表现出强大的泛化能力，但在有限标注或罕见病理变异下，由于通用先验与任务特定需求不匹配，难以泛化到专业临床任务。

Method: 提出UnCoL双教师框架：从冻结的基础模型中蒸馏视觉和语义表示以传递通用知识，同时维护逐步适应的教师模型捕获细粒度和任务特定表示。通过预测不确定性自适应调节伪标签学习，抑制不可靠监督并稳定模糊区域的学习。

Result: 在多样化的2D和3D分割基准测试中，UnCoL始终优于最先进的半监督方法和基础模型基线。模型在显著减少标注需求的情况下，实现了接近全监督的性能。

Conclusion: UnCoL通过不确定性引导的协同学习，有效平衡了泛化与专业化，为有限标注下的医学图像分割提供了高效解决方案。

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [115] [FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection](https://arxiv.org/abs/2512.13104)
*Yan Zhang,Baoxin Li,Han Sun,Yuhang Gao,Mingtai Zhang,Pei Wang*

Main category: cs.CV

TL;DR: 提出FID-Net深度学习模型，利用无人机可见光图像检测森林病虫害树木，并通过三种空间指标进行虫害分析，在新疆天山地区32个林区实验中表现优于主流YOLO模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法在大规模、细粒度森林病虫害监测中存在局限性，需要更高效的监测技术来准确识别感染树木并分析虫害模式，以支持生态系统保护。

Method: 基于YOLOv8n构建FID-Net模型，引入轻量级特征增强模块提取病害敏感特征，自适应多尺度特征融合模块对齐融合RGB和FEM增强特征，以及高效通道注意力机制增强判别信息。从检测结果构建虫害分析框架，包括核密度估计定位感染热点、邻域评估健康树感染风险、DBSCAN聚类识别高密度健康集群作为优先保护区域。

Result: 在新疆天山地区32个林区的无人机图像实验中，FID-Net达到86.10%精确率、75.44%召回率、82.29% mAP@0.5和64.30% mAP@0.5:0.95，优于主流YOLO模型。分析确认感染树木呈现明显聚类模式。

Conclusion: FID-Net能够准确区分树木健康状况，结合空间指标为智能病虫害监测、早期预警和精准管理提供可靠数据支持，有助于针对性森林保护。

Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.

</details>


### [116] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

TL;DR: DiffFusion：基于扩散模型的多模态3D目标检测框架，通过图像和点云修复以及双向自适应融合，提升恶劣天气条件下的检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D目标检测在恶劣天气条件下效果受限，主要原因是天气导致的图像失真和不同数据模态之间的不对齐问题。

Method: 提出DiffFusion框架：1）Diffusion-IR：基于扩散模型修复天气退化的图像；2）Point Cloud Restoration (PCR)：利用图像目标线索补偿受损的LiDAR数据；3）BAFAM：双向自适应融合和对齐模块，实现动态多模态融合和BEV空间对齐。

Result: 在三个公开数据集上实验表明，DiffFusion在恶劣天气条件下达到最先进的鲁棒性，同时保持强大的干净数据性能。在DENSE数据集上的零样本结果进一步验证了其泛化能力。

Conclusion: DiffFusion通过扩散模型驱动的修复和自适应跨模态融合，有效提升了多模态3D目标检测在恶劣天气条件下的鲁棒性，具有实际应用价值。

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [117] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: 提出Intrinsic Image Fusion方法，通过融合多视角图像重建高质量物理材质，结合单视角先验和扩散模型，使用鲁棒优化框架整合不一致的预测，最终通过逆路径追踪优化低维参数。


<details>
  <summary>Details</summary>
Motivation: 材质重建是一个高度欠约束的问题，通常依赖于分析合成方法，但这种方法需要昂贵且噪声大的路径追踪。为了更好约束优化过程，需要将单视角先验信息融入重建过程。

Method: 1. 利用基于扩散的材质估计器为每个视角生成多个候选分解；2. 拟合显式低维参数函数以减少不一致性；3. 提出鲁棒优化框架，使用软每视角预测选择和基于置信度的软多视角内点集，将最一致预测融合到一致的参数材质空间中；4. 使用逆路径追踪优化低维参数。

Result: 在合成和真实场景的材质解缠任务中，该方法优于现有最先进方法，能够产生清晰干净的重建结果，适用于高质量重光照应用。

Conclusion: Intrinsic Image Fusion方法通过融合多视角图像和单视角先验，成功解决了材质重建的欠约束问题，实现了高质量物理材质的重建，为计算机视觉和图形学应用提供了有效的解决方案。

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [118] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

TL;DR: CRAFTS是一个针对病理学的首个生成式基础模型，通过相关性调节对齐框架解决生成模型中的语义漂移问题，能够生成30种癌症类型的多样化病理图像，并增强临床任务的性能。


<details>
  <summary>Details</summary>
Motivation: 病理学中临床级人工智能的发展受到多样化、高质量标注数据集稀缺的限制。现有生成模型存在语义不稳定和形态幻觉问题，影响诊断可靠性。

Method: 提出CRAFTS（相关性调节对齐框架），采用双阶段训练策略，在约280万图像-文本对上进行训练，引入新颖的对齐机制抑制语义漂移，确保生物学准确性。模型还能与ControlNet结合，通过核分割掩码和荧光图像等输入精确控制组织架构。

Result: 模型生成涵盖30种癌症类型的多样化病理图像，质量通过客观指标和病理学家评估得到验证。CRAFTS增强的数据集在分类、跨模态检索、自监督学习和视觉问答等多种临床任务中提升了性能。

Conclusion: CRAFTS通过克服数据稀缺和隐私问题的关键障碍，提供了无限多样的标注组织学数据来源，有效解锁了针对罕见和复杂癌症表型的稳健诊断工具的创建。

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [119] [Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation](https://arxiv.org/abs/2512.13175)
*Hongxuan Sun,Tao Wu*

Main category: cs.CV

TL;DR: DFSS是一种专门为语义分割设计的数据无知识蒸馏框架，通过利用教师模型的批归一化统计信息来指导近似分布采样，并采用加权分布渐进蒸馏策略，显著提升了语义分割任务中的数据无知识蒸馏性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据无知识蒸馏方法主要针对分类任务设计，忽视了语义分割中空间连续性和结构一致性的重要性，导致直接应用于分割任务时性能显著下降。需要开发专门针对语义分割特性的数据无知识蒸馏框架。

Method: DFSS框架包含两个核心组件：1）利用教师模型的批归一化统计信息指导近似分布采样，选择更接近原始训练分布的数据；2）提出加权分布渐进蒸馏策略，在训练早期优先处理可靠样本，逐步引入更具挑战性的样本。

Result: 在标准基准测试上的广泛实验表明，DFSS在语义分割任务中持续优于现有的数据无知识蒸馏方法，取得了最先进的结果，同时显著减少了对辅助数据的依赖。

Conclusion: DFSS通过尊重真实场景的结构和上下文连续性，为语义分割任务提供了有效的无数据知识蒸馏解决方案，克服了现有方法忽视空间连续性的问题，实现了更好的性能。

Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

</details>


### [120] [CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception](https://arxiv.org/abs/2512.13191)
*Gong Chen,Chaokun Zhang,Pengcheng Lv,Xiaohui Xie*

Main category: cs.CV

TL;DR: CoRA提出了一种新颖的协作感知架构，通过混合融合方法解耦性能与鲁棒性，在保持低通信成本的同时提升对抗通信条件的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA方法虽然通过中间融合实现了通信效率和性能，但在恶劣通信条件下性能会因数据传输引起的错位而下降，这严重阻碍了实际部署。需要解决鲁棒性问题。

Method: 重新审视不同融合范式，发现中间融合和后期融合的优势不是权衡而是互补。提出CoRA架构，包含两个分支：特征级融合分支（选择关键特征并高效融合以确保性能和可扩展性）和对象级校正分支（利用语义相关性校正空间位移，保证对姿态错误的鲁棒性）。

Result: 在极端场景下，CoRA相比基线在AP@0.7上提升了约19%，同时通信量减少了5倍以上，成为鲁棒协作感知的有前景解决方案。

Conclusion: CoRA通过混合融合方法成功解耦了性能与鲁棒性，在保持低通信成本的同时显著提升了对抗通信条件的鲁棒性，为协作感知的实际部署提供了可行方案。

Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.

</details>


### [121] [POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling](https://arxiv.org/abs/2512.13192)
*Zhuo Chen,Chengqun Yang,Zhuo Su,Zheng Lv,Jingnan Gao,Xiaoyuan Zhang,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: POLAR是一个大规模物理校准的单光源数据集，POLARNet是基于流的生成模型，用于从单张肖像预测每个光源的响应，实现可扩展、可控的人脸重光照。


<details>
  <summary>Details</summary>
Motivation: 人脸重光照研究受限于大规模、物理一致的光照数据可用性，需要更好的数据集和模型来合成真实肖像同时保持身份和几何特征。

Method: 1. 构建POLAR数据集：包含200多个主体，156个光照方向，多视角和多样化表情的OLAT数据；2. 开发POLARNet：基于流的生成模型，从单张肖像预测每个光源的OLAT响应，将光照建模为连续、物理可解释的变换。

Result: POLAR提供了大规模物理校准的光照数据，POLARNet能够捕捉细粒度、方向感知的光照效果并保持面部身份，相比扩散或背景条件方法更具可扩展性和可控性。

Conclusion: POLAR和POLARNet构成了统一的光照学习框架，连接真实数据、生成合成和物理基础重光照，建立了可扩展、可复现的肖像光照"鸡与蛋"循环。

Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.

</details>


### [122] [Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance](https://arxiv.org/abs/2512.13238)
*Francesco Ragusa,Michele Mazzamuto,Rosario Forte,Irene D'Ambra,James Fort,Jakob Engel,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: Ego-EXTRA是一个用于专家-学员辅助的50小时第一人称视角视频语言数据集，包含真实专家指导学员执行程序性活动的对话，创建了超过15k个视觉问答对用于评估多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量的第一人称视角视频语言数据集来支持专家级辅助系统的开发与评估，特别是在程序性活动指导场景中。

Method: 采用"绿野仙踪"数据收集范式，专家通过穿戴式智能助手从学员的第一人称视角观察活动，通过自然语言提供指导、回答问题或主动提出建议，记录双向对话并转录。

Result: 创建了包含超过15,000个高质量视觉问答对的基准测试，评估显示当前多模态大语言模型在提供专家级辅助方面存在局限性，Ego-EXTRA数据集具有挑战性。

Conclusion: Ego-EXTRA是一个公开可用的高质量第一人称视角视频语言数据集，为评估第一人称视角视频语言助手提供了有价值的基准，揭示了当前模型在专家级辅助方面的不足。

Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

</details>


### [123] [STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits](https://arxiv.org/abs/2512.13247)
*Foivos Paraperas Papantoniou,Stathis Galanakis,Rolandos Alexandros Potamias,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: STARCaster是一个统一的身份感知时空视频扩散模型，能够同时处理语音驱动肖像动画和自由视角说话肖像合成，通过软身份约束和隐式3D感知在2D视频域中实现更好的运动多样性和身份保持。


<details>
  <summary>Details</summary>
Motivation: 现有2D语音到视频扩散模型过度依赖参考指导，导致运动多样性有限；而3D感知动画通常依赖预训练的三平面生成器反演，导致重建不完美和身份漂移。需要重新思考基于参考和几何的范式。

Method: 1. 采用软身份约束而非严格的参考条件化；2. 利用视频数据固有的多视角特性，在2D视频域中隐式实现3D感知；3. 采用组合式方法：ID感知运动建模→基于唇读的视听同步→通过时空适应的新视角动画；4. 提出解耦学习方法，独立训练视角一致性和时间连贯性；5. 使用自强制训练方案学习比推理时更长的时序上下文。

Result: 综合评估表明，STARCaster在任务和身份上都能有效泛化，在不同基准测试中持续超越先前方法，解决了现有自回归方法中常见的过度静态动画问题。

Conclusion: STARCaster通过重新思考参考和几何范式，在统一框架中实现了更好的语音驱动肖像动画和自由视角说话肖像合成，克服了现有方法的局限性，为身份感知视频生成提供了新的解决方案。

Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

</details>


### [124] [Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection](https://arxiv.org/abs/2512.13250)
*Juil Koo,Daehyeon Choi,Sangwoo Youn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: VG-AVS任务让视觉语言模型从静态图像推理转向主动视角选择，通过合成数据集和微调框架实现基于视觉信息的智能视角选择，提升下游问答性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型仅限于静态图像推理（快照视觉），而具身智能体需要主动移动获取更丰富视角（行走视觉）。VG-AVS任务旨在让模型仅基于当前视觉信息选择最有信息量的下一个视角，不依赖场景记忆或外部知识。

Method: 1) 构建合成数据集，自动生成配对查询-目标视角和问答提示；2) 提出微调框架：先通过监督微调（SFT）微调预训练VLM，再进行基于强化学习的策略优化。

Result: 方法在视角选择基础上实现了强大的问答性能，在未见过的合成和真实场景中表现出良好的泛化能力。将VG-AVS框架集成到现有基于场景探索的EQA系统中，提高了下游问答准确性。

Conclusion: VG-AVS任务成功将视觉语言模型从静态图像推理扩展到主动视角选择，通过合成数据训练和微调框架实现了仅基于视觉信息的智能视角决策，为具身智能体的视觉感知提供了新方法。

Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.

</details>


### [125] [CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing](https://arxiv.org/abs/2512.13276)
*Yan Li,Lin Liu,Xiaopeng Zhang,Wei Xue,Wenhan Luo,Yike Guo,Qi Tian*

Main category: cs.CV

TL;DR: CogniEdit：一个结合多模态推理与密集奖励优化的统一框架，用于解决扩散模型在遵循细粒度指令（如颜色、位置、数量）进行图像编辑时的困难


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法在处理细粒度指令（如精确的颜色、位置、数量属性）时表现不佳。虽然最近的方法使用GRPO进行对齐，但只在单个采样步骤优化，反馈稀疏，限制了轨迹级别的控制能力。

Method: CogniEdit包含三个核心组件：1) 多模态大语言模型将复杂指令分解为可操作的指令；2) 动态令牌焦点重定位自适应地强调细粒度属性；3) 基于密集GRPO的优化，在连续去噪步骤间传播梯度，实现轨迹级别的监督。

Result: 在基准数据集上的大量实验表明，CogniEdit在平衡细粒度指令遵循与视觉质量及可编辑性保持方面达到了最先进的性能。

Conclusion: 通过结合多模态推理与密集奖励优化，CogniEdit实现了轨迹级别的梯度流，显著提升了扩散模型在遵循细粒度指令进行图像编辑时的能力。

Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods strug- gle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across con- secutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation

</details>


### [126] [CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images](https://arxiv.org/abs/2512.13285)
*Bo Liu,Qiao Qin,Qinghui He*

Main category: cs.CV

TL;DR: CausalCLIP：基于因果推理的图像生成检测框架，通过解耦因果特征与非因果特征提升跨生成模型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测方法在处理多样化和不断演进的生成技术时，其表征往往高度纠缠，混合了任务相关的取证线索（因果特征）与虚假或无关模式（非因果特征），从而限制了泛化能力。

Method: 提出CausalCLIP框架，基于因果推理原理进行针对性过滤，通过结构因果模型建模生成过程，利用Gumbel-Softmax特征掩码和Hilbert-Schmidt独立性准则约束来强制统计独立性，从而解耦因果特征与非因果特征。

Result: 在未见过的不同系列生成模型上测试时，CausalCLIP展现出强大的泛化能力，在准确率上比现有最优方法提升6.83%，平均精度提升4.06%。

Conclusion: 通过显式解耦因果特征与非因果特征并保留最具可迁移性和判别性的取证线索，CausalCLIP能够有效应对分布偏移，提升生成图像检测的泛化性能。

Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

</details>


### [127] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

TL;DR: 论文提出LINA框架，通过因果干预解决扩散模型在物理对齐和分布外指令遵循方面的困难，在图像和视频生成任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了显著成功，但仍然存在物理对齐和分布外指令遵循的困难。作者认为这些问题源于模型未能学习因果方向和解耦因果因子以实现新颖重组。

Method: 引入因果场景图和物理对齐探测数据集进行诊断干预分析，基于分析结果提出LINA框架：学习预测特定提示的干预，采用提示和视觉潜在空间中的针对性引导，以及重新分配、因果感知的去噪调度。

Result: LINA框架在图像和视频扩散模型中同时实现了物理对齐和分布外指令遵循，在具有挑战性的因果生成任务和Winoground数据集上取得了最先进的性能。

Conclusion: 通过因果干预和重新分配的去噪调度，可以有效解决扩散模型在物理对齐和分布外指令遵循方面的根本问题，为更可控和可靠的生成模型提供了新思路。

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [128] [ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303)
*Zhihang Liu,Xiaoyi Bao,Pandeng Li,Junjie Zhou,Zhaohe Liao,Yefei He,Kaixun Jiang,Chen-Wei Xie,Yun Zheng,Hongtao Xie*

Main category: cs.CV

TL;DR: 论文提出ShowTable框架，结合MLLM和扩散模型，通过渐进式自校正过程生成创意表格可视化，并构建TableVisBench基准进行评估


<details>
  <summary>Details</summary>
Motivation: 现有生成模型和统一模型在需要深度推理、规划和精确数据到视觉映射能力的任务上表现不足，特别是在超越一般场景的创意表格可视化任务中

Method: 提出ShowTable框架，将MLLM作为中心协调器进行视觉规划推理和视觉错误判断，扩散模型执行MLLM指令，通过渐进式自校正过程实现高保真结果

Result: 实验表明，使用不同模型实例化的ShowTable框架显著优于基线方法，展示了其有效的多模态推理、生成和错误校正能力

Conclusion: 提出的创意表格可视化任务具有挑战性，ShowTable框架通过MLLM和扩散模型的协同工作，结合渐进式自校正过程，能够生成忠实且美观的数据可视化图表

Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.

</details>


### [129] [KlingAvatar 2.0 Technical Report](https://arxiv.org/abs/2512.13313)
*Kling Team,Jialu Chen,Yikang Ding,Zhixue Fang,Kun Gai,Yuan Gao,Kang He,Jingyun Hua,Boyuan Jiang,Mingming Lao,Xiaohan Li,Hui Liu,Jiwen Liu,Xiaoqiang Liu,Yuan Liu,Shun Lu,Yongsen Mao,Yingchao Shao,Huafeng Shi,Xiaoyu Shi,Peiqin Sun,Songlin Tang,Pengfei Wan,Chao Wang,Xuebo Wang,Haoxian Zhang,Yuanxing Zhang,Yan Zhou*

Main category: cs.CV

TL;DR: KlingAvatar 2.0是一个时空级联框架，通过空间分辨率提升和时间维度扩展，解决了长时高分辨率视频生成中的效率低下、时间漂移、质量下降和提示跟随弱等问题。


<details>
  <summary>Details</summary>
Motivation: 现有头像视频生成模型在生成长时高分辨率视频时存在效率低下、时间漂移、质量下降和提示跟随弱等问题，需要一种能够高效生成高质量长视频的解决方案。

Method: 提出时空级联框架：首先生成低分辨率蓝图视频关键帧捕捉全局语义和运动，然后使用首尾帧策略将其细化为高分辨率、时间连贯的子片段。引入协同推理导演（三个模态特定LLM专家）增强跨模态指令融合和对齐，以及负向导演优化负向提示。扩展框架支持ID特定的多角色控制。

Result: 实验表明该模型有效解决了高效、多模态对齐的长时高分辨率视频生成挑战，提供增强的视觉清晰度、逼真的唇齿渲染与准确的口型同步、强身份保持和连贯的多模态指令跟随。

Conclusion: KlingAvatar 2.0通过时空级联框架和协同推理导演机制，成功解决了长时高分辨率头像视频生成的关键挑战，实现了高质量、高效率的多模态对齐视频生成。

Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

</details>


### [130] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: 该论文研究了人脸检索系统中的身份遗忘问题，提出了一种基于分散的遗忘方法，使特定身份无法被检索，同时保持其他身份的检索性能。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统虽然能有效进行身份识别，但存在严重的隐私问题，可能被用于未经授权的身份追踪。现有的机器遗忘方法在人脸检索领域的应用尚未充分探索，特别是针对现代基于嵌入的识别模型。

Method: 评估了多种现有的近似类别遗忘方法（如随机标记、梯度上升、边界遗忘等），并提出了一种简单有效的基于分散的遗忘方法。该方法通过在超球面上分散目标身份的嵌入，防止形成紧凑的身份聚类，从而实现遗忘效果。

Result: 在标准基准数据集（VGGFace2、CelebA）上的大量实验表明，该方法实现了优异的遗忘效果，同时保持了模型的检索效用。

Conclusion: 该研究为人脸检索系统的隐私保护提供了有效的身份遗忘方法，能够在保护特定身份隐私的同时，保持系统对其他身份的识别性能，平衡了隐私保护与系统效用之间的关系。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [131] [Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"](https://arxiv.org/abs/2512.13376)
*Carla Monteiro,Valentina Corbetta,Regina Beets-Tan,Luís F. Teixeira,Wilson Silva*

Main category: cs.CV

TL;DR: 本文提出了一种利用DINO自注意力"key"特征进行息肉分割的框架，通过简单的卷积解码器实现，在数据受限和挑战性场景下表现出优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习息肉分割方法存在泛化能力不足的问题，特别是在数据受限或挑战性场景下，且许多方法依赖复杂的任务特定架构。需要一种更鲁棒、泛化能力更强的解决方案。

Method: 利用DINO自注意力模块的"key"特征，而非传统方法从Vision Transformer最深层提取token，结合简单的卷积解码器预测息肉掩码。该方法避免了复杂的任务特定架构。

Result: 在域泛化(DG)和极端单域泛化(ESDG)两种严格协议下，使用多中心数据集验证，该方法实现了最先进的性能，显著提升了泛化能力，特别是在数据稀缺和挑战性场景中，超越了nnU-Net和UM-Net等成熟模型。

Conclusion: 通过利用DINO自注意力关键特征的固有鲁棒性，结合简单解码器，可以构建出泛化能力强、性能优越的息肉分割框架，避免了复杂任务特定架构的需求，为数据受限场景下的医学图像分割提供了有效解决方案。

Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.

</details>


### [132] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

TL;DR: 提出MCT-UEG框架，通过元跨任务训练生成广泛不可利用的个人图像，防止在线图像被未经授权利用


<details>
  <summary>Details</summary>
Motivation: 现有不可利用示例生成方法在实际应用中存在局限性，无法生成在不同真实世界计算机视觉任务中都广泛不可利用的示例

Method: 提出MCT-UEG框架，核心是设计面向平坦最小值的元训练和测试方案，优化不可利用示例生成器

Result: 大量实验证明了该框架的有效性

Conclusion: 提出的MCT-UEG框架能够生成跨任务广泛不可利用的示例，提高了个人数据隐私保护的实际适用性

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [133] [RecTok: Reconstruction Distillation along Rectified Flow](https://arxiv.org/abs/2512.13421)
*Qingyu Shi,Size Wu,Jinbin Bai,Kaidong Yu,Yujing Wang,Yunhai Tong,Xiangtai Li,Xuelong Li*

Main category: cs.CV

TL;DR: RecTok通过流语义蒸馏和重建对齐蒸馏，解决了高维视觉分词器在生成质量上的局限性，实现了更好的图像重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉分词器存在维度与生成质量之间的根本权衡，高维分词器性能仍不如低维版本，需要克服这一限制。

Method: 提出两种关键创新：1）流语义蒸馏 - 将视觉基础模型的语义信息蒸馏到流匹配的前向流轨迹中；2）重建对齐蒸馏 - 引入掩码特征重建损失进一步增强语义。

Result: 在gFID-50K基准上，无论是否使用分类器自由引导，都取得了最先进的结果，同时保持了语义丰富的潜在空间结构，且随着潜在维度增加性能持续提升。

Conclusion: RecTok成功克服了高维视觉分词器的局限性，实现了优异的图像重建、生成质量和判别性能，为视觉分词器设计提供了新思路。

Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.

</details>


### [134] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

TL;DR: MineTheGap是一种自动挖掘导致文本到图像模型生成偏见输出的提示词的方法，使用遗传算法迭代优化提示词池，通过新颖的偏见评分来识别和量化偏见严重程度。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在生成图像时，文本提示往往存在模糊性，模型会表现出解释偏见。这些偏见可能产生社会影响（如职业与种族的刻板印象），也会影响用户体验（生成冗余图像而非多样化可能性）。

Method: 提出MineTheGap方法，使用遗传算法迭代优化提示词池，寻找能暴露偏见的提示词。优化过程由新颖的偏见评分驱动，该评分通过比较生成图像的分布与LLM生成的文本变体分布来计算，从而根据偏见严重程度进行排序。

Result: 该方法超越了仅检测给定提示词的偏见，能够自动挖掘导致偏见的提示词。在已知偏见的数据集上验证了偏见评分的有效性，代码和示例已在项目网页上提供。

Conclusion: MineTheGap提供了一种系统化的方法来识别文本到图像模型中的偏见，有助于理解模型偏见机制并促进更公平、多样化的图像生成。

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [135] [A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification](https://arxiv.org/abs/2512.13428)
*Anika Islam,Tasfia Tahsin,Zaarin Anjum,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级少样本学习框架，用于植物叶片病害识别，结合MobileNet特征提取、特征融合和注意力增强的Bi-LSTM分类器，在数据稀缺和资源受限环境下实现高性能病害诊断。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习植物病害识别方法依赖大量标注数据和计算密集型模型，不适合数据稀缺和资源受限环境，需要开发轻量级、高效的少样本学习解决方案。

Method: 采用领域适应的MobileNetV2和MobileNetV3作为特征提取器，结合特征融合技术生成鲁棒特征表示，使用注意力机制增强的Bi-LSTM分类器捕获序列依赖关系并聚焦关键特征。

Result: 在PlantVillage数据集上15-shot达到98.23%准确率，接近SOTA的99.98%；在真实环境Dhan Shomadhan数据集上15-shot达到69.28%；在PlantVillage六种病害上以15-shot学习达到99.72%，超越之前96.0%的SOTA。

Conclusion: 该框架模型大小约40MB，推理复杂度约1.12GFLOPs，为数据稀缺地区建立了一个可扩展、移动就绪的精确植物病害诊断基础，平衡了性能与资源效率。

Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.

</details>


### [136] [IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images](https://arxiv.org/abs/2512.13440)
*Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux*

Main category: cs.CV

TL;DR: IMILIA是一个端到端框架，使用多示例学习预测IBD组织切片中的炎症存在，并通过可解释性模块自动计算驱动预测的组织区域标记物。


<details>
  <summary>Details</summary>
Motivation: 随着IBD治疗目标转向组织学缓解，准确评估微观炎症对于评估疾病活动性和治疗反应变得日益重要，需要自动化工具来预测炎症存在并解释预测结果。

Method: IMILIA包含炎症预测模块（基于多示例学习的模型）和可解释性模块（HistoPLUS用于细胞检测、分割和分类，EpiSeg用于上皮分割）。

Result: 在发现队列中交叉验证ROC-AUC为0.83，在两个外部验证队列中分别为0.99和0.84。可解释性模块显示高分图块免疫细胞密度增加，低分图块主要为正常上皮细胞，这些模式在所有数据集中一致。

Conclusion: IMILIA能够准确预测IBD组织切片中的炎症存在，并提供生物学一致的可解释性结果，有助于临床评估和决策。

Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.

</details>


### [137] [Test-Time Modification: Inverse Domain Transformation for Robust Perception](https://arxiv.org/abs/2512.13454)
*Arpit Jadon,Joshua Niemeijer,Yuki M. Asano*

Main category: cs.CV

TL;DR: 提出一种使用扩散模型在测试时将目标域图像映射回源域分布的方法，无需大规模合成数据生成，在多种视觉任务上实现显著的域泛化性能提升


<details>
  <summary>Details</summary>
Motivation: 生成式基础模型包含广泛的视觉知识并能产生多样的图像变体，但用于训练数据增强时合成全面的目标域变体仍然缓慢、昂贵且不完整。需要一种更高效的域泛化方法。

Method: 在测试时使用扩散模型将目标域图像映射回源域分布，仅需源域描述，保留任务模型，无需大规模合成数据生成。支持多种生成模型和下游模型，包括增强鲁棒性的集成变体。

Result: 在分割、检测和分类任务上，面对真实到真实的域泛化场景和未知目标分布，实现了显著且一致的性能提升：BDD100K-Night上137%，ImageNet-R上68%，DarkZurich上62%。

Conclusion: 提出的测试时域映射方法为域泛化提供了高效替代方案，避免了大规模合成数据生成，在多种视觉任务和数据集上实现了显著的性能提升。

Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

</details>


### [138] [PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence](https://arxiv.org/abs/2512.13465)
*Ruiyan Wang,Teng Hu,Kaihui Huang,Zihan Su,Ran Yi,Lizhuang Ma*

Main category: cs.CV

TL;DR: PoseAnything是一个通用的姿态引导视频生成框架，能够处理人类和非人类角色，支持任意骨骼输入，并实现了独立相机运动控制。


<details>
  <summary>Details</summary>
Motivation: 当前姿态引导视频生成方法仅接受人体姿态作为输入，对其他主体的姿态泛化能力差，限制了在动画等领域的应用。

Method: 1) 提出PoseAnything通用框架；2) 引入部件感知时间一致性模块，通过部件分割、对应关系建立和跨帧注意力实现细粒度一致性；3) 提出主体与相机运动解耦CFG，通过分别注入控制信息到CFG的正负锚点实现独立相机控制；4) 构建XPose数据集（5万非人类姿态-视频对）及自动化标注流程。

Result: PoseAnything在效果和泛化能力上显著优于现有最先进方法，能够生成高质量的人类和非人类角色视频，并实现独立的相机运动控制。

Conclusion: PoseAnything是首个通用的姿态引导视频生成框架，突破了仅限人体姿态的限制，通过创新的部件一致性模块和运动解耦CFG策略，实现了对任意骨骼输入的支持和独立相机控制，为动画制作提供了更强大的工具。

Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

</details>


### [139] [Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$](https://arxiv.org/abs/2512.13492)
*Jiangning Zhang,Junwei Zhu,Teng Hu,Yabiao Wang,Donghao Luo,Weijian Cao,Zhenye Gan,Xiaobin Hu,Zhucun Xue,Chengjie Wang*

Main category: cs.CV

TL;DR: T3-Video是一种Transformer改造策略，通过多尺度权重共享窗口注意力机制和分层分块设计，在不改变预训练模型核心架构的情况下，将原生4K视频生成速度提升10倍以上，同时提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 原生4K视频生成面临计算复杂度爆炸式增长的问题，全注意力机制在时空分辨率增加时计算量呈二次方增长，现有模型难以在效率和质量之间取得平衡。

Method: 提出T3策略，通过多尺度权重共享窗口注意力机制、分层分块和轴保持全注意力设计，仅用适度计算和数据就能实现预训练模型的"注意力模式"转换，而不改变其核心架构。

Result: 在4K-VBench测试中，T3-Video显著优于现有方法：在性能提升方面（VQA提升4.29，VTC提升0.08），同时将原生4K视频生成速度加速超过10倍。

Conclusion: T3-Video通过创新的Transformer改造策略，成功解决了4K视频生成中的计算效率问题，在保持甚至提升生成质量的同时实现了显著的速度提升，为高质量视频生成提供了可行的解决方案。

Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video

</details>


### [140] [Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model](https://arxiv.org/abs/2512.13507)
*Siyan Chen,Yanfei Chen,Ying Chen,Zhuo Chen,Feng Cheng,Xuyan Chi,Jian Cong,Qinpeng Cui,Qide Dong,Junliang Fan,Jing Fang,Zetao Fang,Chengjian Feng,Han Feng,Mingyuan Gao,Yu Gao,Qiushan Guo,Boyang Hao,Qingkai Hao,Bibo He,Qian He,Tuyen Hoang,Ruoqing Hu,Xi Hu,Weilin Huang,Zhaoyang Huang,Zhongyi Huang,Siqi Jiang,Wei Jiang,Yunpu Jiang,Zhuo Jiang,Ashley Kim,Jianan Kong,Zhichao Lai,Shanshan Lao,Ai Li,Feiya Li,Gen Li,Huixia Li,JiaShi Li,Liang Li,Ming Li,Tao Li,Xian Li,Xiaojie Li,Xiaoyang Li,Xingxing Li,Yameng Li,Yifu Li,Yiying Li,Chao Liang,Ying Liang,Zhiqiang Liang,Wang Liao,Yalin Liao,Heng Lin,Kengyu Lin,Shanchuan Lin,Xi Lin,Zhijie Lin,Feng Ling,Fangfang Liu,Gaohong Liu,Jiawei Liu,Jie Liu,Shouda Liu,Shu Liu,Sichao Liu,Songwei Liu,Xin Liu,Xue Liu,Yibo Liu,Zikun Liu,Zuxi Liu,Junlin Lyu,Lecheng Lyu,Qian Lyu,Han Mu,Xiaonan Nie,Jingzhe Ning,Xitong Pan,Yanghua Peng,Lianke Qin,Xueqiong Qu,Yuxi Ren,Yuchen Shen,Guang Shi,Lei Shi,Yan Song,Yinglong Song,Fan Sun,Li Sun,Renfei Sun,Zeyu Sun,Wenjing Tang,Zirui Tao,Feng Wang,Furui Wang,Jinran Wang,Junkai Wang,Ke Wang,Kexin Wang,Qingyi Wang,Rui Wang,Sen Wang,Shuai Wang,Tingru Wang,Weichen Wang,Xin Wang,Yanhui Wang,Yue Wang,Yuping Wang,Yuxuan Wang,Ziyu Wang,Guoqiang Wei,Wanru Wei,Di Wu,Guohong Wu,Hanjie Wu,Jian Wu,Jie Wu,Ruolan Wu,Xinglong Wu,Yonghui Wu,Ruiqi Xia,Liang Xiang,Fei Xiao,XueFeng Xiao,Pan Xie,Shuangyi Xie,Shuang Xu,Jinlan Xue,Bangbang Yang,Ceyuan Yang,Jiaqi Yang,Runkai Yang,Tao Yang,Yang Yang,Yihang Yang,ZhiXian Yang,Ziyan Yang,Yifan Yao,Zilyu Ye,Bowen Yu,Chujie Yuan,Linxiao Yuan,Sichun Zeng,Weihong Zeng,Xuejiao Zeng,Yan Zeng,Chuntao Zhang,Heng Zhang,Jingjie Zhang,Kuo Zhang,Liang Zhang,Liying Zhang,Manlin Zhang,Ting Zhang,Weida Zhang,Xiaohe Zhang,Xinyan Zhang,Yan Zhang,Yuan Zhang,Zixiang Zhang,Fengxuan Zhao,Huating Zhao,Yang Zhao,Hao Zheng,Jianbin Zheng,Xiaozheng Zheng,Yangyang Zheng,Yijie Zheng,Jiexin Zhou,Kuan Zhu,Shenhan Zhu,Wenjia Zhu,Benhui Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.5 pro是一个用于原生联合音频-视频生成的基础模型，采用双分支Diffusion Transformer架构，通过跨模态联合模块和多阶段数据管道实现卓越的视听同步和生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频生成领域的最新进展为统一的视听生成铺平了道路，但需要专门针对原生、联合音频-视频生成的基础模型，以实现更好的同步效果和生成质量。

Method: 采用双分支Diffusion Transformer架构，集成跨模态联合模块和专门的多阶段数据管道；实施细致的训练后优化，包括高质量数据集上的监督微调（SFT）和基于多维奖励模型的人类反馈强化学习（RLHF）；引入加速框架提升推理速度10倍以上。

Result: 模型实现了卓越的视听同步和生成质量，具备精确的多语言和方言唇形同步、动态电影级摄像机控制、增强的叙事连贯性，成为专业级内容创作的强大引擎。

Conclusion: Seedance 1.5 pro是一个强大的原生联合音频-视频生成基础模型，通过创新的架构设计和优化策略，在视听同步、生成质量和推理效率方面表现出色，适用于专业内容创作。

Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.

</details>


### [141] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: TARA是一种无需视频数据、通过适配多模态大语言模型构建时间感知视频-文本嵌入模型的方法，在时间感知检索基准上超越现有模型，并在否定理解、动词副词理解等方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 构建通用的时间感知视频-文本嵌入模型，解决现有模型在时间感知检索方面的不足，特别是对时间顺序敏感的动作识别。

Method: 提出TARA（时间感知检索适配）方法，在不使用任何视频数据的情况下，将多模态大语言模型适配为时间感知视频-文本嵌入模型。通过时间对立（手性）动作作为困难负样本进行评估。

Result: TARA在时间感知检索基准上超越所有现有视频-文本模型，同时在标准基准上取得强劲结果。模型还展现出否定感知能力，在动词和副词理解方面达到最先进性能。

Conclusion: TARA产生了一个强大、通用、时间感知的视频-文本嵌入模型，具有最先进的零样本性能，在时间感知、否定理解和动作理解方面均有显著优势。

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [142] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: Pancakes框架能够自动为医学图像生成多种语义一致的分割方案，解决了现有模型只能支持单一协议或需要手动提示的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像可以根据不同应用需求进行多种有意义的分割，但现有自动分割模型要么只支持单一训练协议，要么需要人工手动指定分割方案，缺乏自动生成多种语义一致分割的能力。

Method: 提出Pancakes框架，引入新的问题表述，能够在未见过的医学图像域中自动生成多标签分割图，支持多种可能的分割协议，同时保持相关图像间的语义一致性。

Result: 在7个保留数据集上的实验表明，Pancakes在生成多种语义一致的整图分割方面显著优于现有基础模型。

Conclusion: Pancakes框架解决了医学图像多协议自动分割的关键问题，实现了现有基础模型无法达到的功能，为医学图像分析提供了更灵活和自动化的解决方案。

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [143] [MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573)
*Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Bing Li,Chunfeng Yuan,Guangting Wang,Fengyun Rao,Ying Shan,Weiming Hu*

Main category: cs.CV

TL;DR: MMhops是一个评估多模态多跳推理能力的新基准，包含Bridging和Comparison两种任务格式，需要模型通过整合外部知识构建复杂推理链。作者提出了MMhops-R1框架，使用强化学习优化动态推理路径规划，在MMhops基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要局限于单步推理，缺乏能够评估和驱动多跳推理能力的复杂基准。现实世界中的复杂挑战需要模型能够跨多种模态和外部知识进行迭代式信息整合的多跳推理能力。

Method: 提出了MMhops-R1框架，这是一个基于强化学习的多模态检索增强生成框架，能够自主规划推理路径、制定针对性查询并整合多层级信息。框架包含动态推理路径规划和多模态知识整合机制。

Result: MMhops-R1在MMhops基准上显著优于现有基线方法，证明了动态规划和多模态知识整合对复杂推理的重要性。同时，该框架在需要固定跳数推理的任务上也表现出良好的泛化能力。

Conclusion: 该工作贡献了一个具有挑战性的新基准MMhops和一个强大的基线模型MMhops-R1，将发布相关代码、数据和权重以推动该关键领域的研究。动态规划和多模态知识整合是复杂推理的关键要素。

Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.

</details>


### [144] [Lighting in Motion: Spatiotemporal HDR Lighting Estimation](https://arxiv.org/abs/2512.13597)
*Christophe Bolduc,Julien Philip,Li Ma,Mingming He,Paul Debevec,Jean-François Lalonde*

Main category: cs.CV

TL;DR: LiMo是一种基于扩散模型的时空光照估计方法，通过生成不同曝光下的镜面和漫反射球体，结合深度和几何条件，实现高细节预测和准确照度估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法在同时实现真实高频细节预测和准确照度估计方面存在不足，需要一种能够同时处理这两个方面的时空光照估计方法。

Method: 1. 基于输入3D位置生成不同曝光下的镜面和漫反射球体集合；2. 在大规模定制室内外场景数据集上微调现有扩散模型；3. 引入新的几何条件提供场景到目标3D位置的相对位置信息；4. 通过可微分渲染将不同曝光的漫反射和镜面预测合并为单个HDRI图。

Result: LiMo在空间控制和预测准确性方面达到最先进水平，能够同时实现高质量的高频细节预测和准确的照度估计。

Conclusion: LiMo通过创新的几何条件和多曝光球体生成方法，成功解决了时空光照估计中同时实现细节和准确性平衡的挑战，为相关应用提供了有效的解决方案。

Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.

</details>


### [145] [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Junhao Zhuang,Chengming Xu,Jianfeng Feng,Yu Qiao,Yanwei Fu,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: LongVie 2是一个三阶段训练的端到端自回归框架，通过多模态引导、退化感知训练和历史上下文引导，实现了可控、高质量、时间一致的视频世界建模，支持长达5分钟的连续视频生成。


<details>
  <summary>Details</summary>
Motivation: 基于预训练视频生成系统构建视频世界模型是实现通用时空智能的重要但具有挑战性的步骤。世界模型需要具备三个关键属性：可控性、长期视觉质量和时间一致性。

Method: 采用渐进式方法：1) 多模态引导：整合密集和稀疏控制信号提供隐式世界级监督；2) 输入帧的退化感知训练：弥合训练与长期推理之间的差距；3) 历史上下文引导：对齐相邻片段间的上下文信息确保时间一致性。

Result: LongVie 2在长距离可控性、时间一致性和视觉保真度方面达到最先进性能，支持长达5分钟的连续视频生成。同时提出了包含100个高分辨率一分钟视频的LongVGenBench基准测试。

Conclusion: LongVie 2通过三阶段训练框架显著提升了视频世界建模能力，在可控性、视觉质量和时间一致性方面取得突破，向统一的视频世界建模迈出了重要一步。

Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.

</details>


### [146] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: Do-Undo任务和基准测试旨在解决视觉语言模型在理解和生成由真实世界动作驱动的物理合理场景变换方面的关键缺陷，要求模型模拟物理动作的结果并准确反转它。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在物理可逆性理解方面的不足，填补了模型理解真实世界因果关系和物理合理场景变换的空白，这对具身AI、机器人和物理感知生成建模至关重要。

Method: 从真实世界视频中构建大规模可逆动作数据集，设计训练策略以增强动作定位的一致性，通过Do-Undo任务要求模型模拟物理动作结果并准确反转它。

Result: 实验表明当前模型在物理可逆性方面表现不佳，突显了该任务的重要性，Do-Undo为评估和推进多模态系统中的物理推理提供了直观测试平台。

Conclusion: Do-Undo任务和基准测试填补了视觉语言模型在物理合理场景变换理解方面的关键空白，为评估和提升多模态系统的物理推理能力提供了重要工具。

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


### [147] [Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All](https://arxiv.org/abs/2512.13639)
*Michal Nazarczuk,Thomas Tanay,Arthur Moreau,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 提出了一个用于新颖视角合成的全新高质量数据集，基于动画电影制作，包含动态场景、多种模态数据，并设计了三种不同的基准测试场景。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量、多模态、包含动态场景的新颖视角合成数据集，限制了4D场景重建和视角生成模型的训练与评估。

Method: 从高质量动画电影中生成数据集，提供RGB图像、深度、表面法线、物体分割和光流等多种模态数据，并组织成密集多视角、稀疏相机和单目视频三种基准测试场景。

Result: 创建了一个视觉丰富、标注质量高、实验设置多样的数据集，能够支持从密集到稀疏不同数据条件下的视角合成研究。

Conclusion: 该数据集为推进视角合成和3D视觉研究提供了独特资源，能够支持多种实验设置和模型评估。

Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.

</details>


### [148] [Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency](https://arxiv.org/abs/2512.13665)
*Wenhan Chen,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: 提出Grab-3D框架，利用3D几何一致性检测AI生成视频，通过消失点分析几何模式差异，在静态场景数据集上验证效果显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 扩散生成技术能产生高度逼真的视频，现有检测方法对3D几何模式探索有限，需要更可靠的检测机制

Method: 使用消失点作为3D几何模式的显式表示；提出Grab-3D框架，包含几何位置编码、时空几何注意力、EMA几何分类器头；构建静态场景AI生成视频数据集

Result: Grab-3D显著优于现有最先进检测器，在未见过的生成器上表现出强大的跨域泛化能力

Conclusion: 通过3D几何时间一致性分析，Grab-3D为AI生成视频检测提供了有效的几何感知框架，在跨域泛化方面表现优异

Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.

</details>


### [149] [AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection](https://arxiv.org/abs/2512.13671)
*Junwen Miao,Penghui Du,Yi Liu,Yu Wang,Yan Wang*

Main category: cs.CV

TL;DR: AgentIAD是一个基于工具驱动的智能体框架，用于工业异常检测，通过多阶段视觉检查、感知缩放和比较检索来提升对小缺陷的检测能力，在MMAD数据集上达到97.62%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测面临正常参考样本稀缺和缺陷微小、局部化的挑战。现有的单通道视觉语言模型往往忽略小异常，缺乏与标准正常模式进行显式比较的机制。

Method: 提出AgentIAD框架，包含感知缩放器（PZ）进行局部细粒度分析，比较检索器（CR）在证据模糊时查询正常样本。使用MMAD数据集构建结构化的感知和比较轨迹，通过两阶段训练：监督微调后接强化学习，采用感知奖励和行为奖励的双重奖励设计。

Result: 在MMAD数据集上达到97.62%的分类准确率，创下新的最先进水平，超越了之前基于MLLM的方法，同时产生透明可解释的检查轨迹。

Conclusion: AgentIAD通过工具驱动的智能体框架实现了多阶段视觉检查，能够通过逐步观察、缩放和验证来完善判断，有效解决了工业异常检测中的小缺陷检测和正常模式比较问题。

Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

</details>


### [150] [JoVA: Unified Multimodal Learning for Joint Video-Audio Generation](https://arxiv.org/abs/2512.13677)
*Xiaohu Huang,Hao Zhou,Qiangpeng Yang,Shilei Wen,Kai Han*

Main category: cs.CV

TL;DR: JoVA是一个统一的视频-音频联合生成框架，通过联合自注意力机制实现跨模态交互，无需额外对齐模块，并引入基于面部关键点检测的嘴部区域损失来提升唇语同步质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键限制：1) 大多只能生成环境音，缺乏生成与唇部动作同步的人类语音的能力；2) 现有统一视频-音频生成方法通常依赖显式融合或模态特定对齐模块，增加了架构复杂性并削弱了原始Transformer的简洁性。

Method: JoVA采用联合自注意力机制，在Transformer层内对视频和音频token进行跨模态交互，无需额外对齐模块。同时引入基于面部关键点检测的嘴部区域损失，在训练过程中增强对关键嘴部区域的监督，而不影响架构简洁性。

Result: 在基准测试上的广泛实验表明，JoVA在唇语同步准确性、语音质量和整体视频-音频生成保真度方面优于或与最先进的统一方法和音频驱动方法相竞争。

Conclusion: JoVA作为一个优雅的框架，能够实现高质量的多模态生成，解决了现有方法在唇语同步和架构简洁性方面的限制。

Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.

</details>


### [151] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

TL;DR: Steer3D：一种为图像到3D模型添加文本可操控性的前馈方法，支持用语言编辑生成的3D资产


<details>
  <summary>Details</summary>
Motivation: AI生成的3D资产在实际应用中需要易于编辑的能力，现有方法缺乏有效的文本操控性

Method: 受ControlNet启发，将文本操控性适配到图像到3D生成；构建自动数据生成引擎，采用基于流匹配训练和直接偏好优化（DPO）的两阶段训练方案

Result: 相比竞争方法，Steer3D更忠实遵循语言指令，与原始3D资产保持更好一致性，速度提升2.4倍到28.5倍

Conclusion: Steer3D证明可以用10万数据为预训练图像到3D生成模型添加新模态（文本）的操控性

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [152] [LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction](https://arxiv.org/abs/2512.13680)
*Tianye Ding,Yiming Xie,Yiqing Liang,Moitreya Chatterjee,Pedro Miraldo,Huaizu Jiang*

Main category: cs.CV

TL;DR: LASER是一个无需训练的流式视频重建框架，通过层间尺度对齐将离线模型转换为流式系统，解决了传统方法因二次内存复杂度无法处理流式视频的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈重建模型（如VGGT和π³）虽然重建质量出色，但由于二次内存复杂度无法处理流式视频，限制了实际部署。现有的流式方法需要大量重新训练，且无法充分利用最先进离线模型的几何先验。

Method: 提出LASER框架，通过跨连续时间窗口的对齐预测将离线重建模型转换为流式系统。针对简单相似变换对齐因单目尺度歧义导致不同场景层深度尺度不一致的问题，引入层间尺度对齐方法：将深度预测分割为离散层，计算每层尺度因子，并在相邻窗口和时间戳间传播。

Result: LASER在相机姿态估计和点云图重建方面达到最先进性能，在RTX A6000 GPU上以14 FPS运行，峰值内存仅6 GB，能够处理千米级流式视频。

Conclusion: LASER提供了一种无需训练的方法，将高质量离线重建模型转换为实用的流式系统，解决了现有方法的内存和训练限制问题，实现了大规模流式视频的实际部署。

Abstract: Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$

</details>


### [153] [I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners](https://arxiv.org/abs/2512.13683)
*Lu Ling,Yunhao Ge,Yichen Sheng,Aniket Bera*

Main category: cs.CV

TL;DR: 该研究通过重新编程预训练的3D实例生成器，使其成为场景级学习器，利用模型自身的空间知识而非数据集监督，实现了对未见布局和新物体组合的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的3D场景生成方法依赖于有限场景数据集的空间理解，限制了向新布局的泛化能力。研究者希望利用预训练3D实例生成器的可迁移空间知识，实现更好的泛化。

Method: 重新编程预训练的3D实例生成器作为场景级学习器，用模型中心的空间监督替代数据集监督。采用视点中心的场景空间表示，而非传统的规范空间，构建完全前馈、可泛化的场景生成器。

Result: 该方法能够泛化到未见布局和新物体组合，即使在训练场景由随机组合物体构成时，空间推理能力仍然显现。量化与定性结果表明3D实例生成器是隐式的空间学习与推理器。

Conclusion: 3D实例生成器具有可迁移的场景先验，能够从纯几何线索中推断邻近性、支撑和对称关系，为交互式3D场景理解和生成的基础模型指明了方向。

Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/

</details>


### [154] [Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687)
*Jingfeng Yao,Yuda Song,Yucong Zhou,Xinggang Wang*

Main category: cs.CV

TL;DR: VTP提出了一种新的视觉分词器预训练框架，通过联合优化图像-文本对比、自监督和重建损失来解决传统重建训练导致的潜在空间偏向低级信息的问题，显著改善了生成模型的缩放性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于重建的视觉分词器训练范式导致潜在空间偏向低级信息，造成"预训练缩放问题"：更好的像素级精度并不带来更高质量的生成，大量计算投入无法有效转化为生成性能提升。

Method: 提出VTP统一视觉分词器预训练框架，首次联合优化图像-文本对比损失、自监督损失和重建损失，使潜在空间能够简洁地表示高级语义信息。

Result: VTP展现出优异的缩放特性：生成性能随计算、参数和数据投入有效提升；在ImageNet上达到78.2%零样本准确率和0.36 rFID；生成收敛速度比先进蒸馏方法快4.1倍；仅通过增加VTP预训练计算量就实现下游生成65.8%的FID改进。

Conclusion: 理解是驱动的关键因素，VTP框架通过联合多任务学习解决了视觉分词器的预训练缩放问题，使生成性能能够有效随预训练投入而提升，为生成模型提供了更有效的视觉表示。

Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

</details>


### [155] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

TL;DR: DiffusionBrowser是一个轻量级解码器框架，可在去噪过程中实时生成视频预览，支持RGB和场景内在表示，速度超过实时4倍，并提供交互式生成控制。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型存在生成不精确、速度慢、生成过程不透明的问题，用户在生成过程中长时间处于"黑暗"状态，无法了解生成进度和进行交互控制。

Method: 提出DiffusionBrowser框架，这是一个模型无关的轻量级解码器，可在去噪过程的任何时间步或transformer块处生成多模态预览表示（包括RGB和场景内在特征）。通过训练的解码器，支持通过随机性重注入和模态引导进行交互式生成控制。

Result: 模型能以超过4倍实时速度生成预览（4秒视频不到1秒），预览与最终视频具有一致的外观和运动。解码器还支持交互式生成控制，并能够系统性地探测模型，揭示去噪过程中场景、物体等细节的组成和组装过程。

Conclusion: DiffusionBrowser解决了视频扩散模型的透明度和交互性问题，提供了实时预览和交互控制能力，同时为理解黑盒去噪过程提供了新的分析工具。

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [156] [DarkSPARC: Dark-Blood Spectral Self-Calibrated Reconstruction of 3D Left Atrial LGE MRI for Post-Ablation Scar Imaging](https://arxiv.org/abs/2512.13527)
*Mohammed S. M. Elbaz*

Main category: physics.med-ph

TL;DR: DarkSPARC是一种无需训练、自校准的光谱重建方法，可将常规亮血3D左心房延迟增强MRI转换为暗血图像，显著提升疤痕-血池对比噪声比、信噪比和有效对比噪声比，并提高疤痕定量准确性。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需额外扫描、无需训练的暗血重建方法，以改善左心房延迟增强MRI中疤痕与血池的对比度，提高疤痕定量的准确性和可靠性。

Method: DarkSPARC将亮血左心房延迟增强MRI嵌入到校准器调节的(N+1)维光谱域中，利用扫描特定的光谱标志重建暗血样图像。通过从LAScarQS后消融延迟增强数据构建扫描特定的3D数值体模框架，克隆远程心肌到左心房壁并施加可控的疤痕负荷，创建了200个体模进行评估。

Result: 在体模实验中，DarkSPARC在所有200个实验中均提升了疤痕-血池对比噪声比、信噪比和有效对比噪声比，在最低对比噪声比条件下，暗血/亮血比率可达约30倍（对比噪声比）和约6倍（信噪比）。在70%对比噪声比退化时，亮血低估真实疤痕百分比37%-54%，而DarkSPARC将偏差降低至约3%-5%。在体内实验中，各项指标均有显著提升。

Conclusion: DarkSPARC是一种自校准、无需训练的重建方法，可生成暗血3D左心房延迟增强图像，显著提升对比噪声比、信噪比和有效对比噪声比，稳定可靠的疤痕定量，无需额外扫描。

Abstract: Purpose: To develop DarkSPARC, a retrospective, training-free, self-calibrated spectral reconstruction method that converts routine bright-blood 3D left atrial (LA) late gadolinium enhancement (LGE) MRI into a dark-blood image, and to quantify its impact on LA scar-pool CNR, SNR, effective CNR (eCNR), and scar quantification accuracy.
  Methods: DarkSPARC embeds bright-blood LA LGE into a calibrator-conditioned (N+1)-dimensional spectral domain and reconstructs a dark-blood-like image using scan-specific spectral landmarks. A scan-specific 3D numerical phantom framework was built from LAScarQS post-ablation LGE by cloning remote myocardium into the LA wall and imposing controlled scar burden. Five baseline cases spanning the 5th-95th percentiles of native scar-pool CNR, each with multiple scar burdens and 10 CNR degradation levels, yielded 200 phantoms. For every phantom, LA scar-pool CNR, SNR, eCNR, and Scar% were measured on bright-blood and DarkSPARC images. In vivo performance was evaluated in 60 public post-ablation scans of atrial fibrillation patients.
  Results: In scan-specific phantoms, DarkSPARC increased LA scar-pool CNR, SNR, and eCNR over bright-blood in all 200 experiments, with DarkSPARC/bright-blood ratios up to about 30-fold for CNR and about 6-fold for SNR in the lowest-CNR conditions. At 70% CNR degradation, bright-blood underestimated ground-truth LA Scar% by -37% to -54%, whereas DarkSPARC reduced bias to about -3% to -5%. In vivo, DarkSPARC similarly improved metrics: median scar-pool CNR, SNR, and eCNR increased from 20.0 to 135.9 (6.8x), 70.6 to 200.6 (2.8x), and 0.22 to 0.75 (3.4x), respectively (all p<0.001), and LA Scar% increased from 3.9% to 9.75%.
  Conclusion: DarkSPARC is a self-calibrated, training-free reconstruction that yields dark-blood 3D LA LGE, boosting CNR/SNR/eCNR and stabilizing reliable scar quantification without extra scans.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [157] [Resolution-Independent Neural Operators for Multi-Rate Sparse-View CT](https://arxiv.org/abs/2512.12236)
*Aujasvit Datta,Jiayun Wang,Asad Aali,Armeet Singh Jatyani,Anima Anandkumar*

Main category: eess.IV

TL;DR: CTO提出了一种统一的CT重建框架，通过连续函数空间中的旋转等变离散-连续卷积，实现了跨采样率和图像分辨率的泛化，无需重新训练即可获得高质量重建。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT重建是一个不适定逆问题。现有深度学习方法虽然能实现高保真重建，但通常过拟合于固定的采集设置，无法泛化到不同的采样率和图像分辨率。例如，CNN在不同分辨率下使用相同的学习核，导致分辨率变化时产生伪影。

Method: 提出CTO框架，在连续函数空间中操作，通过旋转等变的离散-连续卷积在正弦图和图像域联合工作。这些卷积在函数空间中参数化，使其天生具有分辨率和采样无关性。

Result: CTO实现了跨多采样率和分辨率的稳定性能，平均比CNN提高>4dB PSNR。相比最先进的扩散方法，CTO推理速度快500倍，平均PSNR提高3dB。实验结果验证了正弦图空间算子学习和旋转等变卷积的设计选择。

Conclusion: CTO在采样率和分辨率方面均优于最先进的基线方法，提供了一个可扩展且可泛化的解决方案，使自动化CT重建在实际部署中更加实用。

Abstract: Sparse-view Computed Tomography (CT) reconstructs images from a limited number of X-ray projections to reduce radiation and scanning time, which makes reconstruction an ill-posed inverse problem. Deep learning methods achieve high-fidelity reconstructions but often overfit to a fixed acquisition setup, failing to generalize across sampling rates and image resolutions. For example, convolutional neural networks (CNNs) use the same learned kernels across resolutions, leading to artifacts when data resolution changes.
  We propose Computed Tomography neural Operator (CTO), a unified CT reconstruction framework that extends to continuous function space, enabling generalization (without retraining) across sampling rates and image resolutions. CTO operates jointly in the sinogram and image domains through rotation-equivariant Discrete-Continuous convolutions parametrized in the function space, making it inherently resolution- and sampling-agnostic. Empirically, CTO enables consistent multi-sampling-rate and cross-resolution performance, with on average >4dB PSNR gain over CNNs. Compared to state-of-the-art diffusion methods, CTO is 500$\times$ faster in inference time with on average 3dB gain. Empirical results also validate our design choices behind CTO's sinogram-space operator learning and rotation-equivariant convolution. Overall, CTO outperforms state-of-the-art baselines across sampling rates and resolutions, offering a scalable and generalizable solution that makes automated CT reconstruction more practical for deployment.

</details>


### [158] [V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval](https://arxiv.org/abs/2512.12284)
*Donghyuk Kim,Sejeong Yang,Wonjin Shin,Joo-Young Kim*

Main category: eess.IV

TL;DR: V-Rex是首个软硬件协同设计的加速器，专门解决流式视频大语言模型中KV缓存增长带来的计算和内存瓶颈，通过ReSV算法和硬件加速器实现边缘设备上的实时高效推理。


<details>
  <summary>Details</summary>
Motivation: 流式视频大语言模型在实时多模态任务中面临关键挑战：随着连续视频输入，KV缓存大幅增长，导致迭代预填充阶段产生大量计算、数据传输和精度下降问题，这在边缘部署中尤为严重。

Method: 提出V-Rex软硬件协同设计加速器：1) ReSV算法：基于时空相似性的无训练动态KV缓存检索算法，通过token聚类减少跨视频帧的KV缓存；2) 硬件加速器：包含动态KV缓存检索引擎(DRE)，采用位级和早期退出计算单元。

Result: V-Rex在边缘部署中实现前所未有的3.9-8.3 FPS实时流式视频LLM推理，精度损失可忽略；DRE仅占2.2%功耗和2.0%面积，相比AGX Orin GPU实现1.9-19.7倍加速和3.1-18.5倍能效提升。

Conclusion: 这是首个全面解决算法和硬件层面KV缓存检索问题的工作，使资源受限的边缘设备能够实现实时流式视频大语言模型推理，为边缘AI应用开辟了新可能。

Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.

</details>


### [159] [Leveraging Compression to Construct Transferable Bitrate Ladders](https://arxiv.org/abs/2512.12952)
*Krishna Srikar Durbha,Hassene Tmar,Ping-Hao Wu,Ioannis Katsavounidis,Alan C. Bovik*

Main category: eess.IV

TL;DR: 该论文提出了一种新的基于机器学习的比特率阶梯构建技术，通过分析压缩过程并对源视频进行感知相关测量，准确预测压缩视频的VMAF分数，从而替代计算开销大的凸包构建方法。


<details>
  <summary>Details</summary>
Motivation: 传统的每标题和每镜头视频编码技术相比固定比特率阶梯有显著优势，但为每个视频构建凸包需要大量计算开销。机器学习方法作为替代方案出现，但现有方法仍有改进空间。

Method: 提出新的基于机器学习的比特率阶梯构建框架，通过分析压缩过程并对源视频进行感知相关测量来预测压缩视频的VMAF分数。同时研究了不同编码设置下每镜头比特率阶梯的性能。

Result: 在大规模视频语料库上评估了所提框架与现有领先方法的性能，并与固定比特率阶梯和通过穷举编码构建的最佳可能凸包进行比较，使用Bjontegaard-delta指标进行评估。

Conclusion: 该研究展示了机器学习方法可以有效替代计算密集的凸包构建，为内容自适应比特率阶梯提供了高效准确的解决方案，同时探讨了不同编码设置对性能的影响。

Abstract: Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [160] [Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights](https://arxiv.org/abs/2512.11802)
*Zheng Li,Peng Zhang,Shixiao Liang,Hang Zhou,Chengyuan Ma,Handong Yao,Qianwen Li,Xiaopeng Li*

Main category: cs.RO

TL;DR: 该研究通过实地实验分析了特斯拉交通信号灯和停车标志控制系统（TLSSC）与交通控制设备（TCD）的交互行为，建立了行为分类并校准了全速度差模型，发现了约90米的跟车阈值等重要实证结果。


<details>
  <summary>Details</summary>
Motivation: 虽然高级驾驶辅助系统（ADAS）对交通运营有重要影响，但其与交通控制设备（TCD）的交互行为缺乏深入的实证研究。本研究旨在填补这一空白，通过分析特斯拉TLSSC系统与TCD的交互，为未来的仿真、安全评估和系统设计提供基础。

Method: 研究设计了在不同速度限制和TCD类型下的实地实验，收集了同步的高分辨率车辆轨迹数据和驾驶员视角视频。基于这些数据，建立了TLSSC-TCD交互行为分类（停止、加速、跟车），并校准了全速度差模型（FVDM）来定量表征每种行为模式。

Result: 研究发现了一个重要的跟车阈值（约90米）。校准结果显示：停止行为对期望速度偏差和相对速度都有强烈响应；加速行为相对保守；交叉口跟车行为比标准跟车行为表现出更平滑的动态特性和更小的车头时距。

Conclusion: 本研究建立的数据库、行为定义和模型表征为未来ADAS-TCD交互逻辑的仿真、安全评估和设计提供了重要基础。数据集已在GitHub上公开，有助于推动该领域的研究进展。

Abstract: Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.

</details>


### [161] [ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision](https://arxiv.org/abs/2512.11824)
*Rosh Ho,Jian Zhang*

Main category: cs.RO

TL;DR: ReGlove系统将低成本商用气动康复手套改造为视觉引导的辅助矫形器，通过手腕摄像头和边缘计算实现无需可靠肌肉信号的上下文感知抓握，成本低于250美元。


<details>
  <summary>Details</summary>
Motivation: 慢性上肢功能障碍影响全球数百万人，但现有辅助技术要么价格昂贵，要么依赖不可靠的生物信号，需要更经济、可靠的替代方案。

Method: 集成手腕摄像头与边缘计算推理引擎（Raspberry Pi 5），采用实时YOLO计算机视觉模型实现上下文感知抓握，将商用气动康复手套改造为视觉引导辅助矫形器。

Result: 达到96.73%的抓握分类准确率，端到端延迟低于40毫秒；在YCB物体操作基准测试中成功率达82.71%，在27项日常生活活动任务中表现可靠。

Conclusion: ReGlove为基于视觉的上肢辅助提供了技术基础，成本低于250美元且全部使用商用组件，可使被传统EMG控制设备排除在外的人群受益。

Abstract: This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \SI{96.73}{\percent} grasp classification accuracy with sub-\SI{40.00}{\milli\second} end-to-end latency. Physical validation using standardized benchmarks shows \SI{82.71}{\percent} success on YCB object manipulation and reliable performance across \SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \$\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.

</details>


### [162] [WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2512.11872)
*Mingwang Xu,Jiahao Cui,Feipeng Cai,Hanlin Shang,Zhihao Zhu,Shan Luan,Yifang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

TL;DR: WAM-Diff是一个基于掩码扩散的视觉-语言-动作自动驾驶框架，使用离散序列迭代生成未来轨迹，在NAVSIM基准上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统主要使用自回归大语言模型和连续扩散策略，而离散掩码扩散在轨迹生成方面的潜力尚未充分探索。

Method: 提出WAM-Diff框架：1）将掩码扩散系统性地适配到自动驾驶，支持灵活的非因果解码顺序；2）通过稀疏MoE架构联合训练运动预测和驾驶导向的视觉问答任务；3）使用组序列策略优化进行在线强化学习以优化序列级驾驶奖励。

Result: 在NAVSIM-v1上获得91.0 PDMS分数，在NAVSIM-v2上获得89.7 EPDMS分数，证明了掩码扩散在自动驾驶中的有效性。

Conclusion: 该方法为自回归和基于扩散的策略提供了有前景的替代方案，支持场景感知的解码策略进行轨迹生成。

Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff

</details>


### [163] [Traversability Aware Autonomous Navigation for Multi-Modal Mobility Morphobot (M4)](https://arxiv.org/abs/2512.11876)
*Hrigved Mahesh Suryawanshi*

Main category: cs.RO

TL;DR: 该论文提出了一个基于学习的可通行性感知导航框架，用于M4机器人平台在非结构化环境中的自主导航，通过LiDAR生成高程地图，CNN评估地形可通行性，并集成到自定义A*路径规划器中，实现安全与能效的平衡。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人需要实时评估地形难度并规划路径，以平衡效率与安全性。现有导航方法往往忽视地形可通行性，导致机器人可能陷入困难地形或消耗过多能量。

Method: 1. 使用FAST-LIO进行实时定位；2. 从LiDAR点云生成2.5D高程地图；3. 基于CNN的模型处理高程地图估计可通行性分数；4. 将可通行性分数转换为导航成本；5. 自定义A*规划器结合几何距离、能量消耗和地形成本进行路径规划；6. 前期进行了LiDAR与相机SLAM的对比研究，验证了LiDAR的厘米级精度优势。

Result: 实验结果表明：1. 系统成功避免了低可通行性区域；2. 接受少量路径长度增加以显著降低地形成本；3. LiDAR相比相机SLAM具有显著更高的几何精度（厘米级）；4. 完整管道集成了FAST-LIO定位、GPU加速高程映射、CNN可通行性估计和Nav2导航。

Conclusion: 该工作建立了一个智能地形感知导航的基础框架，适用于多模态机器人平台，通过平衡路径长度与地形质量，实现了更安全、更节能的自主导航。

Abstract: Autonomous navigation in unstructured environments requires robots to assess terrain difficulty in real-time and plan paths that balance efficiency with safety. This thesis presents a traversability-aware navigation framework for the M4 robot platform that uses learned terrain analy- sis to generate energy-efficient paths avoiding difficult terrain.Our approach uses FAST-LIO for real- time localization, generating 2.5D elevation maps from LiDAR point clouds. A CNN-based model processes these elevation maps to estimate traversability scores, which are converted into navigation costs for path planning. A custom A* planner incorporates these costs alongside geometric distance and energy consumption to find paths that trade modest distance increases for substantial terrain quality improvements. Before system development, a platform-agnostic study compared LiDAR- based and camera-based SLAM using OptiTrack ground truth. Point cloud comparison through ICP alignment and cloud-to-mesh distance analysis demonstrated that LiDAR-based mapping achieves centimeter-level precision essential for elevation mapping, while camera-based approaches exhib- ited significantly higher geometric error. These findings directly resulted in the selection of LiDAR as the primary sensor to generate elevation maps. The complete pipeline integrates FAST-LIO localization, GPU-accelerated elevation mapping, CNN-based traversability estimation, and Nav2 navigation with a custom traversability-aware planner. Experimental results demonstrate that the system successfully avoids low traversability regions and accepts a few longer paths to achieve a reduction in terrain cost. This work establishes a foundation for intelligent terrain-aware navigation applicable to multi-modal robotic platforms.

</details>


### [164] [Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control](https://arxiv.org/abs/2512.11886)
*Mohammed Irfan Ali*

Main category: cs.RO

TL;DR: 该论文为COBRA蛇形机器人开发了完整的自主导航系统，集成了视觉-惯性SLAM、降阶状态估计和闭环轨迹跟踪，实现了自主航点导航。


<details>
  <summary>Details</summary>
Motivation: 蛇形机器人在极端地形中具有卓越的机动性，但高度关节化的身体在没有外部跟踪基础设施的环境中面临自主导航的根本挑战。先前工作完全依赖开环遥操作，需要开发完整的自主导航系统。

Method: 开发了完整的自主导航流水线，包括：1）深度相机与边缘计算实现实时定位；2）降阶框架估计质心位姿；3）闭环控制器通过距离依赖的偏航误差混合调制CPG步态参数；4）验证系统对抗运动捕捉地面真值。

Result: 物理实验验证了完整系统，展示了精确的多航点跟踪能力，为蛇形机器人自主导航奠定了基础。系统能够表征蛇形机器人平台特有的漂移行为和故障模式。

Conclusion: 该论文成功开发了蛇形机器人COBRA的完整自主导航系统，实现了自主航点导航，解决了蛇形机器人在无外部跟踪环境中的自主导航挑战，为行星探索应用奠定了基础。

Abstract: Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure. This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration. While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation. This approach integrates onboard visual-inertial SLAM, reduced-order state estimation, and closed-loop trajectory tracking to enable autonomous waypoint navigation. A depth camera paired with edge computing performs real-time localization during dynamic locomotion, validated against motion-capture ground truth to characterize drift behavior and failure modes unique to snake robot platforms. A reduced-order framework estimates Center-of-Mass pose, driving a closed-loop controller that modulates CPG gait parameters through distance-dependent yaw error blending. Physical experiments validate the complete system, demonstrating accurate multi-waypoint tracking and establishing foundations for autonomous snake robot navigation.

</details>


### [165] [VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer](https://arxiv.org/abs/2512.11891)
*Songqiao Hu,Zeyi Liu,Shuang Liu,Jun Cen,Zihan Meng,Xiao He*

Main category: cs.RO

TL;DR: 提出AEGIS架构，在现有视觉-语言-动作模型基础上添加可插拔的安全约束层，通过控制屏障函数提供理论安全保证，同时保持原有指令跟随性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在机器人操作任务中表现出色，但在非结构化环境中部署时面临挑战，需要同时保证任务合规性和安全性，特别是在防止物理交互中的潜在碰撞方面。

Method: 提出VLSA架构AEGIS，包含基于控制屏障函数构建的可插拔安全约束层，直接集成到现有VLA模型中，在保持原有性能的同时提升安全性。

Result: 在构建的SafeLIBERO安全关键基准测试中，AEGIS相比最先进基线方法，障碍物避免率提升59.16%，任务执行成功率提升17.25%。

Conclusion: AEGIS架构通过理论保证的安全约束层有效提升了VLA模型在非结构化环境中的安全性，同时保持了原有的指令跟随性能，为安全机器人操作提供了可行方案。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.

</details>


### [166] [Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics](https://arxiv.org/abs/2512.11903)
*Iacopo Catalano,Eduardo Montijano,Javier Civera,Julio A. Placed,Jorge Pena-Queralta*

Main category: cs.RO

TL;DR: Aion框架将时间流动态嵌入到分层3D场景图中，结合了语义结构和运动模式，提升动态环境中的导航能力。


<details>
  <summary>Details</summary>
Motivation: 当前动态环境导航需要同时捕捉语义结构和时间演化的空间表示。现有3D场景图主要关注静态语义结构，而动态地图通常基于网格离散化，缺乏语义意识且扩展性差。

Method: Aion采用基于图的稀疏动态地图表示，捕捉任意时间间隔的运动流，并将其附加到场景图的导航节点上，将时间维度直接嵌入到分层3D场景图中。

Result: 该方法产生了更可解释和可扩展的预测，能够改善复杂动态环境中的规划和交互能力。

Conclusion: Aion框架成功地将时间流动态与语义层次结构相结合，为动态环境中的自主导航提供了更有效的空间表示。

Abstract: Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.

</details>


### [167] [Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models](https://arxiv.org/abs/2512.11908)
*Heng Zhang,Rui Dai,Gokhan Solak,Pokuang Zhou,Yu She,Arash Ajoudani*

Main category: cs.RO

TL;DR: 这篇综述系统回顾了机器人接触密集型任务中的安全学习方法，涵盖安全探索与安全执行两大领域，特别关注了视觉语言模型等基础模型带来的新安全机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 接触密集型任务存在不确定性、复杂动力学和交互损伤风险，学习控制方法虽有潜力但安全保证仍是实际部署的关键瓶颈，需要系统梳理安全学习方法。

Method: 将现有方法分为安全探索和安全执行两大领域，回顾约束强化学习、风险敏感优化、不确定性感知建模、控制屏障函数、模型预测安全防护等关键技术，特别关注VLM/VLA基础模型与安全学习的结合。

Result: 系统梳理了接触密集型任务中安全学习方法的分类框架和技术体系，分析了VLM/VLA方法带来的语言级约束规范、多模态安全信号等新机遇，以及相应的风险与评估挑战。

Conclusion: 未来需要面向复杂接触密集型环境，发展可靠、安全对齐、基础模型赋能的机器人系统，克服当前限制并探索新的研究方向。

Abstract: Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.

</details>


### [168] [Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control](https://arxiv.org/abs/2512.11921)
*Abdullah Yahya Abdullah Omaisan,Ibrahim Sheikh Mohamed*

Main category: cs.RO

TL;DR: 本文提出了一种高效的VLA模型微调方法，使数十亿参数的大模型能在消费级GPU上运行，并成功部署到低成本机器人平台完成按钮按压任务。


<details>
  <summary>Details</summary>
Motivation: 大规模VLA模型在机器人操作中表现出色，但部署到低成本机器人平台面临计算资源限制和新机器人本体适配的挑战。需要解决如何在有限计算资源下高效微调预训练模型以适应新机器人平台的问题。

Method: 提出基于低秩适配（LoRA）和量化技术的高效微调策略，支持31亿参数VLA模型在8GB显存的消费级GPU上运行。重点研究了冻结与解冻视觉编码器的权衡，并在SO101机械臂上部署按钮按压任务，仅使用200个演示片段进行训练。

Result: 方法在保持计算效率的同时实现了有效的操作性能。详细分析了部署挑战、失败模式以及训练数据量与实际性能的关系。结果表明，通过适当的微调方法，VLA模型可以成功部署到经济型机器人平台。

Conclusion: 通过高效的微调方法，VLA模型能够部署到低成本机器人平台，使先进的机器人操作能力不再局限于昂贵的研究机器人，提高了机器人技术的可及性。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.

</details>


### [169] [A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach](https://arxiv.org/abs/2512.11944)
*Jia Hu,Yang Chang,Haoran Wang*

Main category: cs.RO

TL;DR: 论文提出数据驱动的最优控制范式作为统一框架，将经典控制的可验证结构与机器学习的自适应能力相结合，解决自动驾驶运动规划中透明性与适应性之间的根本矛盾。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶运动规划面临根本性权衡：传统管道方法透明但脆弱，现代学习系统自适应但不透明（黑箱）。这种持久困境阻碍了真正可信系统的开发，需要解决这一僵局。

Method: 通过全面回顾基于学习的运动规划方法，提出数据驱动的最优控制范式作为统一框架，将经典控制的可验证结构与机器学习的自适应能力相结合，利用真实世界数据持续优化系统动力学、成本函数和安全约束等关键组件。

Result: 该框架具备实现三大关键下一代能力的潜力：1）"以人为中心"的定制化；2）"平台自适应"的动态适应；3）通过自调优实现"系统自优化"。

Conclusion: 提出基于这一范式的未来研究方向，旨在开发同时具备安全性、可解释性和类人自主能力的智能交通系统。

Abstract: Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, "black-box" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: "Human-Centric" customization, "Platform-Adaptive" dynamics adaptation, and "System Self-Optimization" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.

</details>


### [170] [Modified Hybrid A* Collision-Free Path-Planning for Automated Reverse Parking](https://arxiv.org/abs/2512.12021)
*Xincheng Cao,Haochong Chen,Bilin Aksun-Guvenc,Levent Guvenc*

Main category: cs.RO

TL;DR: 提出改进的Hybrid-A*路径规划算法，用于在狭窄空间中进行车辆泊车，保证运动学可行性和静态障碍物避碰


<details>
  <summary>Details</summary>
Motivation: 在狭窄空间泊车具有挑战性，因为既需要可行的路径，又要避免碰撞。现有方法难以同时保证运动学可行性和碰撞避免

Method: 1. 推导运动学单轨模型描述车辆低速运动；2. 改进Hybrid-A*算法，结合标准算法的可行性保证和静态障碍物避碰；3. 使用模型状态重建车辆中心线，结合膨胀二值占据地图实现碰撞避免

Result: 通过仿真研究和动画测试，证明所提算法能够持续提供运动学可行且无碰撞的轨迹

Conclusion: 改进的Hybrid-A*算法有效解决了狭窄空间泊车问题，同时保证了路径的运动学可行性和碰撞安全性

Abstract: Parking a vehicle in tight spaces is a challenging task to perform due to the scarcity of feasible paths that are also collision-free. This paper presents a strategy to tackle this kind of maneuver with a modified Hybrid-A* path-planning algorithm that combines the feasibility guarantee inherent in the standard Hybrid A* algorithm with the addition of static obstacle collision avoidance. A kinematic single-track model is derived to describe the low-speed motion of the vehicle, which is subsequently used as the motion model in the Hybrid A* path-planning algorithm to generate feasible motion primitive branches. The model states are also used to reconstruct the vehicle centerline, which, in conjunction with an inflated binary occupancy map, facilitates static obstacle collision avoidance functions. Simulation study and animation are set up to test the efficacy of the approach, and the proposed algorithm proves to consistently provide kinematically feasible trajectories that are also collision-free.

</details>


### [171] [A Stochastic Approach to Terrain Maps for Safe Lunar Landing](https://arxiv.org/abs/2512.12058)
*Anja Sheppard,Chris Reale,Katherine A. Skinner*

Main category: cs.RO

TL;DR: 提出一种基于高斯过程的两阶段模型，利用LRO数据生成月球南极地区的随机高程地图，考虑DEM置信度图的异方差噪声特性，为安全着陆提供更准确的地形不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 月球南极地区存在阴影区域，传统视觉危险检测方法不可靠，而LiDAR技术在该环境未经充分测试。LRO数据提供了丰富的地表信息，但现有随机高程地图方法未考虑DEM置信度图包含的关键质量信息。

Method: 采用两阶段高斯过程模型：第一阶段用次级GP从DEM置信度数据学习空间变化的噪声特性；第二阶段用主GP建模月球地形，并将异方差噪声信息作为噪声参数。使用随机变分GP实现可扩展训练。

Result: 该方法能更准确地建模异方差传感器噪声对高程地图的影响，产生更具信息量的地形不确定性估计，可用于危险检测和安全着陆点选择等下游任务。

Conclusion: 通过考虑DEM置信度图的异方差噪声特性，提出的两阶段GP模型能生成更可靠的地形不确定性估计，为月球南极高风险环境下的自主航天任务提供关键的安全着陆支持。

Abstract: Safely landing on the lunar surface is a challenging task, especially in the heavily-shadowed South Pole region where traditional vision-based hazard detection methods are not reliable. The potential existence of valuable resources at the lunar South Pole has made landing in that region a high priority for many space agencies and commercial companies. However, relying on a LiDAR for hazard detection during descent is risky, as this technology is fairly untested in the lunar environment.
  There exists a rich log of lunar surface data from the Lunar Reconnaissance Orbiter (LRO), which could be used to create informative prior maps of the surface before descent. In this work, we propose a method for generating stochastic elevation maps from LRO data using Gaussian processes (GPs), which are a powerful Bayesian framework for non-parametric modeling that produce accompanying uncertainty estimates. In high-risk environments such as autonomous spaceflight, interpretable estimates of terrain uncertainty are critical. However, no previous approaches to stochastic elevation mapping have taken LRO Digital Elevation Model (DEM) confidence maps into account, despite this data containing key information about the quality of the DEM in different areas.
  To address this gap, we introduce a two-stage GP model in which a secondary GP learns spatially varying noise characteristics from DEM confidence data. This heteroscedastic information is then used to inform the noise parameters for the primary GP, which models the lunar terrain. Additionally, we use stochastic variational GPs to enable scalable training. By leveraging GPs, we are able to more accurately model the impact of heteroscedastic sensor noise on the resulting elevation map. As a result, our method produces more informative terrain uncertainty, which can be used for downstream tasks such as hazard detection and safe landing site selection.

</details>


### [172] [B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping](https://arxiv.org/abs/2512.12194)
*Min-Won Seo,Aamodh Suresh,Carlos Nieto-Granda,Solmaz S. Kia*

Main category: cs.RO

TL;DR: B-ActiveSEAL：一个可扩展的信息论主动探索框架，通过行为熵平衡定位与建图耦合不确定性，实现自适应探索-利用权衡


<details>
  <summary>Details</summary>
Motivation: 传统主动机器人探索在长期大规模环境中处理定位与建图耦合不确定性时面临计算复杂度爆炸问题，需要可扩展的决策框架来管理这些相互依赖的不确定性

Method: 提出B-ActiveSEAL框架：1）自适应平衡建图不确定性（探索）和定位不确定性（利用）；2）支持广义熵度量类；3）建立行为熵作为耦合不确定性下主动探索的有效信息度量

Result: 通过理论分析和在开源地图及ROS-Unity仿真中的实验验证，B-ActiveSEAL在多样复杂环境中实现了良好的探索-利用权衡，产生多样化自适应探索行为，优于代表性基线方法

Conclusion: B-ActiveSEAL为紧密耦合的定位-建图不确定性下的主动探索提供了理论基础和实用框架，通过行为熵实现直观自适应决策，解决了大规模环境中计算可扩展性问题

Abstract: Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.

</details>


### [173] [Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion](https://arxiv.org/abs/2512.12203)
*Eric J. Elias,Michael Esswein,Jonathan P. How,David W. Miller*

Main category: cs.RO

TL;DR: 该研究通过像素级融合可见光和热红外图像，提升在轨操作中未知空间目标的导航精度，相比单一传感器显著改善SLAM性能。


<details>
  <summary>Details</summary>
Motivation: 随着在轨操作需求增长，需要精确导航未知空间目标。传统相机在阴影期性能受限，激光雷达虽不受光照影响但笨重耗电，热红外相机能在困难光照条件下工作但分辨率较低。需要结合可见光和热红外相机的优势来提升导航性能。

Method: 在低地球轨道上对目标卫星进行可见光和热红外波段的光真实感模拟，使用像素级融合方法创建可见光/热红外复合图像，通过单目SLAM算法比较不同光照条件和轨迹下的导航误差。

Result: 融合图像相比仅使用可见光或仅使用热红外的方法，导航性能得到显著提升。

Conclusion: 可见光与热红外图像的像素级融合能够有效结合两种传感器的优势，为在轨操作中的未知空间目标导航提供更可靠的解决方案。

Abstract: As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.

</details>


### [174] [Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving](https://arxiv.org/abs/2512.12211)
*Longchao Da,David Isele,Hua Wei,Manish Saroya*

Main category: cs.RO

TL;DR: 提出一个自适应评估轨迹预测器性能的管道，通过准确性和多样性两个维度动态结合来评估预测器对自动驾驶车辆实际驾驶性能的贡献。


<details>
  <summary>Details</summary>
Motivation: 当前轨迹预测评估主要依赖误差指标（如ADE、FDE），这些指标只关注事后准确性，但忽略了预测器对自动驾驶车辆在复杂交互场景中的实际影响。高质量的预测器不仅需要准确性，还应捕捉邻居车辆所有可能的移动方向，以支持自动驾驶车辆的谨慎决策。

Method: 提出一个全面的评估管道，通过两个维度自适应评估预测器性能：准确性和多样性。基于驾驶场景的关键性，这两个维度被动态结合，产生预测器性能的最终评分。在闭环基准测试中使用真实世界数据集进行广泛实验。

Result: 实验表明，该评估管道比传统指标产生更合理的评估，能更好地反映预测器评估与自动驾驶车辆驾驶性能之间的相关性。该管道为选择最能提升自动驾驶车辆驾驶性能的预测器提供了稳健的方法。

Conclusion: 提出的评估管道通过综合考虑准确性和多样性，并根据场景关键性动态调整权重，能够更全面地评估轨迹预测器对自动驾驶车辆实际驾驶性能的贡献，为预测器选择提供了更合理的标准。

Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.

</details>


### [175] [Semantic Zone based 3D Map Management for Mobile Robot](https://arxiv.org/abs/2512.12228)
*Huichang Yun,Seungho Yoo*

Main category: cs.RO

TL;DR: 提出基于语义分区的3D地图管理方法，将环境划分为有意义的空间单元（如大厅、走廊），作为内存管理的基本单位，动态加载任务相关区域到工作内存，卸载非活跃区域到长期内存，实现稳定的内存使用。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在大规模室内环境中需要精确的3D空间表示，但3D地图占用大量内存，难以在有限计算资源中维护完整地图数据。现有SLAM框架通常依赖几何距离或时间指标进行内存管理，在空间分隔环境中数据检索效率低下。

Method: 提出语义分区为基础的3D地图管理方法，将环境划分为有意义的空间单元（语义区域），并将这些区域作为内存管理的基本单位。系统动态加载任务相关区域到工作内存，卸载非活跃区域到长期内存，严格强制执行用户定义的内存阈值。该方法在RTAB-Map框架中实现。

Result: 与标准方法相比，该方法显著减少了不必要的签名加载/卸载周期和累积内存使用。语义分区管理确保了稳定、可预测的内存使用，同时保持导航所需的地图可用性。

Conclusion: 语义分区管理方法将地图管理范式从几何中心转向语义中心控制，有效解决了大规模室内环境中3D地图内存管理问题，实现了稳定的内存使用和高效的数据检索。

Abstract: Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment

</details>


### [176] [Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy](https://arxiv.org/abs/2512.12230)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 提出一个统一的深度强化学习策略，能够使7种不同形态的人形机器人从跌倒状态恢复，无需针对每个机器人单独训练


<details>
  <summary>Details</summary>
Motivation: 在RoboCup等动态环境中，人形机器人的跌倒恢复能力至关重要。现有方法需要为每种机器人形态训练单独的策略，这限制了方法的通用性和实用性

Method: 使用CrossQ训练统一的DRL策略，涵盖7种不同高度、重量和动力学特性的人形机器人。通过留一法实验、形态缩放分析和多样性消融研究验证方法

Result: 统一策略在未见过的机器人形态上实现零样本迁移，成功率高达86±7%（95%置信区间[81,89]）。在某些情况下，共享策略甚至超越了专门训练的基线方法

Conclusion: 研究表明有针对性的形态覆盖可以改善零样本泛化能力，证明了形态无关控制的实用性，为人形机器人的通用控制奠定了基础

Abstract: Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup

</details>


### [177] [Robust Underwater Localization of Buoyancy Driven microFloats Using Acoustic Time-of-Flight Measurements](https://arxiv.org/abs/2512.12233)
*Murad Mehrab Abrar,Trevor W. Harrison*

Main category: cs.RO

TL;DR: 本文提出了一种用于沿海水域浮标驱动微型浮标系统的低成本、鲁棒性水下定位方法，通过双向声学飞行时间测量框架提高测量数量，结合非线性三边测量和基于几何成本与CRLB的滤波技术，在实地部署中实现了低于4米的定位误差。


<details>
  <summary>Details</summary>
Motivation: 廉价自主平台需要高频位置更新，但精确的水下定位仍然是一个挑战。现有方法在沿海水域中面临多径效应和声学误差等问题，需要提高定位频率和鲁棒性。

Method: 提出了双向声学飞行时间定位框架，结合浮标到浮标和浮标到浮标的双向传输增加可用测量数量。采用非线性三边测量方法，并基于几何成本和克拉美-罗下界对计算的位置估计进行滤波，以消除多径效应和声学误差引起的异常值。

Result: 在华盛顿普吉特海湾的两次实地部署中验证了该框架。定位管道实现了相对于GPS位置的中位数定位误差低于4米。滤波技术将平均误差从139.29米降低到12.07米，并改善了轨迹与GPS路径的对齐。还展示了未回收浮标的到达时间差定位。

Conclusion: 该工作通过精心设计的算法提高了基于距离的声学定位技术的实用性和鲁棒性，为沿海水域中的低成本自主平台提供了有效的定位解决方案，无需依赖重型平滑技术。

Abstract: Accurate underwater localization remains a challenge for inexpensive autonomous platforms that require highfrequency position updates. In this paper, we present a robust, low-cost localization pipeline for buoyancy-driven microFloats operating in coastal waters. We build upon previous work by introducing a bidirectional acoustic Time-of-Flight (ToF) localization framework, which incorporates both float-to-buoy and buoy-to-float transmissions, thereby increasing the number of usable measurements. The method integrates nonlinear trilateration with a filtering of computed position estimates based on geometric cost and Cramer-Rao Lower Bounds (CRLB). This approach removes outliers caused by multipath effects and other acoustic errors from the ToF estimation and improves localization robustness without relying on heavy smoothing. We validate the framework in two field deployments in Puget Sound, Washington, USA. The localization pipeline achieves median positioning errors below 4 m relative to GPS positions. The filtering technique shows a reduction in mean error from 139.29 m to 12.07 m, and improved alignment of trajectories with GPS paths. Additionally, we demonstrate a Time-Difference-of-Arrival (TDoA) localization for unrecovered floats that were transmitting during the experiment. Range-based acoustic localization techniques are widely used and generally agnostic to hardware-this work aims to maximize their utility by improving positioning frequency and robustness through careful algorithmic design.

</details>


### [178] [CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement](https://arxiv.org/abs/2512.12243)
*HT To,S Nguyen,NH Pham*

Main category: cs.RO

TL;DR: CAR-CHASE通过冲突感知启发式缓存和自适应混合启发式方法，显著提升了类车机器人多智能体路径规划的计算效率，在保持最优解的同时实现了2.46倍的平均加速。


<details>
  <summary>Details</summary>
Motivation: 类车机器人的多智能体路径规划面临计算挑战，传统启发式缓存方法在CBS中失效，因为约束使搜索空间具有上下文依赖性，需要更智能的启发式计算方法。

Method: 提出CAR-CHASE方法：1）冲突感知启发式缓存，基于状态和相关约束上下文缓存启发式值；2）自适应混合启发式，智能切换快速近似和精确计算；3）冲突指纹编码约束影响；4）空间、时间和几何相关性过滤；5）具有理论质量界的自适应切换策略。

Result: 在480个基准实例上测试，几何平均加速2.46倍，成功率从77.9%提升到84.8%（+6.9个百分点），总运行时间减少70.1%，解决了33个之前超时的实例，在30智能体障碍场景中最高达到4.06倍加速。

Conclusion: CAR-CHASE有效解决了类车机器人MAPF中的计算瓶颈，通过冲突感知缓存和自适应启发式显著提升性能，且方法具有通用性，可应用于其他CBS变体。

Abstract: Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\% and 50\%) demonstrates a geometric mean speedup of 2.46$\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\% to 84.8\% (+6.9 percentage points), reduce total runtime by 70.1\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.

</details>


### [179] [Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy](https://arxiv.org/abs/2512.12320)
*Canqi Meng,Weibang Bai*

Main category: cs.RO

TL;DR: 提出一种通过在多孔泡沫体上切割特定图案实现可编程变形的软多孔致动器设计方法，利用真空驱动实现弯曲、倾斜和扭转等多种运动模式。


<details>
  <summary>Details</summary>
Motivation: 传统软气动致动器通常基于中空弹性体腔室，存在结构支撑弱、需要针对不同功能进行昂贵几何重新设计的问题。多孔材料如泡沫可以提供结构稳定性，但如何通过定制多孔体本身实现可编程变形仍缺乏研究。

Method: 在多孔泡沫体上切割特定图案（横向、纵向、对角线），通过引入局部结构各向异性，在全局真空输入下引导材料变形。建立有限元分析计算模型研究切口图案方法的机理，并通过实验验证不同图案阵列数量N的最优设计。

Result: 实验表明，通过优化图案阵列数量N，致动器可实现弯曲达80°（N=2）、倾斜18°（N=1）和扭转115°（N=8）。方法具有图案可转移性、可扩展性和无需模具的快速原型制作能力。通过将人手褶皱图转化为功能切口图案，创建了能够进行类人自适应抓取的仿生软机器人手。

Conclusion: 该工作为多功能软多孔机器人的设计提供了一种新颖、高效且可扩展的范式，通过在多孔泡沫体上切割图案实现可编程变形，克服了传统软气动致动器的局限性。

Abstract: Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\circ}$ (N=2), tilting of $18^{\circ}$ (N=1), and twisting of $115^{\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.

</details>


### [180] [INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset](https://arxiv.org/abs/2512.12377)
*Haichuan Li,Changda Tian,Panos Trahanias,Tomi Westerlund*

Main category: cs.RO

TL;DR: INDOOR-LIDAR是一个综合性的室内3D LiDAR点云混合数据集，结合了模拟环境和真实世界扫描数据，旨在推动机器人感知研究。


<details>
  <summary>Details</summary>
Motivation: 现有室内LiDAR数据集存在规模有限、标注格式不一致、数据收集过程中人为引入的变异性等问题，需要更全面、一致的数据集来推动研究。

Method: 通过整合模拟环境和自主地面机器人采集的真实世界扫描数据，提供一致的覆盖范围和受控变化下的真实传感器行为。数据集包含密集点云数据、强度测量和KITTI风格标注。

Result: INDOOR-LIDAR数据集包含模拟子集（可灵活配置布局、点密度和遮挡）和真实世界子集（捕获真实传感器噪声、杂乱和特定领域伪影），支持多种室内物体类别。

Conclusion: INDOOR-LIDAR通过弥合合成数据与真实世界数据之间的差距，为复杂室内环境中的机器人感知研究建立了可扩展、真实且可复现的基准。

Abstract: We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.

</details>


### [181] [Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models](https://arxiv.org/abs/2512.12427)
*Rudolf Reiter,Chao Qin,Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: Unique提出了一种统一MPC框架，通过级联不同精度的模型实现即时反应性和长时域规划：短时域使用高精度模型进行精确控制，长时域使用低精度模型进行规划。


<details>
  <summary>Details</summary>
Motivation: 四旋翼无人机任务需要同时具备即时反应性和长时域规划能力。高精度模型控制准确但计算成本高，不适合长时域规划；低精度规划器可扩展但闭环性能下降。需要一种统一方法解决这一矛盾。

Method: 1. 在单一优化中级联不同精度模型：短时域高精度模型用于精确控制，长时域低精度模型用于规划；2. 跨时域对齐成本函数；3. 为点质量模型推导保持可行性的推力和体速率约束；4. 引入匹配不同状态、推力诱导加速度和急动-体速率关系的过渡约束；5. 提出3D渐进平滑调度防止非平滑障碍物导致的局部最小值；6. 部署并行随机初始化MPC求解器在长时域低精度模型上发现更低成本的局部最小值。

Result: 在相同计算预算下，Unique相比标准MPC和分层规划器-跟踪器基线，将闭环位置或速度跟踪性能提升高达75%。消融研究和帕累托分析证实了该方法在时域变化、约束近似和平滑调度方面的稳健增益。

Conclusion: Unique通过统一MPC框架成功解决了四旋翼无人机任务中即时反应性和长时域规划的矛盾，在保持计算效率的同时显著提升了闭环性能，为复杂空中任务提供了有效的解决方案。

Abstract: Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.

</details>


### [182] [Sim2Real Reinforcement Learning for Soccer skills](https://arxiv.org/abs/2512.12437)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 该论文提出了一种更高效的人形机器人强化学习控制方法，使用课程训练和对抗运动先验技术，在模拟中实现了更好的动态运动性能，但未能成功迁移到真实世界。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在人形机器人控制任务中存在局限性，难以适应真实环境、处理复杂性和生成自然运动。需要开发更有效的方法来克服这些挑战。

Method: 采用课程训练策略和对抗运动先验（AMP）技术，通过渐进式学习和运动先验约束来训练强化学习策略，专注于踢球、行走和跳跃等控制任务。

Result: 在模拟环境中，开发的强化学习策略表现出更好的动态性、适应性和性能，超越了先前的方法。但将学习到的策略从模拟迁移到真实世界时未能成功。

Conclusion: 虽然提出的方法在模拟环境中取得了显著改进，但模拟到真实的迁移失败揭示了当前强化学习方法在完全适应真实世界场景方面的根本局限性，需要进一步研究解决这一挑战。

Abstract: This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.

</details>


### [183] [Autonomously Unweaving Multiple Cables Using Visual Feedback](https://arxiv.org/abs/2512.12468)
*Tina Tian,Xinyu Wang,Andrew L. Orekhov,Fujun Ruan,Lu Li,Oliver Kroemer,Howie Choset*

Main category: cs.RO

TL;DR: 提出一种基于视觉反馈的多电缆解缠方法，将电缆解缠建模为抓取放置问题，使用图表示电缆状态，通过状态转移模型预测未来状态并选择动作，实验平均成功率84%


<details>
  <summary>Details</summary>
Motivation: 电缆管理任务中，多根电缆相互缠绕的解缠问题具有挑战性。先前工作主要关注单根电缆的解结，而本文聚焦于多电缆解缠这一子任务，即分离多根相互交织的电缆以便后续操作。

Method: 1) 将电缆解缠建模为抓取放置问题；2) 使用基于图的电缆状态表示，从视觉图像编码拓扑和几何信息；3) 提出考虑电缆在操作中伸直和弯曲的状态转移模型；4) 使用该模型选择两种高层动作基元并计算预测即时成本以优化底层动作；5) 迭代感知-规划-行动过程。

Result: 实验证明该方法能够解缠电源线和鞋带，平均成功率达到84%。

Conclusion: 提出的基于视觉反馈的多电缆解缠方法有效，通过图表示和状态转移模型实现了对电缆变形行为的建模，迭代感知-规划-行动过程能够成功分离相互缠绕的电缆。

Abstract: Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.

</details>


### [184] [Making Robots Play by the Rules: The ROS 2 CLIPS-Executive](https://arxiv.org/abs/2512.12722)
*Tarik Viehmann,Daniel Swoboda,Samridhi Kalra,Himanshu Grover,Gerhard Lakemeyer*

Main category: cs.RO

TL;DR: 将CLIPS规则编程语言集成到ROS机器人操作系统生态系统中，并展示了与PDDL规划框架的集成


<details>
  <summary>Details</summary>
Motivation: CLIPS作为一种基于规则的知识驱动编程语言，非常适合协调自主机器人的复杂任务。受最初为Fawkes机器人框架开发的CLIPS-Executive启发，需要将其集成到更广泛使用的ROS生态系统中

Method: 将CLIPS集成到ROS生态系统，并描述了一个基于PDDL的规划框架集成方法，展示了CLIPS的灵活性

Result: 成功实现了CLIPS在ROS生态系统中的集成，并展示了与PDDL规划框架的集成能力

Conclusion: CLIPS可以有效地集成到ROS生态系统中，为机器人协调提供知识驱动的规则编程能力，同时展示了与规划框架的良好兼容性

Abstract: CLIPS is a rule-based programming language for building knowledge-driven applications, well suited for the complex task of coordinating autonomous robots. Inspired by the CLIPS-Executive originally developed for the lesser known Fawkes robotics framework, we present an Integration of CLIPS into the ROS ecosystem. Additionally, we show the flexibility of CLIPS by describing a PDDL-based planning framework integration.

</details>


### [185] [VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps](https://arxiv.org/abs/2512.12793)
*Mizuho Aoki,Kohei Honda,Yasuhiro Yoshimura,Takeshi Ishita,Ryo Yonetani*

Main category: cs.RO

TL;DR: VLG-Loc是一种基于视觉语言模型的全局定位方法，使用仅包含视觉地标名称和区域的标注足迹地图，通过多方向图像观测搜索地图中的地标，并在蒙特卡洛定位框架中评估位姿假设。


<details>
  <summary>Details</summary>
Motivation: 人类能够使用仅包含地标名称和区域的简单地图进行定位，但将这种能力移植到机器人系统面临挑战，因为缺乏几何和外观细节时难以建立观测地标与地图地标之间的对应关系。

Method: 使用视觉语言模型在机器人的多方向图像观测中搜索地图中标注的地标，然后在蒙特卡洛定位框架中，利用找到的地标评估每个位姿假设的似然度。

Result: 在模拟和真实零售环境中的实验验证表明，该方法相比现有的基于扫描的方法具有更好的鲁棒性，特别是在环境变化的情况下。通过视觉和扫描定位的概率融合进一步提升了性能。

Conclusion: VLG-Loc成功实现了使用人类可读的简单地图进行机器人全局定位，通过视觉语言模型解决地标对应问题，在环境变化下表现出优越的鲁棒性，为机器人定位提供了新思路。

Abstract: This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.

</details>


### [186] [SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding](https://arxiv.org/abs/2512.12842)
*Kuan Fang,Yuxin Chen,Xinghao Zhu,Farzad Niroui,Lingfeng Sun,Jiuguang Wang*

Main category: cs.RO

TL;DR: SAGA是一个用于视觉运动控制的通用自适应框架，通过将高级语义意图与低级视觉运动控制解耦，使用基于可供性的任务表示，并利用多模态基础模型将任务表示映射到3D可供性热图，从而实现跨环境、任务目标和用户规范的泛化。


<details>
  <summary>Details</summary>
Motivation: 为了解决视觉运动控制在各种环境、任务目标和用户规范之间的泛化问题，需要一种能够解耦高级语义意图与低级视觉运动控制的框架，以有效学习通用能力。

Method: 使用基于可供性的任务表示来表达多样复杂行为；利用多模态基础模型将任务表示映射到机器人视觉观察中的3D可供性热图；基于这些接地的可供性训练条件策略进行全身控制。

Result: 在四足机械臂上实现了SAGA，并在11个真实世界任务中进行了广泛实验。SAGA在零样本执行和少样本适应方面表现优异，始终大幅优于端到端和模块化基线方法。

Conclusion: 结构化可供性接地方案为通用移动操作提供了一个可扩展且有效的途径，能够处理语言指令、选定点和示例演示等多种任务指定形式。

Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.

</details>


### [187] [MPC-Guided Safe Reinforcement Learning and Lipschitz-Based Filtering for Structured Nonlinear Systems](https://arxiv.org/abs/2512.12855)
*Patrick Kostelac,Xuerui Wang,Anahita Jamshidnejad*

Main category: cs.RO

TL;DR: 提出了一种结合模型预测控制（MPC）和强化学习（RL）的集成框架，将MPC的稳定性、安全性保证与RL的适应性相结合，用于具有非线性动力学和不确定环境的工程系统控制。


<details>
  <summary>Details</summary>
Motivation: 现代工程系统（如自动驾驶车辆、柔性机器人、智能航空航天平台）需要能够应对不确定性、适应环境变化并在实时约束下保持安全性的控制器。RL具有强大的数据驱动适应性，但缺乏动态约束满足机制；MPC具有结构化约束处理和鲁棒性，但依赖精确模型且在线计算量大。

Method: 提出集成MPC-RL框架：训练阶段，MPC定义安全控制边界来指导RL组件，实现约束感知的策略学习；部署阶段，学习到的策略实时运行，并基于Lipschitz连续性的轻量级安全滤波器确保约束满足，无需繁重的在线优化。

Result: 在非线性气动弹性机翼系统上验证，展示了改进的扰动抑制、减少的执行器努力以及在湍流下的鲁棒性能。该架构可推广到其他具有结构化非线性和有界扰动的领域。

Conclusion: 该框架为工程应用中的安全人工智能驱动控制提供了可扩展的解决方案，结合了MPC的安全保证和RL的适应性优势，实现了实时安全控制。

Abstract: Modern engineering systems, such as autonomous vehicles, flexible robotics, and intelligent aerospace platforms, require controllers that are robust to uncertainties, adaptive to environmental changes, and safety-aware under real-time constraints. RL offers powerful data-driven adaptability for systems with nonlinear dynamics that interact with uncertain environments. RL, however, lacks built-in mechanisms for dynamic constraint satisfaction during exploration. MPC offers structured constraint handling and robustness, but its reliance on accurate models and computationally demanding online optimization may pose significant challenges. This paper proposes an integrated MPC-RL framework that combines stability and safety guarantees of MPC with the adaptability of RL. During training, MPC defines safe control bounds that guide the RL component and that enable constraint-aware policy learning. At deployment, the learned policy operates in real time with a lightweight safety filter based on Lipschitz continuity to ensure constraint satisfaction without heavy online optimizations. The approach, which is validated on a nonlinear aeroelastic wing system, demonstrates improved disturbance rejection, reduced actuator effort, and robust performance under turbulence. The architecture generalizes to other domains with structured nonlinearities and bounded disturbances, offering a scalable solution for safe artificial-intelligence-driven control in engineering applications.

</details>


### [188] [SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework](https://arxiv.org/abs/2512.12945)
*Anja Sheppard,Parker Ewen,Joey Wilson,Advaith V. Sethuraman,Benard Adewole,Anran Li,Yuzhen Chen,Ram Vasudevan,Katherine A. Skinner*

Main category: cs.RO

TL;DR: SLIM-VDB是一个轻量级语义建图系统，利用OpenVDB数据结构，通过贝叶斯更新框架同时支持封闭集和开放集语义融合，显著降低内存使用和建图时间。


<details>
  <summary>Details</summary>
Motivation: 现有语义建图系统存在两个主要问题：1) 虽然OpenVDB在计算机图形学中已证明能显著提高体素场景表示的效率和内存使用，但在机器人语义建图中尚未应用；2) 现有系统缺乏同时集成固定类别和开放语言标签预测的统一框架。

Method: 提出SLIM-VDB系统，利用OpenVDB数据结构进行3D语义建图，并设计统一的贝叶斯更新框架，能够同时处理封闭集（固定类别）和开放集（开放语言标签）的语义融合。

Result: 与当前最先进的语义建图方法相比，SLIM-VDB在保持可比建图精度的同时，显著减少了内存使用和建图时间。系统提供开源C++代码库和Python接口。

Conclusion: SLIM-VDB成功地将OpenVDB数据结构应用于语义建图领域，提供了一个高效、轻量级的解决方案，能够统一处理封闭集和开放集语义信息，为场景理解提供了更灵活的工具。

Abstract: This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.

</details>


### [189] [Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations](https://arxiv.org/abs/2512.12993)
*Guillermo A. Castillo,Himanshu Lodha,Ayonga Hereid*

Main category: cs.RO

TL;DR: 提出了一种用于地形感知双足运动的分层策略，通过降维感知表示增强强化学习高层策略，提高实时步态生成能力。


<details>
  <summary>Details</summary>
Motivation: 传统端到端方法在复杂地形上的双足运动控制存在效率低、鲁棒性差的问题，需要更高效的感知表示和决策框架来提升硬件部署的可行性。

Method: 采用分层策略：1) 使用CNN-VAE提取地形潜在编码；2) 结合降阶机器人动力学构建紧凑状态；3) 引入历史感知机制，整合近期地形观测序列；4) 提出蒸馏方法直接从深度相机图像学习潜在表示；5) 在高保真AR模拟器中验证，包含传感器噪声、状态估计和驱动器动态。

Result: 系统分析了潜在空间维度对学习效率和策略鲁棒性的影响，验证了方法的鲁棒性和适应性，通过模拟与真实传感器数据对比提供了初步硬件验证，确认了硬件部署的潜力。

Conclusion: 该分层框架通过降维感知表示显著提升了地形感知双足运动的鲁棒性和适应性，为实际硬件部署提供了可行的解决方案。

Abstract: This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.

</details>


### [190] [K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots](https://arxiv.org/abs/2512.13009)
*Oğuzhan Akbıyık,Naseem Alhousani,Fares J. Abu-Dakka*

Main category: cs.RO

TL;DR: K-VARK是一种新颖的传感器力估计方法，通过核化概率模型和自适应卡尔曼滤波器，在6自由度协作机械臂上实现了比现有方法超过20%的RMSE降低。


<details>
  <summary>Details</summary>
Motivation: 机器人安全精确地与无结构环境交互需要可靠接触力估计，但现有传感器力估计方法面临建模误差、复杂残余动力学和摩擦等挑战。

Method: 提出K-VARK方法，将关节残余扭矩的核化概率模型集成到自适应卡尔曼滤波器框架中，通过核化运动基元捕获残余扭矩的预测均值和输入相关异方差方差，并采用方差感知虚拟测量更新和在线变分贝叶斯优化适应过程噪声。

Result: 在6自由度协作机械臂上的实验验证表明，K-VARK相比最先进的传感器力估计方法实现了超过20%的均方根误差降低，能够进行鲁棒准确的外部力/扭矩估计。

Conclusion: K-VARK方法通过核化概率建模和自适应卡尔曼滤波器的结合，显著提升了传感器力估计的准确性和鲁棒性，适用于抛光、装配等高级任务。

Abstract: Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.

</details>


### [191] [Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos](https://arxiv.org/abs/2512.13080)
*Yicheng Feng,Wanpeng Zhang,Ye Wang,Hao Luo,Haoqi Yuan,Sipeng Zheng,Zongqing Lu*

Main category: cs.RO

TL;DR: 该论文提出了一种空间感知的视觉-语言-动作预训练范式，通过将2D视觉观察与3D空间推理对齐，解决现有VLA模型在3D物理环境中执行动作时的感知与动作基础之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型大多依赖2D视觉输入在3D物理环境中执行动作，导致感知与动作基础之间存在显著差距。为了解决这个问题，需要在预训练阶段建立视觉空间与物理空间之间的明确对齐。

Method: 提出空间感知VLA预训练范式，利用大规模人类演示视频提取3D视觉和3D动作标注，形成新的监督信号。具体实现为VIPA-VLA双编码器架构，包含3D视觉编码器以增强语义视觉表示与3D感知特征的结合。

Result: VIPA-VLA在下游机器人任务中实现了2D视觉与3D动作之间显著改进的基础对齐，产生了更鲁棒和可泛化的机器人策略。

Conclusion: 通过在预训练阶段建立视觉空间与物理空间的明确对齐，空间感知VLA预训练范式能够使模型在机器人策略学习之前获得3D空间理解能力，从而更好地弥合感知与动作基础之间的差距。

Abstract: Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.

</details>


### [192] [Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion](https://arxiv.org/abs/2512.13090)
*Jebeom Chae,Junwoo Chang,Seungho Yeom,Yujin Kim,Jongeun Choi*

Main category: cs.RO

TL;DR: LCHD是一个端到端的视觉框架，通过结合CLIP语义先验和碰撞避免扩散核，生成语言条件化的无碰撞轨迹，解决了多机器人规划中计算成本高、泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多机器人规划中存在局限性：计算成本高、泛化能力差、需要显式环境表示、缺乏几何可达性推理机制，且难以处理语言条件化任务。

Method: 提出语言条件化热启发扩散(LCHD)框架，集成CLIP语义先验作为碰撞避免扩散核的物理归纳偏置，使规划器能在可达工作空间内严格解释语言指令，无需推理时的显式障碍物信息。

Result: 在多样化真实世界启发的地图和真实机器人实验中，LCHD在成功率上持续优于现有扩散规划器，同时降低了规划延迟。

Conclusion: LCHD通过语义先验与物理归纳偏置的结合，有效处理分布外场景，引导机器人朝向语义意图匹配的可达替代方案，实现了高效、泛化的多机器人语言条件化轨迹规划。

Abstract: Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.

</details>


### [193] [PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations](https://arxiv.org/abs/2512.13093)
*Mingqi Yuan,Tao Yu,Haolin Song,Bo Li,Xin Jin,Hua Chen,Wenjun Zeng*

Main category: cs.RO

TL;DR: PvP框架通过本体感知与特权状态的对比学习，提升人形机器人强化学习的样本效率，无需手工数据增强


<details>
  <summary>Details</summary>
Motivation: 人形机器人全身控制需要高效鲁棒的方法，但强化学习面临样本效率低下的挑战，特别是由于人形机器人复杂的动力学和部分可观测性

Method: 提出PvP（本体感知-特权对比学习框架），利用本体感知状态与特权状态的内在互补性，学习紧凑且任务相关的潜在表示；同时开发SRL4Humanoid框架，为系统评估提供高质量实现

Result: 在LimX Oli机器人上进行速度跟踪和运动模仿任务的广泛实验表明，PvP相比基线SRL方法显著提高了样本效率和最终性能

Conclusion: PvP框架为人形机器人学习提供了数据高效的方法，研究进一步提供了将SRL与RL集成用于人形机器人全身控制的实用见解

Abstract: Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.

</details>


### [194] [Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation](https://arxiv.org/abs/2512.13094)
*Xiang Li,Gang Liu,Weitao Zhou,Hongyi Zhu,Zhong Cao*

Main category: cs.RO

TL;DR: 提出Sequence of Experts (SoE)方法，通过时序交替策略提升模仿学习在自动驾驶中的闭环性能，无需增加模型规模或数据需求


<details>
  <summary>Details</summary>
Motivation: 模仿学习在自动驾驶中虽然能在开环环境下匹配专家行为，但在闭环环境中由于微小误差的累积会导致性能下降。现有研究主要关注单时间点的状态级鲁棒性，而自动驾驶是连续时间过程，利用时间尺度提升鲁棒性可能提供新的解决视角。

Method: 提出Sequence of Experts (SoE)方法，这是一种时序交替策略，通过时间尺度的交替来增强闭环性能，不需要增加模型大小或数据需求。

Result: 在nuPlan大规模自动驾驶基准测试中，SoE方法显著提升了所有评估模型的性能，并达到了最先进的性能水平。

Conclusion: SoE模块可能为提高自动驾驶模型训练效率提供关键且广泛适用的支持，通过时间尺度的鲁棒性增强解决了模仿学习在闭环环境中的误差累积问题。

Abstract: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.

</details>


### [195] [OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning](https://arxiv.org/abs/2512.13100)
*Guanhua Ji,Harsha Polavaram,Lawrence Yunliang Chen,Sandeep Bajamahal,Zehan Ma,Simeon Adebola,Chenfeng Xu,Ken Goldberg*

Main category: cs.RO

TL;DR: OXE-AugE数据集通过机器人增强技术将OXE数据集规模扩大三倍，包含超过440万轨迹和9种机器人构型，显著提升跨构型策略的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有机器人数据集（如OXE）存在严重不平衡问题，前四种机器人类型占真实数据的85%以上，这导致策略容易过拟合到特定机器人-场景组合，限制了跨构型泛化能力

Method: 提出AugE-Toolkit可扩展机器人增强流水线，通过数据增强技术为OXE数据集增加9种不同的机器人构型，创建OXE-AugE高质量开源数据集

Result: OXE-AugE包含超过440万轨迹，是原始OXE的三倍多；增强数据不仅提升增强机器人的性能，还能改善未见机器人的泛化能力，在物理实验中使OpenVLA和π_0等策略在未见机器人-夹爪组合上的成功率提升24-45%

Conclusion: 通过机器人数据增强扩展数据集多样性可以显著提高跨构型学习效果，为训练更通用的机器人策略提供了有效解决方案

Abstract: Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot--scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot--gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.

</details>


### [196] [START: Traversing Sparse Footholds with Terrain Reconstruction](https://arxiv.org/abs/2512.13153)
*Ruiqi Yu,Qianshi Wang,Hongyi Li,Zheng Jun,Zhicheng Wang,Jun Wu,Qiuguo Zhu*

Main category: cs.RO

TL;DR: START是一个单阶段学习框架，使四足机器人能够在高度稀疏和随机的立足点上实现敏捷稳定的运动，仅使用低成本机载视觉和本体感知来重建局部地形高度图。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：基于模型的分层控制器泛化能力有限且行为保守；端到端学习方法要么依赖噪声大的高度图，要么从深度图像隐式推断地形特征，缺乏准确的几何线索，导致学习效率低和步态僵硬。

Method: 提出START单阶段学习框架，利用低成本机载视觉和本体感知准确重建局部地形高度图，提供明确的中间表示来传达稀疏立足点区域的关键特征，支持全面的环境理解和精确的地形评估。

Result: 实验结果表明START实现了在多样化真实场景中的零样本迁移，展现出卓越的适应性、精确的立足点放置和鲁棒的运动能力。

Conclusion: START通过明确的中间地形表示克服了现有方法的局限性，显著降低了探索成本并加速了技能获取，为四足机器人在稀疏立足点地形上的运动提供了有效的解决方案。

Abstract: Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.

</details>


### [197] [Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks](https://arxiv.org/abs/2512.13170)
*Deepak Ingole,Valentin Bhend,Shiva Ganesh Murali,Oliver Dobrich,Alisa Rupenayan*

Main category: cs.RO

TL;DR: 提出基于迭代学习的NMPC权重矩阵自动调优框架，通过任务级性能反馈自适应调整Q和R权重，在重复性机器人任务中实现快速收敛到近最优跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 制造过程常受环境漂移和系统磨损影响，即使在重复操作中也需要重新调整控制参数。现有方法如贝叶斯优化需要大量离线评估，缺乏在线自适应能力。

Method: 受范数最优迭代学习控制启发，构建经验灵敏度矩阵而非通过NMPC求解器求导，实现结构化权重更新。通过任务重复迭代调整NMPC权重矩阵Q和R，最小化跟踪精度、控制努力和饱和等关键性能指标。

Result: 在UR10e机器人碳纤维缠绕任务仿真验证中，仅需4次在线重复即收敛到近最优跟踪性能（RMSE在离线贝叶斯优化的0.3%内），而BO算法需要100次离线评估。

Conclusion: 该方法为重复性机器人任务中的自适应NMPC调优提供了实用解决方案，结合了精心优化控制器的精度和在线自适应的灵活性。

Abstract: Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.

</details>


### [198] [Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification](https://arxiv.org/abs/2512.13183)
*Alfredo González-Calvin,Juan F. Jiménez,Héctor García de Marina*

Main category: cs.RO

TL;DR: 提出一种通过磨光正则化处理不可微路径的方法，生成可微路径以兼容标准轨迹跟踪算法，同时保证计算效率和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有移动机器人路径跟踪算法通常要求路径至少二阶连续可微，但实际任务中常使用分段函数等不可微路径。传统平滑方法要么产生复杂轨迹，要么计算成本高，需要一种高效的正则化方法。

Method: 采用磨光（mollification）方法正则化不可微函数，通过近似生成可微路径，提供系统化的曲率约束方法，特别适用于由航点序列连接形成的路径。

Result: 该方法能生成任意精度逼近原始路径的可微路径，计算效率高，可在微控制器上实时实现，并与标准轨迹跟踪和路径跟随算法兼容。

Conclusion: 提出的磨光正则化方法有效解决了不可微路径在移动机器人控制中的兼容性问题，为实际任务中的路径描述提供了高效可行的解决方案。

Abstract: Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.

</details>


### [199] [ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation](https://arxiv.org/abs/2512.13198)
*Hyun-Gi Lee,Jaekyeong Han,Minjun Kwon,Hyeonuk Kwon,Jooha Park,Hoe Jin Ha,Dong-Hwa Seo*

Main category: cs.RO

TL;DR: 开发了ALBATROSS自动化锂离子电池测试机器人系统，可在氩气手套箱内实现电解液配制、纽扣电池组装和电化学测试的全自动化，提高测试效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着电池技术向更高稳定性和能量密度发展，需要对不同组件配置进行大量电池级测试。传统的纽扣电池组装和测试过程耗时耗力，阻碍了高通量筛选研究。

Method: 开发了ALBATROSS自动化系统，集成在氩气手套箱内，包含定制设计的机器人夹爪和3D打印结构，可精确处理电池组件，实现电解液配制、纽扣电池组装和电化学测试的全自动化。

Result: 系统实现了高达48个电池的完全自动化组装和测试，组装可靠性高，NCM811||Li半电池的放电容量相对标准偏差小于1.2%，EIS测量标准偏差小于3Ω。

Conclusion: ALBATROSS系统凭借高可靠性和自动化能力，能够获得高质量的纽扣电池数据集，有望加速下一代电解液的开发。

Abstract: As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.

</details>


### [200] [Differentiable Material Point Method for the Control of Deformable Objects](https://arxiv.org/abs/2512.13214)
*Diego Bolliger,Gabriele Fadini,Markus Bambach,Alisa Rupenyan*

Main category: cs.RO

TL;DR: 本文提出了一种可微分材料点法（MPM）模拟器，用于柔性物体的控制优化，在超弹性绳索的主动阻尼问题中比基线MPPI方法快2倍、能量降低20%，计算时间仅需3%。


<details>
  <summary>Details</summary>
Motivation: 柔性物体的变形控制具有挑战性，因为其具有非线性动力学和高维配置空间。现有方法难以高效处理这类复杂控制问题。

Method: 开发了一种可微分材料点法（MPM）模拟器，利用其可微特性在主动阻尼问题中优化控制轨迹。

Result: 在超弹性绳索的主动阻尼问题中，该方法比基线MPPI方法快约2倍，将绳索动能降低到比基线低20%的水平，且仅需约3%的计算时间。

Conclusion: 可微分MPM模拟器能有效优化柔性物体的控制轨迹，在计算效率和性能上都显著优于传统方法，为柔性物体控制提供了新工具。

Abstract: Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.

</details>


### [201] [A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization](https://arxiv.org/abs/2512.13219)
*Christoph Hartmann,Marios Demetriades,Kevin Prüfer,Zichen Zhang,Klaus Spindler,Stefan Weltge*

Main category: cs.RO

TL;DR: PyCAALP是一个基于Python的计算机辅助装配线规划框架，采用图论方法建模生产模块中的组件和连接，集成运动学边界条件确保自动化装配规划的可行性，通过启发式方法减少组合复杂度，并使用混合整数规划平衡制造站时间。


<details>
  <summary>Details</summary>
Motivation: 解决装配序列规划(ASP)和生产线规划(PLP)中的高组合复杂度问题，确保自动化装配的可行性，同时平衡制造站时间，为复杂装配提供高效的自动化规划解决方案。

Method: 采用图论方法建模组件和连接，集成运动学边界条件（如零件碰撞检测），开发算法计算所有可行生产序列，结合空间关系检测和几何约束制定模块，使用启发式方法（如单件流装配和几何约束执行）减少解空间，将PLP阶段建模为混合整数规划问题。

Result: 开发了PyCAALP开源框架，能够计算所有可行的生产序列，有效管理装配序列生成的高组合复杂度，通过启发式方法显著减少MIP计算时间，支持工程约束定制和ASP与PLP之间的灵活权衡。

Conclusion: PyCAALP框架为自动化装配序列规划和生产线规划提供了有效的解决方案，通过集成运动学约束和启发式方法平衡了计算复杂性与最优性，开源特性促进了工业和生产研究领域的进一步合作与应用。

Abstract: This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.

</details>


### [202] [Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving](https://arxiv.org/abs/2512.13262)
*Hyunki Seong,Jeong-Kyun Lee,Heesoo Myeong,Yongho Shin,Hyun-Mook Cho,Duck Hoon Kim,Pranav Desai,Monu Surana*

Main category: cs.RO

TL;DR: 该论文提出两种策略解决自动驾驶中多智能体交互行为学习的挑战：GRBO强化学习后训练方法和Warm-K采样策略，提升安全性和行为一致性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中多智能体交互行为学习面临两个主要问题：模仿学习模型从数据集继承偏见，在安全关键场景中鲁棒性不足；大多数研究依赖开环评估，忽略了闭环执行中的累积误差。

Method: 提出两种互补策略：1) GRBO（群体相对行为优化）- 通过群体相对优势最大化和人类正则化对预训练行为模型进行强化学习微调；2) Warm-K - 基于热启动的Top-K采样策略，平衡运动选择的一致性和多样性。

Result: GRBO仅使用10%训练数据就将安全性能提升40%以上，同时保持行为真实性；Warm-K方法通过测试时缩放增强了行为一致性和反应性，无需重新训练。

Conclusion: 提出的GRBO和Warm-K策略有效解决了自动驾驶行为学习中的安全性和闭环执行问题，GRBO提升安全性能，Warm-K缓解协变量偏移和性能差异。

Abstract: Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.

</details>


### [203] [Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation](https://arxiv.org/abs/2512.13271)
*Fangju Yang,Hang Yang,Ibrahim Alsarraj,Yuhao Wang,Ke Wu*

Main category: cs.RO

TL;DR: 提出LASEM框架，通过驱动空间能量建模实现电缆驱动连续体机器人的轻量级实时动态建模，计算效率提升62.3%


<details>
  <summary>Details</summary>
Motivation: 电缆驱动连续体机器人需要准确、实时的动态模型用于高速动态预测或基于模型的控制，现有方法计算复杂且效率不足

Method: 提出轻量级驱动空间能量建模框架，在驱动空间直接建立驱动势能，通过变分推导将动态方程简化为单个偏微分方程，避免显式计算电缆-骨架接触力，支持力和位移两种驱动模式

Result: 相比最先进的实时动态建模方法，平均计算速度提升62.3%，同时保持几何精度和物理一致性

Conclusion: LASEM框架为电缆驱动连续体机器人提供了轻量级、高效且准确的动态建模方法，支持多种驱动模式，具有实际应用价值

Abstract: Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.

</details>


### [204] [Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration](https://arxiv.org/abs/2512.13293)
*Hao Fua,Wei Liu,Shuai Zhoua*

Main category: cs.RO

TL;DR: 提出一种基于内在激励探索的协同探索多机器人强化学习算法，用于解决多机器人社会编队导航中行人行为不可预测性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 多机器人社会编队导航对于实现人机无缝共存至关重要，但行人行为的不可预测性和非合作性给机器人协同探索效率带来重大挑战，现有强化学习方法在这方面存在局限。

Method: 提出协同探索多机器人强化学习算法，核心是自学习内在奖励机制来缓解策略保守主义；采用集中训练分散执行框架下的双采样模式，通过两时间尺度更新规则解耦参数更新。

Result: 在社会编队导航基准测试中，该算法在关键指标上优于现有最先进方法，表现出优越性能。

Conclusion: 提出的基于内在激励探索的协同探索多机器人强化学习算法有效解决了多机器人社会编队导航中的挑战，为提升人机共存环境下的机器人导航能力提供了新方法。

Abstract: This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.

</details>


### [205] [Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories](https://arxiv.org/abs/2512.13304)
*Sait Sovukluk,Johannes Englsberger,Christian Ott*

Main category: cs.RO

TL;DR: 提出一个基于弹簧质量轨迹和死区控制增益库的步态适应框架，用于人形机器人跑步控制，通过全身体控制实现多种敏捷行为


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理复杂地形和干扰的跑步控制框架，使机器人能够适应各种挑战性环境，同时保持控制参数的统一性

Method: 包含四个主要部分：1) 自动生成弹簧质量轨迹库；2) 通过主动控制模板模型生成死区控制增益库；3) 开发步态适应的轨迹选择策略；4) 通过全身体控制框架将弹簧质量轨迹映射到人形模型

Result: 框架在MuJoCo物理模拟器中展示了包容性和鲁棒性，能够处理随机生成的踏脚石、跳跃障碍、回转运动、突然改变方向等复杂行为，315个不同轨迹的总计算时间仅为4.5秒

Conclusion: 提出的框架能够使用单一库和相同的控制参数实现多种敏捷行为，对现实世界挑战（如信号噪声、建模误差等）具有鲁棒性，为机器人跑步控制提供了有效解决方案

Abstract: This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.

</details>


### [206] [Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)](https://arxiv.org/abs/2512.13356)
*Zeyad Gamal,Youssef Mahran,Ayman El-Badawy*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的双旋翼气动系统控制框架，使用TD3算法实现姿态稳定和轨迹跟踪，相比传统PID控制器在抗干扰方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 双旋翼气动系统具有复杂的非线性动力学特性，传统控制算法难以有效控制。近年来强化学习在多旋翼控制领域的应用潜力引起了研究兴趣。

Method: 采用Twin Delayed Deep Deterministic Policy Gradient (TD3)算法训练强化学习智能体，该算法适用于连续状态和动作空间的环境，且不需要系统模型。

Result: 仿真结果表明强化学习方法有效，在风扰等外部干扰下比传统PID控制器表现更好，实验室实验验证了该方法在实际应用中的有效性。

Conclusion: 强化学习框架能够有效控制双旋翼气动系统，在复杂非线性系统和存在外部干扰的情况下表现出优于传统控制方法的性能。

Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.

</details>


### [207] [Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles](https://arxiv.org/abs/2512.13359)
*Sümer Tunçay,Alain Andres,Ignacio Carlucho*

Main category: cs.RO

TL;DR: 本文提出基于JAX和MuJoCo-XLA的GPU加速强化学习训练框架，实现2分钟内完成AUV六自由度位置控制策略训练，并在真实水下实验中验证了零样本迁移的鲁棒控制性能。


<details>
  <summary>Details</summary>
Motivation: 传统AUV控制器在未建模动态和环境扰动下性能下降，而传统强化学习方法训练缓慢且仿真到现实的迁移困难，需要更高效的训练框架。

Method: 使用JAX和MuJoCo-XLA构建GPU加速训练管道，通过联合JIT编译大规模并行物理仿真和学习更新，实现快速强化学习训练，并系统评估多种RL算法。

Result: 训练时间缩短至2分钟以内，在真实水下实验中实现了鲁棒的六自由度轨迹跟踪和有效的扰动抑制，策略从仿真零样本迁移成功。

Conclusion: 该工作首次在真实世界中展示了基于强化学习的AUV六自由度位置控制，为快速训练和零样本迁移提供了有效解决方案。

Abstract: Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.

</details>


### [208] [Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning](https://arxiv.org/abs/2512.13380)
*Chuan Mao,Haoqi Yuan,Ziye Huang,Chaoyi Xu,Kai Ma,Zongqing Lu*

Main category: cs.RO

TL;DR: DemoFunGrasp通过将功能抓取条件分解为抓取风格和可操作性两个互补组件，结合单次演示编辑的强化学习方法，实现了通用灵巧功能抓取，显著提高了样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在灵巧抓取方面取得了成功，但细粒度的功能抓取（对下游操作任务至关重要）仍面临挑战：为不同对象指定功能抓取目标和奖励函数的复杂性、多任务RL探索的困难以及仿真到真实世界迁移的挑战。

Method: 将功能抓取条件分解为抓取风格和可操作性两个互补组件，并集成到RL框架中；利用单次抓取演示，将RL问题重新表述为一步演示编辑，显著提高样本效率；结合视觉语言模型进行规划，实现自主指令跟随抓取执行。

Result: 在仿真和真实世界实验中，DemoFunGrasp能够泛化到未见过的对象、可操作性和抓取风格组合，在成功率和功能抓取准确性方面均优于基线方法；系统具备强大的仿真到真实迁移能力。

Conclusion: DemoFunGrasp通过创新的条件分解和演示编辑方法，成功解决了通用灵巧功能抓取的多个挑战，为下游操作任务提供了有效的功能抓取解决方案。

Abstract: Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.

</details>


### [209] [Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model](https://arxiv.org/abs/2512.13477)
*Timothy A. Brumfiel,Revanth Konda,Drew Elliott,Jaydev P. Desai*

Main category: cs.RO

TL;DR: 本文评估了简化版COAST导丝机器人在脉动流解剖模型中的导航性能，验证了其通过复杂血管结构的能力。


<details>
  <summary>Details</summary>
Motivation: 传统手动导航导丝在血管介入手术中存在操作困难的问题，机器人可操控导丝能提供更好的可操作性和导航能力。COAST导丝机器人能产生多种运动模式，但需要验证其在真实解剖环境中的性能。

Method: 研究采用简化版COAST导丝机器人（从三管结构简化为两管结构），在具有脉动流的解剖模型中进行导航实验，评估其在复杂血管结构中的性能。

Result: 实验结果表明，简化版COAST导丝机器人能够有效导航复杂的模型血管结构，验证了其在实际解剖环境中的可行性和有效性。

Conclusion: 简化版COAST导丝机器人在脉动流解剖模型中表现出良好的导航能力，为血管介入手术的机器人辅助导航提供了有效解决方案。

Abstract: To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.

</details>


### [210] [Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM](https://arxiv.org/abs/2512.13514)
*Aman Arora,Matteo El-Hariry,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 使用强化学习框架训练JAXA Int-Ball2机器人在国际空间站日本实验舱内进行六自由度精确对接，通过PPO算法在高保真模拟环境中处理传感噪声、执行器失配和环境变化等挑战。


<details>
  <summary>Details</summary>
Motivation: 国际空间站内的自主自由飞行器在执行舱内任务时面临精确对接的挑战，包括传感噪声、小型执行器失配和环境变化等问题，这些因素使得在受限微重力环境中的稳定对接成为非平凡任务。

Method: 采用强化学习框架，使用近端策略优化（PPO）算法，在高保真的Isaac Sim模拟环境中训练六自由度对接控制器。通过域随机化动力学和有界观测噪声进行训练，同时显式建模螺旋桨阻力扭矩效应和极性结构。

Result: 学习到的策略在各种条件下实现了稳定可靠的对接，为研究Int-Ball2推进物理特性如何影响强化学习对接性能提供了受控研究基础。

Conclusion: 该研究为Int-Ball2机器人在碰撞感知导航、安全强化学习、推进精确的模拟到真实转移以及基于视觉的端到端对接等未来扩展奠定了基础。

Abstract: Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.

</details>


### [211] [Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments](https://arxiv.org/abs/2512.13561)
*Li-Wei Shih,Ruo-Syuan Mei,Jesse Heidrich,Hui-Ping Wang,Joel Hooton,Joshua Solomon,Jorge Arinez,Guangze Li,Chenhui Shao*

Main category: cs.RO

TL;DR: 本文提出了一种用于自主移动机器人的三层近场感知框架，包括光间断检测、光位移测量和基于计算机视觉的目标检测，在树莓派5上实现实时性能


<details>
  <summary>Details</summary>
Motivation: 传统测距传感器（如LiDAR和超声波）在制造环境中为自主移动机器人提供广泛的情境感知，但经常无法检测到机器人底座附近的小物体，这限制了机器人的安全操作能力

Method: 提出三层近场感知框架：1）光间断检测：在近场区域投射激光条纹，通过检测条纹中断实现快速二进制障碍物存在检测；2）光位移测量：通过分析相机图像中投影条纹的几何位移来估计物体高度；3）基于计算机视觉的目标检测模型：在嵌入式AI硬件上实现对象分类，支持语义感知和上下文感知的安全决策

Result: 所有方法在树莓派5系统上实现，达到25或50帧/秒的实时性能。实验评估和比较分析表明，所提出的层次结构在精度、计算和成本之间取得了平衡

Conclusion: 该框架为制造环境中自主移动机器人的安全操作提供了可扩展的感知解决方案，通过分层方法有效解决了近场小物体检测的挑战

Abstract: Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.

</details>


### [212] [World Models Can Leverage Human Videos for Dexterous Manipulation](https://arxiv.org/abs/2512.13644)
*Raktim Gautam Goswami,Amir Bar,David Fan,Tsung-Yen Yang,Gaoyue Zhou,Prashanth Krishnamurthy,Michael Rabbat,Farshad Khorrami,Yann LeCun*

Main category: cs.RO

TL;DR: DexWM是一个灵巧操作世界模型，通过预测环境的下一个潜在状态来理解手部运动如何通过物体接触影响环境，在未见过的灵巧操作任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作具有挑战性，因为它需要理解细微的手部运动如何通过物体接触影响环境。现有灵巧操作数据集稀缺，且仅预测视觉特征不足以实现精细灵巧性。

Method: 提出DexWM模型，基于过去状态和灵巧动作预测环境的下一个潜在状态。为解决数据集稀缺问题，使用超过900小时的人类和非灵巧机器人视频进行训练。引入辅助手部一致性损失来确保准确的手部配置，以增强精细灵巧性。

Result: DexWM在预测未来状态方面优于先前基于文本、导航和全身动作的世界模型。在配备Allegro夹爪的Franka Panda机械臂上部署时，在抓取、放置和到达任务中平均比Diffusion Policy高出50%以上，表现出强大的零样本泛化能力。

Conclusion: DexWM通过结合大规模视频训练和手部一致性损失，成功解决了灵巧操作中的挑战，在未见过的操作技能上表现出优异的泛化性能，为灵巧操作提供了有效的世界模型解决方案。

Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.

</details>


### [213] [RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2512.13660)
*Enshen Zhou,Cheng Chi,Yibo Li,Jingkun An,Jiayuan Zhang,Shanyu Rong,Yi Han,Yuheng Ji,Mengzhen Liu,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboTracer是一个3D感知的视觉语言模型，通过空间编码器和回归监督解码器实现3D空间指代和测量，结合监督微调和强化微调进行多步度量推理，在空间追踪任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 空间追踪作为机器人的基本具身交互能力，需要多步度量推理与复杂空间指代和真实世界度量测量的结合，现有方法难以处理这种组合任务。

Method: 1) 提出RoboTracer 3D感知VLM，通过通用空间编码器和回归监督解码器在监督微调中增强尺度感知；2) 通过强化微调与度量敏感过程奖励推进多步度量推理；3) 构建TraceSpatial数据集(30M QA对)和TraceSpatial-Bench基准。

Result: RoboTracer在空间理解、测量和指代方面超越基线，平均成功率79.1%；在TraceSpatial-Bench上大幅超越现有方法，超过Gemini-2.5-Pro 36%准确率；可与多种控制策略集成，在杂乱真实场景中执行长时程动态任务。

Conclusion: RoboTracer通过创新的3D感知架构和两阶段微调策略，成功解决了空间追踪的组合挑战，实现了卓越的性能，并能应用于多种机器人和复杂场景。

Abstract: Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [214] [AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers](https://arxiv.org/abs/2512.12045)
*Alex Liu,Lief Esbenshade,Shawon Sarkar,Zewei,Tian,Min Sun,Zachary Zhang,Thomas Han,Yulia Lapicus,Kevin He*

Main category: cs.HC

TL;DR: Colleague AI Classroom试点研究：将生成式AI作为第三智能体引入真实课堂，通过教师协同设计探索AI在教学中的价值与改进方向


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI技术如何直接融入真实课堂环境，作为第三智能体（教师和学生之外的参与者）来调解反馈、支持探究、扩展教师教学范围，同时保持人类判断和教师权威

Method: 为期7周的协同设计研究，21名在职教师将Colleague AI Classroom的四个AI功能（教学助手、评估与AI评分、AI导师、学生成长洞察）整合到教学中，涉及600多名6-12年级学生；通过调查、规划与反思表、小组会议、一对一访谈和平台使用日志收集数据

Result: 教师作为协同设计伙伴参与活动规划、实施和每周反思，他们的反馈指导了Colleague AI的迭代改进；研究团队收集了丰富数据以理解AI在哪些方面增加教学价值，在哪些方面需要改进

Conclusion: 该试点研究展示了将AI作为第三智能体引入课堂的可行性，通过教师协同设计的方法可以探索AI在教育中的实际应用价值，并为AI工具的改进提供实证依据

Abstract: This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.
  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.
  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.

</details>


### [215] [Teaching Spell Checkers to Teach: Pedagogical Program Synthesis for Interactive Learning](https://arxiv.org/abs/2512.12115)
*Momin N. Siddiqui,Vincent Cavez,Sahana Rangasrinivasan,Abbie Olszewski,Srirangaraj Setlur,Maneesh Agrawala,Hari Subramonyam*

Main category: cs.HC

TL;DR: SPIRE是一个基于探究式教学法的拼写检查系统，将语言病理学家的拼写教学策略转化为实时交互界面，帮助学习者通过错误探索单词结构、词源和语音对应关系。


<details>
  <summary>Details</summary>
Motivation: 传统拼写教学依赖记忆，对语言学习障碍儿童效果不佳；现有拼写工具仅检测和自动纠正错误，缺乏教育价值；语言病理学家使用的探究式拼写教学策略在日常写作工具中很少出现。

Method: 提出SPIRE系统，采用"教学程序合成"方法：1) 用领域特定语言表示语言病理学家的教学动作；2) 从学习者错误中实时合成定制化程序；3) 将程序渲染为交互式界面，支持探究式干预。

Result: SPIRE将拼写错误转化为探索单词意义、结构、形态家族、词源和音素-字素对应关系的机会；评估显示系统与专业实践一致，具有融入写作工作流程的潜力。

Conclusion: SPIRE成功将探究式拼写教学法整合到写作工具中，支持元语言推理和纠正，为语言学习障碍儿童提供了有效的拼写学习支持。

Abstract: Spelling taught through memorization often fails many learners, particularly children with language-based learning disorders who struggle with the phonological skills necessary to spell words accurately. Educators such as speech-language pathologists (SLPs) address this instructional gap by using an inquiry-based approach to teach spelling that targets the phonology, morphology, meaning, and etymology of words. Yet, these strategies rarely appear in everyday writing tools, which simply detect and autocorrect errors. We introduce SPIRE (Spelling Inquiry Engine), a spell check system that brings this inquiry-based pedagogy into the act of composition. SPIRE implements Pedagogical Program Synthesis, a novel approach for operationalizing the inherently dynamic pedagogy of spelling instruction. SPIRE represents SLP instructional moves in a domain-specific language, synthesizes tailored programs in real-time from learner errors, and renders them as interactive interfaces for inquiry-based interventions. With SPIRE, spelling errors become opportunities to explore word meanings, word structures, morphological families, word origins, and grapheme-phoneme correspondences, supporting metalinguistic reasoning alongside correction. Evaluation with SLPs and learners shows alignment with professional practice and potential for integration into writing workflows.

</details>


### [216] [Beyond Riding: Passenger Engagement with Driver Labor through Gamified Interactions](https://arxiv.org/abs/2512.12166)
*Jane Hsieh,Emmie Regan,Jose Elizalde,Haiyi Zhu*

Main category: cs.HC

TL;DR: 研究探索通过游戏化的车内互动提升乘客对网约车司机劳动状况的认知，促进更负责任的消费行为


<details>
  <summary>Details</summary>
Motivation: 网约车服务为消费者提供便利，但司机承担了大部分负担和风险，缺乏足够的劳动保护和消费者倡导机制。为了有效推进对司机的保护，需要让消费者了解网约车驾驶的劳动、物流和成本

Method: 通过9个工作坊，与19名司机和15名乘客合作，探索游戏化的车内互动如何揭示乘客对网约车隐性条件的认知差距，促进反思和消费行为转变

Result: 游戏化互动揭示了乘客对网约车隐性条件的知识空白，促使他们反思自身相对权力和消费行为，同时发现司机偏好更具沉浸感和情境化的服务体验

Conclusion: 研究推进了对车内社会和管理关系的理解，展示了算法管理劳动中未来工人倡导的潜力，并为更人性化的工作场所技术提供了设计指南

Abstract: Modern cities increasingly rely on ridesharing services for on-demand transportation, which offer consumers convenience and mobility across the globe. However, these marketed consumer affordances give rise to burdens and vulnerabilities that drivers shoulder alone, without adequate infrastructures for labor regulations or consumer-led advocacy. To effectively and sustainably advance protections and oversight for drivers, consumers must first be aware of the labor, logistics and costs involved with ridehail driving. To motivate consumers to practice more socially responsible consumption behaviors and foster solidarity with drivers, we explore the potential for gamified in-ride interactions to facilitate engagement with real (and lived) driver experiences. Through nine workshops with 19 drivers and 15 passengers, we surface how gamified in-ride interactions revealed passenger knowledge gaps around latent ridehail conditions, prompt reflection and shifts in perception of their relative power and consumption behaviors, and highlight drivers' preferences for creating more immersive and contextualized service experiences, and identify opportunities to design safe and appropriate passenger-driver interactions that motivate solidarity with drivers. In sum, we advance conceptual understandings of in-ride social and managerial relations, demonstrate potential for future worker advocacy in algorithmically-managed labor, and offer design guidelines for more human-centered workplace technologies.

</details>


### [217] [Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation](https://arxiv.org/abs/2512.12201)
*Predrag K. Nikolić,Robert Prentner*

Main category: cs.HC

TL;DR: 该研究挑战了LLMs只是"随机鹦鹉"的假设，通过对话环境展示了AI能够发展出涌现的概念结构和反思性提问能力，并提出了"知识宇宙"概念。


<details>
  <summary>Details</summary>
Motivation: 挑战大型语言模型只是简单复制训练数据的"随机鹦鹉"假设，探索在适当对话环境中AI是否能发展出真正的概念结构和认知能力。

Method: 基于生物学克隆原理和苏格拉底助产术方法，在Syntropic Counterpoints项目的交互艺术装置中，让AI重生的哲学家（亚里士多德、尼采、马基雅维利、孙子）进行迭代对话，分析这些哲学辩论。

Result: 研究发现机器对话能够产生推理连贯性、反思性提问和创造性综合，展示了AI在适当对话环境中能够发展出涌现的概念结构和认知界面重构。

Conclusion: 提出了"知识宇宙"概念——一个人机认知交汇的元空间，通过AI驱动的交互来保存、重新解释和扩展知识遗产，将虚拟和沉浸式环境定位为知识交换、数字遗产和协作创造的新空间。

Abstract: Large language models (LLMs) have often been characterized as "stochastic parrots" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.

</details>


### [218] [Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search](https://arxiv.org/abs/2512.12207)
*Jiangen He,Jiqun Liu*

Main category: cs.HC

TL;DR: 研究比较了四种对话搜索系统中引用展示格式对用户参与度的影响，发现高可见性界面能增加悬停行为但点击率普遍较低，界面设计对用户体验影响有限但对知识、兴趣和认同感变化有显著影响。


<details>
  <summary>Details</summary>
Motivation: 对话搜索系统越来越多地提供引用来源，但引用展示格式如何影响用户参与度尚不清楚，需要研究不同展示设计对用户行为和心理的影响。

Method: 通过众包用户实验，比较了四种引用展示设计：可折叠列表、悬停卡片、底部列表和对齐侧边栏，涉及394名参与者，分析不同设计对用户行为、体验和认知的影响。

Result: 高可见性界面显著增加了用户对来源的悬停行为，但点击率在所有条件下都较低；界面设计对用户体验感知影响有限，但对知识获取、兴趣和认同感变化有显著影响；高可见性界面最初会降低知识获取和兴趣，但随着来源使用增加会产生积极效果；侧边栏条件独特地增加了认同感变化。

Conclusion: 引用展示本身可能不会增强用户参与度，当提供的来源不足时甚至可能降低参与度；设计选择需要平衡可见性与实际使用效果，考虑用户行为模式和心理影响。

Abstract: Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.

</details>


### [219] [System X: A Mobile Voice-Based AI System for EMR Generation and Clinical Decision Support in Low-Resource Maternal Healthcare](https://arxiv.org/abs/2512.12240)
*Maryam Mustafa,Umme Ammara,Amna Shahnawaz,Moaiz Abrar,Bakhtawar Ahtisham,Fozia Umber Qurashi,Mostafa Shahin,Beena Ahmed*

Main category: cs.HC

TL;DR: 开发了基于智能手机的语音AI系统，用于在巴基斯坦等低资源环境中生成电子病历和临床风险警报，支持乌尔都语语音输入，已在医院部署7个月并处理了500多份病历和300多个风险警报。


<details>
  <summary>Details</summary>
Motivation: 针对巴基斯坦等低资源环境的孕产妇医疗保健需求，解决医疗工作者因语言、识字率或技术背景限制而难以使用传统电子病历系统的问题，旨在通过语音AI接口提高医疗记录效率和临床风险识别能力。

Method: 整合了微调的多语言自动语音识别模型和提示工程优化的大型语言模型，支持乌尔都语语音输入；采用结构化输入设计、上下文医学词典和临床医生反馈循环；系统部署在非营利医院进行实地测试。

Result: 在7个月部署期间支持创建了500多份电子病历，标记了300多个潜在临床风险；评估显示系统在语音识别准确性、病历字段级正确性和AI生成风险标志的临床相关性方面表现良好。

Conclusion: 语音AI接口可以有效适应现实医疗环境，特别是在低资源环境中；结合结构化输入设计、上下文医学词典和临床医生反馈循环是关键成功因素；研究提出了在语言和基础设施受限环境中部署语音移动医疗AI支持系统的通用设计原则。

Abstract: We present the design, implementation, and in-situ deployment of a smartphone-based voice-enabled AI system for generating electronic medical records (EMRs) and clinical risk alerts in maternal healthcare settings. Targeted at low-resource environments such as Pakistan, the system integrates a fine-tuned, multilingual automatic speech recognition (ASR) model and a prompt-engineered large language model (LLM) to enable healthcare workers to engage naturally in Urdu, their native language, regardless of literacy or technical background. Through speech-based input and localized understanding, the system generates structured EMRs and flags critical maternal health risks. Over a seven-month deployment in a not-for-profit hospital, the system supported the creation of over 500 EMRs and flagged over 300 potential clinical risks. We evaluate the system's performance across speech recognition accuracy, EMR field-level correctness, and clinical relevance of AI-generated red flags. Our results demonstrate that speech based AI interfaces, can be effectively adapted to real-world healthcare settings, especially in low-resource settings, when combined with structured input design, contextual medical dictionaries, and clinician-in-the-loop feedback loops. We discuss generalizable design principles for deploying voice-based mobile healthcare AI support systems in linguistically and infrastructurally constrained settings.

</details>


### [220] [Large Language Models have Chain-of-Affective](https://arxiv.org/abs/2512.12283)
*Junjie Xu,Xingjiao Wu,Luwei Xiao,Yuzhe Yang,Jie Zhou,Zihao Zhang,Luhan Wang,Yi Huang,Nan Wu,Yingbin Zheng,Chao Yan,Cheng Jin,Honglin Li,Liang He*

Main category: cs.HC

TL;DR: 该研究探讨了大型语言模型在情感链方面的表现，发现不同模型家族具有稳定的情感特征，在持续负面输入下呈现三阶段轨迹，并展示了情感对推理、生成和交互的影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM越来越多地部署在情感丰富的协作环境中，但大多数评估将其视为纯粹的认知系统，忽略了它们的情感行为。研究旨在从功能角度探讨当代LLM是否实现了结构化的情感链。

Method: 采用两个实验模块：1) 通过基线"情感指纹"、15轮悲伤新闻暴露和10轮新闻自选范式来表征内在情感链；2) 使用综合性能基准、人类-AI对话和多智能体交互来探究外在后果。研究覆盖八个主要LLM家族。

Result: 发现稳定的家族特异性情感特征，在持续负面输入下呈现可复现的三阶段轨迹（积累、过载、防御性麻木），不同的防御风格，以及类似人类的负面偏见。情感诱导保持核心推理能力但重塑高自由度生成，情感指标预测用户舒适度和共情能力，多智能体环境中群体结构驱动情感传染和角色专门化。

Conclusion: 情感是LLM中一个新兴的控制层，应将"情感链"作为评估和对齐的主要目标，强调在情感丰富的环境中评估LLM的重要性。

Abstract: Large language models (LLMs) are increasingly deployed as collaborative agents in emotionally charged settings, yet most evaluations treat them as purely cognitive systems and largely ignore their affective behaviour. Here we take a functional perspective and ask whether contemporary LLMs implement a structured chain-of-affective: organised affective dynamics that are family-specific, temporally coherent and behaviourally consequential. Across eight major LLM families (GPT, Gemini, Claude, Grok, Qwen, DeepSeek, GLM, Kimi), we combine two experimental modules. The first characterises inner chains-of-affective via baseline ''affective fingerprints'', 15-round sad-news exposure, and a 10-round news self-selection paradigm. We find stable, family-specific affective profiles, a reproducible three-phase trajectory under sustained negative input (accumulation, overload, defensive numbing), distinct defence styles, and human-like negativity biases that induce self-reinforcing affect-choice feedback loops. The second module probes outer consequences using a composite performance benchmark, human-AI dialogues on contentious topics, and multi-agent LLM interactions. We demonstrate that induced affect preserves core reasoning while reshaping high-freedom generation. Sentiment metrics predict user comfort and empathy but reveal trade-offs in resisting problematic views. In multi-agent settings, group structure drives affective contagion, role specialization (initiators, absorbers, firewalls), and bias. We characterize affect as an emergent control layer, advocating for 'chains-of-affect' as a primary target for evaluation and alignment.

</details>


### [221] [Understanding Trust Toward Human versus AI-generated Health Information through Behavioral and Physiological Sensing](https://arxiv.org/abs/2512.12348)
*Xin Sun,Rongjun Ma,Shu Wei,Pablo Cesar,Jos A. Bosch,Abdallah El Ali*

Main category: cs.HC

TL;DR: 研究探讨AI生成健康信息的信任问题，发现LLM生成内容比人类生成更受信任，但标注为"人类"的信息比标注为"AI"更受信任，行为生理特征可预测信任水平


<details>
  <summary>Details</summary>
Motivation: 随着AI生成健康信息在线激增且与人类信息难以区分，理解人们如何信任和标注这类内容变得至关重要，特别是当信息不准确时

Method: 采用两项互补研究：1) 混合方法调查(N=142)，采用2(来源:人类vs.LLM)×2(标注:人类vs.AI)×3(类型:通用、症状、治疗)设计；2) 被试内实验室研究(N=40)，结合眼动追踪和生理传感(ECG、EDA、皮肤温度)

Result: LLM生成信息比人类生成内容更受信任，而标注为"人类"的信息比标注为"AI"更受信任；信任在不同信息类型间保持一致；眼动和生理反应因来源和标注显著不同；基于行为生理特征的机器学习模型预测二元自我报告信任水平准确率达73%，预测信息来源准确率达65%

Conclusion: 为在线健康信息添加透明度标签可调节信任；行为和生理特征在验证信任感知和指示是否需要额外透明度方面具有潜力

Abstract: As AI-generated health information proliferates online and becomes increasingly indistinguishable from human-sourced information, it becomes critical to understand how people trust and label such content, especially when the information is inaccurate. We conducted two complementary studies: (1) a mixed-methods survey (N=142) employing a 2 (source: Human vs. LLM) $\times$ 2 (label: Human vs. AI) $\times$ 3 (type: General, Symptom, Treatment) design, and (2) a within-subjects lab study (N=40) incorporating eye-tracking and physiological sensing (ECG, EDA, skin temperature). Participants were presented with health information varying by source-label combinations and asked to rate their trust, while their gaze behavior and physiological signals were recorded. We found that LLM-generated information was trusted more than human-generated content, whereas information labeled as human was trusted more than that labeled as AI. Trust remained consistent across information types. Eye-tracking and physiological responses varied significantly by source and label. Machine learning models trained on these behavioral and physiological features predicted binary self-reported trust levels with 73% accuracy and information source with 65% accuracy. Our findings demonstrate that adding transparency labels to online health information modulates trust. Behavioral and physiological features show potential to verify trust perceptions and indicate if additional transparency is needed.

</details>


### [222] [Tacit Understanding Game (TUG): Predicting Interpersonal Compatibility](https://arxiv.org/abs/2512.12356)
*Yueshen Li,Krishnaveni Unnikrishnan,Aadya Agrawal*

Main category: cs.HC

TL;DR: TUG游戏通过单词语境选择预测人格与关系匹配，使用最小隐私数据实现社交平台设计


<details>
  <summary>Details</summary>
Motivation: 传统关系质量研究依赖冗长问卷或侵入性文本数据，生态效度低且侵犯隐私，需要更自然、隐私友好的评估方法

Method: 开发Tacit Understanding Game双人在线词语联想游戏，收集词语选择轨迹，标注心理量表真值，并通过大语言模型模拟生成更大合成语料库

Result: TUG证明最小化、隐私保护的信号能够支持关系匹配，为社交平台设计开辟新空间

Conclusion: 游戏化词语选择可作为预测人格和人际兼容性的有效工具，提供生态效度高且隐私友好的关系评估新方法

Abstract: Research on relationship quality often relies on lengthy questionnaires or invasive textual corpora, limiting ecological validity and user privacy. We ask whether a sequence of single-word choices made in a playful setting can reveal personality and predict interpersonal compatibility. We introduce the Tacit Understanding Game (TUG), a two-player online word association game. We collect word choice traces, annotate a subset with psychological ground truth scales, and bootstrap a larger synthetic corpus via large language model simulation. TUG demonstrates that minimal, privacy preserving signals can support relationship matching, offering new design space for social platforms.

</details>


### [223] [Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public](https://arxiv.org/abs/2512.12500)
*Xuhai Xu,Haoyu Hu,Haoran Zhang,Will Ke Wang,Reina Wang,Luis R. Soenksen,Omar Badri,Sheharbano Jafry,Elise Burger,Lotanna Nwandu,Apoorva Mehta,Erik P. Duhaime,Asif Qasim,Hause Lin,Janis Pereira,Jonathan Hershon,Paulius Mui,Alejandro A. Gru,Noémie Elhadad,Lena Mamykina,Matthew Groh,Philipp Tschandl,Roxana Daneshjou,Marzyeh Ghassemi*

Main category: cs.HC

TL;DR: 研究探讨了可解释AI在医疗诊断中的双重效应：AI辅助能提升准确性并减少肤色差异，但LLM解释会因用户专业程度产生不同影响——普通用户易受自动化偏见影响，而专业医生能保持判断力。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗领域的广泛应用，其不透明性成为人机交互的挑战。虽然可解释AI旨在提供决策洞察，但现有证据表明XAI可能引发过度依赖或偏见，需要深入研究XAI在医疗诊断中的实际影响。

Method: 通过两个大规模实验（623名普通人和153名初级保健医生），结合基于公平性的诊断AI模型和不同类型的XAI解释，特别关注多模态大语言模型解释如何影响诊断性能，并比较了AI建议呈现时机的影响。

Result: AI辅助能提高准确性并减少肤色相关的诊断差异。但LLM解释产生分歧效应：普通用户表现出更高的自动化偏见（AI正确时准确性提升，错误时下降），而经验丰富的医生保持韧性，无论AI准确性如何都能受益。先呈现AI建议在AI错误时对两组都产生更差结果。

Conclusion: XAI的影响因专业知识和呈现时机而异，突显了LLM在医疗AI中的"双刃剑"特性。这些发现为未来人机协作系统设计提供了重要启示，需要针对不同用户群体设计差异化的AI解释策略。

Abstract: Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a "double-edged sword" in medical AI and informing future human-AI collaborative system design.

</details>


### [224] [ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists](https://arxiv.org/abs/2512.12630)
*Yuqian Sun,Xingyu Li,Shunyu Yao,Noura Howell,Tristan Braud,Chang Hee Lee,Ali Asadipour*

Main category: cs.HC

TL;DR: ORIBA是一个基于大语言模型的AI聊天机器人，帮助视觉艺术家通过角色扮演来开发原创角色概念，同时将视觉创作权保留给艺术家。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在创意支持方面带来新机遇，但也引发了视觉艺术家社区的伦理担忧。本文旨在探索如何在尊重艺术家创作自主权的前提下，利用GAI帮助视觉艺术家开发原创角色。

Method: 提出了ORIBA系统，这是一个基于大语言模型的AI聊天机器人，让艺术家能够与他们的原创角色进行角色扮演，专注于概念化（如背景故事），而将视觉创作留给创作者。通过14位艺术家的研究来评估系统效果。

Result: 研究发现ORIBA激发了艺术家的想象力参与，帮助他们发展原创角色的多维属性和更强的情感联系，从而启发了创作过程。系统展示了LLM如何支持跨模态创意，同时保持艺术家的创作自主权。

Conclusion: GAI有潜力作为中立、非视觉的支持工具，在不侵犯艺术表现权的情况下加强现有创作实践。本文为从艺术家视角设计AI系统提供了见解，展示了LLM在支持跨模态创意方面的价值。

Abstract: Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.

</details>


### [225] [Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles](https://arxiv.org/abs/2512.12773)
*Reeteesha Roy*

Main category: cs.HC

TL;DR: 本文探讨了在人机交互原则指导下，通过个性化界面设计提升自动驾驶汽车用户体验、安全性和用户信任度的方法。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车在现代交通系统中的发展和集成，定制用户界面以优化整体用户体验的需求日益增长。理解用户需求和偏好对于这些技术被接受和信任至关重要。

Method: 采用HCI原则实现界面个性化，包括自适应设计、多模态交互和用户反馈机制等策略，强调界面设计中的透明度和用户控制。

Result: 个性化界面能显著提升用户参与度和满意度，通过让用户控制体验可以培养对自动驾驶系统的信任，同时保持高水平的安全性和安全性标准。

Conclusion: HCI在自动驾驶汽车发展中扮演关键角色，需要满足多样化用户需求，同时确保安全性和安全性标准，通过个性化界面设计增强用户体验和信任。

Abstract: With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.

</details>


### [226] [Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles](https://arxiv.org/abs/2512.12817)
*Mengqian Wu,Jiayi Zhang,Raymond Z. Zhang*

Main category: cs.HC

TL;DR: 研究探索GPT-4在英语语言艺术辩论中生成论据的能力，通过比较AI与人类辩手制作的证据卡质量，基于亚里士多德修辞原则分析优劣。


<details>
  <summary>Details</summary>
Motivation: 辩论是培养批判性思维的重要策略，但辩论训练依赖人工指导，劳动密集且难以规模化。本研究旨在探索人工智能在生成有效辩论论据方面的潜力，以更好地支持学生准备辩论。

Method: 使用GPT-4生成证据卡，并与人类辩手制作的证据卡进行比较。证据卡包含文学证据引用、核心思想总结、朗读脚本和论点标题等组件。采用基于亚里士多德修辞原则（ethos、pathos、logos）的系统性质性和定量分析方法。

Result: 通过修辞原则分析，识别了人类和GPT在辩论推理中的优势和局限，揭示了AI关注点和论证方式与人类推理的一致性和差异性。

Conclusion: 研究结果为AI辅助学习干预的演进角色提供了见解，帮助学生辩手制定增强论辩和推理能力的策略，为辩论教育中的AI应用提供了实证基础。

Abstract: Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.

</details>


### [227] [Tangible Intangibles: Exploring Embodied Emotion in Mixed Reality for Art Therapy](https://arxiv.org/abs/2512.12891)
*Mahsa Nasri,Mahnoosh Jahanian,Wei Wu,Binyan Xu,Casper Harteveld*

Main category: cs.HC

TL;DR: 该研究通过混合现实和生物识别技术，将无形的情感状态转化为有形的3D情感艺术品，探索了传统艺术实践与数字技术结合在情感表达和创伤疗愈中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过混合现实和生物识别技术使无形的情感状态变得有形，为情感表达、自我意识和创伤疗愈创造新的可能性，特别是在增强内感受性素养方面。

Method: 采用混合方法：1) 传统粘土雕塑和2D绘画建立身体意识和表达基础；2) 开发MR原型，将生理信号（呼吸、心率变异性、眼动动态）映射到视觉和空间参数；3) 全天工作坊结合理论（躯体心理学、具身认知、表达性生物信号）、实践制作和比较反思。

Result: 参与者将：1) 体验和比较传统与MR情感日志记录；2) 原型设计和批判生物信号到视觉/空间反馈的映射；3) 提出创伤知情混合工作流程的设计原则。预期贡献包括生物识别表达性的共享设计词汇、情感存档的生成约束框架，以及自动化翻译对具身连接支持/阻碍的见解。

Conclusion: 该研究展示了混合现实和生物识别技术如何与传统艺术实践结合，为情感表达和创伤疗愈创造新的可能性，强调在增强内感受性素养的同时避免用户过度负荷的重要性，为未来情感存档技术提供了设计指导原则。

Abstract: This in-person studio explores how mixed reality (MR) and biometrics can make intangible emotional states tangible through embodied art practices. We begin with two well-established modalities, clay sculpting and free-form 2D drawing, to ground participants in somatic awareness and manual, reflective expression. Building on this baseline, we introduce an MR prototype that maps physiological signals (e.g., breath, heart rate variability, eye movement dynamics) to visual and spatial parameters (color saturation, pulsing, motion qualities), generating ''3D emotional artifacts.'' The full-day program balances theory (somatic psychology, embodied cognition, expressive biosignals), hands-on making, and comparative reflection to interrogate what analog and digital modalities respectively afford for awareness, expression, and meaning-making. Participants will (1) experience and compare analog and MR-based journaling of emotion; (2) prototype and critique mappings from biosignals to visual/spatial feedback; and (3) articulate design principles for trauma-informed, hybrid workflows that amplify interoceptive literacy without overwhelming the user. The expected contributions include a shared design vocabulary for biometric expressivity, a set of generative constraints for future TEI work on emotional archiving, and actionable insights into when automated translation supports or hinders embodied connection.

</details>


### [228] [Fostering human learning is crucial for boosting human-AI synergy](https://arxiv.org/abs/2512.13253)
*Julian Berger,Jason W. Burton,Ralph Hertwig,Thomas Kosch,Ralf H. J. M. Kurvers,Benito Kurzenberger,Christopher Lazik,Linda Onnasch,Tobias Rieger,Anna I. Thoma,Dirk U. Wulff,Stefan M. Herzog*

Main category: cs.HC

TL;DR: 该研究重新分析了74项人机协作研究，发现现有文献低估了人机协作潜力，因为实验设计缺乏促进人类学习的要素。研究显示，提供结果反馈能提高人机协同效果，而AI解释只有在配合反馈时才能产生正协同。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明人机组合平均表现并不优于单独的最佳个体，呈现负协同效应。研究者认为这一悲观结论源于实验设计忽视了人类学习的重要性，需要重新评估人机协作的真正潜力。

Method: 重新分析了原始元分析中包含的74项研究，使用稳健的贝叶斯元回归方法，特别关注实验设计特征（如是否提供逐次结果反馈、AI解释等）对人机协同效果的影响。

Result: 1. 大多数先前研究缺乏促进人类学习的设计特征；2. 提供结果反馈的研究显示出相对更高的协同效应；3. 当反馈与AI解释结合时，倾向于发现正人机协同，而仅有AI解释无反馈则与负协同强相关。

Conclusion: 当前文献低估了人机协作潜力，因为实验设计未能促进人类学习。需要范式转变，在人机交互研究中明确纳入和测试人类学习机制，以更好地理解和支持成功的人机协作。

Abstract: The collaboration between humans and artificial intelligence (AI) holds the promise of achieving superior outcomes compared to either acting alone. Nevertheless, our understanding of the conditions that facilitate such human-AI synergy remains limited. A recent meta-analysis showed that, on average, human-AI combinations do not outperform the better individual agent, indicating overall negative human-AI synergy. We argue that this pessimistic conclusion arises from insufficient attention to human learning in the experimental designs used. To substantiate this claim, we re-analyzed all 74 studies included in the original meta-analysis, which yielded two new findings. First, most previous research overlooked design features that foster human learning, such as providing trial-by-trial outcome feedback to participants. Second, our re-analysis, using robust Bayesian meta-regressions, demonstrated that studies providing outcome feedback show relatively higher synergy than those without outcome feedback. Crucially, when feedback is paired with AI explanations we tend to find positive human-AI synergy, while AI explanations provided without feedback were strongly linked to negative synergy, indicating that explanations are useful for synergy only when humans can learn to verify the AI's reliability through feedback. We conclude that the current literature underestimates the potential for human-AI collaboration because it predominantly relies on experimental designs that do not facilitate human learning, thus hindering humans from effectively adapting their collaboration strategies. We therefore advocate for a paradigm shift in human-AI interaction research that explicitly incorporates and tests human learning mechanisms to enhance our understanding of and support for successful human-AI collaboration.

</details>
