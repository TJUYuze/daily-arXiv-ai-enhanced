<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training](https://arxiv.org/abs/2511.21592)
*Haotian Xue,Qi Chen,Zhonghao Wang,Xun Huang,Eli Shechtman,Jinrong Xie,Yongxin Chen*

Main category: cs.CV

TL;DR: MoGAN是一个专注于提升视频生成运动真实性的后训练框架，通过光流判别器和分布匹配正则化器，在保持视觉保真度的同时显著改善运动质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型在帧级保真度上表现良好，但在运动连贯性、动态性和真实性方面存在不足，经常产生抖动、重影或不合理的动态。标准去噪MSE目标缺乏对时间一致性的直接监督。

Method: 基于3步蒸馏的视频扩散模型，训练基于DiT的光流判别器来区分真实与生成的运动，结合分布匹配正则化器来保持视觉保真度。

Result: 在VBench上，MoGAN相比50步教师模型提升运动得分7.3%，相比3步DMD模型提升13.3%；在VideoJAM-Bench上分别提升7.4%和8.8%，同时保持相当或更好的美学和图像质量得分。人类研究也证实MoGAN在运动质量上更受青睐。

Conclusion: MoGAN在不牺牲视觉保真度或效率的前提下，显著提升了运动真实性，为快速高质量视频生成提供了实用路径。

Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

</details>


### [2] [ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images](https://arxiv.org/abs/2511.21606)
*M. Naseer Subhani*

Main category: cs.CV

TL;DR: 提出了一种自提示的点监督框架，通过Refine-Requery-Reinforce循环，仅使用稀疏点标注来适应SAM模型到遥感图像分割任务。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在遥感图像上由于领域偏移和密集标注稀缺导致的性能不佳问题。

Method: 采用Refine-Requery-Reinforce循环：从初始点生成粗伪掩码（Refine），使用自构建的框提示改进（Requery），并通过嵌入对齐减少确认偏差（Reinforce）。

Result: 在WHU、HRSID和NWPU VHR-10三个遥感图像基准数据集上，该方法始终优于预训练SAM和最近的点监督分割方法。

Conclusion: 自提示和语义对齐为遥感应用中基础分割模型的可扩展点级适应提供了有效途径。

Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.

</details>


### [3] [Qwen3-VL Technical Report](https://arxiv.org/abs/2511.21631)
*Shuai Bai,Yuxuan Cai,Ruizhe Chen,Keqin Chen,Xionghui Chen,Zesen Cheng,Lianghao Deng,Wei Ding,Chang Gao,Chunjiang Ge,Wenbin Ge,Zhifang Guo,Qidong Huang,Jie Huang,Fei Huang,Binyuan Hui,Shutong Jiang,Zhaohai Li,Mingsheng Li,Mei Li,Kaixin Li,Zicheng Lin,Junyang Lin,Xuejing Liu,Jiawei Liu,Chenglong Liu,Yang Liu,Dayiheng Liu,Shixuan Liu,Dunjie Lu,Ruilin Luo,Chenxu Lv,Rui Men,Lingchen Meng,Xuancheng Ren,Xingzhang Ren,Sibo Song,Yuchong Sun,Jun Tang,Jianhong Tu,Jianqiang Wan,Peng Wang,Pengfei Wang,Qiuyue Wang,Yuxuan Wang,Tianbao Xie,Yiheng Xu,Haiyang Xu,Jin Xu,Zhibo Yang,Mingkun Yang,Jianxin Yang,An Yang,Bowen Yu,Fei Zhang,Hang Zhang,Xi Zhang,Bo Zheng,Humen Zhong,Jingren Zhou,Fan Zhou,Jing Zhou,Yuanzhi Zhu,Ke Zhu*

Main category: cs.CV

TL;DR: Qwen3-VL是Qwen系列中最强大的视觉语言模型，支持256K令牌的交错上下文，在文本理解、长上下文处理和多模态推理方面表现卓越，提供密集和MoE两种架构变体。


<details>
  <summary>Details</summary>
Motivation: 开发一个在文本理解、长上下文处理和多模态推理方面都表现卓越的视觉语言模型，满足现实工作流程中图像推理、智能决策和多模态代码智能的需求。

Method: 采用增强的交错MRoPE进行时空建模，集成DeepStack以利用多级ViT特征加强视觉语言对齐，以及基于文本的时间对齐技术来提升视频处理精度。

Result: 在多个多模态基准测试中取得领先性能，特别是在MMMU和视觉数学基准测试中表现突出，在密集和MoE架构下均实现优越性能。

Conclusion: Qwen3-VL可作为现实工作流程中图像推理、智能决策和多模态代码智能的基础引擎，具有广泛的应用前景。

Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.

</details>


### [4] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 提出了一种新颖的AI错误纠正系统，通过少样本学习在资源受限设备上实现高效错误修正，结合服务器端基础模型训练和设备端原型分类，无需模型重训练即可更新原型。


<details>
  <summary>Details</summary>
Motivation: AI模型在移动设备上的预测错误会降低用户体验，现有解决方案主要关注错误检测而缺乏高效的纠正机制，特别是在资源受限设备上。

Method: 系统包含两个关键组件：(1) 服务器端管道利用知识蒸馏将基础模型的鲁棒特征表示转移到设备兼容架构；(2) 设备端机制通过原型适配实现超高效错误纠正。

Result: 在Food-101和Flowers-102数据集上，单样本场景下实现超过50%的错误纠正率，遗忘率低于0.02%，计算开销可忽略不计。Android演示应用验证了系统的实用性。

Conclusion: 该系统证明了在资源受限设备上通过原型更新的方式实现高效AI错误纠正的可行性，具有实际应用价值。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


### [5] [CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow](https://arxiv.org/abs/2511.21653)
*Ruisheng Han,Kanglei Zhou,Shuang Chen,Amir Atapour-Abarghouei,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: CaFlow是一个用于长期动作质量评估的统一框架，结合了反事实去混淆和双向时间条件流，通过因果反事实正则化和双向流模块提升长期动作评估的鲁棒性和表示一致性。


<details>
  <summary>Details</summary>
Motivation: 长期动作质量评估（如花样滑冰、艺术体操）面临建模长时程动态和对抗上下文混淆因子的挑战，现有方法依赖昂贵标注或单向时序建模，容易受到伪相关性和不稳定长期表示的影响。

Method: 提出CaFlow框架，包含因果反事实正则化（CCR）模块和双向时间条件流（BiT-Flow）模块。CCR以自监督方式解耦因果和混淆特征，通过反事实干预增强因果鲁棒性；BiT-Flow建模前向和后向动态，通过循环一致性约束产生更平滑连贯的表示。

Result: 在多个长期AQA基准测试上的广泛实验表明，CaFlow达到了最先进的性能。

Conclusion: CaFlow通过整合反事实去混淆和双向时序建模，有效解决了长期动作质量评估中的挑战，为细粒度动作评分提供了鲁棒且一致的解决方案。

Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow

</details>


### [6] [Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following](https://arxiv.org/abs/2511.21662)
*Tianyi Xiong,Yi Ge,Ming Li,Zuolong Zhang,Pranav Kulkarni,Kaishen Wang,Qi He,Zeying Zhu,Chenxi Liu,Ruibo Chen,Tong Zheng,Yanshuo Chen,Xiyao Wang,Renrui Zhang,Wenhu Chen,Heng Huang*

Main category: cs.CV

TL;DR: Multi-Crit是一个评估多模态模型作为评判者能力的基准，重点关注模型遵循多样化、细粒度评估标准的能力，涵盖开放式生成和可验证推理任务。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型作为评估系统评判者时，其遵循多样化细粒度评估标准的能力尚未充分探索，需要系统评估其多元标准遵循能力。

Method: 通过严格的数据筛选流程构建Multi-Crit基准，收集具有多标准人工标注的挑战性响应对，并引入三个新指标系统评估多元遵循性、标准切换灵活性和识别标准级偏好冲突的能力。

Result: 对25个LMM的全面分析显示：专有模型在遵循多元标准方面仍有困难，开源模型在灵活遵循多样化标准方面更落后，基于整体判断信号的批评微调增强了视觉基础但无法泛化到多元标准级判断。

Conclusion: Multi-Crit为构建可靠且可引导的多模态AI评估奠定了基础，揭示了当前多模态评判者在遵循多样化评估标准方面的局限性。

Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.

</details>


### [7] [Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21663)
*Naifu Zhang,Wei Tao,Xi Xiao,Qianpu Sun,Yuxin Zheng,Wentao Mo,Peiqiang Wang,Nan Zhang*

Main category: cs.CV

TL;DR: ADVLA是一种针对视觉-语言-动作(VLA)模型的特征空间对抗攻击框架，通过在视觉编码器投影到文本特征空间的特征上直接施加扰动，以低幅度和局部稀疏的方式有效破坏下游动作预测。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法需要昂贵的端到端训练，且往往生成明显的扰动块。为了解决这些限制，需要开发更高效、隐蔽的攻击方法。

Method: 提出ADVLA框架，直接在视觉编码器投影到文本特征空间的特征上施加对抗扰动，采用注意力引导使扰动既集中又稀疏，引入三种策略增强敏感性、强制稀疏性和集中扰动。

Result: 在L∞=4/255约束下，ADVLA结合Top-K掩码修改少于10%的补丁，攻击成功率接近100%。扰动集中在关键区域，在整体图像中几乎不可察觉，单步迭代仅需约0.06秒，显著优于传统基于补丁的攻击方法。

Conclusion: ADVLA在低幅度和局部稀疏条件下有效削弱VLA模型的下游动作预测，避免了传统补丁攻击的高训练成本和明显扰动，展示了攻击VLA特征空间的独特有效性和实用价值。

Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.

</details>


### [8] [Seeing without Pixels: Perception from Camera Trajectories](https://arxiv.org/abs/2511.21681)
*Zihui Xue,Kristen Grauman,Dima Damen,Andrew Zisserman,Tengda Han*

Main category: cs.CV

TL;DR: 本文首次系统研究仅通过相机轨迹（而非像素）感知视频内容的可行性，提出了CamFormer对比学习框架，发现相机轨迹是揭示视频内容的强有力信号。


<details>
  <summary>Details</summary>
Motivation: 探索仅通过相机运动轨迹（而非图像像素）能否感知视频内容，验证"如何移动"是否能揭示"在做什么"或"在观察什么"。

Method: 提出对比学习框架训练CamFormer编码器，将相机位姿轨迹投影到联合嵌入空间，与自然语言对齐。

Result: 相机轨迹是异常丰富的信息信号，能够有效揭示视频内容，学习到的表示在跨模态对齐、分类和时间分析等任务中表现优异，且对不同的相机位姿估计方法具有鲁棒性。

Conclusion: 相机轨迹是一种轻量级、鲁棒且多功能的视频内容感知模态，"如何移动"确实能够揭示"在做什么"或"在观察什么"。

Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

</details>


### [9] [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/abs/2511.21691)
*Yusuf Dalva,Guocheng Gordon Qian,Maya Goldenberg,Tsai-Shien Chen,Kfir Aberman,Sergey Tulyakov,Pinar Yanardag,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: Canvas-to-Image是一个统一框架，将文本提示、主题参考、空间布局、姿态约束和布局注释等多种控制信号整合到单一画布界面中，通过多任务画布训练策略实现高质量的多模态图像生成。


<details>
  <summary>Details</summary>
Motivation: 现代扩散模型在生成高质量多样化图像方面表现出色，但在处理高保真度的组合和多模态控制方面仍有困难，特别是当用户同时指定文本提示、主题参考、空间安排、姿态约束和布局注释时。

Method: 将多样控制信号编码为单一复合画布图像，模型可直接解释进行集成视觉空间推理；策划多任务数据集并提出了多任务画布训练策略，在统一学习范式中优化扩散模型以联合理解和集成异构控制。

Result: 在包括多人组合、姿态控制组合、布局约束生成和多控制生成等具有挑战性的基准测试中，Canvas-to-Image在身份保持和控制依从性方面显著优于最先进的方法。

Conclusion: 联合训练使Canvas-to-Image能够跨多个控制模态进行推理，而不是依赖任务特定的启发式方法，在推理过程中能够很好地泛化到多控制场景。

Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [10] [Entropy Coding for Non-Rectangular Transform Blocks using Partitioned DCT Dictionaries for AV1](https://arxiv.org/abs/2511.21609)
*Priyanka Das,Tim Classen,Mathias Wien*

Main category: eess.IV

TL;DR: 本文提出了一种针对非矩形分区变换系数的高效熵编码方法，通过有效建模这些系数的特性，在实验设置中实现了显著的理论码率节省。


<details>
  <summary>Details</summary>
Motivation: 当前视频编码标准如VVC和AV1采用非矩形分区结合平滑融合技术，但缺乏相应的变换支持。现有方法使用2D DCT基的相同分区来稀疏表示非矩形信号，但现有熵编码方案主要针对DCT系数设计，无法最优编码这些变换系数。

Method: 设计了一种专门针对非矩形分区变换系数的熵编码方法，通过有效建模这些系数的特性来优化编码效率。该方法利用条件熵估计理论码率节省，特别适用于与DCT差异较大的场景。

Result: 该方法在实验设置中提供了显著的理论码率节省，特别是在与DCT特性差异较大的场景下效果更为明显。

Conclusion: 所提出的熵编码方法能够高效编码非矩形分区变换系数，为视频编码标准中非矩形变换技术的实际应用提供了可行的解决方案，同时保持了解码器改动最小的设计优势。

Abstract: Recent video codecs such as VVC and AV1 apply a Non-rectangular (NR) partitioning to combine prediction signals using a smooth blending around the boundary, followed by a rectangular transform on the whole block. The NR signal transformation is not yet supported. A transformation technique that applies the same partitioning to the 2D Discrete Cosine Transform (DCT) bases and finds a sparse representation of the NR signal in such a dictionary showed promising gains in an experimental setup outside the reference software. This method uses the regular inverse transformation at the decoder to reconstruct a rectangular signal and discards the signal outside the region of interest. This design is appealing due to the minimal changes required at the decoder. However, current entropy coding schemes are not well-suited for optimally encoding these coefficients because they are primarily designed for DCT coefficients. This work introduces an entropy coding method that efficiently codes these transform coefficients by effectively modeling their properties. The design offers significant theoretical rate savings, estimated using conditional entropy, particularly for scenarios that are more dissimilar to DCT in an experimental setup.

</details>
