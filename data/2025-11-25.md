<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 258]
- [cs.RO](#cs.RO) [Total: 56]
- [cs.HC](#cs.HC) [Total: 16]
- [eess.IV](#eess.IV) [Total: 11]
- [physics.med-ph](#physics.med-ph) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks](https://arxiv.org/abs/2511.17576)
*Rayan Aldajani*

Main category: cs.CV

TL;DR: 本研究评估了使用正面身体图像和基本人体测量数据的人工智能模型作为低成本体脂率估算替代方法的可行性，图像模型取得了RMSE 4.44%和R² 0.807的良好结果。


<details>
  <summary>Details</summary>
Motivation: 传统金标准体脂测量方法（如DEXA扫描）昂贵且难以普及，需要开发低成本、易获取的替代方案来支持大众健康管理。

Method: 开发了两种方法：基于ResNet的图像模型和使用人体测量数据（体重、身高、颈围、踝围、腕围）的回归模型，并提出了多模态融合框架。数据集包含535个样本，其中282个来自Reddit的图像数据和253个带人体测量数据的案例。

Result: 图像模型在体脂率估算方面表现良好，RMSE为4.44%，R²为0.807，表明AI模型能够提供准确的体脂估计。

Conclusion: AI辅助模型能够提供可访问且低成本的体脂估算，为未来健康和健身领域的消费者应用提供了支持。

Abstract: Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.

</details>


### [2] [Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding](https://arxiv.org/abs/2511.17596)
*Yassir Benhammou,Suman Kalyan,Sujay Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种多模态自编码器（MMAE），用于学习文本、音频和视觉数据的统一表示，实现广播内容元数据提取和语义聚类的端到端自动化。


<details>
  <summary>Details</summary>
Motivation: 广播和媒体组织日益依赖AI自动化内容索引、标记和元数据生成，但现有AI系统通常只处理单一模态，限制了其对广播材料中复杂跨模态关系的理解。

Method: 使用多模态自编码器（MMAE），在LUMA数据集上训练，通过最小化跨模态联合重构损失来发现模态不变的语义结构，无需依赖大型配对或对比数据集。

Result: 与线性基线相比，在聚类和对齐指标（Silhouette、ARI、NMI）上显示出显著改进，表明基于重构的多模态嵌入可作为广播档案中可扩展元数据生成和跨模态检索的基础。

Conclusion: 重构驱动的多模态学习有潜力增强现代广播工作流程中的自动化、可搜索性和内容管理效率。

Abstract: Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.

</details>


### [3] [BCWildfire: A Long-term Multi-factor Dataset and Deep Learning Benchmark for Boreal Wildfire Risk Prediction](https://arxiv.org/abs/2511.17597)
*Zhengsen Xu,Sibo Cheng,Hongjie He,Lanying Wang,Wentao Sun,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 本文提出了一个覆盖25年、每日分辨率的野火数据集，包含38个协变量，评估了多种时间序列预测模型在野火风险预测中的表现。


<details>
  <summary>Details</summary>
Motivation: 野火风险预测因燃料条件、气象、地形和人类活动等复杂相互作用而具有挑战性，且缺乏支持长期时间建模、大规模空间覆盖和多模态驱动因素的公开基准数据集。

Method: 构建了一个覆盖不列颠哥伦比亚省及周边地区2.4亿公顷的25年每日分辨率野火数据集，包含38个协变量，评估了CNN-based、linear-based、Transformer-based和Mamba-based等时间序列预测模型。

Result: 创建了一个包含活跃火点检测、天气变量、燃料条件、地形特征和人为因素的综合数据集，并比较了不同模型架构的性能。

Conclusion: 该数据集填补了野火预测领域基准数据的空白，为研究多模态驱动因素和长期时间建模提供了重要资源。

Abstract: Wildfire risk prediction remains a critical yet challenging task due to the complex interactions among fuel conditions, meteorology, topography, and human activity. Despite growing interest in data-driven approaches, publicly available benchmark datasets that support long-term temporal modeling, large-scale spatial coverage, and multimodal drivers remain scarce. To address this gap, we present a 25-year, daily-resolution wildfire dataset covering 240 million hectares across British Columbia and surrounding regions. The dataset includes 38 covariates, encompassing active fire detections, weather variables, fuel conditions, terrain features, and anthropogenic factors. Using this benchmark, we evaluate a diverse set of time-series forecasting models, including CNN-based, linear-based, Transformer-based, and Mamba-based architectures. We also investigate effectiveness of position embedding and the relative importance of different fire-driving factors. The dataset and the corresponding code can be found at https://github.com/SynUW/mmFire

</details>


### [4] [Robustness of Structured Data Extraction from Perspectively Distorted Documents](https://arxiv.org/abs/2511.17607)
*Hyakka Nakada,Yoshiyasu Tanaka*

Main category: cs.CV

TL;DR: 本文研究了透视失真对多模态大语言模型（如Gemini-1.5-pro）在文档数据提取任务中性能的影响，发现结构识别准确率显著下降，并提出通过简单的旋转校正可以改善性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的文档图像通常不仅存在平面内旋转，还存在透视失真，这些扰动会影响多模态大语言模型的数据提取准确性，但目前缺乏系统研究。

Method: 通过观察典型文档图像失真，发现大多数近似遵循等腰梯形变换，将独立参数从8个减少到2个（旋转角度和失真比例），然后在合成生成的样本文档上提取特定实体。

Result: 文档失真显著降低了结构识别准确率（与阅读顺序正确性相关），而字符识别准确率受影响较小；通过简单的旋转校正可以改善结构识别准确率。

Conclusion: 透视失真对多模态大语言模型的OCR任务性能有显著影响，特别是结构识别准确率，但可以通过旋转校正来缓解，这对多模态LLMs在OCR任务中的实际应用具有重要意义。

Abstract: Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.

</details>


### [5] [Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression](https://arxiv.org/abs/2511.17612)
*Siddiqua Namrah*

Main category: cs.CV

TL;DR: 提出了一种完全无监督的多阶段深度学习框架，用于增强低光照交通图像，通过分解图像为光照和反射率分量，并使用三个专门模块进行渐进式优化。


<details>
  <summary>Details</summary>
Motivation: 低光照交通图像在自动驾驶、智能交通和城市监控系统中存在能见度差、噪声、运动模糊、光照不均和眩光等问题，影响目标检测和场景理解任务的可靠性。

Method: 采用多阶段深度学习框架，将图像分解为光照和反射率分量，通过三个专门模块进行优化：光照适应模块用于全局和局部亮度校正；反射率恢复模块使用空间通道注意力进行噪声抑制和结构细节恢复；过曝光补偿模块用于重建饱和区域和平衡场景亮度。使用自监督重建、反射率平滑度、感知一致性和领域感知正则化损失进行训练。

Result: 在通用和交通专用数据集上的实验表明，该方法在定量指标（PSNR、SSIM、LPIPS、NIQE）和定性视觉质量方面均优于现有最先进方法。

Conclusion: 该方法能有效增强低光照交通场景的可见度，保持结构完整性，并提高下游感知任务的可靠性。

Abstract: Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.

</details>


### [6] [Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2511.17615)
*Young-Beom Woo*

Main category: cs.CV

TL;DR: PnP-MIX是一种无需调优的多概念个性化文本到图像生成方法，通过引导外观注意力、掩码引导噪声混合和背景稀释++策略，解决多对象场景中个性化区域和非个性化区域的保真度问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂多对象场景中表现不佳，会导致个性化区域和非个性化区域发生意外改变，破坏提示结构完整性并造成语义不一致。

Method: 1）使用引导外观注意力准确反映每个个性化概念的外观；2）采用掩码引导噪声混合策略保护非个性化区域完整性；3）提出背景稀释++策略减少概念泄漏。

Result: 实验结果表明PnP-MIX在单概念和多概念个性化场景中均优于现有方法，展现了其鲁棒性和优越性能。

Conclusion: PnP-MIX提供了一种高效、无需额外模型调优的多概念个性化图像生成解决方案，能够保持高保真度和语义一致性。

Abstract: Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.

</details>


### [7] [Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach](https://arxiv.org/abs/2511.17618)
*Ju-Young Oh*

Main category: cs.CV

TL;DR: FIQ框架通过生成描述性Q&A对来增强视频问答模型的推理能力，提出VQ-CAlign模块对齐问题嵌入与视觉特征，在SUTD-TrafficQA数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统VQA方法依赖事件中心的Q&A对，缺乏对场景基础信息（如物体类别、空间配置、视觉属性）的全面理解，限制了模型的泛化和推理能力。

Method: 提出FIQ框架：1）从视频中提取描述性信息生成Q&A对，丰富数据集；2）设计VQ-CAlign模块对齐任务特定问题嵌入与视觉特征，保留上下文线索。

Result: 在SUTD-TrafficQA数据集上的实验结果表明，FIQ超越了现有基线方法，实现了最先进的性能。

Conclusion: FIQ通过增强对视频内容的基础理解，显著提升了VQA模型的推理能力和泛化性能，验证了描述性信息生成和对齐机制的有效性。

Abstract: Conventional VQA approaches primarily rely on question-answer (Q&A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.

</details>


### [8] [Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds](https://arxiv.org/abs/2511.17619)
*Qinghao Meng,Junbo Yin,Jianbing Shen,Yunde Jia*

Main category: cs.CV

TL;DR: 该论文提出了一种基于角点对齐回归的LiDAR 3D目标检测方法，通过将预测目标从不稳定的中心点转移到几何信息丰富的角点，解决了中心对齐回归在稀疏区域不稳定的问题，并支持弱监督学习。


<details>
  <summary>Details</summary>
Motivation: 传统的中心对齐回归在LiDAR 3D检测中存在根本性不稳定问题，因为物体中心经常落在BEV视图的稀疏或空区域，导致边界框预测噪声大且不准确。

Method: 提出角点对齐回归方法，将预测目标从不稳定的中心转移到几何信息丰富的角点；利用角点和图像2D框之间的几何约束，从角点标注中恢复3D边界框的部分参数；设计简单有效的角点感知检测头，可插入现有检测器中。

Result: 在KITTI数据集上的实验表明，该方法比基于中心的基线提高了3.5% AP，仅使用BEV角点点击就达到了全监督精度的83%。

Conclusion: 角点感知回归策略有效解决了中心对齐回归的不稳定性问题，同时支持弱监督学习，在LiDAR 3D目标检测中表现出色。

Abstract: Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.

</details>


### [9] [Efficient Score Pre-computation for Diffusion Models via Cross-Matrix Krylov Projection](https://arxiv.org/abs/2511.17634)
*Kaikwan Lau,Andrew S. Na,Justin W. L. Wan*

Main category: cs.CV

TL;DR: 提出了一种加速基于分数的扩散模型的新框架，通过将稳定扩散模型转换为Fokker-Planck形式，并开发跨矩阵Krylov投影方法来利用矩阵间的数学相似性，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 标准稳定扩散模型转换为Fokker-Planck形式后需要为每张图像求解大型线性系统，当处理大量图像时会产生高昂的计算成本，因此需要开发高效的求解方法。

Method: 将稳定扩散模型转换为Fokker-Planck形式，提出跨矩阵Krylov投影方法，利用从"种子"矩阵构建的共享子空间来快速求解后续"目标"矩阵。

Result: 实验显示该方法比标准稀疏求解器减少15.8%到43.7%的时间，在去噪任务中比DDPM基线快达115倍，在固定计算预算下能生成高质量图像而DDPM无法生成可识别内容。

Conclusion: 该方法是一种在资源受限环境下进行高效生成的实际可行方法，显著提升了扩散模型的效率。

Abstract: This paper presents a novel framework to accelerate score-based diffusion models. It first converts the standard stable diffusion model into the Fokker-Planck formulation which results in solving large linear systems for each image. For training involving many images, it can lead to a high computational cost. The core innovation is a cross-matrix Krylov projection method that exploits mathematical similarities between matrices, using a shared subspace built from ``seed" matrices to rapidly solve for subsequent ``target" matrices. Our experiments show that this technique achieves a 15.8\% to 43.7\% time reduction over standard sparse solvers. Additionally, we compare our method against DDPM baselines in denoising tasks, showing a speedup of up to 115$\times$. Furthermore, under a fixed computational budget, our model is able to produce high-quality images while DDPM fails to generate recognizable content, illustrating our approach is a practical method for efficient generation in resource-limited settings.

</details>


### [10] [Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification](https://arxiv.org/abs/2511.17635)
*Max A. Nelson,Elif Keles,Eminenur Sen Tasci,Merve Yazol,Halil Ertugrul Aktas,Ziliang Hong,Andrea Mia Bejar,Gorkem Durak,Oznur Leman Boyunaga,Ulas Bagci*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级增强策略UPMI，通过在低维元特征空间而非图像空间操作，解决儿科胰腺炎诊断中样本有限和多模态成像复杂性的挑战。


<details>
  <summary>Details</summary>
Motivation: 儿科胰腺炎是一种进行性炎症疾病，临床诊断面临挑战。基于机器学习的方法由于样本有限和多模态成像复杂性也面临诊断困难。

Method: 提出上游概率元插补(UPMI)策略，使用模态特异性逻辑回归生成概率输出，转换为7维元特征向量，然后使用类条件高斯混合模型在交叉验证折叠内采样合成元特征，结合真实元特征训练随机森林元分类器。

Result: 在67名儿科患者的配对T1W/T2W MRI数据上，UPMI实现了0.908±0.072的平均AUC，相比仅使用真实数据的基线(AUC 0.864±0.061)获得了约5%的相对增益。

Conclusion: UPMI是一种有效的轻量级数据增强策略，能够显著提升儿科胰腺炎的多模态MRI诊断性能。

Abstract: Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\pm$ 0.072, a $\sim$5% relative gain over a real-only baseline (AUC 0.864 $\pm$ 0.061).

</details>


### [11] [SWITCH: Benchmarking Modeling and Handling of Tangible Interfaces in Long-horizon Embodied Scenarios](https://arxiv.org/abs/2511.17649)
*Jieru Lin,Zhiwei Yu,Börje F. Karlsson*

Main category: cs.CV

TL;DR: SWITCH是一个具身智能基准测试，通过351个任务评估AI在真实世界交互中的能力，包括任务感知VQA、语义UI接地、动作生成、状态转换预测和结果验证。


<details>
  <summary>Details</summary>
Motivation: 自主智能需要与现实世界及其基础设施有效交互，而现有基准很少测试在具体环境中的接地、部分可观测性和事后验证能力。

Method: 创建SWITCH基准测试，包含SWITCH-Basic迭代，评估五个互补能力：任务感知VQA、语义UI接地、动作生成、状态转换预测和结果验证，使用自我中心RGB视频输入和设备多样性。

Result: 在98个真实设备和家电的351个任务中，商业和开源LMM在单步交互上表现不一致，经常过度依赖文本线索而未能充分利用视觉或视频证据。

Conclusion: SWITCH提供了数据、代码和保留集，支持可重复评估和社区贡献，以推动更具挑战性的基准迭代和训练数据集的创建。

Abstract: Autonomous intelligence requires not only perception and reasoning, but critically, effective interaction with the existing world and its infrastructure. Everyday environments are rich in tangible control interfaces (TCIs), e.g., light switches, appliance panels, and embedded GUIs, that demand commonsense and physics reasoning, but also causal prediction and outcome verification in time and space (e.g., delayed heating, remote lights). Moreover, failures here have potential safety implications, yet current benchmarks rarely test grounding, partial observability (video), or post-hoc verification in situated settings. We introduce SWITCH (Semantic World Interface Tasks for Control and Handling), an embodied, task-driven benchmark created through iterative releases to probe these gaps. Its first iteration, SWITCH-Basic, evaluates five complementary abilities:task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification, under egocentric RGB video input and device diversity. Across 351 tasks spanning 98 real devices and appliances, commercial and open LMMMs exhibit inconsistent performance even on single-step interactions, often over-relying on textual cues and under-using visual or video evidence (and high aggregate scores can mask such failures). SWITCH provides data, code, and held-out splits to enable reproducible evaluation and community contributions toward more challenging future iterations of the benchmark and the creation of training datasets. Benchmark resources are available at: https://github.com/BAAI-Agents/SWITCH.

</details>


### [12] [Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment](https://arxiv.org/abs/2511.17655)
*Md. Mohaiminul Islam,Md. Mofazzal Hossen,Maher Ali Rusho,Nahiyan Nazah Ridita,Zarin Tasnia Shanta,Md. Simanto Haider,Ahmed Faizul Haque Dhrubo,Md. Khurshid Jahan,Mohammad Abdul Qayum*

Main category: cs.CV

TL;DR: 该研究开发了一个完整的深度学习系统，用于从MRI图像自动分类脑肿瘤，包含六个基准架构和自定义紧凑CNN，在标准化评估、可解释性、轻量模型和全面评估方面取得进展。


<details>
  <summary>Details</summary>
Motivation: 开发一个完整的深度学习系统来解决脑肿瘤MRI图像自动分类问题，特别关注标准化评估、模型可解释性、轻量化部署以及在资源受限环境中的应用。

Method: 使用六个基准架构（五个ImageNet预训练模型和自定义紧凑CNN），采用标准化预处理、训练协议（AdamW优化器、余弦退火学习率、早停机制），并使用Grad-CAM和GradientShap进行可解释性分析。

Result: Inception-ResNet V2达到99.53%的测试准确率，自定义紧凑CNN（1.31M参数）达到96.49%准确率，比Inception-ResNet V2小100倍，在边缘设备上实现375ms实时推理。

Conclusion: 该研究提供了一个端到端的解决方案，综合考虑准确性、可解释性和部署能力，为先进和低资源医疗系统中的性能评估和部署创建了必要框架，适合临床筛查和分诊级别的应用。

Abstract: Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.

</details>


### [13] [MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation](https://arxiv.org/abs/2511.17668)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: 提出了MedPEFT-CL框架，通过双阶段架构解决医学视觉语言分割模型在适应新解剖结构时的灾难性遗忘问题，实现高效学习新任务并保留先前知识。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言分割模型在适应新解剖结构时遭受灾难性遗忘，需要完全重新训练，限制了临床部署。针对医学视觉语言任务的持续学习方法研究不足。

Method: 基于CLIPSeg的双阶段架构：自适应学习阶段使用语义相似度适配器分配和参数高效微调；知识巩固阶段采用双向Fisher记忆协调。包含语义驱动适配器分配、双模态LoRA适应和双向Fisher记忆协调。

Result: 在多样化医学数据集上的广泛实验显示，该框架在最小参数开销下实现了优越的遗忘缓解和性能保持。

Conclusion: MedPEFT-CL框架在医学视觉语言场景中实现了有效的持续学习，显著减少可训练参数同时保持跨模态学习能力。

Abstract: Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.

</details>


### [14] [Person Recognition in Aerial Surveillance: A Decade Survey](https://arxiv.org/abs/2511.17674)
*Kien Nguyen,Feng Liu,Clinton Fookes,Sridha Sridharan,Xiaoming Liu,Arun Ross*

Main category: cs.CV

TL;DR: 本文对过去10年150多篇关于以人为中心的空中监视任务的论文进行了系统性综述，重点分析了无人机等空中平台在人类检测、识别和重识别任务中的计算机视觉和机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 随着空中平台和成像传感器的快速发展，空中监视因其在规模、移动性、部署和隐蔽观察能力方面的优势而展现出新的应用形式。本文旨在从计算机视觉和机器学习角度对空中监视任务进行深入系统的技术分析。

Method: 首先识别空中环境相比地面环境在执行人类检测、识别和重识别任务时的独特挑战，然后整理分析每个任务的公开空中数据集，深入探讨现有方法如何应对空中挑战并提出改进技术。

Result: 提供了150多篇论文的全面综述，系统分析了空中监视任务的技术现状，包括数据集收集和方法评估。

Conclusion: 通过讨论现有差距和开放研究问题，为未来研究方向提供信息，推动空中监视技术的发展。

Abstract: The rapid emergence of airborne platforms and imaging sensors is enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment, and covert observation capabilities. This paper provides a comprehensive overview of 150+ papers over the last 10 years of human-centric aerial surveillance tasks from a computer vision and machine learning perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs, and other airborne platforms. The object of interest is humans, where human subjects are to be detected, identified, and re-identified. More specifically, for each of these tasks, we first identify unique challenges in performing these tasks in an aerial setting compared to the popular ground-based setting and subsequently compile and analyze aerial datasets publicly available for each task. Most importantly, we delve deep into the approaches in the aerial surveillance literature with a focus on investigating how they presently address aerial challenges and techniques for improvement. We conclude the paper by discussing the gaps and open research questions to inform future research avenues.

</details>


### [15] [Data Augmentation Strategies for Robust Lane Marking Detection](https://arxiv.org/abs/2511.18668)
*Flora Lian,Dinh Quang Huynh,Hector Penades,J. Stephany Berrio Perez,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 提出基于生成式AI的数据增强方法，通过几何透视变换、AI修复和车辆覆盖来模拟特定视角，提升车道检测模型在侧置摄像头场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决车道检测模型在公共数据集（如CULane）上训练后，无法很好泛化到不同摄像头视角（特别是侧置摄像头）的领域偏移问题。

Method: 使用生成式AI数据增强管道，结合几何透视变换、AI驱动的修复技术和车辆车身覆盖，模拟部署特定的视角同时保持车道连续性。

Result: 在SCNN和UFLDv2两个先进模型上评估，增强数据训练后模型在不同条件（包括阴影）下表现出更好的鲁棒性，精确率、召回率和F1分数均有提升。

Conclusion: 该方法通过弥合公开数据集与部署特定场景之间的差距，为提高车道检测在试点部署场景中的可靠性提供了可扩展且实用的框架。

Abstract: Robust lane detection is essential for advanced driver assistance and autonomous driving, yet models trained on public datasets such as CULane often fail to generalise across different camera viewpoints. This paper addresses the challenge of domain shift for side-mounted cameras used in lane-wheel monitoring by introducing a generative AI-based data enhancement pipeline. The approach combines geometric perspective transformation, AI-driven inpainting, and vehicle body overlays to simulate deployment-specific viewpoints while preserving lane continuity. We evaluated the effectiveness of the proposed augmentation in two state-of-the-art models, SCNN and UFLDv2. With the augmented data trained, both models show improved robustness to different conditions, including shadows. The experimental results demonstrate gains in precision, recall, and F1 score compared to the pre-trained model.
  By bridging the gap between widely available datasets and deployment-specific scenarios, our method provides a scalable and practical framework to improve the reliability of lane detection in a pilot deployment scenario.

</details>


### [16] [Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models](https://arxiv.org/abs/2511.17681)
*Weiyi Lv,Ning Zhang,Hanyang Sun,Haoran Jiang,Kai Zhao,Jing Xiao,Dan Zeng*

Main category: cs.CV

TL;DR: 本文提出VMRMOT框架，通过引入运动模态来增强视觉模态与语言参考之间的对齐，解决现有RMOT基准中静态描述无法捕捉物体运动动态变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RMOT基准仅描述物体外观、相对位置和初始运动状态，这种静态调节无法捕捉物体运动的动态变化（如速度变化和运动方向转变），导致静态参考与动态视觉模态之间存在时间差异，限制了多模态跟踪性能。

Method: 提出VMRMOT框架，集成从物体动态中提取的运动模态，通过多模态大语言模型增强视觉模态与语言参考的对齐。具体包括：从物体动态行为中提取运动感知描述，利用MLLMs的时间推理能力提取运动特征；设计视觉-运动-参考对齐模块进行层次化对齐；开发运动引导预测头来利用运动模态增强预测性能。

Result: 在多个RMOT基准上的广泛实验表明，VMRMOT优于现有的最先进方法。

Conclusion: VMRMOT是首个在RMOT任务中采用MLLMs进行视觉-参考对齐的方法，通过引入运动模态有效解决了静态参考与动态视觉之间的时间差异问题，显著提升了多模态跟踪性能。

Abstract: Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.

</details>


### [17] [Understanding Counting Mechanisms in Large Language and Vision-Language Models](https://arxiv.org/abs/2511.17699)
*Hosein Hasani,Amirmohammad Izadi,Fatemeh Askari,Mobin Bagherian,Sadegh Mohammadian,Mohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 本文研究了大型语言模型和视觉语言模型在计数任务中如何表示和处理数字信息，通过因果干预和激活修补分析发现模型通过层级结构逐步构建数字表示，并识别出内部计数器机制。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs和LVLMs在计数任务中如何表示和计算数字信息，理解其内部工作机制。

Method: 使用重复文本和视觉项目的受控实验，通过因果干预和激活修补分析模型行为，开发了CountScope工具进行机制可解释性分析。

Result: 发现单个token或视觉特征编码潜在位置计数信息，可跨上下文提取和转移；识别出内部计数器机制，存储在最终token或区域中；视觉语言模型中数字信息也出现在视觉嵌入中。

Conclusion: 计数在LLMs中作为结构化、分层过程出现，在LVLMs中遵循相同模式，但受视觉编码器特性影响；模型依赖结构线索如分隔符来跟踪项目计数。

Abstract: This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.

</details>


### [18] [Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions](https://arxiv.org/abs/2511.17722)
*Saurav Sengupta,Nazanin Moradinasab,Jiebei Liu,Donald E. Brown*

Main category: cs.CV

TL;DR: 该研究构建了合成基准数据集和评估框架，分析视觉语言模型在计数任务中的表现如何随图像和提示属性变化，并通过注意力干预来调节模型对视觉标记的关注。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在回答关于图像视觉属性的查询时，往往依赖训练中学到的固有偏见，特别是在需要聚焦图像特定区域进行计数的任务中，这些偏见会被放大。

Method: 开发合成基准数据集和评估框架，使用开源视觉语言模型分析注意力分配如何随输入参数变化，并实施基于注意力的干预来调节不同层级的视觉标记关注度。

Result: 实验表明，虽然视觉语言模型的计数性能仍然具有挑战性，特别是在高视觉或语言复杂度条件下，但某些注意力干预可以在计数性能上带来适度的提升。

Conclusion: 视觉语言模型在计数任务中面临挑战，但通过注意力干预可以一定程度上改善其性能，特别是在处理复杂视觉和语言条件时。

Abstract: Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.

</details>


### [19] [AngioDG: Interpretable Channel-informed Feature-modulated Single-source Domain Generalization for Coronary Vessel Segmentation in X-ray Angiography](https://arxiv.org/abs/2511.17724)
*Mohammad Atwany,Mojtaba Lashgari,Robin P. Choudhury,Vicente Grau,Abhirup Banerjee*

Main category: cs.CV

TL;DR: AngioDG是一种用于X射线冠状动脉造影血管分割的单源域泛化方法，通过通道正则化策略提升模型泛化能力，在6个数据集上取得了最佳域外性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，XCA是实时心脏介入的金标准。血管分割有助于定量评估，但由于成像协议和患者差异导致的域偏移，以及标注数据缺乏，使得单源域泛化成为必要解决方案。

Method: 提出AngioDG方法，通过通道正则化策略识别早期特征通道对任务特定指标的贡献，重新加权通道以校准和放大域不变特征，同时减弱域特定特征。

Result: 在6个X射线血管造影数据集上评估，相比其他方法取得了最佳域外性能，同时保持一致的域内测试性能。

Conclusion: AngioDG通过通道正则化有效提升了XCA血管分割模型的泛化能力，在域偏移情况下表现优异。

Abstract: Cardiovascular diseases are the leading cause of death globally, with X-ray Coronary Angiography (XCA) as the gold standard during real-time cardiac interventions. Segmentation of coronary vessels from XCA can facilitate downstream quantitative assessments, such as measurement of the stenosis severity and enhancing clinical decision-making. However, developing generalizable vessel segmentation models for XCA is challenging due to variations in imaging protocols and patient demographics that cause domain shifts. These limitations are exacerbated by the lack of annotated datasets, making Single-source Domain Generalization (SDG) a necessary solution for achieving generalization. Existing SDG methods are largely augmentation-based, which may not guarantee the mitigation of overfitting to augmented or synthetic domains. We propose a novel approach, ``AngioDG", to bridge this gap by channel regularization strategy to promote generalization. Our method identifies the contributions of early feature channels to task-specific metrics for DG, facilitating interpretability, and then reweights channels to calibrate and amplify domain-invariant features while attenuating domain-specific ones. We evaluate AngioDG on 6 x-ray angiography datasets for coronary vessels segmentation, achieving the best out-of-distribution performance among the compared methods, while maintaining consistent in-domain test performance.

</details>


### [20] [The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation](https://arxiv.org/abs/2511.17727)
*Victor Li,Naveenraj Kamalakannan,Avinash Parnandi,Heidi Schambra,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 本研究评估了视觉语言模型(VLMs)在卒中康复视频分析中的应用，发现当前VLM在精细运动理解方面存在局限，但通过优化提示和后处理，在高级活动分类、运动检测和剂量估算方面显示出潜力。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在数字健康领域的应用潜力，特别是解决卒中康复中的两个关键挑战：从视频中自动量化康复剂量和运动障碍。

Method: 将康复剂量和运动障碍量化问题构建为运动识别任务，使用VLM框架在29名健康对照和51名卒中幸存者队列上进行评估，采用优化提示和后处理技术。

Result: 当前VLM缺乏精细运动理解能力：剂量估计与排除视觉信息的基线相当，障碍评分无法可靠预测。但通过优化，VLM能够从少量帧中分类高级活动，中等准确度检测运动和抓握，对轻度障碍和健康参与者的剂量计数误差在25%以内。

Conclusion: 研究揭示了VLM在数据驱动卒中康复中的当前局限性和新兴机遇，为更广泛的临床视频分析提供了参考。

Abstract: Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.

</details>


### [21] [VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.17731)
*Lingxiao Li,Yifan Wang,Xinyan Gao,Chen Tang,Xiangyu Yue,Chenyu You*

Main category: cs.CV

TL;DR: VisReason是一个大规模视觉思维链推理数据集，包含489K标注样本和165K专家级子集VisReason-Pro，通过微调Qwen2.5-VL模型显著提升了多模态大语言模型的逐步视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉思维链资源规模小、领域特定或缺乏人类逐步推理结构，限制了多模态大语言模型的视觉推理能力开发。

Method: 构建VisReason大规模数据集，包含多轮人类式推理链，并通过专家级GPT标注器创建VisReason-Pro子集，添加详细推理轨迹和3D空间标注。

Result: 在Qwen2.5-VL模型上微调VisReason和VisReason-Pro后，在逐步视觉推理准确性、可解释性和跨基准泛化方面取得显著提升。

Conclusion: VisReason为培养人类式视觉推理提供了基础，推动了下一代多模态智能的发展。

Abstract: Chain-of-Thought (CoT) prompting has proven remarkably effective for eliciting complex reasoning in large language models (LLMs). Yet, its potential in multimodal large language models (MLLMs) remains largely untapped, hindered by the absence of large-scale datasets that capture the rich, spatially grounded reasoning intrinsic to visual understanding. Existing visual-CoT resources are typically small, domain-specific, or lack the human-like stepwise structure necessary for compositional visual reasoning. In this paper, we introduce VisReason, a large-scale dataset designed to advance visual Chain-of-Thought reasoning. VisReason comprises 489K annotated examples spanning four diverse domains, each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps. Building upon this, we curate VisReason-Pro, a 165K subset produced with a stronger expert-level GPT annotator, enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations. Fine-tuning the state-of-the-art Qwen2.5-VL model on VisReason and VisReason-Pro yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization. These results demonstrate that VisReason equips MLLMs with more systematic and generalizable reasoning capabilities. We envision VisReason as a cornerstone for cultivating human-like visual reasoning, paving the way toward the next generation of multimodal intelligence.

</details>


### [22] [Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders](https://arxiv.org/abs/2511.17735)
*Samuel Stevens,Jacob Beattie,Tanya Berger-Wolf,Yu Su*

Main category: cs.CV

TL;DR: 本文探讨了稀疏自编码器（SAEs）是否能够从基础模型表示中实现开放式特征发现，通过控制性重新发现研究评估了SAE特征与语义概念的对齐情况，并在生态图像上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 科学档案包含海量数据，但现有方法只能针对预设目标提取结构，无法支持未知模式的开放式发现。本文旨在探索稀疏自编码器是否能够实现从基础模型表示中的开放式特征发现。

Method: 使用稀疏自编码器（SAEs）从基础模型表示中学习特征，通过控制性重新发现研究评估特征与语义概念的对齐，并与无标签替代方法在概念对齐指标上进行比较。

Result: 在生态图像应用中，该方法无需分割或部件标签即可发现细粒度解剖结构，提供了具有真实验证的科学案例研究。稀疏分解为探索科学基础模型学习内容提供了实用工具。

Conclusion: 稀疏分解为探索科学基础模型学习内容提供了实用工具，是从确认转向真正发现的重要前提条件。该方法具有领域无关性，可应用于蛋白质、基因组学、天气等其他科学领域的模型。

Abstract: Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.

</details>


### [23] [CORA: Consistency-Guided Semi-Supervised Framework for Reasoning Segmentation](https://arxiv.org/abs/2511.17755)
*Prantik Howlader,Hoang Nguyen-Canh,Srijan Das,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: CORA是一个半监督推理分割框架，通过结合有限标注数据和大量未标注图像，在推理分割任务中实现最先进性能，仅需少量标注即可超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型在指令跟随分割方面取得进展，但泛化能力有限，主要瓶颈在于获取多样化高质量像素标注与丰富语言监督的成本过高，导致在分布偏移下性能脆弱。

Method: CORA包含三个主要组件：1）条件视觉指令编码对象间的空间和上下文关系；2）基于多模态LLM在语义等价查询输出一致性的噪声伪标签过滤器；3）标注样本与伪标注样本间的标记级对比对齐以增强特征一致性。

Result: 在Cityscapes数据集上仅需100张标注图像即可超越基线+2.3%，在PanNuke数据集上仅需180张标注图像提升性能+2.4%。

Conclusion: CORA能够在最小监督下执行稳健的推理分割，在受限标注设置下优于现有基线，实现了最先进的结果。

Abstract: Reasoning segmentation seeks pixel-accurate masks for targets referenced by complex, often implicit instructions, requiring context-dependent reasoning over the scene. Recent multimodal language models have advanced instruction following segmentation, yet generalization remains limited. The key bottleneck is the high cost of curating diverse, high-quality pixel annotations paired with rich linguistic supervision leading to brittle performance under distribution shift. Therefore, we present CORA, a semi-supervised reasoning segmentation framework that jointly learns from limited labeled data and a large corpus of unlabeled images. CORA introduces three main components: 1) conditional visual instructions that encode spatial and contextual relationships between objects; 2) a noisy pseudo-label filter based on the consistency of Multimodal LLM's outputs across semantically equivalent queries; and 3) a token-level contrastive alignment between labeled and pseudo-labeled samples to enhance feature consistency. These components enable CORA to perform robust reasoning segmentation with minimal supervision, outperforming existing baselines under constrained annotation settings. CORA achieves state-of-the-art results, requiring as few as 100 labeled images on Cityscapes, a benchmark dataset for urban scene understanding, surpassing the baseline by $+2.3\%$. Similarly, CORA improves performance by $+2.4\%$ with only 180 labeled images on PanNuke, a histopathology dataset.

</details>


### [24] [Latent Dirichlet Transformer VAE for Hyperspectral Unmixing with Bundled Endmembers](https://arxiv.org/abs/2511.17757)
*Giancarlo Giannetti,Faisal Z. Qureshi*

Main category: cs.CV

TL;DR: 提出LDVAE-T模型用于高光谱解混，结合Transformer的全局上下文建模能力和狄利克雷先验的物理约束，将材料视为捆绑端元而非固定光谱，在三个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像包含丰富的光谱信息，但光谱混合常常掩盖纯材料特征，需要开发能够处理材料固有变异性同时保持物理可解释性的解混方法。

Method: 结合Transformer架构和狄利克雷变分自编码器，在潜在空间施加狄利克雷先验以强制丰度估计的求和为一和非负约束，将端元建模为具有均值和结构化协方差的捆绑光谱。

Result: 在Samson、Jasper Ridge和HYDICE Urban三个基准数据集上，LDVAE-T在丰度估计（均方根误差）和端元提取（光谱角距离）方面持续优于最先进模型。

Conclusion: LDVAE-T通过将材料建模为捆绑端元并施加物理约束，能够有效处理光谱混合问题，同时保持物理可解释性，在高光谱解混任务中表现出优越性能。

Abstract: Hyperspectral images capture rich spectral information that enables per-pixel material identification; however, spectral mixing often obscures pure material signatures. To address this challenge, we propose the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T) for hyperspectral unmixing. Our model combines the global context modeling capabilities of transformer architectures with physically meaningful constraints imposed by a Dirichlet prior in the latent space. This prior naturally enforces the sum-to-one and non-negativity conditions essential for abundance estimation, thereby improving the quality of predicted mixing ratios. A key contribution of LDVAE-T is its treatment of materials as bundled endmembers, rather than relying on fixed ground truth spectra. In the proposed method our decoder predicts, for each endmember and each patch, a mean spectrum together with a structured (segmentwise) covariance that captures correlated spectral variability. Reconstructions are formed by mixing these learned bundles with Dirichlet-distributed abundances garnered from a transformer encoder, allowing the model to represent intrinsic material variability while preserving physical interpretability. We evaluate our approach on three benchmark datasets, Samson, Jasper Ridge, and HYDICE Urban and show that LDVAE-T consistently outperforms state-of-the-art models in abundance estimation and endmember extraction, as measured by root mean squared error and spectral angle distance, respectively.

</details>


### [25] [Deepfake Geography: Detecting AI-Generated Satellite Images](https://arxiv.org/abs/2511.17766)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: 本研究比较了CNN和ViT在检测AI生成卫星图像方面的性能，发现ViT在准确率和鲁棒性上显著优于CNN，主要得益于其建模长距离依赖和全局语义结构的能力。


<details>
  <summary>Details</summary>
Motivation: 随着StyleGAN2和Stable Diffusion等生成模型的快速发展，卫星图像的真实性面临严重威胁，而现有的深度伪造检测方法主要针对人脸图像，卫星图像检测面临地形不一致和结构伪影等独特挑战。

Method: 使用包含13万张标记RGB图像的DM-AER和FSI数据集，系统比较CNN和ViT的性能，并采用Grad-CAM和Chefer注意力归因等架构特定可解释性方法来增强模型透明度。

Result: ViT在准确率上显著优于CNN（95.11% vs 87.02%），在鲁棒性方面也表现更好，能够有效检测合成图像中的结构不一致和重复纹理模式。

Conclusion: ViT在检测AI生成卫星图像方面具有优越性能，未来研究将扩展到多光谱和SAR模态，并整合频域分析以进一步增强检测能力。

Abstract: The rapid advancement of generative models such as StyleGAN2 and Stable Diffusion poses a growing threat to the authenticity of satellite imagery, which is increasingly vital for reliable analysis and decision-making across scientific and security domains. While deepfake detection has been extensively studied in facial contexts, satellite imagery presents distinct challenges, including terrain-level inconsistencies and structural artifacts. In this study, we conduct a comprehensive comparison between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for detecting AI-generated satellite images. Using a curated dataset of over 130,000 labeled RGB images from the DM-AER and FSI datasets, we show that ViTs significantly outperform CNNs in both accuracy (95.11 percent vs. 87.02 percent) and overall robustness, owing to their ability to model long-range dependencies and global semantic structures. We further enhance model transparency using architecture-specific interpretability methods, including Grad-CAM for CNNs and Chefer's attention attribution for ViTs, revealing distinct detection behaviors and validating model trustworthiness. Our results highlight the ViT's superior performance in detecting structural inconsistencies and repetitive textural patterns characteristic of synthetic imagery. Future work will extend this research to multispectral and SAR modalities and integrate frequency-domain analysis to further strengthen detection capabilities and safeguard satellite imagery integrity in high-stakes applications.

</details>


### [26] [Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?](https://arxiv.org/abs/2511.17792)
*Dingrui Wang,Hongyuan Ye,Zhihao Liang,Zhexiao Sun,Zhaowei Lu,Yuchen Zhang,Yuyu Zhao,Yuan Gao,Marvin Seegert,Finn Schäfer,Haotong Qin,Wei Li,Luigi Palmieri,Felix Jahncke,Mattia Piccinini,Johannes Betz*

Main category: cs.CV

TL;DR: Target-Bench是首个专门评估世界模型在真实环境中无地图路径规划能力的基准，包含450个机器人收集的视频序列和SLAM基准轨迹。评估显示当前最先进模型在机器人规划任务中表现有限，但通过微调可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管当前世界模型能生成高度逼真的视频，但它们在机器人路径规划方面的能力尚不清楚且缺乏量化评估。

Method: 构建Target-Bench基准，包含450个视频序列和SLAM基准轨迹；开发评估流程，从生成视频中恢复相机运动，使用5个互补指标量化规划性能；评估Sora 2、Veo 3.1和Wan系列等最先进模型。

Result: 最佳现成模型(Wan2.2-Flash)仅得0.299总分；在数据集上微调5B参数开源模型后达到0.345总分，比基础版本提升400%以上，比最佳现成模型高15%。

Conclusion: 当前世界模型在机器人规划任务中存在显著局限性，但通过针对性微调可大幅提升性能；将开源代码和数据集。

Abstract: While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.

</details>


### [27] [Attention Guided Alignment in Efficient Vision-Language Models](https://arxiv.org/abs/2511.17793)
*Shweta Mahajan,Hoang Le,Hyojin Park,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文分析了高效视觉语言模型中的注意力模式，发现基于拼接的架构难以区分语义匹配和非匹配的图像-文本对，这是导致物体幻觉的关键因素。为此提出了AGE-VLM框架，通过交叉注意力层增强视觉定位能力，显著减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有高效视觉语言模型在视觉和语言信息融合时存在注意力模式问题，导致无法有效区分匹配和非匹配的图像-文本对，这是物体幻觉的主要原因。

Method: 提出AGE-VLM框架，使用交叉注意力层将视觉能力注入预训练的小语言模型中，利用Segment Anything Model的空间知识蒸馏来引导模型关注正确的图像区域。

Result: 在多个视觉中心基准测试中，该方法优于或与先前的高效视觉语言模型相当，显著减少了幻觉现象。

Conclusion: 该研究为未来实现增强视觉和语言理解的视觉语言模型提供了有价值的见解，注意力引导方法能有效提升模型的视觉定位能力。

Abstract: Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability "look" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.

</details>


### [28] [Pillar-0: A New Frontier for Radiology Foundation Models](https://arxiv.org/abs/2511.17803)
*Kumar Krishna Agrawal,Longchao Liu,Long Lian,Michael Nercessian,Natalia Harguindeguy,Yufu Wu,Peter Mikhael,Gigin Lin,Lecia V. Sequist,Florian Fintelmann,Trevor Darrell,Yutong Bai,Maggie Chung,Adam Yala*

Main category: cs.CV

TL;DR: Pillar-0是一个放射学基础模型，在42,990个腹部-盆腔CT、86,411个胸部CT、14,348个头部CT和11,543个乳腺MRI上预训练，结合RATE框架提取366个放射学发现的结构化标签，在多个任务上超越现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 放射学在现代医学中至关重要，但影像量增长远超人力增长。现有医学模型处理3D CT和MRI为低质量2D切片，丢弃关键灰度对比信息，缺乏反映真实临床实践的评价框架。

Method: 使用Pillar-0放射学基础模型，在大量CT和MRI数据上预训练，结合RATE框架利用LLM高精度提取结构化放射学发现标签。

Result: 在内部测试集上，Pillar-0平均AUROC达86.4-90.1，比MedGemma、MedImageInsight等模型高7.8-15.8 AUROC点，在87.2%任务中排名第一。在外部验证和肺癌风险预测等任务中也表现优异。

Conclusion: Pillar-0和RATE共同提供了一个开放、临床严谨的基础，用于构建高性能放射学系统，使之前因计算、数据和评估限制而不可行的应用成为可能。

Abstract: Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.

</details>


### [29] [A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking](https://arxiv.org/abs/2511.17805)
*Chengan Che,Chao Wang,Xinyue Chen,Sophia Tsoka,Luis C. Garcia-Peraza-Herrera*

Main category: cs.CV

TL;DR: PL-Stitch是一个自监督学习框架，通过利用视频帧的固有时间顺序作为监督信号，解决现有方法忽略程序性活动时序结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在静态图像和短视频上表现成功，但往往忽略了程序性活动的结构化特性，即动作按特定时间顺序执行。实验表明，现有模型无法区分正向和时间反转序列，证明它们对程序顺序是盲目的。

Method: 提出PL-Stitch框架，基于Plackett-Luce模型集成两个概率目标：主要PL目标训练模型按时间顺序排序采样帧，学习全局工作流程进展；次要目标通过时空拼图损失捕捉细粒度的跨帧对象相关性。

Result: 在五个手术和烹饪基准测试中持续实现卓越性能，特别是在手术阶段识别（如Cholec80上k-NN准确率提升11.4个百分点）和烹饪动作分割（如Breakfast上线性探测准确率提升5.7个百分点）方面取得显著增益。

Conclusion: PL-Stitch通过利用程序性视频的时间顺序作为监督信号，有效解决了现有自监督学习方法缺乏程序意识的问题，在程序性视频表示学习方面表现出色。

Abstract: Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.

</details>


### [30] [REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion](https://arxiv.org/abs/2511.17806)
*Ryoma Yataka,Pu Perry Wang,Petros Boufounos,Ryuhei Takahashi*

Main category: cs.CV

TL;DR: REXO提出了一种基于3D边界框扩散的多视角雷达物体检测方法，通过显式的跨视角雷达特征关联来提升复杂室内场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖隐式的跨视角雷达特征关联，在复杂室内场景中容易导致特征匹配模糊和检测性能下降，需要更明确的关联机制。

Method: 将DiffusionDet的2D边界框扩散过程提升到3D雷达空间，利用噪声3D边界框指导显式的跨视角雷达特征关联，并结合人与地面接触的先验知识减少扩散参数。

Result: 在两个公开室内雷达数据集上评估，HIBER数据集AP提升+4.22，MMVR数据集AP提升+11.02，显著超越现有最优方法。

Conclusion: REXO通过3D边界框扩散和显式跨视角特征关联，有效解决了复杂室内场景下的多视角雷达检测问题，取得了显著的性能提升。

Abstract: Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.

</details>


### [31] [Importance-Weighted Non-IID Sampling for Flow Matching Models](https://arxiv.org/abs/2511.17812)
*Xinshuang Liu,Runfa Blark Li,Shaoxiu Wei,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种重要性加权的非独立同分布采样框架，用于在有限采样预算下准确估计流匹配模型的期望值，通过联合采样覆盖分布的关键区域并保持无偏估计。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型能有效表示复杂分布，但在有限采样预算下估计函数期望值具有挑战性。独立采样往往产生高方差估计，特别是当罕见但高影响结果主导期望时。

Method: 1. 重要性加权的非独立同分布采样框架，联合抽取多个样本覆盖分布的多样化重要区域；2. 基于分数的正则化机制，使用分数函数确保样本在高密度区域内分散；3. 学习残差速度场来重现非独立同分布样本的边际分布，实现重要性加权。

Result: 实验表明，该方法能产生多样化、高质量的样本，准确估计重要性权重和期望值，提升了流匹配模型输出的可靠表征能力。

Conclusion: 所提出的重要性加权非独立同分布采样框架有效解决了流匹配模型期望估计的挑战，在保持无偏估计的同时提高了采样效率和准确性。

Abstract: Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.

</details>


### [32] [QAL: A Loss for Recall Precision Balance in 3D Reconstruction](https://arxiv.org/abs/2511.17824)
*Pranay Meshram,Yash Turkar,Kartikeya Singh,Praveen Raj Masilamani,Charuvahan Adhivarahan,Karthik Dantu*

Main category: cs.CV

TL;DR: 提出Quality-Aware Loss (QAL)作为Chamfer Distance和Earth Mover's Distance的替代方案，通过解耦召回率和精确度来改善3D体积学习的覆盖质量。


<details>
  <summary>Details</summary>
Motivation: 现有的3D体积学习训练目标（如CD和EMD）无法平衡召回率和精确度，导致薄结构和代表性不足区域被忽略。

Method: QAL结合了覆盖加权的最近邻项和未覆盖真实值吸引项，将召回率和精确度显式解耦为可调组件。

Result: 在多样化流程中，QAL平均比CD提高4.3个百分点，比最佳替代方案提高2.8个百分点，能可靠恢复薄结构和代表性不足区域。

Conclusion: QAL为稳健的3D视觉和安全关键机器人流程提供了原则性、可解释且实用的目标函数。

Abstract: Volumetric learning underpins many 3D vision tasks such as completion, reconstruction, and mesh generation, yet training objectives still rely on Chamfer Distance (CD) or Earth Mover's Distance (EMD), which fail to balance recall and precision. We propose Quality-Aware Loss (QAL), a drop-in replacement for CD/EMD that combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into tunable components.
  Across diverse pipelines, QAL achieves consistent coverage gains, improving by an average of +4.3 pts over CD and +2.8 pts over the best alternatives. Though modest in percentage, these improvements reliably recover thin structures and under-represented regions that CD/EMD overlook. Extensive ablations confirm stable performance across hyperparameters and across output resolutions, while full retraining on PCN and ShapeNet demonstrates generalization across datasets and backbones. Moreover, QAL-trained completions yield higher grasp scores under GraspNet evaluation, showing that improved coverage translates directly into more reliable robotic manipulation.
  QAL thus offers a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines

</details>


### [33] [Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations](https://arxiv.org/abs/2511.17828)
*Guilherme J. Cavalcante,José Gabriel A. Moreira,Gabriel A. B. do Nascimento,Vincent Dong,Alex Nguyen,Thaís G. do Rêgo,Yuri Malheiros,Telmo M. Silva Filho,Carla R. Zeballos Torrez,James C. Gee,Anne Marie McCarthy,Andrew D. A. Maidment,Bruno Barufaldi*

Main category: cs.CV

TL;DR: 该研究利用BiomedCLIP基础模型进行乳腺密度BI-RADS分类，通过多模态乳腺成像数据训练，在96,995张图像上验证了模型的泛化能力和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在专业医学影像任务中的有效性，特别是在乳腺成像领域，解决模型泛化能力不足的问题。

Method: 采用BiomedCLIP基础模型，使用多模态乳腺成像数据（合成2D图像、数字乳腺摄影和数字乳腺断层合成），通过加权对比学习处理类别不平衡问题，比较单模态和多模态训练方法。

Result: 多模态和单模态方法准确率相近（0.74 vs 0.73），多模态模型在跨模态应用和AUC值方面表现更优（AUC>0.84），在外部验证数据集上表现出强泛化能力（AUC: 0.80-0.93）。

Conclusion: 基础模型在乳腺成像应用中具有巨大潜力，为未来扩展到诊断任务奠定了基础，模型具有可解释性和鲁棒性。

Abstract: Foundation models hold promise for specialized medical imaging tasks, though their effectiveness in breast imaging remains underexplored. This study leverages BiomedCLIP as a foundation model to address challenges in model generalization. BiomedCLIP was adapted for automated BI-RADS breast density classification using multi-modality mammographic data (synthesized 2D images, digital mammography, and digital breast tomosynthesis). Using 96,995 images, we compared single-modality (s2D only) and multi-modality training approaches, addressing class imbalance through weighted contrastive learning. Both approaches achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with the multi-modality model offering broader applicability across different imaging modalities and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on the RSNA and EMBED datasets showed strong generalization capabilities (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent and clinically relevant attention patterns, highlighting the models interpretability and robustness. This research underscores the potential of foundation models for breast imaging applications, paving the way for future extensions for diagnostic tasks.

</details>


### [34] [Show Me: Unifying Instructional Image and Video Generation with Diffusion Models](https://arxiv.org/abs/2511.17839)
*Yujiang Pu,Zhanbo Huang,Vishnu Boddeti,Yu Kong*

Main category: cs.CV

TL;DR: ShowMe是一个统一框架，通过选择性激活视频扩散模型的空间和时间组件，同时支持文本引导的图像编辑和视频预测任务，解决了现有方法将这两个任务分离的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将文本引导图像编辑和视频预测视为独立任务，导致图像编辑方法忽略动作的时间展开，而视频预测模型忽视预期结果。需要统一框架来同时处理这两个任务。

Method: 提出ShowMe框架，选择性激活视频扩散模型的空间和时间组件，引入结构和运动一致性奖励来提高结构保真度和时间连贯性。

Result: 在多个基准测试中，该方法在指令性图像和视频生成方面都优于专家模型，展示了视频扩散模型作为统一动作-状态转换器的优势。

Conclusion: 视频扩散模型可以作为统一的动作-状态转换器，这种统一带来了双重好处：视频预训练获得的空间知识增强了非刚性图像编辑的上下文一致性和真实感，而指令引导的编辑阶段为视频预测提供了更强的目标导向推理能力。

Abstract: Generating visual instructions in a given context is essential for developing interactive world simulators. While prior works address this problem through either text-guided image manipulation or video prediction, these tasks are typically treated in isolation. This separation reveals a fundamental issue: image manipulation methods overlook how actions unfold over time, while video prediction models often ignore the intended outcomes. To this end, we propose ShowMe, a unified framework that enables both tasks by selectively activating the spatial and temporal components of video diffusion models. In addition, we introduce structure and motion consistency rewards to improve structural fidelity and temporal coherence. Notably, this unification brings dual benefits: the spatial knowledge gained through video pretraining enhances contextual consistency and realism in non-rigid image edits, while the instruction-guided manipulation stage equips the model with stronger goal-oriented reasoning for video prediction. Experiments on diverse benchmarks demonstrate that our method outperforms expert models in both instructional image and video generation, highlighting the strength of video diffusion models as a unified action-object state transformer.

</details>


### [35] [JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception](https://arxiv.org/abs/2511.17843)
*Chenyi Wang,Zhaowei Li,Ming F. Li,Wujie Wen*

Main category: cs.CV

TL;DR: JigsawComm是一个端到端训练的语义感知多智能体协同感知框架，通过提取语义相关特征和预测特征效用，在有限带宽下实现高效通信和准确感知。


<details>
  <summary>Details</summary>
Motivation: 多智能体协同感知受限于通信带宽，现有方法未考虑语义相关性和跨智能体冗余性，需要最大化每个传输比特对感知任务的贡献。

Method: 使用正则化编码器提取语义相关稀疏特征，通过轻量级特征效用估计器预测特征贡献，交换元效用图并计算最优传输策略，选择每个位置效用最高的特征。

Result: 在OPV2V和DAIR-V2X基准测试中，JigsawComm将总数据量减少500倍以上，同时达到或优于最先进方法的准确率。

Conclusion: JigsawComm通过语义感知的特征选择和传输策略，在保持高精度的同时显著降低通信成本，实现可扩展的O(1)通信复杂度。

Abstract: Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.

</details>


### [36] [Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation](https://arxiv.org/abs/2511.17844)
*Shihan Cheng,Nilesh Kulkarni,David Hyde,Dmitriy Smirnov*

Main category: cs.CV

TL;DR: 提出了一种数据高效的精调策略，能够从稀疏、低质量的合成数据中学习生成控制，效果优于使用逼真真实数据精调的模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量高质量数据集来精调文本到视频扩散模型以添加生成控制，但这些数据集难以获取。

Method: 使用稀疏、低质量的合成数据进行精调，而非依赖逼真的真实数据。

Result: 在简单数据上精调不仅能够实现期望的控制，而且效果优于在逼真真实数据上精调的模型。

Conclusion: 提供了一种框架来解释这一现象，证明使用简单合成数据可以更有效地学习生成控制。

Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.

</details>


### [37] [MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use](https://arxiv.org/abs/2511.17881)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: MGA-VQA是一个多模态文档视觉问答框架，通过整合标记级编码、空间图推理、记忆增强推理和问题引导压缩，解决了现有方法在空间关系建模、高分辨率文档处理、多跳推理和可解释性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前文档视觉问答方法在显式空间关系建模、高分辨率文档处理效率、多跳推理能力和模型可解释性方面存在不足，需要一种更有效的解决方案。

Method: 提出MGA-VQA框架，集成标记级编码、空间图推理、记忆增强推理和问题引导压缩，引入可解释的基于图的决策路径和结构化内存访问机制。

Result: 在六个基准测试（FUNSD、CORD、SROIE、DocVQA、STE-VQA和RICO）上评估，显示出在答案预测和空间定位方面的优越准确性和效率，获得了一致的改进。

Conclusion: MGA-VQA通过多模态集成和可解释的推理机制，显著提升了文档视觉问答的性能和透明度，为复杂文档理解任务提供了有效解决方案。

Abstract: Document Visual Question Answering (DocVQA) requires models to jointly understand textual semantics, spatial layout, and visual features. Current methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability. We propose MGA-VQA, a multi-modal framework that integrates token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways and structured memory access for enhanced reasoning transparency. Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO) demonstrates superior accuracy and efficiency, with consistent improvements in both answer prediction and spatial localization.

</details>


### [38] [ArticFlow: Generative Simulation of Articulated Mechanisms](https://arxiv.org/abs/2511.17883)
*Jiong Lin,Jinchen Ruan,Hod Lipson*

Main category: cs.CV

TL;DR: ArticFlow是一个两阶段流匹配框架，通过动作控制从噪声生成可控制的3D关节形状，可作为生成模型和神经模拟器使用。


<details>
  <summary>Details</summary>
Motivation: 生成模型在静态3D形状方面取得了显著进展，但关节3D生成由于动作依赖变形和数据集有限而面临挑战。

Method: 采用两阶段流匹配框架：(i) 潜在流将噪声传输到形状先验代码；(ii) 点流在动作和形状先验条件下传输点，使单个模型能够表示多样化的关节类别并在动作间泛化。

Result: 在MuJoCo Menagerie上，ArticFlow作为生成模型和神经模拟器，从紧凑先验预测动作条件运动学，并通过潜在插值合成新形态。相比特定对象模拟器和静态点云生成器的动作条件变体，ArticFlow实现了更高的运动学精度和更好的形状质量。

Conclusion: 动作条件流匹配是实现可控高质量关节机制生成的实用途径。

Abstract: Recent advances in generative models have produced strong results for static 3D shapes, whereas articulated 3D generation remains challenging due to action-dependent deformations and limited datasets. We introduce ArticFlow, a two-stage flow matching framework that learns a controllable velocity field from noise to target point sets under explicit action control. ArticFlow couples (i) a latent flow that transports noise to a shape-prior code and (ii) a point flow that transports points conditioned on the action and the shape prior, enabling a single model to represent diverse articulated categories and generalize across actions. On MuJoCo Menagerie, ArticFlow functions both as a generative model and as a neural simulator: it predicts action-conditioned kinematics from a compact prior and synthesizes novel morphologies via latent interpolation. Compared with object-specific simulators and an action-conditioned variant of static point-cloud generators, ArticFlow achieves higher kinematic accuracy and better shape quality. Results show that action-conditioned flow matching is a practical route to controllable and high-quality articulated mechanism generation.

</details>


### [39] [FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning](https://arxiv.org/abs/2511.17885)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Wei Chen,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CV

TL;DR: FastMMoE是一个针对基于专家混合(MoE)的多模态大语言模型的训练免费加速框架，通过专家激活减少和路由感知令牌剪枝两种策略，在保持约95.5%性能的同时减少高达55.0%的计算量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的高分辨率视觉输入会产生大量视觉令牌，导致推理延迟显著增加。减少冗余视觉令牌对于在资源受限或延迟敏感场景中部署MLLM至关重要。

Method: 提出FastMMoE框架，包含两种互补策略：(1) 视觉令牌的专家激活减少，最小化不必要的专家计算；(2) 路由感知令牌剪枝，利用路由概率分布的相似性识别和移除高度冗余的视觉令牌。

Result: 在DeepSeek-VL2和InternVL3.5等大规模MoE-MLLM上的实验表明，FastMMoE可减少高达55.0%的FLOPs，同时保留约95.5%的原始性能，在多个保留率下持续优于包括FastV和SparseVLM在内的密集模型剪枝基线。

Conclusion: FastMMoE为MoE-based MLLMs提供了一种有效的训练免费加速解决方案，显著减少了计算开销，同时保持了模型性能，优于现有的密集模型剪枝方法。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.

</details>


### [40] [When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA](https://arxiv.org/abs/2511.17886)
*Pume Tuchinda,Parinthapat Pengpun,Romrawin Chumpu,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CV

TL;DR: 本文系统研究了CLIP风格视觉语言模型的知识蒸馏，发现与NLP和视觉领域不同，更强的教师模型并不总能产生更好的学生模型，现有蒸馏框架在多模态任务中难以扩展。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型计算需求大，知识蒸馏是构建轻量级模型的有效方法，但在CLIP风格模型中的应用有限，主要局限于小规模教师模型和窄范围评估任务。

Method: 对一系列CLIP风格教师模型进行系统蒸馏研究，从标准基线到大规模最先进模型，评估现有蒸馏框架的可扩展性。

Result: 发现更强的教师模型并不总是产生更好的学生模型，现有蒸馏框架在多模态任务中往往导致性能下降，特别是在视觉问答等下游任务中。

Conclusion: 研究结果挑战了知识蒸馏中的普遍假设，为设计参数高效的多模态模型指出了新方向。

Abstract: Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.

</details>


### [41] [MINDiff: Mask-Integrated Negative Attention for Controlling Overfitting in Text-to-Image Personalization](https://arxiv.org/abs/2511.17888)
*Seulgi Jeong,Jaeil Kim*

Main category: cs.CV

TL;DR: MINDiff提出了一种新的负注意力机制，通过修改推理时的交叉注意力来抑制主题在无关区域的影响，从而解决文本到图像模型个性化过程中的过拟合问题，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决大规模文本到图像模型个性化过程中，从有限图像学习特定主题时出现的过拟合问题。现有方法如DreamBooth需要增加训练计算成本，且在推理时限制用户控制。

Method: 提出Mask-Integrated Negative Attention Diffusion (MINDiff)，引入负注意力概念，在推理时修改交叉注意力机制，抑制主题在掩码无关区域的影响，用户可通过调整lambda参数平衡主题保真度和文本对齐。

Result: 在DreamBooth模型上的定性和定量实验表明，MINDiff比类特定先验保持损失更有效地缓解过拟合，且完全在推理时运行，不改变模型架构，可直接应用于现有DreamBooth模型。

Conclusion: MINDiff提供了一种有效的推理时解决方案，通过负注意力机制改善语义控制和文本对齐，同时保持主题保真度，无需重新训练模型。

Abstract: In the personalization process of large-scale text-to-image models, overfitting often occurs when learning specific subject from a limited number of images. Existing methods, such as DreamBooth, mitigate this issue through a class-specific prior-preservation loss, which requires increased computational cost during training and limits user control during inference time. To address these limitations, we propose Mask-Integrated Negative Attention Diffusion (MINDiff). MINDiff introduces a novel concept, negative attention, which suppresses the subject's influence in masked irrelevant regions. We achieve this by modifying the cross-attention mechanism during inference. This enables semantic control and improves text alignment by reducing subject dominance in irrelevant regions. Additionally, during the inference time, users can adjust a scale parameter lambda to balance subject fidelity and text alignment. Our qualitative and quantitative experiments on DreamBooth models demonstrate that MINDiff mitigates overfitting more effectively than class-specific prior-preservation loss. As our method operates entirely at inference time and does not alter the model architecture, it can be directly applied to existing DreamBooth models without re-training. Our code is available at https://github.com/seuleepy/MINDiff.

</details>


### [42] [Decoupled Audio-Visual Dataset Distillation](https://arxiv.org/abs/2511.17890)
*Wenyuan Li,Guang Li,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: DAVDD是一个基于预训练的音频-视觉数据集蒸馏框架，通过解耦表示学习和跨模态匹配策略，解决了传统方法在跨模态对齐和模态特定信息保护方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统分布匹配方法难以捕捉内在的跨模态对齐，现有方法存在模态映射空间不一致和模态特定信息受损的问题。

Method: 使用预训练特征库获取稳定模态特征，通过轻量化解耦器将特征分解为公共和私有表示，采用公共跨模态匹配和样本-分布联合对齐策略。

Result: 在多个基准测试中，DAVDD在所有IPC设置下均实现了最先进的性能。

Conclusion: 解耦表示学习对于高质量音频-视觉数据集蒸馏具有有效性。

Abstract: Audio-Visual Dataset Distillation aims to compress large-scale datasets into compact subsets while preserving the performance of the original data. However, conventional Distribution Matching (DM) methods struggle to capture intrinsic cross-modal alignment. Subsequent studies have attempted to introduce cross-modal matching, but two major challenges remain: (i) independently and randomly initialized encoders lead to inconsistent modality mapping spaces, increasing training difficulty; and (ii) direct interactions between modalities tend to damage modality-specific (private) information, thereby degrading the quality of the distilled data. To address these challenges, we propose DAVDD, a pretraining-based decoupled audio-visual distillation framework. DAVDD leverages a diverse pretrained bank to obtain stable modality features and uses a lightweight decoupler bank to disentangle them into common and private representations. To effectively preserve cross-modal structure, we further introduce Common Intermodal Matching together with a Sample-Distribution Joint Alignment strategy, ensuring that shared representations are aligned both at the sample level and the global distribution level. Meanwhile, private representations are entirely isolated from cross-modal interaction, safeguarding modality-specific cues throughout distillation. Extensive experiments across multiple benchmarks show that DAVDD achieves state-of-the-art results under all IPC settings, demonstrating the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation. Code will be released.

</details>


### [43] [CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation](https://arxiv.org/abs/2511.17904)
*Yuhang Ming,Chenxin Fang,Xingyuan Yu,Fan Zhang,Weichen Dai,Wanzeng Kong,Guofeng Zhang*

Main category: cs.CV

TL;DR: CUS-GS是一种紧凑的统一结构化高斯溅射表示，通过连接多模态语义特征与结构化3D几何来弥合语义导向和结构导向方法之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯溅射的3D场景表示存在两个主要趋势：语义导向方法关注高层理解但缺乏明确的3D几何建模，而结构导向方法捕获空间结构但提供有限的语义抽象。需要弥合这一差距。

Method: 设计了体素化锚点结构构建空间支架，从基础模型（如CLIP、DINOv2、SEEM）提取多模态语义特征；引入多模态潜在特征分配机制统一外观、几何和语义；提出特征感知显著性评估策略动态指导锚点生长和修剪。

Result: 广泛实验表明，CUS-GS在使用仅600万参数的情况下实现了与最先进方法竞争的性能，比最接近的竞争对手（3500万参数）小一个数量级。

Conclusion: CUS-GS框架在性能和模型效率之间实现了出色的权衡，通过紧凑的统一表示有效连接了多模态语义特征与结构化3D几何。

Abstract: Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.

</details>


### [44] [Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation](https://arxiv.org/abs/2511.17914)
*Chenyang Jiang,Hang Zhao,Xinyu Zhang,Zhengcen Li,Qiben Shan,Shaocong Wu,Jingyong Su*

Main category: cs.CV

TL;DR: 本文提出ADSA（自适应软标签对齐模块）来解决长尾数据集蒸馏中的性能下降问题，通过校准软标签偏差显著提升尾类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法主要针对平衡数据集，在真实世界长尾分布下表现不佳，需要解决软标签偏差问题。

Method: 提出ADSA模块，通过系统扰动数据不平衡水平识别软标签偏差来源，并设计自适应软标签对齐机制来校准这些偏差。

Result: 在ImageNet-1k-LT数据集上，ADSA将尾类准确率提升高达11.8%，整体准确率达到41.4%。

Conclusion: ADSA为有限标签预算下的长尾数据集蒸馏提供了鲁棒且通用的解决方案，可无缝集成到现有蒸馏流程中。

Abstract: Dataset distillation compresses large-scale datasets into compact, highly informative synthetic data, significantly reducing storage and training costs. However, existing research primarily focuses on balanced datasets and struggles to perform under real-world long-tailed distributions. In this work, we emphasize the critical role of soft labels in long-tailed dataset distillation and uncover the underlying mechanisms contributing to performance degradation. Specifically, we derive an imbalance-aware generalization bound for model trained on distilled dataset. We then identify two primary sources of soft-label bias, which originate from the distillation model and the distilled images, through systematic perturbation of the data imbalance levels. To address this, we propose ADSA, an Adaptive Soft-label Alignment module that calibrates the entangled biases. This lightweight module integrates seamlessly into existing distillation pipelines and consistently improves performance. On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. Extensive experiments demonstrate that ADSA provides a robust and generalizable solution under limited label budgets and across a range of distillation techniques. Code is available at: https://github.com/j-cyoung/ADSA_DD.git.

</details>


### [45] [Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization](https://arxiv.org/abs/2511.17918)
*Youngsik Yun,Dongjun Gu,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出频率自适应锐度正则化(FASR)方法，通过改进3D高斯泼溅(3DGS)的训练目标，解决其在少样本场景下对新视角泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 3DGS在大多数配置下表现优异，但在少样本场景下由于对稀疏观测的过拟合，缺乏对新视角的泛化能力。本文从机器学习角度重新审视3DGS优化，将新视角合成视为对未见视角的泛化问题。

Method: 提出FASR方法，通过反映图像的局部频率来设置正则化权重和邻域半径，从而估计局部锐度。相比直接使用SAM方法，FASR避免了过度正则化导致高频细节丢失的问题，同时防止了对锐度惩罚不足的情况。

Result: 在各种配置的数据集上，该方法持续改进了广泛的基线方法，能够防止新视角中的漂浮伪影，并重建SAM倾向于过度平滑的精细细节。

Conclusion: FASR通过频率自适应锐度正则化有效提升了3DGS在新视角合成任务中的泛化性能，解决了少样本场景下的过拟合问题。

Abstract: Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.

</details>


### [46] [PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning](https://arxiv.org/abs/2511.17927)
*Yingjie Ma,Xun Lin,Yong Xu,Weicheng Xie,Zitong Yu*

Main category: cs.CV

TL;DR: PA-FAS方法通过构建高质量扩展推理序列和答案打乱机制，解决了多模态人脸反欺诈中监督微调+强化学习方法的局限性，显著提升了多模态推理准确性和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态人脸反欺诈中监督微调+强化学习方法的两大局限：有限的多模态推理路径限制了互补模态的使用，以及单任务监督与多样化推理路径不匹配导致的推理混淆问题。

Method: 提出PA-FAS方法：1）从有限标注构建高质量扩展推理序列，丰富推理路径并放松探索约束；2）在监督微调阶段引入答案打乱机制，强制进行全面的多模态分析而非依赖表面线索。

Result: PA-FAS显著提高了多模态推理准确性和跨域泛化能力，更好地统一了多模态融合、泛化性和可解释性，为可信赖的人脸反欺诈系统提供了支持。

Conclusion: PA-FAS通过增强推理路径和强制深度推理，有效解决了多模态人脸反欺诈中的关键挑战，为构建更可靠的多模态人脸反欺诈系统提供了新思路。

Abstract: Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.

</details>


### [47] [UniRSCD: A Unified Novel Architectural Paradigm for Remote Sensing Change Detection](https://arxiv.org/abs/2511.17930)
*Yuan Qu,Zhipeng Zhang,Chaojun Xu,Qiao Wan,Mengying Xie,Yuzeng Chen,Zhenqi Liu,Yanfei Zhong*

Main category: cs.CV

TL;DR: 本文提出了一个统一的遥感变化检测框架UniRSCD，基于状态空间模型，通过频率变化提示生成器作为统一编码器，无需专门解码器即可适应BCD、SCD、BDA等不同输出粒度的变化检测任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量专家知识设计专门解码器来补偿编码过程中的信息损失，这不仅在选择最优模型时引入不确定性，还限制了架构的通用性。

Method: 基于状态空间模型，引入频率变化提示生成器作为统一编码器，动态扫描双时相全局上下文信息，集成高频细节和低频整体信息；统一解码器和预测头通过分层特征交互和任务自适应输出映射建立共享表示空间。

Result: 实验结果表明，该架构能适应多种变化检测任务，在LEVIR-CD、SECOND、xBD等五个数据集上取得了领先性能。

Conclusion: UniRSCD框架成功将二进制变化检测、语义变化检测等不同任务集成到统一架构中，适应了不同变化检测任务的输出粒度要求。

Abstract: In recent years, remote sensing change detection has garnered significant attention due to its critical role in resource monitoring and disaster assessment. Change detection tasks exist with different output granularities such as BCD, SCD, and BDA. However, existing methods require substantial expert knowledge to design specialized decoders that compensate for information loss during encoding across different tasks. This not only introduces uncertainty into the process of selecting optimal models for abrupt change scenarios (such as disaster outbreaks) but also limits the universality of these architectures. To address these challenges, this paper proposes a unified, general change detection framework named UniRSCD. Building upon a state space model backbone, we introduce a frequency change prompt generator as a unified encoder. The encoder dynamically scans bitemporal global context information while integrating high-frequency details with low-frequency holistic information, thereby eliminating the need for specialized decoders for feature compensation. Subsequently, the unified decoder and prediction head establish a shared representation space through hierarchical feature interaction and task-adaptive output mapping. This integrating various tasks such as binary change detection and semantic change detection into a unified architecture, thereby accommodating the differing output granularity requirements of distinct change detection tasks. Experimental results demonstrate that the proposed architecture can adapt to multiple change detection tasks and achieves leading performance on five datasets, including the binary change dataset LEVIR-CD, the semantic change dataset SECOND, and the building damage assessment dataset xBD.

</details>


### [48] [Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion](https://arxiv.org/abs/2511.17932)
*Yan Xu,Yixing Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: 提出了一种零样本、生成引导的稀疏输入新视角合成框架，利用预训练视频扩散模型的先验知识，通过不确定性感知机制生成伪视图，结合3D高斯溅射进行场景重建，无需场景特定训练即可从稀疏输入生成高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏输入新视角合成问题，不仅填补空间视图间的空白，还要完成自然视频的时空连续性，利用预训练视频扩散模型的强大先验知识进行测试时自然视频补全。

Method: 采用零样本、生成引导框架，通过不确定性感知机制在新型相机位姿生成伪视图，使用3D高斯溅射进行场景重建，建立迭代反馈循环让3D几何和2D视图合成相互促进。

Result: 在LLFF、DTU、DL3DV和MipNeRF-360数据集上，该方法在极端稀疏条件下显著优于强3D-GS基线，能够生成连贯、高保真的渲染结果。

Conclusion: 该方法成功实现了无需场景特定训练或微调的稀疏输入新视角合成，通过结合视频扩散模型先验和3D高斯溅射，在多个基准测试中表现出优越性能。

Abstract: Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \emph{completing a natural video} unfolding through space.
  We recast the task as \emph{test-time natural video completion}, using powerful priors from \emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.
  The result is coherent, high-fidelity renderings from sparse inputs \emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.

</details>


### [49] [V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction](https://arxiv.org/abs/2511.17941)
*Xiangyan Kong,Xuecheng Wu,Xiongwei Zhao,Xiaodong Li,Yunyun Shi,Gang Wang,Dingkang Yang,Yang Liu,Hong Chen,Yulong Gao*

Main category: cs.CV

TL;DR: V2X-RECT是一个针对高密度交通场景的轨迹预测框架，通过多源身份匹配校正、交通信号引导的交互模块和局部时空坐标编码，解决了目标身份频繁切换、冗余交互和历史轨迹重复编码等问题，显著提升了预测准确性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 在密集交通场景中，目标身份频繁切换阻碍了跨视角关联和融合，多源信息在编码阶段产生冗余交互，传统车辆中心编码导致大量重复历史轨迹特征编码，降低了实时推理性能。

Method: 1. 多源身份匹配校正模块：利用多视角时空关系实现稳定一致的目标关联；2. 交通信号引导交互模块：编码交通灯变化趋势特征，准确过滤关键交互车辆；3. 局部时空坐标编码：实现历史轨迹和地图特征的可重用性，支持并行解码。

Result: 在V2X-Seq和V2X-Traj数据集上的广泛实验表明，V2X-RECT相比最先进方法取得了显著改进，同时在不同交通密度下增强了鲁棒性和推理效率。

Conclusion: V2X-RECT通过增强数据关联一致性、减少冗余交互和重用历史信息，实现了更高效准确的轨迹预测，特别适用于高密度交通环境。

Abstract: V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.

</details>


### [50] [Test-Time Temporal Sampling for Efficient MLLM Video Understanding](https://arxiv.org/abs/2511.17945)
*Kaibin Wang,Mingbao Lin*

Main category: cs.CV

TL;DR: T3S是一种无需训练、即插即用的推理包装器，通过生成多个短而多样的视频标记子序列，在单次前向传播中处理并聚合预测，从而高效处理长视频，降低计算成本并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 处理长视频时，多模态大语言模型的自注意力机制计算复杂度随视频标记数量呈二次方增长，导致高计算需求和慢推理速度。现有解决方案在准确性、额外训练需求或推理速度方面存在权衡。

Method: 提出测试时时间采样（T3S），利用时空冗余性，在推理时生成多个短而多样的视频标记子序列，将它们打包在单次前向传播中，并聚合它们的预测结果。

Result: 在长视频理解基准测试中，T3S将准确性提高了高达3.1%，并将首个标记延迟减少了2.04倍，且集成工作量最小。

Conclusion: T3S将视频冗余转化为计算优势，为长视频理解提供了可扩展的解决方案，完全在推理时运行，无需模型修改或微调，并与多种预训练MLLMs兼容。

Abstract: Processing long videos with multimodal large language models (MLLMs) poses a significant computational challenge, as the model's self-attention mechanism scales quadratically with the number of video tokens, resulting in high computational demand and slow inference speed. Current solutions, such as rule-based sub-sampling, learned frame selector, or memory-based summarization, often introduce their own trade-offs: they compromise accuracy, necessitate additional training, or decrease inference speed. In this paper, we propose Test-Time Temporal Sampling (T3S), a training-free, plug-and-play inference wrapper that enables MLLMs to process long videos both efficiently and effectively. T3S exploits spatiotemporal redundancy by generating multiple short and diverse subsequences of video tokens at inference time, packing them within a single forward pass, and aggregating their predictions. This multi-subsequence formulation broadens visual coverage while reducing the computational cost of self-attention from $O(L^2)$ to $O(\sum_{i=1}^m α_i^2L^2)$, where $\sum_{i=1}^m α_i^2 < 1$. Extensive experiments on long video understanding benchmarks demonstrate that T3S improves accuracy by up to 3.1% and reduces first token delay by $2.04\times$, all with minimal integration effort. Our approach operates entirely at inference time, requires no model modifications or fine-tuning, and is compatible with a wide range of pretrained MLLMs. T3S turns video redundancy into a computational advantage, offering a scalable solution for long-video understanding. The code is available at https://github.com/kaibinwang3/T3S.

</details>


### [51] [Multi-speaker Attention Alignment for Multimodal Social Interaction](https://arxiv.org/abs/2511.17952)
*Liangyang Ouyang,Yifei Huang,Mingfang Zhang,Caixin Kang,Ryosuke Furuta,Yoichi Sato*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态多说话者注意力对齐方法，用于解决多说话者场景中视觉和文本标记缺乏说话者一致对齐的问题，从而提升多模态大语言模型在社交推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在多说话者社交场景中表现不佳，主要原因是视觉和文本标记缺乏说话者一致的对齐，导致跨模态注意力较弱。

Method: 提出动态跨模态头选择和自适应社交感知注意力偏置方法，通过识别负责接地的注意力头并注入基于现有注意力模式和说话者位置的偏置，强化说话者视觉表示与其话语之间的对齐。

Result: 在三个不同的多模态大语言模型和三个基准测试上验证了方法的有效性，在四个社交任务中取得了最先进的结果，注意力可视化证实了方法能成功聚焦于说话者相关区域。

Conclusion: 该方法能够有效提升多模态大语言模型在多说话者社交推理任务中的表现，无需引入可训练参数或架构改变，实现了更稳健的多方社交推理。

Abstract: Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.

</details>


### [52] [HEAL: Learning-Free Source Free Unsupervised Domain Adaptation for Cross-Modality Medical Image Segmentation](https://arxiv.org/abs/2511.17958)
*Yulong Shi,Jiapeng Li,Lin Qi*

Main category: cs.CV

TL;DR: HEAL是一个新颖的源自由无监督域自适应框架，通过分层去噪、边缘引导选择、尺寸感知融合和无学习特性来解决临床数据隐私和存储限制问题。


<details>
  <summary>Details</summary>
Motivation: 临床数据隐私需求和存储限制推动了源自由无监督域自适应的发展，但该方法面临源域数据缺失和目标域无标签监督的挑战。

Method: 提出HEAL框架，整合分层去噪、边缘引导选择、尺寸感知融合和无学习特性来适应未见目标域。

Result: 大规模跨模态实验表明，该方法优于现有SFUDA方法，达到最先进性能。

Conclusion: HEAL框架有效解决了SFUDA中的关键挑战，在跨模态任务中表现出色，源代码已公开。

Abstract: Growing demands for clinical data privacy and storage constraints have spurred advances in Source Free Unsupervised Domain Adaptation (SFUDA). SFUDA addresses the domain shift by adapting models from the source domain to the unseen target domain without accessing source data, even when target-domain labels are unavailable. However, SFUDA faces significant challenges: the absence of source domain data and label supervision in the target domain due to source free and unsupervised settings. To address these issues, we propose HEAL, a novel SFUDA framework that integrates Hierarchical denoising, Edge-guided selection, size-Aware fusion, and Learning-free characteristic. Large-scale cross-modality experiments demonstrate that our method outperforms existing SFUDA approaches, achieving state-of-the-art (SOTA) performance. The source code is publicly available at: https://github.com/derekshiii/HEAL.

</details>


### [53] [VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment](https://arxiv.org/abs/2511.17962)
*Ziheng Jia,Linhan Cao,Jinliang Han,Zicheng Zhang,Jiaying Qian,Jiarui Wang,Zijian Chen,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了一种视觉编码器中心的生成预训练流程，开发了VITAL-Series多模态模型，通过机器执行的标注-审查范式构建了450万视觉语言对，采用多任务训练增强模型的定量评分精度和质量解释能力，实现了高效的模型库扩展。


<details>
  <summary>Details</summary>
Motivation: 现有视觉质量评估多模态模型通常专注于单一任务并依赖全参数微调，容易在特定模态或任务类型上过拟合，限制了泛化能力和可迁移性。

Method: 采用视觉编码器中心的生成预训练流程，构建大规模视觉质量评估训练数据集，使用多任务训练工作流同时增强定量评分精度和质量解释能力，基于视觉编码器实现高效的模型库扩展。

Result: 构建了450万视觉语言对的最大视觉质量评估训练数据集，模型库展现出强大的零样本性能，每个配对解码器仅需少于1/1000预训练数据的快速预热即可达到与完全训练模型相当的性能。

Conclusion: 该工作为推进视觉质量评估基础多模态模型奠定了基石。

Abstract: Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.
  However, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.

</details>


### [54] [X-ReID: Multi-granularity Information Interaction for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.17964)
*Chenyang Yu,Xuehu Liu,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出X-ReID框架解决视频可见光-红外行人重识别问题，通过跨模态原型协作和多粒度信息交互来减少模态差异并增强时空建模。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型在检索任务中表现优异，但在视频可见光-红外行人重识别中的潜力尚未充分探索，主要挑战是缩小模态差距和利用视频序列的时空信息。

Method: 提出跨模态原型协作(CPC)来对齐和整合不同模态特征，设计多粒度信息交互(MII)包含短时相邻帧交互、长时跨帧信息融合和跨模态特征对齐。

Result: 在两个大规模VVI-ReID基准测试(HITSZ-VCM和BUPTCampus)上的广泛实验证明了该方法优于现有最先进方法。

Conclusion: 通过整合多粒度信息实现了鲁棒的序列级表示，X-ReID框架在视频可见光-红外行人重识别任务中表现出优越性能。

Abstract: Large-scale vision-language models (e.g., CLIP) have recently achieved remarkable performance in retrieval tasks, yet their potential for Video-based Visible-Infrared Person Re-Identification (VVI-ReID) remains largely unexplored. The primary challenges are narrowing the modality gap and leveraging spatiotemporal information in video sequences. To address the above issues, in this paper, we propose a novel cross-modality feature learning framework named X-ReID for VVI-ReID. Specifically, we first propose a Cross-modality Prototype Collaboration (CPC) to align and integrate features from different modalities, guiding the network to reduce the modality discrepancy. Then, a Multi-granularity Information Interaction (MII) is designed, incorporating short-term interactions from adjacent frames, long-term cross-frame information fusion, and cross-modality feature alignment to enhance temporal modeling and further reduce modality gaps. Finally, by integrating multi-granularity information, a robust sequence-level representation is achieved. Extensive experiments on two large-scale VVI-ReID benchmarks (i.e., HITSZ-VCM and BUPTCampus) demonstrate the superiority of our method over state-of-the-art methods. The source code is released at https://github.com/AsuradaYuci/X-ReID.

</details>


### [55] [Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning](https://arxiv.org/abs/2511.17973)
*Hiroto Honda*

Main category: cs.CV

TL;DR: 本文提出了一种名为对抗性伪重放（APR）的方法，用于解决无示例类增量学习中的可塑性-稳定性困境，通过对抗攻击扰动新任务图像来在线合成伪重放图像，无需存储任何重放样本。


<details>
  <summary>Details</summary>
Motivation: 无示例类增量学习（EFCIL）面临的主要挑战是在不存储先前任务图像的情况下，既要学习新任务又要避免灾难性遗忘，这被称为可塑性-稳定性困境。

Method: APR方法通过对抗攻击扰动新任务图像，以增强的旧类均值原型为目标，生成伪重放图像用于知识蒸馏；同时通过校准协方差矩阵来补偿语义漂移。

Result: 该方法在标准EFCIL基准测试的冷启动设置中实现了最先进的性能，成功平衡了稳定性和可塑性。

Conclusion: APR方法有效解决了EFCIL中的可塑性-稳定性困境，无需存储重放样本即可实现知识保留，在冷启动设置下表现优异。

Abstract: Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.

</details>


### [56] [Plan-X: Instruct Video Generation via Semantic Planning](https://arxiv.org/abs/2511.17986)
*Lun Huang,You Xie,Hongyi Xu,Tianpei Gu,Chenxu Zhang,Guoxian Song,Zenan Li,Xiaochen Zhao,Linjie Luo,Guillermo Sapiro*

Main category: cs.CV

TL;DR: Plan-X是一个通过显式语义规划来指导视频生成的框架，包含一个语义规划器来生成时空语义标记，有效减少视觉幻觉并实现细粒度、指令对齐的视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决扩散变换器在高级语义推理和长时程规划方面的局限性，这些限制导致视觉幻觉和用户指令不对齐，特别是在复杂场景理解、人-物交互、多阶段动作和上下文运动推理等场景中。

Method: 提出Plan-X框架，包含一个可学习的多模态语言模型作为语义规划器，从文本提示和视觉上下文中推理用户意图，并自回归生成基于文本的时空语义标记序列，这些语义标记作为视频扩散模型的结构化"语义草图"。

Result: 大量实验表明，该框架显著减少了视觉幻觉，并实现了与多模态上下文一致的细粒度、指令对齐的视频生成。

Conclusion: Plan-X有效整合了语言模型在多模态上下文推理和规划方面的优势，以及扩散模型在逼真视频合成方面的优势，解决了现有方法在语义推理和规划方面的不足。

Abstract: Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured "semantic sketches" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.

</details>


### [57] [HyM-UNet: Synergizing Local Texture and Global Context via Hybrid CNN-Mamba Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.17988)
*Haodong Chen,Xianfei Han,Qwen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的混合架构HyM-UNet，将CNN的局部特征提取能力与Mamba的全局建模能力相结合，用于医学图像分割任务。通过在浅层使用卷积模块保留高频纹理细节，在深层引入Visual Mamba模块捕获长距离语义依赖，并设计了Mamba引导的跳跃连接来增强边界感知。


<details>
  <summary>Details</summary>
Motivation: 准确的分割是计算机辅助诊断的关键前提。传统CNN受限于局部感受野，难以捕捉复杂的全局解剖结构。为了解决这一挑战，需要结合局部特征提取和全局建模能力。

Method: 提出HyM-UNet混合架构：1）分层编码器在浅层使用卷积模块保留纹理细节，深层引入Visual Mamba模块捕获长距离依赖；2）Mamba引导融合跳跃连接利用深层语义特征作为门控信号，动态抑制浅层特征中的背景噪声。

Result: 在ISIC 2018基准数据集上的实验表明，HyM-UNet在Dice系数和IoU指标上显著优于现有最先进方法，同时保持较低的参数数量和推理延迟。

Conclusion: HyM-UNet在处理具有复杂形状和尺度变化的医学分割任务中表现出有效性和鲁棒性，验证了CNN与Mamba协同设计的优势。

Abstract: Accurate organ and lesion segmentation is a critical prerequisite for computer-aided diagnosis. Convolutional Neural Networks (CNNs), constrained by their local receptive fields, often struggle to capture complex global anatomical structures. To tackle this challenge, this paper proposes a novel hybrid architecture, HyM-UNet, designed to synergize the local feature extraction capabilities of CNNs with the efficient global modeling capabilities of Mamba. Specifically, we design a Hierarchical Encoder that utilizes convolutional modules in the shallow stages to preserve high-frequency texture details, while introducing Visual Mamba modules in the deep stages to capture long-range semantic dependencies with linear complexity. To bridge the semantic gap between the encoder and the decoder, we propose a Mamba-Guided Fusion Skip Connection (MGF-Skip). This module leverages deep semantic features as gating signals to dynamically suppress background noise within shallow features, thereby enhancing the perception of ambiguous boundaries. We conduct extensive experiments on public benchmark dataset ISIC 2018. The results demonstrate that HyM-UNet significantly outperforms existing state-of-the-art methods in terms of Dice coefficient and IoU, while maintaining lower parameter counts and inference latency. This validates the effectiveness and robustness of the proposed method in handling medical segmentation tasks characterized by complex shapes and scale variations.

</details>


### [58] [SD-PSFNet: Sequential and Dynamic Point Spread Function Network for Image Deraining](https://arxiv.org/abs/2511.17993)
*Jiayu Wang,Haoyu Bian,Haoran Sun,Shaoning Zeng*

Main category: cs.CV

TL;DR: 提出了一种名为SD-PSFNet的新型图像去雨方法，采用多阶段恢复架构结合点扩散函数机制，通过动态物理建模和序列特征融合实现高效雨纹去除。


<details>
  <summary>Details</summary>
Motivation: 图像去雨对视觉应用至关重要，但面临雨的多尺度物理复杂性及其与场景耦合的挑战，需要更有效的物理感知方法。

Method: 采用三阶段级联序列恢复架构，利用学习PSF机制动态模拟雨纹光学特性，结合自适应门控融合实现跨阶段特征集成，从粗雨纹去除到精细细节恢复逐步优化。

Result: 在Rain100H数据集上达到33.12dB/0.9371，RealRain-1k-L数据集上达到42.28dB/0.9872，RealRain-1k-H数据集上达到41.08dB/0.9838的先进PSNR/SSIM指标。

Conclusion: SD-PSFNet在复杂场景和密集降雨条件下表现出色，为图像去雨提供了新的物理感知方法。

Abstract: Image deraining is crucial for vision applications but is challenged by the complex multi-scale physics of rain and its coupling with scenes. To address this challenge, a novel approach inspired by multi-stage image restoration is proposed, incorporating Point Spread Function (PSF) mechanisms to reveal the image degradation process while combining dynamic physical modeling with sequential feature fusion transfer, named SD-PSFNet. Specifically, SD-PSFNet employs a sequential restoration architecture with three cascaded stages, allowing multiple dynamic evaluations and refinements of the degradation process estimation. The network utilizes components with learned PSF mechanisms to dynamically simulate rain streak optics, enabling effective rain-background separation while progressively enhancing outputs through novel PSF components at each stage. Additionally, SD-PSFNet incorporates adaptive gated fusion for optimal cross-stage feature integration, enabling sequential refinement from coarse rain removal to fine detail restoration. Our model achieves state-of-the-art PSNR/SSIM metrics on Rain100H (33.12dB/0.9371), RealRain-1k-L (42.28dB/0.9872), and RealRain-1k-H (41.08dB/0.9838). In summary, SD-PSFNet demonstrates excellent capability in complex scenes and dense rainfall conditions, providing a new physics-aware approach to image deraining.

</details>


### [59] [RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale](https://arxiv.org/abs/2511.18005)
*Shengyuan Wang,Zhiheng Zheng,Yu Shang,Lixuan He,Yangcheng Yu,Fan Hangyu,Jie Feng,Qingmin Liao,Yong Li*

Main category: cs.CV

TL;DR: RAISECity是一个面向城市级3D生成的真实对齐智能合成引擎，通过多模态基础工具和智能体框架解决现有方法在质量、保真度和可扩展性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在城市级3D世界生成中面临质量、保真度和可扩展性的重大挑战，需要开发能够创建详细、城市级3D世界的新方法。

Method: 提出智能体框架，利用多样化多模态基础工具获取真实世界知识，维护鲁棒的中间表示，并构建复杂3D场景，包括动态数据处理、迭代自反思和优化以及调用先进多模态工具。

Result: 在真实世界对齐、形状精度、纹理保真度和美学水平方面表现出优越性能，在整体感知质量上相对于现有基线实现了超过90%的胜率。

Conclusion: RAISECity在3D质量、真实对齐、可扩展性和与计算机图形管道的无缝兼容性方面的结合，使其成为沉浸式媒体、具身智能和世界模型应用的有前景基础。

Abstract: City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \textbf{R}eality-\textbf{A}ligned \textbf{I}ntelligent \textbf{S}ynthesis \textbf{E}ngine that creates detailed, \textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.

</details>


### [60] [Is Complete Labeling Necessary? Understanding Active Learning in Longitudinal Medical Imaging](https://arxiv.org/abs/2511.18007)
*Siteng Ma,Honghui Du,Prateek Mathur,Brendan S. Kelly,Ronan P. Killeen,Aonghus Lawlor,Ruihai Dong*

Main category: cs.CV

TL;DR: 提出了一种专门针对纵向医学影像的深度主动学习框架LMI-AL，通过配对和差分基线期与随访期的3D图像切片，仅需标注不到8%的数据即可达到全标注数据集的性能水平。


<details>
  <summary>Details</summary>
Motivation: 纵向医学影像标注成本高且耗时，现有深度主动学习方法主要针对静态任务，无法直接应用于需要识别多图像间细微差异的变化检测任务。

Method: LMI-AL框架将基线和随访期3D图像的所有2D切片配对并差分，使用深度主动学习迭代选择最具信息量的图像对进行标注，以最小化手动标注训练深度学习模型。

Result: 实验结果表明，仅标注不到8%的数据，LMI-AL就能达到与使用全标注数据集训练模型相当的性能水平。

Conclusion: LMI-AL为纵向医学影像变化检测提供了一种高效的主动学习解决方案，显著降低了标注成本，并为未来研究提供了详细性能分析指导。

Abstract: Detecting changes in longitudinal medical imaging using deep learning requires a substantial amount of accurately labeled data. However, labeling these images is notably more costly and time-consuming than labeling other image types, as it requires labeling across various time points, where new lesions can be minor, and subtle changes are easily missed. Deep Active Learning (DAL) has shown promise in minimizing labeling costs by selectively querying the most informative samples, but existing studies have primarily focused on static tasks like classification and segmentation. Consequently, the conventional DAL approach cannot be directly applied to change detection tasks, which involve identifying subtle differences across multiple images. In this study, we propose a novel DAL framework, named Longitudinal Medical Imaging Active Learning (LMI-AL), tailored specifically for longitudinal medical imaging. By pairing and differencing all 2D slices from baseline and follow-up 3D images, LMI-AL iteratively selects the most informative pairs for labeling using DAL, training a deep learning model with minimal manual annotation. Experimental results demonstrate that, with less than 8% of the data labeled, LMI-AL can achieve performance comparable to models trained on fully labeled datasets. We also provide a detailed analysis of the method's performance, as guidance for future research. The code is publicly available at https://github.com/HelenMa9998/Longitudinal_AL.

</details>


### [61] [RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios](https://arxiv.org/abs/2511.18011)
*Jun Zhang,Jie Feng,Long Chen,Junhui Wang,Zhicheng Liu,Depeng Jin,Yong Li*

Main category: cs.CV

TL;DR: RoadBench是一个系统性基准测试，专注于评估多模态大语言模型在复杂城市场景中的细粒度空间理解和推理能力，特别是针对道路标线。该基准包含6个任务、9,121个严格人工验证的测试用例，使用BEV和FPV图像输入。


<details>
  <summary>Details</summary>
Motivation: 填补多模态大语言模型在复杂城市场景中细粒度空间理解和推理能力评估的空白，重点关注道路标线这一典型细粒度空间元素。

Method: 提出RoadBench基准测试，包含6个任务组成的系统性评估框架，使用BEV和FPV图像输入，涵盖从局部空间范围到全局推理的评估。

Result: 评估了14个主流MLLM，发现RoadBench对这些模型具有挑战性，现有MLLM在城市场景中的细粒度空间理解和推理能力存在显著不足，某些任务中甚至不如简单的基于规则或随机选择的基线方法。

Conclusion: RoadBench基准测试及其发现将有助于全面推动MLLM空间理解能力的发展，基准代码、示例数据集和原始评估结果已提供。

Abstract: Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.

</details>


### [62] [ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models](https://arxiv.org/abs/2511.18082)
*Wencheng Ye,Tianshi Wang,Lei Zhu,Fengling Li,Guoli Yang*

Main category: cs.CV

TL;DR: ActDistill是一个动作引导的自蒸馏框架，将现有VLA模型的动作预测能力转移到轻量级模型中，通过动作先验指导知识转移和模型压缩，实现VLA模型的动作导向效率优化。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在机器人操作中面临计算开销大和推理延迟高的问题，限制了实际部署。

Method: 使用训练好的VLA模型作为教师，引入图结构封装策略建模动作预测的层次演化，学生模型配备动态路由器自适应选择计算路径，并通过层次图监督确保平滑高效演化。

Result: 在具身基准测试中，ActDistill达到与全尺寸VLA模型相当或更优的性能，同时减少超过50%的计算量，实现高达1.67倍的加速。

Conclusion: ActDistill为高效具身智能建立了一个通用范式，显著提升了VLA模型在机器人操作中的部署效率。

Abstract: Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.

</details>


### [63] [State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection](https://arxiv.org/abs/2511.18012)
*Jiaying Zhou,Qingchao Chen*

Main category: cs.CV

TL;DR: 本文提出两种原型增强策略来解决弱监督开放词汇目标检测中的关键挑战：状态增强语义原型（SESP）捕获类内视觉变化，场景增强伪原型（SAPP）解决语义不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有语义原型存在两个关键问题：一是静态原型无法捕捉由不同物体状态引起的丰富类内视觉变化；二是标准伪框生成导致视觉区域提议与物体中心文本嵌入之间的语义不匹配。

Method: 提出SESP生成状态感知的文本描述来捕获多样化的物体外观，以及SAPP结合上下文语义并使用软对齐机制来促进上下文一致的视觉-文本表示。

Result: 通过整合SESP和SAPP，方法有效增强了语义原型的丰富性和视觉-文本对齐，实现了显著改进。

Conclusion: 所提出的两种互补原型增强策略成功解决了弱监督开放词汇目标检测中的关键挑战，提升了检测性能。

Abstract: Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.

</details>


### [64] [PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation](https://arxiv.org/abs/2511.18570)
*Samarth Chopra,Jing Liang,Gershom Seneviratne,Dinesh Manocha*

Main category: cs.CV

TL;DR: PhysGS是一个基于贝叶斯推理的3D高斯泼溅扩展方法，能够从视觉线索和视觉-语言先验中估计密集的每点物理属性，如质量、硬度和摩擦系数。


<details>
  <summary>Details</summary>
Motivation: 现有的3D重建方法主要关注几何和外观，无法推断基础的物理属性，而这些属性对于机器人安全有效地与环境交互至关重要。

Method: 将属性估计建模为高斯泼溅上的贝叶斯推理，其中材料和属性信念随着新观察的到来而迭代优化，同时建模了偶然和认知不确定性。

Result: 在物体尺度、室内和室外真实世界数据集上，PhysGS将质量估计准确率提高了22.8%，肖氏硬度误差降低了61.2%，动摩擦误差降低了18.1%。

Conclusion: PhysGS在单一空间连续框架中统一了3D重建、不确定性建模和物理推理，实现了密集物理属性估计。

Abstract: Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.

</details>


### [65] [Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents](https://arxiv.org/abs/2511.18685)
*Dayong Liu,Chao Xu,Weihong Chen,Suyu Zhang,Juncheng Wang,Jiankang Deng,Baigui Sun,Yang Liu*

Main category: cs.CV

TL;DR: 提出了CFG-Bench基准，系统评估多模态大语言模型在具身物理交互中的细粒度动作智能，包含1,368个视频和19,562个多模态问答对，涵盖四个认知维度。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注高层次规划或空间推理，而忽视了具身物理交互所需的细粒度动作智能，需要填补这一研究空白。

Method: 构建CFG-Bench基准，包含四个认知维度：物理交互、时序因果关系、意图理解和评估判断，通过监督微调验证方法有效性。

Result: 主流MLLMs在生成物理交互的详细指令方面表现不佳，在意图和评估等高阶推理方面存在显著局限；在CFG-Bench数据上进行监督微调能显著提升在现有具身基准上的性能。

Conclusion: CFG-Bench揭示了MLLMs在具身物理交互中的关键局限，为开发更有能力的具身智能体提供了重要见解。

Abstract: Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.

</details>


### [66] [MambaX: Image Super-Resolution with State Predictive Control](https://arxiv.org/abs/2511.18028)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Naoto Yokoya,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 本文提出了MambaX模型，通过非线性状态预测控制来解决图像超分辨率中的误差传播问题，在单图像和多模态超分辨率任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率方法主要关注最终分辨率提升，但忽视了中间阶段的误差传播和积累控制。Mamba的固定线性映射器受限于狭窄感受野和有限灵活性，在细粒度图像中效果不佳。

Method: 创建非线性状态预测控制模型MambaX，将连续光谱带映射到潜在状态空间，通过动态学习控制方程的非线性状态参数来泛化超分辨率任务。采用动态状态预测控制学习、状态交叉控制范式和渐进过渡学习。

Result: 评估显示动态光谱状态表示模型在单图像超分辨率和多模态融合超分辨率任务中表现优异，展现了在任意维度和模态下推进光谱泛化建模的巨大潜力。

Conclusion: MambaX模型通过非线性状态预测控制有效解决了超分辨率中的误差传播问题，为跨维度和模态的光谱泛化建模提供了重要进展。

Abstract: Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities.

</details>


### [67] [Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation](https://arxiv.org/abs/2511.18037)
*Yunfan Lu,Nico Messikommer,Xiaogang Xu,Liming Chen,Yuhan Chen,Nikola Zubic,Davide Scaramuzza,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出了首个统一的事件帧混合传感器噪声模型，联合描述APS和EVS像素的噪声行为，并开发了校准流程和统计模拟器HESIM。


<details>
  <summary>Details</summary>
Motivation: 事件帧混合传感器集成了APS和EVS，但复杂的电路架构引入了难以理解的噪声模式，目前缺乏统一的噪声模型。

Method: 开发了基于统计的统一成像噪声模型，包含光子散粒噪声、暗电流噪声、固定模式噪声和量化噪声，并建立了校准流程来估计噪声参数。

Result: 在两个混合传感器上的实验验证了模型在多个成像任务（如视频帧插值和去模糊）中的有效性，展示了从模拟到真实数据的强迁移能力。

Conclusion: 提出的统一噪声模型和HESIM模拟器为事件帧混合传感器的噪声建模提供了统计基础，在多种成像应用中表现出良好的性能。

Abstract: Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.

</details>


### [68] [UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios](https://arxiv.org/abs/2511.18050)
*Tian Ye,Song Fei,Lei Zhu*

Main category: cs.CV

TL;DR: UltraFlux是一个基于Flux的扩散变换器，通过数据模型协同设计原生支持4K分辨率图像生成，解决了位置编码、VAE压缩和优化等方面的耦合问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器在扩展到原生4K分辨率时存在位置编码、VAE压缩和优化的紧密耦合问题，单独解决任一因素都无法达到理想质量。

Method: 采用数据模型协同设计：构建MultiAspect-4K-1M数据集，结合Resonance 2D RoPE位置编码、VAE后训练方案、SNR-Aware Huber Wavelet目标和分阶段美学课程学习策略。

Result: UltraFlux在4096基准测试和多纵横比4K设置中，在保真度、美学质量和对齐度方面持续优于开源基线，并在配备LLM提示优化器时达到或超越专有Seedream 4.0。

Conclusion: 通过综合解决4K生成中的耦合问题，UltraFlux实现了稳定、保留细节的4K扩散变换器，能够泛化到宽屏、方形和竖屏等多种纵横比。

Abstract: Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.

</details>


### [69] [IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment](https://arxiv.org/abs/2511.18055)
*Bowen Qu,Shangkun Sun,Xiaoyu Liang,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出了IE-Bench基准套件和IE-Critic-R1评估方法，用于改进文本驱动图像编辑的质量评估。IE-Bench包含多样化的源图像、编辑提示和编辑结果，以及近4000个带有人类评分的数据样本。IE-Critic-R1利用强化学习从可验证奖励中学习，提供更全面、可解释且符合人类感知的质量评估。


<details>
  <summary>Details</summary>
Motivation: 文本驱动图像编辑的评估面临挑战，现有方法要么只关注文本-图像对齐，要么未能很好地对齐人类感知。编辑后的图像与原始图像存在内在联系，且随文本语义动态变化，需要更全面的评估方法。

Method: 构建IE-Bench数据库，包含多样化源图像、编辑提示和编辑结果，收集近4000个带有人类评分的数据样本。开发IE-Critic-R1评估方法，利用强化学习从可验证奖励（RLVR）中学习，提供更全面、可解释的质量评估。

Result: 大量实验证明，IE-Critic-R1在文本驱动图像编辑任务上相比先前指标具有更优越的主观对齐性。

Conclusion: IE-Bench和IE-Critic-R1为文本驱动图像编辑提供了更有效的评估基准和方法，能够更好地对齐人类感知，相关数据和代码已公开。

Abstract: Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.

</details>


### [70] [Hierarchical Semi-Supervised Active Learning for Remote Sensing](https://arxiv.org/abs/2511.18058)
*Wei Huang,Zhitong Xiong,Chenying Liu,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了一种分层半监督主动学习框架HSSAL，通过结合半监督学习和分层主动学习，在遥感场景分类中显著提高了标签效率，仅需少量标注数据即可达到接近全监督模型的性能。


<details>
  <summary>Details</summary>
Motivation: 遥感领域深度学习模型性能严重依赖高质量标注数据，但大规模标注成本高昂且耗时，大量未标注图像未被充分利用。

Method: HSSAL框架将半监督学习和分层主动学习结合在闭环迭代中：SSL通过监督学习和弱到强自训练改进特征表示和不确定性估计；HAL基于精炼表示和不确定性线索，通过渐进聚类策略选择最具信息量的样本。

Result: 在UCM、AID和NWPU-RESISC45三个遥感场景分类数据集上的实验表明，HSSAL始终优于仅使用SSL或AL的基线方法。仅使用8%、4%和2%的标注数据即可达到超过95%的全监督准确率。

Conclusion: HSSAL通过有效利用未标注数据的信息性，实现了卓越的标签效率，为遥感领域数据标注成本高的问题提供了有效解决方案。

Abstract: The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be released at https://github.com/zhu-xlab/RS-SSAL.

</details>


### [71] [VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection](https://arxiv.org/abs/2511.18075)
*Jianhang Yao,Yongbin Zheng,Siqi Lu,Wanying Xu,Peng Sun*

Main category: cs.CV

TL;DR: VK-Det是一个无需额外监督的视觉知识引导开放词汇目标检测框架，通过利用视觉编码器的固有区域感知能力和原型感知伪标签策略，在航空图像上实现了最先进的开放词汇检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇航空目标检测方法依赖文本监督，导致语义偏差，限制了向文本指定概念之外的扩展。需要一种无需额外监督的方法来突破这一限制。

Method: 1. 利用视觉编码器的固有信息区域感知能力实现细粒度定位和自适应蒸馏；2. 引入原型感知伪标签策略，通过特征聚类建模类间决策边界，通过原型匹配将检测区域映射到潜在类别。

Result: 在DIOR数据集上达到30.1 mAP^N，在DOTA数据集上达到23.3 mAP^N，超越了现有方法，甚至优于使用额外监督的方法。

Conclusion: VK-Det框架通过视觉知识引导和原型感知伪标签，成功实现了无需额外监督的开放词汇目标检测，显著提升了新颖类别的检测性能。

Abstract: To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\textbf{VK-Det}$, a $\textbf{V}$isual $\textbf{K}$nowledge-guided open-vocabulary object $\textbf{Det}$ection framework $\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\mathrm{mAP}^{N}$ on DIOR and 23.3 $\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.

</details>


### [72] [Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective](https://arxiv.org/abs/2511.18089)
*Wenjing Liu,Qin Ren,Wen Zhang,Yuewei Lin,Chenyu You*

Main category: cs.CV

TL;DR: 本文提出Together-Then-Apart (TTA)框架，通过双阶段优化同时建模共享和模态特定表示，解决了多模态生存分析中过度强调对齐导致表示崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度强调跨模态对齐，牺牲了模态特异性特征，导致表示崩溃和多样性减少。需要同时考虑对齐和差异性，保持模态特定结构。

Method: TTA框架包含Together阶段（最小化语义差异，通过共享原型对齐嵌入）和Apart阶段（最大化表示多样性，通过模态锚点和对比正则化器）。

Result: 在五个TCGA基准测试上的广泛实验表明，TTA始终优于最先进的方法。

Conclusion: TTA为多模态生存分析提供了新的理论视角，展示了如何同时实现对齐和差异性，获得稳健、可解释且具有生物学意义的结果。

Abstract: Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.

</details>


### [73] [Versatile Recompression-Aware Perceptual Image Super-Resolution](https://arxiv.org/abs/2511.18090)
*Mingwei He,Tongda Xu,Xingtong Ge,Ming Sun,Chao Zhou,Yan Wang*

Main category: cs.CV

TL;DR: VRPSR是一种感知超分辨率方法，通过将压缩建模为条件文本到图像生成，利用预训练扩散模型构建可泛化的编解码器模拟器，使现有感知SR方法能够适应多种压缩格式，节省超过10%的比特率。


<details>
  <summary>Details</summary>
Motivation: 传统感知图像超分辨率方法忽略重新压缩过程，导致恢复后的图像在存储和传输时可能产生额外伪影，联合优化SR和重新压缩具有挑战性。

Method: 将压缩建模为条件文本到图像生成，利用预训练扩散模型构建编解码器模拟器，采用针对感知SR的训练技术，包括使用感知目标优化模拟器，并以轻微压缩图像作为训练目标。

Result: 基于Real-ESRGAN和S3Diff，在H.264/H.265/H.266压缩下节省超过10%比特率，并促进SR与重新压缩后处理模型的联合优化。

Conclusion: VRPSR成功使感知超分辨率方法能够适应多种压缩格式，显著提升压缩效率，为SR与压缩的联合优化提供了有效解决方案。

Abstract: Perceptual image super-resolution (SR) methods restore degraded images and produce sharp outputs. In practice, those outputs are usually recompressed for storage and transmission. Ignoring recompression is suboptimal as the downstream codec might add additional artifacts to restored images. However, jointly optimizing SR and recompression is challenging, as the codecs are not differentiable and vary in configuration. In this paper, we present Versatile Recompression-Aware Perceptual Super-Resolution (VRPSR), which makes existing perceptual SR aware of versatile compression. First, we formulate compression as conditional text-to-image generation and utilize a pre-trained diffusion model to build a generalizable codec simulator. Next, we propose a set of training techniques tailored for perceptual SR, including optimizing the simulator using perceptual targets and adopting slightly compressed images as the training target. Empirically, our VRPSR saves more than 10\% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression. Besides, our VRPSR facilitates joint optimization of the SR and post-processing model after recompression.

</details>


### [74] [Spotlight: Identifying and Localizing Video Generation Errors Using VLMs](https://arxiv.org/abs/2511.18102)
*Aditya Chinchure,Sahithya Ravi,Pushkar Shukla,Vered Shwartz,Leonid Sigal*

Main category: cs.CV

TL;DR: 本文提出了Spotlight任务，用于定位和解释视频生成中的错误，通过标注1600多个细粒度错误发现当前VLM在错误识别和定位方面显著落后于人类。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频模型(T2V)虽然能生成高质量视频，但仍存在错误，且现有评估方法通常整体评估视频而无法识别具体错误发生的时间和性质。

Method: 使用200个多样化文本提示和三种最先进的视频生成器生成600个视频，标注六种类型的1600多个细粒度错误，包括运动、物理和提示遵循等。

Result: 发现遵循性和物理错误占主导地位且持续时间较长，而外观消失和身体姿态错误出现在较短片段中；当前VLM在错误识别和定位方面显著落后于人类，通过推理时策略可将性能提升近2倍。

Conclusion: Spotlight任务为构建细粒度评估工具和更复杂的视频生成器奖励模型开辟了道路。

Abstract: Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.

</details>


### [75] [Consolidating Diffusion-Generated Video Detection with Unified Multimodal Forgery Learning](https://arxiv.org/abs/2511.18104)
*Xiaohong Liu,Xiufeng Song,Huayu Zheng,Lei Bai,Xiaoming Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: MM-Det++是一个用于检测扩散模型生成视频的多模态检测算法，包含时空分支和多模态分支，通过统一多模态学习模块整合两种表示，在大型DVF数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的视频日益增多引发信息安全担忧，现有方法主要关注图像级伪造检测，视频级伪造检测研究不足，需要可靠的合成媒体检测方法。

Method: 提出MM-Det++算法，包含两个创新分支：时空分支使用帧中心视觉变换器(FC-ViT)聚合时空信息；多模态分支利用多模态大语言模型(MLLMs)的可学习推理范式获取多模态伪造表示；通过统一多模态学习(UML)模块整合多模态表示。

Result: 在大型扩散视频取证(DVF)数据集上的广泛实验表明MM-Det++的优越性，突显了统一多模态伪造学习在检测扩散生成视频中的有效性。

Conclusion: MM-Det++通过整合时空和多模态信息，在检测扩散生成的视频方面表现出色，统一多模态学习显著提升了检测性能，为视频取证领域提供了有效解决方案。

Abstract: The proliferation of videos generated by diffusion models has raised increasing concerns about information security, highlighting the urgent need for reliable detection of synthetic media. Existing methods primarily focus on image-level forgery detection, leaving generic video-level forgery detection largely underexplored. To advance video forensics, we propose a consolidated multimodal detection algorithm, named MM-Det++, specifically designed for detecting diffusion-generated videos. Our approach consists of two innovative branches and a Unified Multimodal Learning (UML) module. Specifically, the Spatio-Temporal (ST) branch employs a novel Frame-Centric Vision Transformer (FC-ViT) to aggregate spatio-temporal information for detecting diffusion-generated videos, where the FC-tokens enable the capture of holistic forgery traces from each video frame. In parallel, the Multimodal (MM) branch adopts a learnable reasoning paradigm to acquire Multimodal Forgery Representation (MFR) by harnessing the powerful comprehension and reasoning capabilities of Multimodal Large Language Models (MLLMs), which discerns the forgery traces from a flexible semantic perspective. To integrate multimodal representations into a coherent space, a UML module is introduced to consolidate the generalization ability of MM-Det++. In addition, we also establish a large-scale and comprehensive Diffusion Video Forensics (DVF) dataset to advance research in video forgery detection. Extensive experiments demonstrate the superiority of MM-Det++ and highlight the effectiveness of unified multimodal forgery learning in detecting diffusion-generated videos.

</details>


### [76] [AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens](https://arxiv.org/abs/2511.18105)
*Purvish Jajal,Nick John Eliopoulos,Benjamin Shiue-Hal Chou,George K. Thiruvathukal,Yung-Hsiang Lu,James C. Davis*

Main category: cs.CV

TL;DR: AdaPerceiver是一种新型transformer架构，首次在单一模型中实现了深度、宽度和token的统一自适应能力，能够在不同硬件和延迟约束下动态调整计算分配。


<details>
  <summary>Details</summary>
Motivation: 现有transformer架构在推理时计算分配方式固定，无法适应多样化的硬件和延迟约束。大多数动态计算方法只关注单一维度（如减少token数量），缺乏统一的自适应能力。

Method: 提出支持深度、宽度和token三个维度自适应的架构，并配合高效的联合训练机制，确保模型在各种配置下保持性能。

Result: 在图像分类任务中，AdaPerceiver扩展了精度-吞吐量Pareto前沿，达到85.4%精度，吞吐量比FlexiViT-L高36%。在密集预测任务中，匹配ViT-H/14性能的同时，语义分割和深度估计的编码器FLOPs减少约26倍。配备策略后，能在保持ImageNet1K精度（±0.1个百分点）的同时减少24-33%的FLOPs。

Conclusion: AdaPerceiver通过统一的自适应架构，在保持性能的同时显著提升了计算效率，为transformer模型在多样化部署场景中的应用提供了有效解决方案。

Abstract: Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.

</details>


### [77] [PromptMoE: Generalizable Zero-Shot Anomaly Detection via Visually-Guided Prompt Mixtures](https://arxiv.org/abs/2511.18116)
*Yuheng Shao,Lizhang Wang,Changhao Li,Peixian Chen,Qinyuan Liu*

Main category: cs.CV

TL;DR: 提出了PromptMoE方法，通过混合专家机制动态组合多个专家提示来解决零样本异常检测中的表示瓶颈和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的零样本异常检测方法受限于提示工程策略，存在表示瓶颈和过拟合问题，难以泛化到复杂多样的未见异常。

Method: 学习专家提示池作为可组合语义基元，通过视觉引导的混合专家机制为每个实例动态组合提示，生成语义丰富的文本表示。

Result: 在工业和医疗领域的15个数据集上的广泛实验证明了PromptMoE的有效性和最先进性能。

Conclusion: PromptMoE通过组合式提示学习方法显著提升了零样本异常检测的泛化能力和性能。

Abstract: Zero-Shot Anomaly Detection (ZSAD) aims to identify and localize anomalous regions in images of unseen object classes. While recent methods based on vision-language models like CLIP show promise, their performance is constrained by existing prompt engineering strategies. Current approaches, whether relying on single fixed, learnable, or dense dynamic prompts, suffer from a representational bottleneck and are prone to overfitting on auxiliary data, failing to generalize to the complexity and diversity of unseen anomalies. To overcome these limitations, we propose $\mathtt{PromptMoE}$. Our core insight is that robust ZSAD requires a compositional approach to prompt learning. Instead of learning monolithic prompts, $\mathtt{PromptMoE}$ learns a pool of expert prompts, which serve as a basis set of composable semantic primitives, and a visually-guided Mixture-of-Experts (MoE) mechanism to dynamically combine them for each instance. Our framework materializes this concept through a Visually-Guided Mixture of Prompt (VGMoP) that employs an image-gated sparse MoE to aggregate diverse normal and abnormal expert state prompts, generating semantically rich textual representations with strong generalization. Extensive experiments across 15 datasets in industrial and medical domains demonstrate the effectiveness and state-of-the-art performance of $\mathtt{PromptMoE}$.

</details>


### [78] [MVS-TTA: Test-Time Adaptation for Multi-View Stereo via Meta-Auxiliary Learning](https://arxiv.org/abs/2511.18120)
*Hannuo Zhang,Zhixiang Chi,Yang Wang,Xinxin Zuo*

Main category: cs.CV

TL;DR: MVS-TTA是一个高效的测试时自适应框架，通过元辅助学习策略将优化式方法的自适应能力集成到基于学习的多视图立体方法中，提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 基于学习的多视图立体方法受限于有限训练数据分布，泛化能力不足；而优化式方法虽然支持场景特定适应但缺乏可扩展性且需要昂贵的逐场景优化。

Method: 提出自监督的跨视图一致性损失作为辅助任务指导推理时自适应，采用元辅助学习策略训练模型从辅助任务更新中获益，框架与模型无关且只需最小架构改动。

Result: 在标准数据集（DTU、BlendedMVS）和具有挑战性的跨数据集泛化设置上的广泛实验表明，MVS-TTA能持续提升性能，即使应用于最先进的MVS模型。

Conclusion: 这是首次使用元学习将基于优化的测试时自适应集成到基于学习的MVS中的尝试，显著提升了方法的适应性和泛化能力。

Abstract: Recent learning-based multi-view stereo (MVS) methods are data-driven and have achieved remarkable progress due to large-scale training data and advanced architectures. However, their generalization remains sub-optimal due to fixed model parameters trained on limited training data distributions. In contrast, optimization-based methods enable scene-specific adaptation but lack scalability and require costly per-scene optimization. In this paper, we propose MVS-TTA, an efficient test-time adaptation (TTA) framework that enhances the adaptability of learning-based MVS methods by bridging these two paradigms. Specifically, MVS-TTA employs a self-supervised, cross-view consistency loss as an auxiliary task to guide inference-time adaptation. We introduce a meta-auxiliary learning strategy to train the model to benefit from auxiliary-task-based updates explicitly. Our framework is model-agnostic and can be applied to a wide range of MVS methods with minimal architectural changes. Extensive experiments on standard datasets (DTU, BlendedMVS) and a challenging cross-dataset generalization setting demonstrate that MVS-TTA consistently improves performance, even when applied to state-of-the-art MVS models. To our knowledge, this is the first attempt to integrate optimization-based test-time adaptation into learning-based MVS using meta-learning. The code will be available at https://github.com/mart87987-svg/MVS-TTA.

</details>


### [79] [VCU-Bridge: Hierarchical Visual Connotation Understanding via Semantic Bridging](https://arxiv.org/abs/2511.18121)
*Ming Zhong,Yuanlei Wang,Liuzhou Zhang,Arctanx An,Renrui Zhang,Hao Liang,Ming Lu,Ying Shen,Wentao Zhang*

Main category: cs.CV

TL;DR: VCU-Bridge框架提出人类视觉内涵理解层次结构，从基础感知到语义桥接再到抽象内涵，构建HVCU-Bench基准进行层次诊断，发现模型性能随推理层次提升而下降，通过MCTS引导的数据生成增强低层能力可提升高层性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM模型处理范式与人类整合视觉信息能力不同，模型倾向于孤立处理细节和高级概念，而现有评估协议往往将低级感知与高级推理解耦，忽略了它们的语义和因果依赖关系。

Method: 提出VCU-Bridge框架，构建人类视觉内涵理解层次结构，创建HVCU-Bench基准进行层次诊断，开发基于蒙特卡洛树搜索的数据生成管道进行指令调优。

Result: 实验显示模型性能随推理层次提升而一致下降，增强低级能力可带来高层性能提升，不仅改善HVCU-Bench表现，还在通用基准上平均提升2.53%，MMStar上提升7.26%。

Conclusion: 分层思维模式对增强MLLM能力具有重要意义，通过强化低层能力可有效提升高层推理性能。

Abstract: While Multimodal Large Language Models (MLLMs) excel on benchmarks, their processing paradigm differs from the human ability to integrate visual information. Unlike humans who naturally bridge details and high-level concepts, models tend to treat these elements in isolation. Prevailing evaluation protocols often decouple low-level perception from high-level reasoning, overlooking their semantic and causal dependencies, which yields non-diagnostic results and obscures performance bottlenecks. We present VCU-Bridge, a framework that operationalizes a human-like hierarchy of visual connotation understanding: multi-level reasoning that advances from foundational perception through semantic bridging to abstract connotation, with an explicit evidence-to-inference trace from concrete cues to abstract conclusions. Building on this framework, we construct HVCU-Bench, a benchmark for hierarchical visual connotation understanding with explicit, level-wise diagnostics. Comprehensive experiments demonstrate a consistent decline in performance as reasoning progresses to higher levels. We further develop a data generation pipeline for instruction tuning guided by Monte Carlo Tree Search (MCTS) and show that strengthening low-level capabilities yields measurable gains at higher levels. Interestingly, it not only improves on HVCU-Bench but also brings benefits on general benchmarks (average +2.53%), especially with substantial gains on MMStar (+7.26%), demonstrating the significance of the hierarchical thinking pattern and its effectiveness in enhancing MLLM capabilities. The project page is at https://vcu-bridge.github.io .

</details>


### [80] [Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models](https://arxiv.org/abs/2511.18123)
*Dachuan Zhao,Weiyue Li,Zhenda Shen,Yushu Qiu,Bowen Xu,Haoyu Chen,Yongchao Chen*

Main category: cs.CV

TL;DR: 本文提出SPD方法，通过识别和移除线性可解码偏见的整个子空间来解决视觉语言模型中的偏见问题，相比现有方法在公平性指标上平均提升18.5%。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)的表征经常编码并放大人口统计偏见，导致下游任务中出现偏见关联和错误预测。现有的事后方法通过替换最相关的嵌入坐标来缓解偏见，但存在特征纠缠、跨数据集泛化能力差和不完全偏见移除三个关键问题。

Method: 提出子空间投影去偏见(SPD)框架，识别并移除线性可解码偏见的整个子空间，同时重新插入中性均值分量以保持语义保真度。

Result: 在零样本分类、文本到图像检索和图像生成等任务上的广泛实验验证了SPD的有效性：相比最佳去偏见基线，该方法实现了更稳健的去偏见效果，在四个公平性指标上平均提升18.5%，同时任务性能损失最小。

Conclusion: 偏见并非局限于少数坐标，而是分布在几个线性子空间中。SPD方法通过几何原理框架有效解决了现有方法的局限性，实现了更好的去偏见效果。

Abstract: Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\textbf{S}$ubspace $\textbf{P}$rojection $\textbf{D}$ebiasing ($\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.

</details>


### [81] [SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation](https://arxiv.org/abs/2511.18127)
*Ruicong Liu,Yifei Huang,Liangyang Ouyang,Caixin Kang,Yoichi Sato*

Main category: cs.CV

TL;DR: SFHand是首个用于语言引导3D手部预测的流式框架，能够从连续的视频和语言指令流中自回归预测未来3D手部状态，在3D手部预测任务上比现有方法提升35.8%，并在下游操作任务中提升13.4%的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法在实时交互场景中处理连续视频流和语言指导的问题，为AR和辅助机器人等应用提供更流畅的人机交互能力。

Method: 结合流式自回归架构和ROI增强记忆层，从连续的视频和语言指令流中预测手部类型、2D边界框、3D姿态和轨迹等完整手部状态。

Result: 在3D手部预测任务上达到新的最先进水平，比之前方法提升35.8%；在下游操作任务中提升13.4%的成功率。

Conclusion: SFHand框架成功解决了实时3D手部预测中的流式处理和语言指导问题，为实际应用提供了有效的解决方案。

Abstract: Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.

</details>


### [82] [Video4Edit: Viewing Image Editing as a Degenerate Temporal Process](https://arxiv.org/abs/2511.18131)
*Xiaofan Li,Yanpeng Sun,Chenming Wu,Fan Duan,YuAn Wang,Weihao Bo,Yumeng Zhang,Dingkang Liang*

Main category: cs.CV

TL;DR: 本文提出了一种基于时间建模视角的图像编辑方法，通过迁移视频预训练中的单帧演化先验，实现了仅需1%监督数据即可达到主流编辑模型性能的高效微调方案。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型虽然推动了指令驱动的图像生成和编辑，但现有编辑流程成本高昂，需要大量高质量的三元组数据（指令、源图像、编辑图像），且视觉替换的保真度依赖于指令对目标语义的精确引用。

Method: 将图像编辑视为退化的时间过程，从视频预训练中迁移单帧演化先验，采用高度数据高效的微调机制。

Result: 实证表明，该方法在使用仅约1%监督数据的情况下，能够匹配领先开源基线的性能。

Conclusion: 通过时间建模视角重新审视图像编辑挑战，实现了显著的数据效率提升，为指令驱动的图像编辑提供了更经济的解决方案。

Abstract: We observe that recent advances in multimodal foundation models have propelled instruction-driven image generation and editing into a genuinely cross-modal, cooperative regime. Nevertheless, state-of-the-art editing pipelines remain costly: beyond training large diffusion/flow models, they require curating massive high-quality triplets of \{instruction, source image, edited image\} to cover diverse user intents. Moreover, the fidelity of visual replacements hinges on how precisely the instruction references the target semantics. We revisit this challenge through the lens of temporal modeling: if video can be regarded as a full temporal process, then image editing can be seen as a degenerate temporal process. This perspective allows us to transfer single-frame evolution priors from video pre-training, enabling a highly data-efficient fine-tuning regime. Empirically, our approach matches the performance of leading open-source baselines while using only about one percent of the supervision demanded by mainstream editing models.

</details>


### [83] [SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation](https://arxiv.org/abs/2511.18136)
*Chunming He,Rihan Zhang,Longxiang Tang,Ziyun Yang,Kai Li,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: SCALER是一个统一的协作框架，用于标签不足的隐蔽目标分割，通过联合优化均值教师分割器和可学习的SAM模型，在交替的两个阶段中实现相互监督和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在标签不足的隐蔽目标分割中性能有限，本研究探索能否将一致性约束和SAM监督联合集成，并实现分割器和SAM之间的相互指导。

Method: 提出SCALER框架，包含两个交替阶段：阶段I在固定SAM监督下优化分割器，使用基于熵的图像级和基于不确定性的像素级加权；阶段II通过增强不变性和噪声抵抗损失更新SAM。

Result: 实验表明SCALER在八个半监督和弱监督COS任务中均获得一致性能提升，能够作为通用训练范式增强轻量分割器和大型基础模型。

Conclusion: SCALER框架成功实现了分割器和SAM的协同优化，在标签稀缺条件下显著提升了隐蔽目标分割性能，具有通用性和实用性。

Abstract: Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \textbf{Phase \uppercase\expandafter{\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \textbf{Phase \uppercase\expandafter{\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.

</details>


### [84] [Compact neural networks for astronomy with optimal transport bias correction](https://arxiv.org/abs/2511.18139)
*Shuhuan Wang,Yuzhen Xie,Jiayi Li*

Main category: cs.CV

TL;DR: WaveletMamba是一个将小波分解与状态空间建模相结合的天文成像框架，在64x64分辨率下实现81.72%的分类准确率，仅需3.54M参数，并在低分辨率输入下获得高分辨率性能，计算效率提升9.7倍。


<details>
  <summary>Details</summary>
Motivation: 解决天文成像中效率与分辨率之间的权衡问题，该问题限制了大尺度形态分类和红移预测的能力。

Method: 集成小波分解、状态空间建模、数学正则化和多级偏差校正的理论驱动框架，包括HK距离（分布级最优传输）和颜色感知加权（样本级微调）。

Result: 在64x64分辨率下达到81.72%±0.53%的分类准确率，在244x244分辨率下达到80.93%±0.27%的准确率；实现22.96%的Log-MSE改进和26.10%的异常值减少；展现分辨率多稳态性。

Conclusion: 数学严谨性能够在科学AI中实现前所未有的效率和全面偏差校正，连接计算机视觉和天体物理学，革新跨学科科学发现。

Abstract: Astronomical imaging confronts an efficiency-resolution tradeoff that limits large-scale morphological classification and redshift prediction. We introduce WaveletMamba, a theory-driven framework integrating wavelet decomposition with state-space modeling, mathematical regularization, and multi-level bias correction. WaveletMamba achieves 81.72% +/- 0.53% classification accuracy at 64x64 resolution with only 3.54M parameters, delivering high-resolution performance (80.93% +/- 0.27% at 244x244) at low-resolution inputs with 9.7x computational efficiency gains. The framework exhibits Resolution Multistability, where models trained on low-resolution data achieve consistent accuracy across different input scales despite divergent internal representations. The framework's multi-level bias correction synergizes HK distance (distribution-level optimal transport) with Color-Aware Weighting (sample-level fine-tuning), achieving 22.96% Log-MSE improvement and 26.10% outlier reduction without explicit selection function modeling. Here, we show that mathematical rigor enables unprecedented efficiency and comprehensive bias correction in scientific AI, bridging computer vision and astrophysics to revolutionize interdisciplinary scientific discovery.

</details>


### [85] [Matching-Based Few-Shot Semantic Segmentation Models Are Interpretable by Design](https://arxiv.org/abs/2511.18163)
*Pasquale De Marinis,Uzay Kaymak,Rogier Brussee,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: 本文提出了首个专门用于解释基于匹配的少样本语义分割模型的方法——Affinity Explainer，通过利用模型固有结构特性生成归因图，揭示支持图像中哪些像素对查询图像分割预测贡献最大。


<details>
  <summary>Details</summary>
Motivation: 少样本语义分割模型在分割新类别方面表现优异，但其决策过程仍不透明。尽管可解释AI在标准计算机视觉任务中已有显著进展，但在少样本分割领域的可解释性研究几乎空白，这对于理解模型行为和在数据稀缺场景中指导支持集选择至关重要。

Method: Affinity Explainer方法通过计算支持图像和查询图像特征在多个特征层级上的匹配分数，提取归因图来突出显示支持图像中对查询分割预测贡献最大的像素。

Result: 在少样本分割基准数据集上的综合实验表明，Affinity Explainer显著优于适应标准归因方法。定性分析显示，该方法提供的解释具有结构化、连贯的注意力模式，与模型架构一致，并能实现有效的模型诊断。

Conclusion: 这项工作为可解释的少样本分割研究奠定了基础，能够实现更好的模型理解和诊断，从而构建更可靠的少样本分割系统。

Abstract: Few-Shot Semantic Segmentation (FSS) models achieve strong performance in segmenting novel classes with minimal labeled examples, yet their decision-making processes remain largely opaque. While explainable AI has advanced significantly in standard computer vision tasks, interpretability in FSS remains virtually unexplored despite its critical importance for understanding model behavior and guiding support set selection in data-scarce scenarios. This paper introduces the first dedicated method for interpreting matching-based FSS models by leveraging their inherent structural properties. Our Affinity Explainer approach extracts attribution maps that highlight which pixels in support images contribute most to query segmentation predictions, using matching scores computed between support and query features at multiple feature levels. We extend standard interpretability evaluation metrics to the FSS domain and propose additional metrics to better capture the practical utility of explanations in few-shot scenarios. Comprehensive experiments on FSS benchmark datasets, using different models, demonstrate that our Affinity Explainer significantly outperforms adapted standard attribution methods. Qualitative analysis reveals that our explanations provide structured, coherent attention patterns that align with model architectures and and enable effective model diagnosis. This work establishes the foundation for interpretable FSS research, enabling better model understanding and diagnostic for more reliable few-shot segmentation systems. The source code is publicly available at https://github.com/pasqualedem/AffinityExplainer.

</details>


### [86] [Unified Spherical Frontend: Learning Rotation-Equivariant Representations of Spherical Images from Any Camera](https://arxiv.org/abs/2511.18174)
*Mukai Yu,Mosam Dabhi,Liuyue Xie,Sebastian Scherer,László A. Jeni*

Main category: cs.CV

TL;DR: USF是一个统一球形前端框架，可将任何校准相机的图像转换为单位球面表示，直接在空间域执行球形重采样、卷积和池化操作，避免了昂贵的球谐变换，提供可配置的旋转等变性。


<details>
  <summary>Details</summary>
Motivation: 现代感知系统越来越多地使用鱼眼、全景等广角相机，但大多数管道仍使用为针孔图像设计的平面CNN，在2D网格上处理时图像空间邻域无法正确表示物理相邻关系，且模型对全局旋转敏感。

Method: 通过光线方向对应关系将图像转换为单位球面表示，在空间域直接执行球形重采样、卷积和池化，使用仅距离的球形核提供可配置的旋转等变性，模块化设计分离投影、位置采样、插值和分辨率控制。

Result: 在合成和真实数据集上的分类、检测和分割任务中，USF能高效处理高分辨率球形图像，在随机测试时旋转下性能下降小于1%，无需旋转增强，并能实现从一种镜头类型到未见过的广角镜头的零样本泛化。

Conclusion: USF提供了一个镜头无关的框架，有效解决了广角相机图像处理中的旋转敏感性和物理相邻关系表示问题，避免了球谐变换的计算成本，具有良好的泛化能力和鲁棒性。

Abstract: Modern perception increasingly relies on fisheye, panoramic, and other wide field-of-view (FoV) cameras, yet most pipelines still apply planar CNNs designed for pinhole imagery on 2D grids, where image-space neighborhoods misrepresent physical adjacency and models are sensitive to global rotations. Frequency-domain spherical CNNs partially address this mismatch but require costly spherical harmonic transforms that constrain resolution and efficiency. We introduce the Unified Spherical Frontend (USF), a lens-agnostic framework that transforms images from any calibrated camera into a unit-sphere representation via ray-direction correspondences, and performs spherical resampling, convolution, and pooling directly in the spatial domain. USF is modular: projection, location sampling, interpolation, and resolution control are fully decoupled. Its distance-only spherical kernels offer configurable rotation-equivariance (mirroring translation-equivariance in planar CNNs) while avoiding harmonic transforms entirely. We compare standard planar backbones with their spherical counterparts across classification, detection, and segmentation tasks on synthetic (Spherical MNIST) and real-world datasets (PANDORA, Stanford 2D-3D-S), and stress-test robustness to extreme lens distortions, varying FoV, and arbitrary rotations. USF processes high-resolution spherical imagery efficiently and maintains less than 1% performance drop under random test-time rotations, even without rotational augmentation, and even enables zero-shot generalization from one lens type to unseen wide-FoV lenses with minimal performance degradation.

</details>


### [87] [Early Lung Cancer Diagnosis from Virtual Follow-up LDCT Generation via Correlational Autoencoder and Latent Flow Matching](https://arxiv.org/abs/2511.18185)
*Yutong Wu,Yifan Wang,Qining Zhang,Chuan Zhou,Lei Ying*

Main category: cs.CV

TL;DR: 本文提出了一种名为CorrFlowNet的生成方法，利用扩散模型生成虚拟的一年随访CT扫描，用于早期检测肺结节的恶性/良性，减少等待临床随访的需求。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期诊断具有挑战性，特别是区分恶性肿瘤的早期信号与良性病变。现有AI方法主要关注从单次早期CT扫描中提取影像组学特征，而患者通常需要多次随访检查才能获得明确诊断，可能错过最佳治疗时机。

Method: 采用相关性自编码器将早期基线和随访CT图像编码到潜在空间，捕捉结节进展动态及其相关性，然后在潜在空间上使用神经常微分方程进行流匹配算法，并利用辅助分类器提高诊断准确性。

Result: 在真实临床数据集上的评估表明，该方法相比现有基线模型能显著改善下游肺结节风险评估，其诊断准确性与真实临床CT随访相当。

Conclusion: CorrFlowNet方法具有改善癌症诊断的潜力，通过生成虚拟随访CT扫描，可以减少对临床随访的依赖，实现更早期的肺癌检测。

Abstract: Lung cancer is one of the most commonly diagnosed cancers, and early diagnosis is critical because the survival rate declines sharply once the disease progresses to advanced stages. However, achieving an early diagnosis remains challenging, particularly in distinguishing subtle early signals of malignancy from those of benign conditions. In clinical practice, a patient with a high risk may need to undergo an initial baseline and several annual follow-up examinations (e.g., CT scans) before receiving a definitive diagnosis, which can result in missing the optimal treatment. Recently, Artificial Intelligence (AI) methods have been increasingly used for early diagnosis of lung cancer, but most existing algorithms focus on radiomic features extraction from single early-stage CT scans. Inspired by recent advances in diffusion models for image generation, this paper proposes a generative method, named CorrFlowNet, which creates a virtual, one-year follow-up CT scan after the initial baseline scan. This virtual follow-up would allow for an early detection of malignant/benign nodules, reducing the need to wait for clinical follow-ups. During training, our approach employs a correlational autoencoder to encode both early baseline and follow-up CT images into a latent space that captures the dynamics of nodule progression as well as the correlations between them, followed by a flow matching algorithm on the latent space with a neural ordinary differential equation. An auxiliary classifier is used to further enhance the diagnostic accuracy. Evaluations on a real clinical dataset show our method can significantly improve downstream lung nodule risk assessment compared with existing baseline models. Moreover, its diagnostic accuracy is comparable with real clinical CT follow-ups, highlighting its potential to improve cancer diagnosis.

</details>


### [88] [Generating Synthetic Human Blastocyst Images for In-Vitro Fertilization Blastocyst Grading](https://arxiv.org/abs/2511.18204)
*Pavan Narahari,Suraj Rajendran,Lorena Bori,Jonas E. Malmsten,Qiansheng Zhan,Zev Rosenwaks,Nikica Zaninovic,Iman Hajirasouliha*

Main category: cs.CV

TL;DR: 提出了DIA框架，使用潜在扩散模型生成高质量的第5天囊胚图像，通过条件控制形态类别和焦距，显著改善胚胎AI评估的数据稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决体外受精中囊胚形态评估的主观性和不一致性问题，以及AI模型训练所需的大规模、多样化数据集难以获取的挑战。

Method: 开发基于潜在扩散模型的DIA框架，能够根据Gardner形态分类和z轴焦距生成高保真度的囊胚图像，并进行多维度评估。

Result: 生成的合成图像在胚胎学家图灵测试中无法与真实图像区分，数据增强显著提高了分类准确性，合成数据可替代高达40%的真实数据而不损失精度。

Conclusion: DIA框架为胚胎数据集中的数据稀缺和类别不平衡问题提供了稳健解决方案，能够提高AI胚胎评估工具的性能、公平性和标准化程度。

Abstract: The success of in vitro fertilization (IVF) at many clinics relies on the accurate morphological assessment of day 5 blastocysts, a process that is often subjective and inconsistent. While artificial intelligence can help standardize this evaluation, models require large, diverse, and balanced datasets, which are often unavailable due to data scarcity, natural class imbalance, and privacy constraints. Existing generative embryo models can mitigate these issues but face several limitations, such as poor image quality, small training datasets, non-robust evaluation, and lack of clinically relevant image generation for effective data augmentation. Here, we present the Diffusion Based Imaging Model for Artificial Blastocysts (DIA) framework, a set of latent diffusion models trained to generate high-fidelity, novel day 5 blastocyst images. Our models provide granular control by conditioning on Gardner-based morphological categories and z-axis focal depth. We rigorously evaluated the models using FID, a memorization metric, an embryologist Turing test, and three downstream classification tasks. Our results show that DIA models generate realistic images that embryologists could not reliably distinguish from real images. Most importantly, we demonstrated clear clinical value. Augmenting an imbalanced dataset with synthetic images significantly improved classification accuracy (p < 0.05). Also, adding synthetic images to an already large, balanced dataset yielded statistically significant performance gains, and synthetic data could replace up to 40% of real data in some cases without a statistically significant loss in accuracy. DIA provides a robust solution for mitigating data scarcity and class imbalance in embryo datasets. By generating novel, high-fidelity, and controllable synthetic images, our models can improve the performance, fairness, and standardization of AI embryo assessment tools.

</details>


### [89] [Large-Scale Pre-training Enables Multimodal AI Differentiation of Radiation Necrosis from Brain Metastasis Progression on Routine MRI](https://arxiv.org/abs/2511.18208)
*Ahmed Gomaa,Annette Schwarz,Ludwig Singer,Arnd Dörfler,Matthias Stefan May,Pluvio Stephan,Ishita Sheth,Juliane Szkitsak,Katharina Breininger,Yixing Huang,Benjamin Frey,Oliver Schnell,Daniel Delev,Roland Coras,Daniel Höfler,Philipp Schubert,Jenny Stritzelberger,Sabine Semrau,Andreas Maier,Dieter H Heiland,Udo S. Gaipl,Andrea Wittig,Rainer Fietkau,Christoph Bert,Stefanie Corradini,Florian Putz*

Main category: cs.CV

TL;DR: 该研究提出了一种基于自监督学习的双阶段深度学习策略，用于区分脑转移瘤放疗后的放射性坏死与肿瘤进展。通过在10,167个未标记MRI子体积上进行预训练，然后在109个活检确认病例上微调，模型在相同中心和外部验证集上均显著优于全监督方法和放射组学。


<details>
  <summary>Details</summary>
Motivation: 脑转移瘤立体定向放射外科治疗后，区分放射性坏死与肿瘤进展是临床关键挑战。活检作为金标准具有侵入性，而传统监督学习受限于活检确认训练数据的稀缺性。自监督学习可以利用大规模未标记脑转移瘤影像数据集克服这一限制。

Method: 采用双阶段深度学习策略：首先在10,167个未标记多源T1CE MRI子体积上通过自监督学习预训练Vision Transformer，然后在公开MOLAB数据集（n=109）上使用双通道输入（T1CE MRI和分割掩码）进行微调。使用20%数据集作为相同中心测试集，并在第二中心测试队列（n=28）上进行外部验证。

Result: 自监督模型在相同中心测试集上AUC为0.916，在第二中心测试集上AUC为0.764，显著优于全监督ViT（AUC 0.624/0.496）和放射组学（AUC 0.807/0.691）。多模态集成进一步提升了性能（AUC 0.947/0.821）。注意力图可视化显示模型聚焦于临床相关病变亚区域。

Conclusion: 大规模预训练显著提高了AI模型性能。双阶段多模态深度学习策略仅使用常规T1CE MRI和标准临床数据即可高精度区分放射性坏死与肿瘤进展，提供了可解释、临床可访问的解决方案，值得进一步验证。

Abstract: Background: Differentiating radiation necrosis (RN) from tumor progression after stereotactic radiosurgery (SRS) remains a critical challenge in brain metastases. While histopathology represents the gold standard, its invasiveness limits feasibility. Conventional supervised deep learning approaches are constrained by scarce biopsy-confirmed training data. Self-supervised learning (SSL) overcomes this by leveraging the growing availability of large-scale unlabeled brain metastases imaging datasets. Methods: In a two-phase deep learning strategy inspired by the foundation model paradigm, a Vision Transformer (ViT) was pre-trained via SSL on 10,167 unlabeled multi-source T1CE MRI sub-volumes. The pre-trained ViT was then fine-tuned for RN classification using a two-channel input (T1CE MRI and segmentation masks) on the public MOLAB dataset (n=109) using 20% of datasets as same-center held-out test set. External validation was performed on a second-center test cohort (n=28). Results: The self-supervised model achieved an AUC of 0.916 on the same-center test set and 0.764 on the second center test set, surpassing the fully supervised ViT (AUC 0.624/0.496; p=0.001/0.008) and radiomics (AUC 0.807/0.691; p=0.005/0.014). Multimodal integration further improved performance (AUC 0.947/0.821; p=0.073/0.001). Attention map visualizations enabled interpretability showing the model focused on clinically relevant lesion subregions. Conclusion: Large-scale pre-training on increasingly available unlabeled brain metastases datasets substantially improves AI model performance. A two-phase multimodal deep learning strategy achieved high accuracy in differentiating radiation necrosis from tumor progression using only routine T1CE MRI and standard clinical data, providing an interpretable, clinically accessible solution that warrants further validation.

</details>


### [90] [EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning](https://arxiv.org/abs/2511.18242)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: EgoVITA是一个基于强化学习的框架，通过交替的自我中心规划和异中心验证阶段，提升多模态大语言模型在自我中心视频中的意图和行动推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在自我中心视角下推理意图和行动的根本挑战，因为自我中心视频具有部分可观察性、有限视野和自我参考运动等特点。

Method: 基于组相对策略优化(GRPO)，交替进行自我中心规划阶段（从第一人称视角预测未来行动步骤）和异中心验证阶段（从第三人称视角检查计划的视觉和逻辑一致性）。

Result: 在自我中心推理任务上取得显著提升，相比基线Qwen2.5-VL-7B，在EgoBlind上提升+7.7，在EgoOrient上提升+4.4，同时在异中心视频任务上保持强泛化能力。

Conclusion: EgoVITA通过结构化规划和验证，使模型能够学习对即将到来的视觉观察具有因果预测性的计划，从而实现更连贯和视觉基础化的推理。

Abstract: Reasoning about intentions and actions from a first-person (egocentric) perspective remains a fundamental challenge for multimodal large language models (MLLMs). Unlike third-person (exocentric) videos that capture scenes from an outside observer, egocentric videos reflect the actor's continuously changing viewpoint, introducing partial observability, limited field of view, and self-referenced motion. We introduce $\textbf{EgoVITA}$, a reinforcement learning framework that enables MLLMs to reason through structured planning and verification. Built on Group Relative Policy Optimization (GRPO), EgoVITA alternates between two stages: (1) an $\textbf{egocentric planning phase}$, where the model reasons from a first-person viewpoint to predict a step-by-step plan of future actions, and (2) an $\textbf{exocentric verification phase}$, where it switches to a third-person perspective to check the visual and logical consistency of that plan. Through GRPO, the model learns to make plans that are causally predictive of upcoming visual observations, leading to more coherent and visually grounded reasoning. EgoVITA achieves significant gains on egocentric reasoning tasks, outperforming the baseline Qwen2.5-VL-7B by $\mathbf{+7.7}$ on EgoBlind and $\mathbf{+4.4}$ on EgoOrient, while maintaining strong generalization on exocentric video tasks.

</details>


### [91] [UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization](https://arxiv.org/abs/2511.18254)
*Siyi Li,Qingwen Zhang,Ishan Khatri,Kyle Vedder,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: 本文提出UniFlow方法，通过跨数据集训练学习通用的LiDAR场景流估计先验，在多个数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR场景流方法通常在单一传感器上训练和评估，缺乏泛化能力。本文旨在学习能够迁移到不同LiDAR传感器的通用运动先验。

Method: 提出UniFlow系列前馈模型，统一训练多个大规模LiDAR场景流数据集，包含不同的传感器布局和点云密度。

Result: 在Waymo和nuScenes数据集上分别提升5.1%和35.2%，在未见过的TruckScenes数据集上超越特定模型30.1%。

Conclusion: 运动估计等低级任务对传感器配置不敏感，跨数据集训练能显著提升LiDAR场景流估计的泛化性能。

Abstract: LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.

</details>


### [92] [Sequence-Adaptive Video Prediction in Continuous Streams using Diffusion Noise Optimization](https://arxiv.org/abs/2511.18255)
*Sina Mokhtarzadeh Azar,Emad Bahrami,Enrico Pallotta,Gianpiero Francesca,Radu Timofte,Juergen Gall*

Main category: cs.CV

TL;DR: 本文提出了一种针对连续视频流的扩散模型自适应预测方法SAVi-DNO，通过优化扩散噪声而非模型参数来实现对视频流的持续适应，在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对连续视频流预测问题，现有模型无法利用不断出现的新训练样本来改进预测效果，需要一种能够持续适应视频流的预测方法。

Method: 提出SAVi-DNO方法，在推理过程中优化扩散噪声而保持模型参数不变，使模型能够自适应地确定合适的采样噪声。

Result: 在Ego4D、OpenDV-YouTube、UCF-101和SkyTimelapse数据集上的实验结果表明，该方法在FVD、SSIM和PSNR指标上均有提升。

Conclusion: SAVi-DNO方法能够有效适应连续视频流，提高扩散模型的视频预测性能，且计算效率高。

Abstract: In this work, we investigate diffusion-based video prediction models, which forecast future video frames, for continuous video streams. In this context, the models observe continuously new training samples, and we aim to leverage this to improve their predictions. We thus propose an approach that continuously adapts a pre-trained diffusion model to a video stream. Since fine-tuning the parameters of a large diffusion model is too expensive, we refine the diffusion noise during inference while keeping the model parameters frozen, allowing the model to adaptively determine suitable sampling noise. We term the approach Sequence Adaptive Video Prediction with Diffusion Noise Optimization (SAVi-DNO). To validate our approach, we introduce a new evaluation setting on the Ego4D dataset, focusing on simultaneous adaptation and evaluation on long continuous videos. Empirical results demonstrate improved performance based on FVD, SSIM, and PSNR metrics on long videos of Ego4D and OpenDV-YouTube, as well as videos of UCF-101 and SkyTimelapse, showcasing SAVi-DNO's effectiveness.

</details>


### [93] [MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2511.18262)
*Tao Shen,Xin Wan,Taicai Chen,Rui Zhang,Junwen Pan,Dawei Lu,Fanding Lei,Zhilin Lu,Yunfei Yang,Chen Cheng,Qi She,Chang Liu,Zhenbang Sun*

Main category: cs.CV

TL;DR: Mammoth2是一个统一的自动回归-扩散框架，通过耦合自动回归语义规划和扩散生成，实现了高质量图像生成和编辑，同时在多模态理解任务上保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型需要在单个框架中集成理解和生成，但离散语义推理与高保真视觉合成之间的差距仍然存在挑战。

Method: 采用串行设计：AR路径进行全局语义建模，单流DiT解码器处理高保真图像合成；通过AR-Diffusion特征对齐模块稳定对齐表示；使用联合Next-Token Prediction和Flow Matching目标进行端到端训练。

Result: 在公开基准测试中表现优异：GenEval得分0.87，DPGBench得分87.2，ImgEdit得分4.06；同时在多模态理解任务上与纯理解模型保持竞争力。

Conclusion: 精心耦合的AR-Diffusion架构可以在单个参数和数据高效的模型中提供高保真生成和编辑，同时保持强大的多模态理解能力。

Abstract: Unified multimodal models aim to integrate understanding and generation within a single framework, yet bridging the gap between discrete semantic reasoning and high-fidelity visual synthesis remains challenging. We present MammothModa2 (Mammoth2), a unified autoregressive-diffusion (AR-Diffusion) framework designed to effectively couple autoregressive semantic planning with diffusion-based generation. Mammoth2 adopts a serial design: an AR path equipped with generation experts performs global semantic modeling over discrete tokens, while a single-stream Diffusion Transformer (DiT) decoder handles high-fidelity image synthesis. A carefully designed AR-Diffusion feature alignment module combines multi-layer feature aggregation, unified condition encoding, and in-context conditioning to stably align AR's representations with the diffusion decoder's continuous latents. Mammoth2 is trained end-to-end with joint Next-Token Prediction and Flow Matching objectives, followed by supervised fine-tuning and reinforcement learning over both generation and editing. With roughly 60M supervised generation samples and no reliance on pre-trained generators, Mammoth2 delivers strong text-to-image and instruction-based editing performance on public benchmarks, achieving 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit, while remaining competitive with understanding-only backbones (e.g., Qwen3-VL-8B) on multimodal understanding tasks. These results suggest that a carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.

</details>


### [94] [Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation](https://arxiv.org/abs/2511.18272)
*Richard J. Young*

Main category: cs.CV

TL;DR: 本研究首次系统评估了在医疗文档OCR中使用视觉token掩码作为隐私保护机制的效果，发现所有掩码策略对PHI的减少率都收敛于42.9%，能有效抑制长格式空间分布标识符但无法阻止短结构化标识符的泄露。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医疗环境中部署OCR功能时，存在受保护健康信息暴露的风险，需要评估隐私保护机制的有效性。

Method: 使用DeepSeek-OCR，引入七种针对不同架构层的掩码策略，在100份合成医疗账单上评估PHI减少效果，并进行掩码扩展半径的消融研究。

Result: 所有掩码策略对PHI的减少率均为42.9%，能100%有效抑制长格式标识符但对短结构化标识符完全无效。模拟混合架构结合NLP后处理可实现88.6%的总PHI减少。

Conclusion: 纯视觉隐私干预存在局限性，需要区分适合视觉级与语言级编辑的PHI类型，未来研究应转向解码器级微调和混合防御架构以实现HIPAA合规的医疗文档处理。

Abstract: Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.

</details>


### [95] [Point-to-Point: Sparse Motion Guidance for Controllable Video Editing](https://arxiv.org/abs/2511.18277)
*Yeji Song,Jaehyun Lee,Mijin Koo,JunHoo Lee,Nojun Kwak*

Main category: cs.CV

TL;DR: 本文提出了一种名为锚点令牌的新型运动表示方法，通过视频扩散模型的丰富先验来捕捉关键运动模式，解决了视频编辑中运动保真度和编辑保真度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在运动保真度和编辑保真度之间存在权衡，因为它们依赖的运动表示要么过度拟合布局，要么只是隐式定义。准确保持运动同时编辑主体仍然是视频编辑任务的核心挑战。

Method: 提出了锚点令牌运动表示方法，通过少量信息丰富的点轨迹紧凑编码视频动态，并可以灵活重新定位以与新主体对齐。该方法名为Point-to-Point，能够泛化到多样化场景。

Result: 大量实验表明，锚点令牌能够实现更可控和语义对齐的视频编辑，在编辑保真度和运动保真度方面都取得了优越性能。

Conclusion: 锚点令牌作为一种新颖的运动表示方法，有效解决了视频编辑中运动保真的核心挑战，为多样化场景下的视频编辑提供了更优的解决方案。

Abstract: Accurately preserving motion while editing a subject remains a core challenge in video editing tasks. Existing methods often face a trade-off between edit and motion fidelity, as they rely on motion representations that are either overfitted to the layout or only implicitly defined. To overcome this limitation, we revisit point-based motion representation. However, identifying meaningful points remains challenging without human input, especially across diverse video scenarios. To address this, we propose a novel motion representation, anchor tokens, that capture the most essential motion patterns by leveraging the rich prior of a video diffusion model. Anchor tokens encode video dynamics compactly through a small number of informative point trajectories and can be flexibly relocated to align with new subjects. This allows our method, Point-to-Point, to generalize across diverse scenarios. Extensive experiments demonstrate that anchor tokens lead to more controllable and semantically aligned video edits, achieving superior performance in terms of edit and motion fidelity.

</details>


### [96] [RoadSceneVQA: Benchmarking Visual Question Answering in Roadside Perception Systems for Intelligent Transportation System](https://arxiv.org/abs/2511.18286)
*Runwei Guan,Rongsheng Hu,Shangshu Chen,Ningyuan Xiao,Xue Xia,Jiayang Liu,Beibei Chen,Ziren Tang,Ningwei Ouyang,Shaofeng Liang,Yuxuan Fan,Wanjie Sun,Yutao Yue*

Main category: cs.CV

TL;DR: 提出了RoadSceneVQA数据集和RoadMind基线模型，用于路边场景的视觉问答，通过CAF融合模块和AD-CoT推理方法提升多模态大语言模型在交通感知和推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有路边感知系统主要关注实例级感知，缺乏自然语言交互和交通行为上下文推理能力，需要构建专门的路边场景VQA数据集和方法。

Method: 构建RoadSceneVQA数据集（34,736个QA对），提出CogniAnchor Fusion视觉语言融合模块和Assisted Decoupled Chain-of-Thought推理方法，开发RoadMind基线模型。

Result: 在RoadSceneVQA和CODA-LM基准测试中，该方法在推理准确性和计算效率方面均有提升，在结构化交通感知和推理任务中达到最先进性能。

Conclusion: RoadSceneVQA数据集和RoadMind模型有效解决了路边场景的视觉问答和推理问题，为交通感知系统提供了自然语言交互和上下文推理能力。

Abstract: Current roadside perception systems mainly focus on instance-level perception, which fall short in enabling interaction via natural language and reasoning about traffic behaviors in context. To bridge this gap, we introduce RoadSceneVQA, a large-scale and richly annotated visual question answering (VQA) dataset specifically tailored for roadside scenarios. The dataset comprises 34,736 diverse QA pairs collected under varying weather, illumination, and traffic conditions, targeting not only object attributes but also the intent, legality, and interaction patterns of traffic participants. RoadSceneVQA challenges models to perform both explicit recognition and implicit commonsense reasoning, grounded in real-world traffic rules and contextual dependencies. To fully exploit the reasoning potential of Multi-modal Large Language Models (MLLMs), we further propose CogniAnchor Fusion (CAF), a vision-language fusion module inspired by human-like scene anchoring mechanisms. Moreover, we propose the Assisted Decoupled Chain-of-Thought (AD-CoT) to enhance the reasoned thinking via CoT prompting and multi-task learning. Based on the above, we propose the baseline model RoadMind. Experiments on RoadSceneVQA and CODA-LM benchmark show that the pipeline consistently improves both reasoning accuracy and computational efficiency, allowing the MLLM to achieve state-of-the-art performance in structural traffic perception and reasoning tasks.

</details>


### [97] [SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes](https://arxiv.org/abs/2511.18290)
*Jungho Lee,Minhyeok Lee,Sunghun Yang,Minseok Kang,Sangyoun Lee*

Main category: cs.CV

TL;DR: SwiftVGGT是一种无需训练的3D重建方法，在大规模场景中显著减少推理时间同时保持高质量稠密重建，通过消除外部VPR模型依赖和简化点采样对齐过程实现高效重建。


<details>
  <summary>Details</summary>
Motivation: 现有大规模3D重建方法在精度和计算效率之间存在固有权衡，要么优先速度但质量低，要么高质量但推理慢。需要一种既能保持高质量又能显著减少推理时间的方法。

Method: 1) 无需外部VPR模型执行闭环检测以保持全局一致性；2) 提出简单有效的点采样方法，使用单次Sim(3) SVD对齐相邻块，消除IRLS优化需求。

Result: 在多个数据集上评估显示，SwiftVGGT达到最先进的重建质量，同时仅需最近VGGT基大规模重建方法33%的推理时间。

Conclusion: SwiftVGGT通过消除冗余计算和简化对齐过程，成功解决了大规模3D重建中精度与效率的权衡问题，在千米级环境中实现准确高效重建。

Abstract: 3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.

</details>


### [98] [DiVE-k: Differential Visual Reasoning for Fine-grained Image Recognition](https://arxiv.org/abs/2511.18305)
*Raja Kumar,Arka Sadhu,Ram Nevatia*

Main category: cs.CV

TL;DR: DiVE-k是一个利用模型自身top-k预测作为训练信号的框架，通过创建多项选择题并使用强化学习训练模型选择正确答案，解决LVLMs在细粒度图像识别中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型拥有丰富的文本知识，但在细粒度图像识别中难以利用这些知识区分视觉相似类别，现有基于精确匹配奖励信号的强化学习方法容易导致记忆训练类别而缺乏泛化能力。

Method: 提出DiVE-k框架：为每个训练图像从模型top-k输出创建多项选择题，使用强化学习训练模型选择正确答案，要求模型在合理选项间进行细粒度差分推理。

Result: 在五个标准细粒度数据集上的实验表明，该方法显著优于现有方法。在基础到新类泛化设置中，DiVE-k在调和平均数指标上分别超过QWEN2.5-VL-7B和ViRFT 10.04%和6.16%。

Conclusion: DiVE-k通过利用模型自身top-k预测作为训练信号，提供简单可验证的奖励信号，有效缓解记忆问题并提升泛化能力，在多种场景下均表现出显著优势。

Abstract: Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\textbf{DiVE-k}$, $\textbf{Di}$fferential $\textbf{V}$isual r$\textbf{E}$asoning using top-$\textbf{k}$ generations, framework that leverages model's own top-k predictions as a training signal. For each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. Experiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. In the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios.

</details>


### [99] [ScriptViT: Vision Transformer-Based Personalized Handwriting Generation](https://arxiv.org/abs/2511.18307)
*Sajjan Acharya,Rajendra Baskota*

Main category: cs.CV

TL;DR: 提出一个统一框架，通过Vision Transformer风格编码器学习全局风格模式，结合交叉注意力机制生成更忠实反映特定书写者风格的笔迹图像，并使用显著性笔画注意力分析提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉书写者特定属性（特别是跨越长距离空间依赖的全局风格模式）方面存在困难，难以同时保持生成文本的准确性和风格一致性。

Method: 使用Vision Transformer风格编码器从多张参考图像学习全局风格模式，通过交叉注意力机制将风格线索与目标文本集成，并采用显著性笔画注意力分析进行可解释性分析。

Result: 该方法能够更好地表示笔迹的长距离结构特征，生成更风格一致的手写图像，同时提供笔画级别的可解释性分析。

Conclusion: 所提出的框架不仅提高了笔迹合成的风格一致性，还使其更易于理解和分析，为风格化笔迹生成提供了更有效的解决方案。

Abstract: Styled handwriting generation aims to synthesize handwritten text that looks both realistic and aligned with a specific writer's style. While recent approaches involving GAN, transformer and diffusion-based models have made progress, they often struggle to capture the full spectrum of writer-specific attributes, particularly global stylistic patterns that span long-range spatial dependencies. As a result, capturing subtle writer-specific traits such as consistent slant, curvature or stroke pressure, while keeping the generated text accurate is still an open problem. In this work, we present a unified framework designed to address these limitations. We introduce a Vision Transformer-based style encoder that learns global stylistic patterns from multiple reference images, allowing the model to better represent long-range structural characteristics of handwriting. We then integrate these style cues with the target text using a cross-attention mechanism, enabling the system to produce handwritten images that more faithfully reflect the intended style. To make the process more interpretable, we utilize Salient Stroke Attention Analysis (SSAA), which reveals the stroke-level features the model focuses on during style transfer. Together, these components lead to handwriting synthesis that is not only more stylistically coherent, but also easier to understand and analyze.

</details>


### [100] [Stro-VIGRU: Defining the Vision Recurrent-Based Baseline Model for Brain Stroke Classification](https://arxiv.org/abs/2511.18316)
*Subhajeet Das,Pritam Paul,Rohit Bahadur,Sohan Das*

Main category: cs.CV

TL;DR: 提出基于预训练Vision Transformer的迁移学习框架，通过冻结部分编码器块并微调其余部分来学习脑卒中特异性特征，结合双向GRU进行分类，在脑卒中数据集上达到94.06%的准确率。


<details>
  <summary>Details</summary>
Motivation: 脑卒中是全球主要致死和致残原因，早期识别对成功治疗至关重要。CT扫描是常用诊断方法，但手动分析耗时且易出错，需要自动化解决方案。

Method: 使用预训练的Vision Transformer模型，冻结部分编码器块，微调其余部分以学习脑卒中特征。提取的特征输入单层双向GRU进行分类，通过数据增强处理类别不平衡问题。

Result: 在脑卒中数据集上实现了94.06%的分类准确率，证明了该方法的有效性。

Conclusion: 提出的基于Vision Transformer的迁移学习框架能够有效识别脑卒中，为早期诊断提供了可靠的自动化解决方案。

Abstract: Stroke majorly causes death and disability worldwide, and early recognition is one of the key elements of successful treatment of the same. It is common to diagnose strokes using CT scanning, which is fast and readily available, however, manual analysis may take time and may result in mistakes. In this work, a pre-trained Vision Transformer-based transfer learning framework is proposed for the early identification of brain stroke. A few of the encoder blocks of the ViT model are frozen, and the rest are allowed to be fine-tuned in order to learn brain stroke-specific features. The features that have been extracted are given as input to a single-layer Bi-GRU to perform classification. Class imbalance is handled by data augmentation. The model has achieved 94.06% accuracy in classifying brain stroke from the Stroke Dataset.

</details>


### [101] [Optimal Pose Guidance for Stereo Calibration in 3D Deformation Measurement](https://arxiv.org/abs/2511.18317)
*Dongcai Tan,Shunkun Liang,Bin Li,Banglei Guan,Ang Su,Yuan Lin,Dapeng Zhang,Minggang Wan,Zibin Liu,Chenglong Wang,Jiajian Zhu,Zhang Li,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种交互式立体标定框架，通过自动生成最优姿态来实现高精度3D变形测量，相比随机姿态方法具有更高的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 当前立体标定方法缺乏直观的最优姿态指导，导致变形测量效率低下且精度不理想，需要开发能够自动生成最优姿态的交互式标定框架。

Method: 提出姿态优化方法，引入相对和绝对外部参数的联合优化，以协方差矩阵迹最小化为损失函数求解下一个最优姿态，并集成用户友好的图形界面。

Result: 该方法在效率和精度上均优于随机姿态方法，需要更少的图像且测量误差更低，在S形试件热变形测量中与有限元分析结果高度一致。

Conclusion: 提出的姿态引导方法在3D变形测量中具有显著应用潜力，仿真实验、真实实验和热变形测量应用均验证了其有效性。

Abstract: Stereo optical measurement techniques, such as digital image correlation (DIC), are widely used in 3D deformation measurement as non-contact, full-field measurement methods, in which stereo calibration is a crucial step. However, current stereo calibration methods lack intuitive optimal pose guidance, leading to inefficiency and suboptimal accuracy in deformation measurements. The aim of this study is to develop an interactive calibration framework that automatically generates the next optimal pose, enabling high-accuracy stereo calibration for 3D deformation measurement. We propose a pose optimization method that introduces joint optimization of relative and absolute extrinsic parameters, with the minimization of the covariance matrix trace adopted as the loss function to solve for the next optimal pose. Integrated with this method is a user-friendly graphical interface, which guides even non-expert users to capture qualified calibration images. Our proposed method demonstrates superior efficiency (requiring fewer images) and accuracy (demonstrating lower measurement errors) compared to random pose, while maintaining robustness across varying FOVs. In the thermal deformation measurement tests on an S-shaped specimen, the results exhibit high agreement with finite element analysis (FEA) simulations in both deformation magnitude and evolutionary trends. We present a pose guidance method for high-precision stereo calibration in 3D deformation measurement. The simulation experiments, real-world experiments, and thermal deformation measurement applications all demonstrate the significant application potential of our proposed method in the field of 3D deformation measurement.
  Keywords: Stereo calibration, Optimal pose guidance, 3D deformation measurement, Digital image correlation

</details>


### [102] [General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain MRI Tumor Classification](https://arxiv.org/abs/2511.18326)
*Helia Abedini,Saba Rahimi,Reza Vaziri*

Main category: cs.CV

TL;DR: 本研究系统评估了三种预训练CNN架构在脑肿瘤MRI分类中的表现，发现尽管RadImageNet DenseNet121使用医学领域预训练，但在小数据集条件下表现最差，而现代通用CNN模型ConvNeXt-Tiny和EfficientNetV2S表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨在小数据集条件下，基于医学领域预训练的模型与基于大规模通用数据集预训练的现代CNN模型在脑肿瘤检测任务中的性能差异，为医学影像分析中的迁移学习策略提供指导。

Method: 在相同条件下训练和微调三种预训练CNN架构：RadImageNet DenseNet121（医学领域预训练）、EfficientNetV2S和ConvNeXt-Tiny（通用预训练），使用有限规模的脑部MRI数据集进行公平比较。

Result: ConvNeXt-Tiny获得最高准确率，其次是EfficientNetV2S，而RadImageNet DenseNet121尽管使用医学领域预训练，但表现出较差的泛化能力，准确率较低且损失较高。

Conclusion: 在小数据条件下，领域特定预训练可能泛化能力不佳，而基于大规模数据集预训练的现代深度通用CNN在专业医学影像任务中能提供更优越的迁移学习性能。

Abstract: Brain tumor detection from MRI scans plays a crucial role in early diagnosis and treatment planning. Deep convolutional neural networks (CNNs) have demonstrated strong performance in medical imaging tasks, particularly when pretrained on large datasets. However, it remains unclear which type of pretrained model performs better when only a small dataset is available: those trained on domain-specific medical data or those pretrained on large general datasets. In this study, we systematically evaluate three pretrained CNN architectures for brain tumor classification: RadImageNet DenseNet121 with medical-domain pretraining, EfficientNetV2S, and ConvNeXt-Tiny, which are modern general-purpose CNNs. All models were trained and fine-tuned under identical conditions using a limited-size brain MRI dataset to ensure a fair comparison. Our results reveal that ConvNeXt-Tiny achieved the highest accuracy, followed by EfficientNetV2S, while RadImageNet DenseNet121, despite being pretrained on domain-specific medical data, exhibited poor generalization with lower accuracy and higher loss. These findings suggest that domain-specific pretraining may not generalize well under small-data conditions. In contrast, modern, deeper general-purpose CNNs pretrained on large-scale datasets can offer superior transfer learning performance in specialized medical imaging tasks.

</details>


### [103] [SciPostLayoutTree: A Dataset for Structural Analysis of Scientific Posters](https://arxiv.org/abs/2511.18329)
*Shohei Tanaka,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 本文构建了SciPostLayoutTree数据集，包含约8000个标注了阅读顺序和父子关系的学术海报，并开发了Layout Tree Decoder模型来改进空间挑战性关系的预测精度。


<details>
  <summary>Details</summary>
Motivation: 学术海报在学术交流中扮演重要角色，但相比论文，海报的结构分析研究不足。分析海报的阅读顺序和父子关系对于构建结构感知界面至关重要，以促进对研究内容的清晰准确理解。

Method: 开发了Layout Tree Decoder模型，该模型结合了视觉特征和边界框特征（包括位置和类别信息），并使用波束搜索来预测关系，同时捕捉序列级合理性。

Result: 实验结果表明，该模型提高了对空间挑战性关系（包括向上、水平和长距离关系）的预测准确性，并为海报结构分析建立了坚实的基线。

Conclusion: SciPostLayoutTree数据集和Layout Tree Decoder模型填补了海报结构分析的研究空白，提供了有效的解决方案来处理海报中复杂的空间关系。

Abstract: Scientific posters play a vital role in academic communication by presenting ideas through visual summaries. Analyzing reading order and parent-child relations of posters is essential for building structure-aware interfaces that facilitate clear and accurate understanding of research content. Despite their prevalence in academic communication, posters remain underexplored in structural analysis research, which has primarily focused on papers. To address this gap, we constructed SciPostLayoutTree, a dataset of approximately 8,000 posters annotated with reading order and parent-child relations. Compared to an existing structural analysis dataset, SciPostLayoutTree contains more instances of spatially challenging relations, including upward, horizontal, and long-distance relations. As a solution to these challenges, we develop Layout Tree Decoder, which incorporates visual features as well as bounding box features including position and category information. The model also uses beam search to predict relations while capturing sequence-level plausibility. Experimental results demonstrate that our model improves the prediction accuracy for spatially challenging relations and establishes a solid baseline for poster structure analysis. The dataset is publicly available at https://huggingface.co/datasets/omron-sinicx/scipostlayouttree. The code is also publicly available at https://github.com/omron-sinicx/scipostlayouttree.

</details>


### [104] [ConsistCompose: Unified Multimodal Layout Control for Image Composition](https://arxiv.org/abs/2511.18333)
*Xuanke Shi,Boxuan Li,Xiaoyang Han,Zhongang Cai,Lei Yang,Dahua Lin,Quan Wang*

Main category: cs.CV

TL;DR: ConsistCompose是一个统一的多模态框架，通过将布局坐标嵌入语言提示，实现布局控制的多实例图像生成，并构建了包含340万数据对的大规模数据集来支持布局条件生成。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型主要关注视觉基础（语言与图像区域对齐），而其生成对应部分——基于布局的语言嵌入生成（LELG）在布局可控的多实例生成方面研究不足，限制了精确的组合控制能力。

Method: 通过实例-坐标绑定提示和坐标感知的无分类器引导，将语言布局线索转化为精确的空间控制，无需特定任务分支，在单一生成接口中实现布局控制的多实例图像生成。

Result: 在COCO-Position和MS-Bench上的实验表明，ConsistCompose相比布局控制基线显著提高了空间准确性，同时保持了身份保真度和竞争力的通用多模态理解能力。

Conclusion: ConsistCompose为布局可控的多模态图像生成建立了一个统一的范式，实现了精确的空间控制和多实例生成能力。

Abstract: Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.

</details>


### [105] [A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles](https://arxiv.org/abs/2511.18344)
*Tianyang Xu,Jinjie Gu,Xuefeng Zhu,XiaoJun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 提出了MM-UAV，首个大规模多模态无人机跟踪基准数据集，包含RGB、红外和事件信号三种模态，涵盖30多个挑战性场景，包含1321个同步多模态序列和超过280万标注帧。同时提出了一个专门针对无人机跟踪的多模态多目标跟踪框架，包含偏移引导自适应对齐模块和自适应动态融合模块等创新技术。


<details>
  <summary>Details</summary>
Motivation: 随着低空无人机的普及，视觉多目标跟踪成为关键安全技术，但单模态跟踪在复杂环境条件下容易失效。多模态跟踪虽然更具鲁棒性，但缺乏专门的公共数据集阻碍了有效解决方案的开发。

Method: 提出了一个多模态多无人机跟踪框架，包含两个关键技术创新：偏移引导自适应对齐模块解决传感器间的空间不匹配问题，自适应动态融合模块平衡不同模态的互补信息。还引入了事件增强关联机制，利用事件模态的运动线索进行更可靠的身份维护。

Result: 综合实验表明，所提出的框架持续优于最先进的方法。

Conclusion: MM-UAV数据集和提出的多模态跟踪框架为多模态无人机跟踪研究提供了重要基础，将促进该领域的进一步发展。数据集和源代码将公开提供。

Abstract: With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.

</details>


### [106] [FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement](https://arxiv.org/abs/2511.18346)
*Wenshuo Gao,Junyi Fan,Jiangyue Zeng,Shuai Yang*

Main category: cs.CV

TL;DR: FlowPortal是一个无需训练的基于光流的视频重光照框架，通过残差校正光流机制实现高结构一致性，结合解耦条件设计和高频传输机制，在保持高效的同时实现优越的时间连贯性、结构保持和光照真实感。


<details>
  <summary>Details</summary>
Motivation: 视频重光照与背景替换是电影制作和创意媒体中的关键任务，现有方法难以平衡时间一致性、空间保真度和光照自然度。

Method: 提出残差校正光流机制将标准光流模型转换为编辑模型，结合解耦条件设计实现精确光照控制，高频传输机制保持细节，以及掩码策略分离前景重光照和背景生成过程。

Result: 实验证明FlowPortal在时间连贯性、结构保持和光照真实感方面达到优越性能，同时保持高效率。

Conclusion: FlowPortal通过创新的训练免费框架有效解决了视频重光照中的关键挑战，为实际应用提供了高效可靠的解决方案。

Abstract: Video relighting with background replacement is a challenging task critical for applications in film production and creative media. Existing methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness. To address these issues, we introduce FlowPortal, a novel training-free flow-based video relighting framework. Our core innovation is a Residual-Corrected Flow mechanism that transforms a standard flow-based model into an editing model, guaranteeing perfect reconstruction when input conditions are identical and enabling faithful relighting when they differ, resulting in high structural consistency. This is further enhanced by a Decoupled Condition Design for precise lighting control and a High-Frequency Transfer mechanism for detail preservation. Additionally, a masking strategy isolates foreground relighting from background pure generation process. Experiments demonstrate that FlowPortal achieves superior performance in temporal coherence, structural preservation, and lighting realism, while maintaining high efficiency. Project Page: https://gaowenshuo.github.io/FlowPortalProject/.

</details>


### [107] [TRANSPORTER: Transferring Visual Semantics from VLM Manifolds](https://arxiv.org/abs/2511.18359)
*Alexandros Stergiou*

Main category: cs.CV

TL;DR: 本文提出了logits-to-video (L2V)任务和TRANSPORTER方法，通过生成视频来揭示视频理解模型的内部决策过程，为模型可解释性提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型能够理解复杂场景，但其内部推理过程难以理解和控制。受文本到视频生成模型进展的启发，需要开发新方法来解释VLMs的预测机制。

Method: 提出TRANSPORTER方法，利用最优传输耦合将VLM的高语义嵌入空间与T2V模型连接，通过logit分数定义嵌入方向进行条件视频生成。

Result: TRANSPORTER能够生成反映不同对象属性、动作副词和场景上下文变化的视频，定量和定性评估表明L2V为模型可解释性提供了高保真的新途径。

Conclusion: L2V任务和TRANSPORTER方法为理解视频理解模型的内部推理过程提供了新颖且有效的可解释性工具，填补了该领域的研究空白。

Abstract: How do video understanding models acquire their answers? Although current Vision Language Models (VLMs) reason over complex scenes with diverse objects, action performances, and scene dynamics, understanding and controlling their internal processes remains an open challenge. Motivated by recent advancements in text-to-video (T2V) generative models, this paper introduces a logits-to-video (L2V) task alongside a model-independent approach, TRANSPORTER, to generate videos that capture the underlying rules behind VLMs' predictions. Given the high-visual-fidelity produced by T2V models, TRANSPORTER learns an optimal transport coupling to VLM's high-semantic embedding spaces. In turn, logit scores define embedding directions for conditional video generation. TRANSPORTER generates videos that reflect caption changes over diverse object attributes, action adverbs, and scene context. Quantitative and qualitative evaluations across VLMs demonstrate that L2V can provide a fidelity-rich, novel direction for model interpretability that has not been previously explored.

</details>


### [108] [Alias-free 4D Gaussian Splatting](https://arxiv.org/abs/2511.18367)
*Zilong Chen,Huan-ang Gao,Delin Qu,Haohan Chi,Hao Tang,Kai Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种针对4D高斯泼溅的尺度自适应滤波器和尺度损失方法，解决了动态场景重建中因相机焦距或距离调整导致的高频伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯泼溅的动态场景重建方法在调整相机焦距或高斯基元与相机距离时会产生强烈伪影，这源于4D高斯的频率约束和2D膨胀滤波器引起的高斯尺度不匹配。

Method: 推导了4D高斯泼溅的最大采样频率公式，引入了4D尺度自适应滤波器和尺度损失，灵活调节4D高斯泼溅的采样频率。

Result: 该方法在增加渲染频率时消除了高频伪影，同时有效减少了多视图视频重建中的冗余高斯基元。通过单目和多视图视频重建实验验证了所提方法。

Conclusion: 提出的4D尺度自适应滤波器和尺度损失方法成功解决了动态场景重建中的高频伪影问题，提高了渲染质量并减少了冗余计算。

Abstract: Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/

</details>


### [109] [MimiCAT: Mimic with Correspondence-Aware Cascade-Transformer for Category-Free 3D Pose Transfer](https://arxiv.org/abs/2511.18370)
*Zenghao Chai,Chen Tang,Yongkang Wong,Xulei Yang,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: 本文提出了MimiCAT模型，用于实现类别无关的3D姿态迁移，能够将不同结构角色（如人形到四足动物）的姿态进行迁移，解决了现有方法局限于相似结构角色的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D姿态迁移方法主要局限于相似结构的角色之间，无法泛化到类别无关的设置（如将人形姿态迁移到四足动物）。主要挑战在于不同角色类型的结构和变换多样性，导致区域不匹配和迁移质量差。

Method: 构建了百万级跨数百个不同角色的姿态数据集；提出MimiCAT级联transformer模型，利用语义关键点标签学习软对应关系，实现灵活的多对多匹配；将姿态迁移表述为条件生成过程，通过软对应匹配将源变换投影到目标，并使用形状条件表示进行细化。

Result: 广泛的定性和定量实验表明，MimiCAT能够在不同角色之间迁移合理的姿态，显著优于局限于狭窄类别迁移的先前方法。

Conclusion: MimiCAT通过软对应学习和条件生成方法，成功实现了类别无关的3D姿态迁移，解决了跨结构角色姿态迁移的挑战。

Abstract: 3D pose transfer aims to transfer the pose-style of a source mesh to a target character while preserving both the target's geometry and the source's pose characteristic. Existing methods are largely restricted to characters with similar structures and fail to generalize to category-free settings (e.g., transferring a humanoid's pose to a quadruped). The key challenge lies in the structural and transformation diversity inherent in distinct character types, which often leads to mismatched regions and poor transfer quality. To address these issues, we first construct a million-scale pose dataset across hundreds of distinct characters. We further propose MimiCAT, a cascade-transformer model designed for category-free 3D pose transfer. Instead of relying on strict one-to-one correspondence mappings, MimiCAT leverages semantic keypoint labels to learn a novel soft correspondence that enables flexible many-to-many matching across characters. The pose transfer is then formulated as a conditional generation process, in which the source transformations are first projected onto the target through soft correspondence matching and subsequently refined using shape-conditioned representations. Extensive qualitative and quantitative experiments demonstrate that MimiCAT transfers plausible poses across different characters, significantly outperforming prior methods that are limited to narrow category transfer (e.g., humanoid-to-humanoid).

</details>


### [110] [MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models](https://arxiv.org/abs/2511.18373)
*Xiyang Wu,Zongxia Li,Jihui Jin,Guangyao Shi,Gouthaman KV,Vishnu Raj,Nilotpal Sinha,Jingxi Chen,Fan Du,Dinesh Manocha*

Main category: cs.CV

TL;DR: 该论文提出了MASS方法，通过将物理世界上下文线索转化为可解释表示来增强视觉语言模型在物理推理方面的能力，并引入了包含4,350个视频和8,361个问答对的MASS-Bench基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在标准视频任务上表现良好，但在涉及运动动力学和空间交互的物理驱动推理方面存在困难，这限制了它们解释真实或AI生成内容视频以及生成物理一致内容的能力。

Method: 提出了MASS方法，这是一种模型无关的方法，通过基于深度的3D编码和视觉接地将时空信号注入VLM语言空间，结合用于对象动态的运动跟踪器。为了加强跨模态对齐和推理，应用了强化微调。

Result: 实验表明，经过改进的VLMs在物理推理和理解方面比可比和更大的基线模型以及先前的最先进模型分别提高了8.7%和6.0%，性能可与Gemini-2.5-Flash等闭源SoTA VLMs相媲美。

Conclusion: 这些结果验证了该方法的有效性，表明通过将物理世界上下文转化为可解释表示可以显著提升视觉语言模型的物理推理能力。

Abstract: Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.

</details>


### [111] [Synthetic Curriculum Reinforces Compositional Text-to-Image Generation](https://arxiv.org/abs/2511.18378)
*Shijian Wang,Runhao Fu,Siyi Zhao,Qingqin Zhan,Xingjian Wang,Jiarui Jin,Yuan Lu,Hanqian Wu,Cunjian Chen*

Main category: cs.CV

TL;DR: 本文提出CompGen框架，通过场景图建立难度标准，使用自适应MCMC图采样算法生成渐进式训练课程数据，结合GRPO强化学习优化T2I模型的组合生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中组合合成问题，特别是复杂场景中多个对象及其属性、空间和语义关系的准确渲染需求。

Method: 利用场景图建立组合能力难度标准，开发自适应MCMC图采样算法，结合GRPO强化学习进行课程学习。

Result: CompGen显著提升了基于扩散和自回归T2I模型的组合生成能力，易到难和高斯采样策略表现出优越的扩展性能。

Conclusion: CompGen框架有效提升了T2I模型的组合生成能力，为组合文本到图像生成系统提供了有效改进方案。

Abstract: Text-to-Image (T2I) generation has long been an open problem, with compositional synthesis remaining particularly challenging. This task requires accurate rendering of complex scenes containing multiple objects that exhibit diverse attributes as well as intricate spatial and semantic relationships, demanding both precise object placement and coherent inter-object interactions. In this paper, we propose a novel compositional curriculum reinforcement learning framework named CompGen that addresses compositional weakness in existing T2I models. Specifically, we leverage scene graphs to establish a novel difficulty criterion for compositional ability and develop a corresponding adaptive Markov Chain Monte Carlo graph sampling algorithm. This difficulty-aware approach enables the synthesis of training curriculum data that progressively optimize T2I models through reinforcement learning. We integrate our curriculum learning approach into Group Relative Policy Optimization (GRPO) and investigate different curriculum scheduling strategies. Our experiments reveal that CompGen exhibits distinct scaling curves under different curriculum scheduling strategies, with easy-to-hard and Gaussian sampling strategies yielding superior scaling performance compared to random sampling. Extensive experiments demonstrate that CompGen significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models, highlighting its effectiveness in improving the compositional T2I generation systems.

</details>


### [112] [ViMix-14M: A Curated Multi-Source Video-Text Dataset with Long-Form, High-Quality Captions and Crawl-Free Access](https://arxiv.org/abs/2511.18382)
*Timing Yang,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CV

TL;DR: 本文介绍了ViMix-14M，一个包含约1400万视频-文本对的精心策划的多源数据集，旨在解决开源视频生成模型面临的数据瓶颈问题。该数据集通过合并多样开放视频源，进行统一去重和质量过滤，并采用多粒度、基于真实信息的重新标注流程来提升描述质量。


<details>
  <summary>Details</summary>
Motivation: 当前开源视频生成模型面临数据瓶颈：缺乏大规模、高质量、易获取的视频-文本语料库。现有公共数据集通常需要手动爬取YouTube视频，存在链接失效、访问限制和版权不确定性等问题，导致可用数据量有限。

Method: 通过合并多样开放视频源构建ViMix-14M数据集，采用统一去重和质量过滤流程，并实施多粒度、基于真实信息的重新标注管道，以优化描述与视频动作、场景和时间结构的匹配度。

Result: 在多模态检索、文本到视频生成和视频问答任务上的评估显示，相比对照数据集，ViMix-14M带来了持续的性能提升。

Conclusion: ViMix-14M有助于消除训练和微调开源视频基础模型的关键障碍，并为构建高质量、可泛化的视频-文本数据集提供了见解。

Abstract: Text-to-video generation has surged in interest since Sora, yet open-source models still face a data bottleneck: there is no large, high-quality, easily obtainable video-text corpus. Existing public datasets typically require manual YouTube crawling, which yields low usable volume due to link rot and access limits, and raises licensing uncertainty. This work addresses this challenge by introducing ViMix-14M, a curated multi-source video-text dataset of around 14 million pairs that provides crawl-free, download-ready access and long-form, high-quality captions tightly aligned to video. ViMix-14M is built by merging diverse open video sources, followed by unified de-duplication and quality filtering, and a multi-granularity, ground-truth-guided re-captioning pipeline that refines descriptions to better match actions, scenes, and temporal structure. We evaluate the dataset by multimodal retrieval, text-to-video generation, and video question answering tasks, observing consistent improvements over counterpart datasets. We hope this work can help removing the key barrier to training and fine-tuning open-source video foundation models, and provide insights of building high-quality and generalizable video-text datasets.

</details>


### [113] [Can a Second-View Image Be a Language? Geometric and Semantic Cross-Modal Reasoning for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.18385)
*Chuang Peng,Renshuai Tao,Zhongwei Ren,Xianglong Liu,Yunchao Wei*

Main category: cs.CV

TL;DR: 该研究提出了首个包含多视角和多模态的X射线安检基准DualXrayBench，并开发了GSR模型，通过将第二视角图像视为"类语言模态"，联合学习跨视角几何和跨模态语义的对应关系，在X射线安检任务上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统X射线违禁品检测方法依赖视觉模态，在复杂威胁检测上存在困难。虽然近期研究引入了语言指导单视角图像，但实际安检中检查员通常使用双视角图像。研究探索第二视角是否能提供类似语言模态的约束。

Method: 提出GSR模型，将第二视角图像视为"类语言模态"，联合学习跨视角几何和跨模态语义的对应关系。构建了GSXray数据集，包含结构化的思维链序列：<top>、<side>、<conclusion>。

Result: 在DualXrayBench基准上的综合评估表明，GSR在所有X射线任务上都取得了显著改进，为实际X射线安检提供了新的视角。

Conclusion: 第二视角图像确实能够提供类似语言模态的约束，GSR模型通过联合学习跨视角几何和跨模态语义对应关系，显著提升了X射线安检性能。

Abstract: Automatic X-ray prohibited items detection is vital for security inspection and has been widely studied. Traditional methods rely on visual modality, often struggling with complex threats. While recent studies incorporate language to guide single-view images, human inspectors typically use dual-view images in practice. This raises the question: can the second view provide constraints similar to a language modality? In this work, we introduce DualXrayBench, the first comprehensive benchmark for X-ray inspection that includes multiple views and modalities. It supports eight tasks designed to test cross-view reasoning. In DualXrayBench, we introduce a caption corpus consisting of 45,613 dual-view image pairs across 12 categories with corresponding captions. Building upon these data, we propose the Geometric (cross-view)-Semantic (cross-modality) Reasoner (GSR), a multimodal model that jointly learns correspondences between cross-view geometry and cross-modal semantics, treating the second-view images as a "language-like modality". To enable this, we construct the GSXray dataset, with structured Chain-of-Thought sequences: <top>, <side>, <conclusion>. Comprehensive evaluations on DualXrayBench demonstrate that GSR achieves significant improvements across all X-ray tasks, offering a new perspective for real-world X-ray inspection.

</details>


### [114] [SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation](https://arxiv.org/abs/2511.18386)
*Peter Siegel,Federico Tombari,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: SegSplat是一个新颖框架，通过从多视角2D基础模型特征构建紧凑语义记忆库，为3D高斯预测离散语义索引，实现快速3D重建与开放词汇语义理解的结合。


<details>
  <summary>Details</summary>
Motivation: 弥合快速前馈3D重建与丰富开放词汇语义理解之间的差距，为机器人交互、增强现实等智能系统提供实用的语义感知3D环境生成。

Method: 构建紧凑语义记忆库，为每个3D高斯预测离散语义索引以及几何和外观属性，单次前向传播即可实现语义注入。

Result: 在几何保真度上与最先进的前馈3D高斯泼溅方法相当，同时实现鲁棒的开放集语义分割，无需任何场景特定优化的语义特征集成。

Conclusion: SegSplat代表了向实用、实时生成语义感知3D环境的重要进展，对推进机器人交互、增强现实等应用具有重要意义。

Abstract: We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.

</details>


### [115] [Exploring Weak-to-Strong Generalization for CLIP-based Classification](https://arxiv.org/abs/2511.18396)
*Jinhao Li,Sarah M. Erfani,Lei Feng,James Bailey,Feng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为类别原型学习（CPL）的方法，用于增强CLIP模型的分类能力，通过弱监督学习更代表性的类别原型，在预训练有限的情况下取得了3.67%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着模型复杂度的增加，依赖人工监督来对齐大型商业模型与用户意图变得不切实际。当模型超越人类知识时，提供准确反馈变得困难且低效。因此需要探索使用较弱模型监督较强模型的新方法。

Method: 提出了类别原型学习（CPL）方法，通过弱监督学习更代表性的类别原型来增强CLIP模型的分类能力，使用简单的损失函数在弱监督设置下进行训练。

Result: 实验表明CPL方法在目标场景下取得了稳健的改进，特别是在预训练有限的情况下，相比强基线方法实现了3.67%的性能提升。

Conclusion: 弱到强泛化方法在视觉语言模型中同样有效，CPL方法通过弱监督学习类别原型，在有限预训练条件下显著提升了CLIP模型的分类性能。

Abstract: Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.

</details>


### [116] [ChineseVideoBench: Benchmarking Multi-modal Large Models for Chinese Video Question Answering](https://arxiv.org/abs/2511.18399)
*Yuxiang Nie,Han Wang,Yongjie Ye,Haiyang Yu,Weitao Jia,Tao Zeng,Hao Feng,Xiang Fei,Yang Li,Xiaohui Lv,Guozhi Tang,Jingqun Tang,Jinghui Lu,Zehui Dai,Jiacong Wang,Dingkang Yang,An-Lan Wang,Can Huang*

Main category: cs.CV

TL;DR: 该论文提出了ChineseVideoBench基准，专门用于评估多模态大语言模型在中文视频问答任务中的表现。该基准包含8个主类和12个子类，涵盖需要深度视频理解和中文语言文化意识的任务。实证评估显示当前MLLMs在该基准上表现仍有挑战，Gemini 2.5 Pro表现最佳（77.9%），InternVL-38B是最具竞争力的开源模型。


<details>
  <summary>Details</summary>
Motivation: 随着对复杂视频分析能力需求的增长，迫切需要全面且具有文化意识的评估框架。现有基准在中文视频内容评估方面存在不足，因此需要专门针对中文视频问答的评估基准。

Method: 开发了ChineseVideoBench基准，包含8个主类和12个子类的任务，设计了专门的数据集和评估指标，用于对最先进的MLLMs进行严格评估。

Result: 实证评估表明ChineseVideoBench对当前MLLMs构成显著挑战。Gemini 2.5 Pro获得最高分77.9%，InternVL-38B是最具竞争力的开源模型。

Conclusion: ChineseVideoBench填补了中文视频问答评估的空白，为评估多模态大语言模型在复杂中文视频内容上的表现提供了重要工具，揭示了当前模型在该领域的局限性。

Abstract: This paper introduces ChineseVideoBench, a pioneering benchmark specifically designed for evaluating Multimodal Large Language Models (MLLMs) in Chinese Video Question Answering. The growing demand for sophisticated video analysis capabilities highlights the critical need for comprehensive, culturally-aware evaluation frameworks. ChineseVideoBench addresses this gap by providing a robust dataset and tailored evaluation metrics, enabling rigorous assessment of state-of-the-art MLLMs on complex Chinese video content. Specifically, ChineseVideoBench comprises 8 main classes and 12 sub-classes, encompassing tasks that demand both deep video understanding and nuanced Chinese linguistic and cultural awareness. Our empirical evaluations reveal that ChineseVideoBench presents a significant challenge to current MLLMs. Among the models assessed, Gemini 2.5 Pro achieves the highest performance with an overall score of 77.9%, while InternVL-38B emerges as the most competitive open-source model.

</details>


### [117] [4D-VGGT: A General Foundation Model with SpatioTemporal Awareness for Dynamic Scene Geometry Estimation](https://arxiv.org/abs/2511.18416)
*Haonan Wang,Hanyu Zhou,Haoyue Liu,Luxin Yan*

Main category: cs.CV

TL;DR: 提出4D-VGGT模型，采用分而治之的时空表示方法来解决动态场景几何估计中的时空特征异质性问题，支持多设置输入、多层次表示和多任务预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时空特征对齐到统一潜在空间中，但由于时空特征的异质性，这种统一范式存在表示不匹配的问题，需要更好的动态场景几何估计方法。

Method: 设计自适应视觉网格支持任意视图和时间步的输入；提出跨视图全局融合进行空间表示和跨时间局部融合进行时间表示；添加多个任务特定头实现多任务预测。

Result: 通过整合多个几何数据集进行训练，在多个动态场景几何基准测试上验证了方法的有效性。

Conclusion: 4D-VGGT框架通过分而治之的时空表示增强了特征可区分性和应用通用性，为动态场景几何估计提供了有效的解决方案。

Abstract: We investigate a challenging task of dynamic scene geometry estimation, which requires representing both spatial and temporal features. Typically, existing methods align the two features into a unified latent space to model scene geometry. However, this unified paradigm suffers from potential mismatched representation due to the heterogeneous nature between spatial and temporal features. In this work, we propose 4D-VGGT, a general foundation model with divide-and-conquer spatiotemporal representation for dynamic scene geometry. Our model is divided into three aspects: 1) Multi-setting input. We design an adaptive visual grid that supports input sequences with arbitrary numbers of views and time steps. 2) Multi-level representation. We propose a cross-view global fusion for spatial representation and a cross-time local fusion for temporal representation. 3) Multi-task prediction. We append multiple task-specific heads to spatiotemporal representations, enabling a comprehensive visual geometry estimation for dynamic scenes. Under this unified framework, these components enhance the feature discriminability and application universality of our model for dynamic scenes. In addition, we integrate multiple geometry datasets to train our model and conduct extensive experiments to verify the effectiveness of our method across various tasks on multiple dynamic scene geometry benchmarks.

</details>


### [118] [NeuroVascU-Net: A Unified Multi-Scale and Cross-Domain Adaptive Feature Fusion U-Net for Precise 3D Segmentation of Brain Vessels in Contrast-Enhanced T1 MRI](https://arxiv.org/abs/2511.18422)
*Mohammad Jafari Vayeghan,Niloufar Delfan,Mehdi Tale Masouleh,Mansour Parvaresh Rizi,Behzad Moshiri*

Main category: cs.CV

TL;DR: NeuroVascU-Net是一种专门用于从临床标准T1CE MRI中分割脑血管结构的深度学习架构，在脑肿瘤患者中实现了精确的3D血管分割，平衡了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 手动分割脑血管耗时且存在观察者间变异性，现有自动化方法在准确性和计算成本之间存在权衡，限制了临床应用。需要一种专门针对神经肿瘤患者T1CE MRI的血管分割方法。

Method: 基于扩张U-Net架构，集成了两个专门模块：瓶颈处的多尺度上下文特征融合模块（MSC²F）和深层层次结构中的跨域自适应特征融合模块（CDA²F），分别捕获局部和全局信息并动态整合领域特定特征。

Result: 在137名脑肿瘤活检患者的T1CE扫描数据集上验证，获得Dice分数0.8609和精度0.8841，准确分割主要和细微血管结构，仅需12.4M参数，显著少于基于Transformer的模型。

Conclusion: NeuroVascU-Net在准确性和效率之间实现了良好平衡，为计算机辅助神经外科规划提供了实用的解决方案。

Abstract: Precise 3D segmentation of cerebral vasculature from T1-weighted contrast-enhanced (T1CE) MRI is crucial for safe neurosurgical planning. Manual delineation is time-consuming and prone to inter-observer variability, while current automated methods often trade accuracy for computational cost, limiting clinical use. We present NeuroVascU-Net, the first deep learning architecture specifically designed to segment cerebrovascular structures directly from clinically standard T1CE MRI in neuro-oncology patients, addressing a gap in prior work dominated by TOF-MRA-based approaches. NeuroVascU-Net builds on a dilated U-Net and integrates two specialized modules: a Multi-Scale Contextual Feature Fusion ($MSC^2F$) module at the bottleneck and a Cross-Domain Adaptive Feature Fusion ($CDA^2F$) module at deeper hierarchical layers. $MSC^2F$ captures both local and global information via multi-scale dilated convolutions, while $CDA^2F$ dynamically integrates domain-specific features, enhancing representation while keeping computation low. The model was trained and validated on a curated dataset of T1CE scans from 137 brain tumor biopsy patients, annotated by a board-certified functional neurosurgeon. NeuroVascU-Net achieved a Dice score of 0.8609 and precision of 0.8841, accurately segmenting both major and fine vascular structures. Notably, it requires only 12.4M parameters, significantly fewer than transformer-based models such as Swin U-NetR. This balance of accuracy and efficiency positions NeuroVascU-Net as a practical solution for computer-assisted neurosurgical planning.

</details>


### [119] [CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images](https://arxiv.org/abs/2511.18424)
*Avishka Perera,Kumal Hewagamage,Saeedha Nazar,Kavishka Abeywardana,Hasitha Gallella,Ranga Rodrigo,Mohamed Afham*

Main category: cs.CV

TL;DR: CrossJEPA是一种跨模态联合嵌入预测架构，利用图像基础模型知识训练预测器从3D点云推断2D视图嵌入，在3D表示学习中实现高性能、内存效率和快速训练。


<details>
  <summary>Details</summary>
Motivation: 解决当前利用2D数据的3D表示学习方法模型庞大、训练缓慢、计算成本高的问题，探索JEPA架构在跨模态环境中的应用潜力。

Method: 提出CrossJEPA架构，利用冻结的图像基础模型作为教师，训练点云编码器预测特定渲染2D视图的嵌入，采用跨域投影信息净化监督信号，并实现一次性目标嵌入缓存机制。

Result: 在ModelNet40上达到94.2%的线性探测准确率，在ScanObjectNN上达到88.3%，仅使用14.1M预训练参数（点编码器8.5M），在单GPU上约6小时完成预训练。

Conclusion: CrossJEPA是一个性能优异、内存高效、训练快速的3D表示学习框架，通过知识蒸馏实现了新的最先进水平。

Abstract: Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.

</details>


### [120] [LungX: A Hybrid EfficientNet-Vision Transformer Architecture with Multi-Scale Attention for Accurate Pneumonia Detection](https://arxiv.org/abs/2511.18425)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: LungX是一种结合EfficientNet多尺度特征、CBAM注意力机制和Vision Transformer全局上下文建模的混合架构，用于肺炎检测，在2万张胸部X光片上达到86.5%准确率和0.943 AUC，比基准模型提升6.7% AUC。


<details>
  <summary>Details</summary>
Motivation: 肺炎是全球主要死亡原因，及时诊断至关重要，需要开发更准确的AI诊断工具。

Method: 提出LungX混合架构，整合EfficientNet的多尺度特征提取、CBAM注意力机制和Vision Transformer的全局上下文建模能力。

Result: 在RSNA和CheXpert的2万张胸部X光片上测试，达到86.5%准确率和0.943 AUC，比EfficientNet-B0基准提升6.7% AUC，注意力图显示更好的病灶定位能力。

Conclusion: LungX在肺炎检测方面达到最先进性能，未来将进行多中心验证和架构优化，目标达到88%准确率用于临床部署。

Abstract: Pneumonia remains a leading global cause of mortality where timely diagnosis is critical. We introduce LungX, a novel hybrid architecture combining EfficientNet's multi-scale features, CBAM attention mechanisms, and Vision Transformer's global context modeling for enhanced pneumonia detection. Evaluated on 20,000 curated chest X-rays from RSNA and CheXpert, LungX achieves state-of-the-art performance (86.5 percent accuracy, 0.943 AUC), representing a 6.7 percent AUC improvement over EfficientNet-B0 baselines. Visual analysis demonstrates superior lesion localization through interpretable attention maps. Future directions include multi-center validation and architectural optimizations targeting 88 percent accuracy for clinical deployment as an AI diagnostic aid.

</details>


### [121] [DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation](https://arxiv.org/abs/2511.18434)
*Yongkun Du,Pinxuan Chen,Xuye Ying,Zhineng Chen*

Main category: cs.CV

TL;DR: DocPTBench是一个专门为拍摄文档解析和翻译设计的基准测试，包含1300多张高分辨率拍摄文档，涵盖8种翻译场景，揭示了现有模型在处理真实世界拍摄文档时的性能显著下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文档解析和翻译基准测试主要针对扫描或数字生成的文档，无法充分反映真实世界拍摄条件下的几何畸变和光度变化等复杂挑战。

Method: 构建DocPTBench基准测试，包含1300多张高分辨率拍摄文档，涵盖多个领域和8种翻译场景，并提供人工验证的解析和翻译标注。

Result: 从数字文档转换到拍摄文档时，主流MLLM在端到端解析中的平均准确率下降18%，翻译下降12%；专业文档解析模型平均下降25%。

Conclusion: 真实世界拍摄文档对现有模型构成了独特挑战，现有模型的鲁棒性有限，DocPTBench填补了这一重要空白。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has unlocked the potential for end-to-end document parsing and translation. However, prevailing benchmarks such as OmniDocBench and DITrans are dominated by pristine scanned or digital-born documents, and thus fail to adequately represent the intricate challenges of real-world capture conditions, such as geometric distortions and photometric variations. To fill this gap, we introduce DocPTBench, a comprehensive benchmark specifically designed for Photographed Document Parsing and Translation. DocPTBench comprises over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides meticulously human-verified annotations for both parsing and translation. Our experiments demonstrate that transitioning from digital-born to photographed documents results in a substantial performance decline: popular MLLMs exhibit an average accuracy drop of 18% in end-to-end parsing and 12% in translation, while specialized document parsing models show significant average decrease of 25%. This substantial performance gap underscores the unique challenges posed by documents captured in real-world conditions and reveals the limited robustness of existing models. Dataset and code are available at https://github.com/Topdu/DocPTBench.

</details>


### [122] [When Generative Replay Meets Evolving Deepfakes: Domain-Aware Relative Weighting for Incremental Face Forgery Detection](https://arxiv.org/abs/2511.18436)
*Hao Shen,Jikang Cheng,Renye Yan,Zhongyuan Wang,Wei Peng,Baojin Huang*

Main category: cs.CV

TL;DR: 本文提出了一种针对人脸伪造检测增量学习的新方法DARW，通过领域感知相对加权策略有效利用生成回放技术，解决了传统样本回放方法多样性低和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 随着人脸生成技术的快速发展，伪造方法日益多样化。增量伪造检测需要不断更新模型以适应新数据，但现有基于样本回放的方法存在多样性不足和隐私问题。生成回放技术提供了潜在解决方案，但其在伪造检测中的可行性尚不明确。

Method: 提出领域感知相对加权(DARW)策略，识别两种生成回放场景：当回放生成器与新伪造模型相似时，生成的真实样本会模糊领域边界；当差异显著时，生成样本可安全监督。DARW直接监督领域安全样本，对领域风险样本应用相对分离损失来平衡监督与潜在混淆，并通过领域混淆分数动态调整权衡。

Result: 大量实验表明，DARW在不同生成回放设置下持续提升伪造检测的增量学习性能，并减轻了领域重叠的不利影响。

Conclusion: DARW方法有效利用了生成回放技术，通过领域感知策略改善了增量伪造检测的性能，为处理不断演变的伪造技术提供了可行解决方案。

Abstract: The rapid advancement of face generation techniques has led to a growing variety of forgery methods. Incremental forgery detection aims to gradually update existing models with new forgery data, yet current sample replay-based methods are limited by low diversity and privacy concerns. Generative replay offers a potential solution by synthesizing past data, but its feasibility for forgery detection remains unclear. In this work, we systematically investigate generative replay and identify two scenarios: when the replay generator closely resembles the new forgery model, generated real samples blur the domain boundary, creating domain-risky samples; when the replay generator differs significantly, generated samples can be safely supervised, forming domain-safe samples. To exploit generative replay effectively, we propose a novel Domain-Aware Relative Weighting (DARW) strategy. DARW directly supervises domain-safe samples while applying a Relative Separation Loss to balance supervision and potential confusion for domain-risky samples. A Domain Confusion Score dynamically adjusts this tradeoff according to sample reliability. Extensive experiments demonstrate that DARW consistently improves incremental learning performance for forgery detection under different generative replay settings and alleviates the adverse impact of domain overlap.

</details>


### [123] [Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning](https://arxiv.org/abs/2511.18437)
*Chi Zhang,Haibo Qiu,Qiming Zhang,Yufei Xu,Zhixiong Zeng,Siqi Yang,Peng Shi,Lin Ma,Jing Zhang*

Main category: cs.CV

TL;DR: PEARL是一种针对视觉语言模型的双分支感知-推理协同强化学习方法，通过显式地将多模态推理锚定到经过验证的视觉证据来解决传统RLVR方法忽视视觉感知的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的RLVR方法仅验证最终文本输出，忽视了视觉感知这一基础步骤，导致视觉幻觉和奖励攻击问题。基于有缺陷感知的推理本质上是不可靠的。

Method: 提出PEARL方法：1）为每个推理问题生成感知检查清单——包含可验证答案的感知导向子问题；2）训练时通过辅助rollout获得感知奖励，既直接增强感知能力，又作为推理的保真度门控；3）通过双分支协同机制，只有当模型通过感知检查时才进行推理策略更新。

Result: 在多项多模态推理基准测试中取得显著提升，例如在MathVerse上比基线提升+9.7%，比GRPO提升+6.6%。

Conclusion: PEARL通过显式锚定视觉证据有效解决了视觉语言模型中的感知-推理脱节问题，显著提升了多模态推理的可靠性和性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.

</details>


### [124] [SineProject: Machine Unlearning for Stable Vision Language Alignment](https://arxiv.org/abs/2511.18444)
*Arpit Garg,Hemanth Saratchandran,Simon Lucey*

Main category: cs.CV

TL;DR: 本文提出SineProject方法，通过在冻结投影器中添加正弦调制可训练参数，改善雅可比矩阵的光谱条件，稳定多模态大语言模型在遗忘特定知识时的跨模态对齐，减少良性查询拒绝同时实现目标信息的完全遗忘。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需要遗忘不安全或隐私信息而不需要完全重新训练，但现有遗忘方法会破坏视觉语言对齐，导致模型拒绝有害和良性查询。

Method: 引入SineProject方法，在冻结投影器中添加正弦调制可训练参数，改善雅可比矩阵的光谱条件，稳定跨模态嵌入的优化过程。

Result: 在LLaVA v1.5 7B和13B模型的安全和隐私遗忘基准测试中，SineProject显著减少良性查询拒绝，同时实现目标信息的完全遗忘，在遗忘-保留权衡方面达到最先进水平，且计算开销可忽略。

Conclusion: SineProject通过改善投影器雅可比矩阵条件，有效解决了多模态大语言模型遗忘过程中的对齐稳定性问题，实现了更好的遗忘性能与模型实用性平衡。

Abstract: Multimodal Large Language Models (MLLMs) increasingly need to forget specific knowledge such as unsafe or private information without requiring full retraining. However, existing unlearning methods often disrupt vision language alignment, causing models to reject both harmful and benign queries. We trace this failure to the projector network during unlearning, its Jacobian becomes severely illconditioned, leading to unstable optimization and drift in cross modal embeddings. We introduce SineProject, a simple method that augments the frozen projector with sinusoidally modulated trainable parameters, improving the Jacobian's spectral conditioning and stabilizing alignment throughout unlearning. Across standard safety and privacy unlearning benchmarks using LLaVA v1.5 7B and 13B, SineProject reduces benign query refusals while achieving complete forgetting of targeted information, yielding state of the art forget retain trade offs with negligible computational overhead.

</details>


### [125] [EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs](https://arxiv.org/abs/2511.18448)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: EventBench是一个针对多模态大语言模型的事件视觉能力评估基准，包含8个多样化任务指标和大规模事件流数据集，评估了GPT-5、Gemini-2.5 Pro、Qwen2.5-VL等先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有事件视觉领域缺乏统一的能力评估基准，需要全面评估多模态大语言模型在事件流处理方面的能力。

Method: 构建EventBench基准，包含8个任务指标、大规模事件流数据集和超过100万事件-文本对的训练集，涵盖理解、识别和空间推理任务。

Result: 评估显示当前基于事件的MLLM在事件流理解方面表现良好，但在细粒度识别和空间推理方面仍有困难。

Conclusion: EventBench为事件视觉领域的模型评估提供了统一标准，揭示了当前模型在空间推理和细粒度识别方面的局限性。

Abstract: Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.

</details>


### [126] [NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering](https://arxiv.org/abs/2511.18452)
*Loick Chambon,Paul Couairon,Eloi Zablocki,Alexandre Boulch,Nicolas Thome,Matthieu Cord*

Main category: cs.CV

TL;DR: 本文提出了邻域注意力滤波(NAF)方法，通过跨尺度邻域注意力和旋转位置编码学习自适应空间-内容权重，实现零样本特征上采样，在多个下游任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型(VFMs)提取空间下采样表示，对像素级任务构成挑战。传统上采样方法存在固定形式与可学习形式之间的权衡：经典滤波器快速通用但依赖固定形式，现代上采样器通过可学习形式获得更高精度但需要为每个VFM重新训练。

Method: NAF通过跨尺度邻域注意力和旋转位置编码学习自适应空间-内容权重，仅由高分辨率输入图像引导，实现零样本特征上采样，无需为每个VFM重新训练。

Result: NAF是首个超越VFM特定上采样器的VFM无关架构，在多个下游任务中达到最先进性能，保持高效率，可在2K特征图上扩展到18 FPS。在图像恢复任务中也表现出色。

Conclusion: NAF成功弥合了传统滤波器和现代上采样器之间的差距，提供了一种高效、通用且高性能的特征上采样解决方案。

Abstract: Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.

</details>


### [127] [RegDeepLab: A Two-Stage Decoupled Framework for Interpretable Embryo Fragmentation Grading](https://arxiv.org/abs/2511.18454)
*Ming-Jhe Lee*

Main category: cs.CV

TL;DR: 本文提出RegDeepLab，一种双分支多任务学习框架，结合语义分割和回归任务，通过两阶段解耦训练策略解决胚胎碎片化程度自动分级问题，实现高精度分级预测同时保持分割准确性。


<details>
  <summary>Details</summary>
Motivation: 当前IVF临床中胚胎碎片化程度的手动分级过程耗时且存在观察者间差异，现有深度学习方法在临床可解释性和精确分级方面存在挑战。

Method: 提出RegDeepLab双分支多任务学习框架，集成DeepLabV3+语义分割和多尺度回归头，采用两阶段解耦训练策略解决梯度冲突和负迁移问题，并引入范围损失进行半监督学习。

Result: 实验结果显示，端到端多任务训练可最小化分级误差(MAE=0.046)，但会损害分割边界完整性；解耦策略在保持SOTA级分割精度(Dice=0.729)的同时提供稳健的高精度分级预测。

Conclusion: 本研究最终提出了一个结合高精度和视觉可解释性的双模块临床辅助解决方案，为胚胎碎片化程度自动分级提供了有效方法。

Abstract: The degree of embryo fragmentation serves as a critical morphological indicator for assessing embryo developmental potential in In Vitro Fertilization (IVF) clinical decision-making. However, current manual grading processes are not only time-consuming but also limited by significant inter-observer variability and efficiency bottlenecks. Although deep learning has demonstrated potential in automated grading in recent years, existing solutions face a significant challenge: pure regression models lack the visual explainability required for clinical practice, while pure segmentation models struggle to directly translate pixel-level masks into precise clinical grades. This study proposes RegDeepLab, a dual-branch Multi-Task Learning (MTL) framework that integrates State-of-the-Art (SOTA) semantic segmentation (DeepLabV3+) with a multi-scale regression head. Addressing the common issues of "Gradient Conflict" and "Negative Transfer" in multi-task training, we propose a "Two-Stage Decoupled Training Strategy." Experimental results demonstrate that while standard end-to-end MTL training can minimize grading error (MAE=0.046) through our designed "Feature Injection" mechanism, it compromises the integrity of segmentation boundaries. In contrast, our decoupled strategy successfully provides robust and high-precision grading predictions while preserving SOTA-level segmentation accuracy (Dice=0.729). Furthermore, we introduce a "Range Loss" to effectively utilize large-scale discrete grading data for semi-supervised learning. This study ultimately presents a dual-module clinical auxiliary solution that combines high accuracy with visual explainability.

</details>


### [128] [Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding](https://arxiv.org/abs/2511.18463)
*Bowei Pu,Chuanbin Liu,Yifan Ge,Peichen Zhou,Yiwei Sun,Zhiyin Lu,Jiankang Wang,Hongtao Xie*

Main category: cs.CV

TL;DR: 提出了Video-PLR框架，通过感知循环推理范式和反幻觉奖励机制解决视频推理中的感知不足和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理大语言模型存在感知捷径问题，采用单步感知范式存在证据不足和幻觉风险。

Method: 1. 感知循环推理(PLR)范式：分步骤描述视频片段并分析；2. 事实感知评估器(FAE)：提供反幻觉奖励；3. 使用AnetHallu-117K数据集训练FAE。

Result: 在3B和7B参数规模上达到最先进水平，具有最佳数据效率。FAE性能与GPT-4o相当。

Conclusion: Video-PLR框架有效解决了视频推理中的感知不足和幻觉问题，实现了state-of-the-art性能。

Abstract: Sufficient visual perception is the foundation of video reasoning. Nevertheless, existing Video Reasoning LLMs suffer from perception shortcuts, relying on a flawed single-step perception paradigm. This paradigm describes the video and then conducts reasoning, which runs the risk of insufficient evidence and emergent hallucinations. To address these issues, we introduce a new framework that integrates a loop-based paradigm with an anti-hallucination reward. First, to address the insufficient evidence, we introduce the Perception Loop Reasoning (PLR) paradigm. Instead of describing the video at once, each loop requires the model to describe a video segment with precise timestamps, analyze this segment, and decide the next action. Second, for the risk of hallucinations, the Factual-Aware Evaluator (FAE) evaluates each perception result as a reliable anti-hallucination reward. This reward encourages the model to provide sufficient and precise video evidence. Our FAE, which performs comparably to GPT-4o, is tuned on our AnetHallu-117K, a large-scale hallucination judgment preference dataset. Extensive experiments show that our Video-PLR achieves the state-of-the-art in both 3B and 7B parameter scales and has the best data efficiency. Our code, models, and datasets are released on: https://github.com/BoweiPu/VideoPLR.

</details>


### [129] [Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span](https://arxiv.org/abs/2511.18470)
*Heeseung Yun,Joonil Na,Jaeyeon Kim,Calvin Murdock,Gunhee Kim*

Main category: cs.CV

TL;DR: 本文提出EgoSpanLift方法，将自我中心视角的视觉跨度预测从2D图像平面转换到3D场景，通过结合3D U-Net和单向transformer实现时空融合，在3D网格中预测未来视觉焦点。


<details>
  <summary>Details</summary>
Motivation: 现有自我中心用户和场景理解研究主要关注运动和接触交互，而预测人类视觉感知本身仍较少探索，尽管其在指导人类行为和AR/VR、辅助技术中具有基础作用。

Method: EgoSpanLift将SLAM关键点转换为与注视兼容的几何结构，提取体积视觉跨度区域，结合3D U-Net和单向transformer进行时空融合预测。

Result: 方法在自我中心2D注视预测和3D定位方面优于竞争基线，即使投影回2D图像平面也取得可比结果，无需额外2D特定训练。

Conclusion: EgoSpanLift成功实现了从2D到3D的视觉跨度预测转换，为自我中心3D视觉感知预测提供了有效解决方案。

Abstract: People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.

</details>


### [130] [Robust Posterior Diffusion-based Sampling via Adaptive Guidance Scale](https://arxiv.org/abs/2511.18471)
*Liav Hen,Tom Tirer,Raja Giryes,Shady Abu-Hussein*

Main category: cs.CV

TL;DR: 本文提出了一种自适应似然步长策略（AdaPS），通过基于两种不同似然梯度近似之间一致性的观测依赖加权方案，在扩散模型逆问题求解中平衡先验和数据保真度，无需超参数调整即可在多种成像任务中提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为生成先验在逆问题求解中面临核心挑战：如何平衡先验贡献与数据保真度。过于激进的似然更新可能引入伪影，而保守更新则会减缓收敛或产生次优重建。

Method: 开发了自适应似然步长策略，基于两种不同不可行中间似然梯度近似之间的一致性构建观测依赖加权方案，该方案自然适应扩散调度、时间重采样和注入的随机性。

Result: AdaPS在CelebA-HQ和ImageNet-256验证集上的超分辨率、高斯去模糊和运动去模糊等多样化成像任务中，在感知质量上持续超越现有基于扩散的基线方法，失真度损失最小或无损失，且无需任何任务特定调优。

Conclusion: AdaPS是一种超参数自由的方法，通过自适应平衡先验与数据保真度，在各种成像任务中显著提升重建质量，且对扩散步数、观测噪声水平和不同随机性具有鲁棒性。

Abstract: Diffusion models have recently emerged as powerful generative priors for solving inverse problems, achieving state-of-the-art results across various imaging tasks. A central challenge in this setting lies in balancing the contribution of the prior with the data fidelity term: overly aggressive likelihood updates may introduce artifacts, while conservative updates can slow convergence or yield suboptimal reconstructions. In this work, we propose an adaptive likelihood step-size strategy to guide the diffusion process for inverse-problem formulations. Specifically, we develop an observation-dependent weighting scheme based on the agreement between two different approximations of the intractable intermediate likelihood gradients, that adapts naturally to the diffusion schedule, time re-spacing, and injected stochasticity. The resulting approach, Adaptive Posterior diffusion Sampling (AdaPS), is hyperparameter-free and improves reconstruction quality across diverse imaging tasks - including super-resolution, Gaussian deblurring, and motion deblurring - on CelebA-HQ and ImageNet-256 validation sets. AdaPS consistently surpasses existing diffusion-based baselines in perceptual quality with minimal or no loss in distortion, without any task-specific tuning. Extensive ablation studies further demonstrate its robustness to the number of diffusion steps, observation noise levels, and varying stochasticity.

</details>


### [131] [Uncertainty Quantification in HSI Reconstruction using Physics-Aware Diffusion Priors and Optics-Encoded Measurements](https://arxiv.org/abs/2511.18473)
*Juan Romero,Qiang Fu,Matteo Ravasi,Wolfgang Heidrich*

Main category: cs.CV

TL;DR: HSDiff是一个基于贝叶斯推理的高光谱图像重建框架，使用无条件训练的像素级扩散先验和后验扩散采样，通过增强的元色增强技术提高先验多样性，提供不确定性感知的重建结果。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动方法由于现有高光谱图像数据集缺乏光谱多样性，在评估元色现象时容易出现幻觉问题，需要一种能够生成多样化且与测量一致的样本的方法。

Method: 将高光谱图像重建建模为贝叶斯推理问题，使用无条件训练的像素级扩散先验和后验扩散采样，提出基于区域的元色黑和分区联合光谱上采样的增强元色增强技术。

Result: HSDiff能够生成与各种高光谱图像形成模型测量一致的多样化样本，通过有效光谱编码提供校准的信息不确定性，相比非编码模型表现更好。

Conclusion: HSDiff提供了一个完整的高性能不确定性感知高光谱图像重建方法，并重申了有效光谱编码在快照高光谱成像中的重要性。

Abstract: Hyperspectral image reconstruction from a compressed measurement is a highly ill-posed inverse problem. Current data-driven methods suffer from hallucination due to the lack of spectral diversity in existing hyperspectral image datasets, particularly when they are evaluated for the metamerism phenomenon. In this work, we formulate hyperspectral image (HSI) reconstruction as a Bayesian inference problem and propose a framework, HSDiff, that utilizes an unconditionally trained, pixel-level diffusion prior and posterior diffusion sampling to generate diverse HSI samples consistent with the measurements of various hyperspectral image formation models. We propose an enhanced metameric augmentation technique using region-based metameric black and partition-of-union spectral upsampling to expand training with physically valid metameric spectra, strengthening the prior diversity and improving uncertainty calibration. We utilize HSDiff to investigate how the studied forward models shape the posterior distribution and demonstrate that guiding with effective spectral encoding provides calibrated informative uncertainty compared to non-encoded models. Through the lens of the Bayesian framework, HSDiff offers a complete, high-performance method for uncertainty-aware HSI reconstruction. Our results also reiterate the significance of effective spectral encoding in snapshot hyperspectral imaging.

</details>


### [132] [Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression](https://arxiv.org/abs/2511.18504)
*Md Tasnin Tanvir,Soumitra Das,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Main category: cs.CV

TL;DR: 本文提出两种自适应压缩技术STTF和ANC，用于在资源受限的边缘设备上实现实时视觉语言任务。STTF通过事件驱动变化检测动态重用视觉token，ANC通过学习路由器条件激活编码器分支。3B参数的TinyGPT-STTF在COCO 2017测试集上超越LLaVA-1.5 7B，同时使用更少参数和计算量。


<details>
  <summary>Details</summary>
Motivation: 边缘AI对视觉语言任务的需求要求模型在资源受限的设备上实现实时性能，需要解决有限功耗和内存的挑战。

Method: 提出Sparse Temporal Token Fusion (STTF) 和 Adaptive Neural Compression (ANC) 两种技术：STTF通过事件驱动变化检测动态重用视觉token；ANC通过学习路由器条件激活编码器分支，实现细粒度场景复杂度适应。

Result: TinyGPT-STTF在COCO 2017测试集上达到CIDEr 131.2，超越LLaVA-1.5 7B 17.6分，参数减少2.3倍，FLOPs减少62倍。STTF在事件视觉任务中减少84% token数量，保持95.6%准确率；ANC在低运动场景中减少90% FLOPs。相比基线，准确率提升4.4%，延迟降低13倍。

Conclusion: 这些技术能够在真实世界边缘设备上高效部署强大的视觉语言模型。

Abstract: The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.

</details>


### [133] [Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives](https://arxiv.org/abs/2511.18507)
*Kai Jiang,Siqi Huang,Xiangyu Chen,Jiawei Shao,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出UNIFIER方法解决多模态大语言模型在持续学习中的灾难性遗忘问题，通过构建包含四种场景的多模态视觉理解数据集MSVQA，并将不同场景的视觉信息解耦到不同分支中，在相同特征空间中进行投影和一致性约束。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在动态场景变化下的灾难性遗忘问题，使模型能够持续适应下游任务中的背景和视角变化，有效执行复杂视觉任务。

Method: 提出UNIFIER方法，将不同场景的视觉信息解耦到每个视觉块的不同分支中，投影到相同特征空间，并对每个分支的特征施加一致性约束以保持跨场景视觉表示的稳定性。

Result: 在MSVQA数据集上的广泛实验表明，UNIFIER有效缓解了跨场景任务的遗忘，并在同一场景内实现了知识积累。

Conclusion: UNIFIER方法能够有效解决多模态大语言模型在持续学习中的灾难性遗忘问题，特别是在动态场景变化的环境下表现出色。

Abstract: Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.

</details>


### [134] [Unified Deep Learning Platform for Dust and Fault Diagnosis in Solar Panels Using Thermal and Visual Imaging](https://arxiv.org/abs/2511.18514)
*Abishek Karthik,Sreya Mynampati,Pandiyaraju V*

Main category: cs.CV

TL;DR: 开发了一个集中式平台，用于检测太阳能电池板上的灰尘和故障，结合CNN、ResNet和KerNet模型，通过图像预处理和热成像分析实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 太阳能电池板输出受多种因素影响，如灰尘、故障等，需要有效的检测系统来维护效率。不同地理环境下的效率差异也促使开发集中式检测平台。

Method: 使用伽马去除和高斯滤波预处理图像，结合CNN、ResNet和带自注意力机制的KerNet模型进行分类，通过功率输出、正弦波、电压等参数检测灰尘和故障。

Result: 模型在检测灰尘和故障方面表现出更高的效率和准确性，相比现有模型有更好的性能表现。

Conclusion: 该多应用模型在检测太阳能电池板灰尘和故障方面高效且优化，适用于从小型家庭到大型太阳能农场的各种规模需求。

Abstract: Solar energy is one of the most abundant and tapped sources of renewable energies with enormous future potential. Solar panel output can vary widely with factors like intensity, temperature, dirt, debris and so on affecting it. We have implemented a model on detecting dust and fault on solar panels. These two applications are centralized as a single-platform and can be utilized for routine-maintenance and any other checks. These are checked against various parameters such as power output, sinusoidal wave (I-V component of solar cell), voltage across each solar cell and others. Firstly, we filter and preprocess the obtained images using gamma removal and Gaussian filtering methods alongside some predefined processes like normalization. The first application is to detect whether a solar cell is dusty or not based on various pre-determined metrics like shadowing, leaf, droppings, air pollution and from other human activities to extent of fine-granular solar modules. The other one is detecting faults and other such occurrences on solar panels like faults, cracks, cell malfunction using thermal imaging application. This centralized platform can be vital since solar panels have different efficiency across different geography (air and heat affect) and can also be utilized for small-scale house requirements to large-scale solar farm sustentation effectively. It incorporates CNN, ResNet models that with self-attention mechanisms-KerNet model which are used for classification and results in a fine-tuned system that detects dust or any fault occurring. Thus, this multi-application model proves to be efficient and optimized in detecting dust and faults on solar panels. We have performed various comparisons and findings that demonstrates that our model has better efficiency and accuracy results overall than existing models.

</details>


### [135] [Breaking Forgetting: Training-Free Few-Shot Class-Incremental Learning via Conditional Diffusion](https://arxiv.org/abs/2511.18516)
*Haidong Kang,Ketong Qian,Yi Lu*

Main category: cs.CV

TL;DR: 本文提出了一种无训练的少样本类增量学习（FSCIL）框架CD-FSCIL，通过条件扩散过程替代传统的梯度优化，解决了梯度学习带来的灾难性遗忘和计算成本爆炸问题，并利用LLM生成的多模态特征缓解样本稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 传统FSCIL方法依赖梯度优化，随着新类别增加会导致计算成本爆炸，且在样本稀缺情况下既会导致基础类灾难性遗忘又阻碍新类适应。本文旨在设计完全去除梯度优化的无训练FSCIL范式。

Method: 基于条件扩散过程与梯度优化的关联性洞察，提出CD-FSCIL框架，用扩散生成转换替代梯度更新过程；引入多模态学习策略，整合视觉特征和LLM自动生成的自然语言描述。

Result: 在主流FSCIL基准测试中实现了最先进性能，同时大幅降低了计算和内存开销。

Conclusion: 该方法标志着向无训练持续适应的范式转变，为FSCIL提供了高效且有效的解决方案。

Abstract: Efforts to overcome catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL) have primarily focused on developing more effective gradient-based optimization strategies. In contrast, little attention has been paid to the training cost explosion that inevitably arises as the number of novel classes increases, a consequence of relying on gradient learning even under extreme data scarcity. More critically, since FSCIL typically provides only a few samples for each new class, gradient-based updates not only induce severe catastrophic forgetting on base classes but also hinder adaptation to novel ones. This paper seeks to break this long-standing limitation by asking: Can we design a training-free FSCIL paradigm that entirely removes gradient optimization? We provide an affirmative answer by uncovering an intriguing connection between gradient-based optimization and the Conditional Diffusion process. Building on this observation, we propose a Conditional Diffusion-driven FSCIL (CD-FSCIL) framework that substitutes the conventional gradient update process with a diffusion-based generative transition, enabling training-free incremental adaptation while effectively mitigating forgetting. Furthermore, to enhance representation under few-shot constraints, we introduce a multimodal learning strategy that integrates visual features with natural language descriptions automatically generated by Large Language Models (LLMs). This synergy substantially alleviates the sample scarcity issue and improves generalization across novel classes. Extensive experiments on mainstream FSCIL benchmarks demonstrate that our method not only achieves state-of-the-art performance but also drastically reduces computational and memory overhead, marking a paradigm shift toward training-free continual adaptation.

</details>


### [136] [DE-KAN: A Kolmogorov Arnold Network with Dual Encoder for accurate 2D Teeth Segmentation](https://arxiv.org/abs/2511.18533)
*Md Mizanur Rahman Mustakim,Jianwu Li,Sumya Bhuiyan,Mohammad Mehedi Hasan,Bing Han*

Main category: cs.CV

TL;DR: 提出DE-KAN模型用于全景X射线图像中的牙齿分割，通过双编码器架构和KAN瓶颈层提升分割精度，在基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决全景X射线图像中牙齿分割的挑战，包括解剖变异、不规则形状和结构重叠等问题，这些限制了传统深度学习模型的性能。

Method: 使用ResNet-18编码器处理增强输入，定制CNN编码器处理原始输入，通过KAN瓶颈层融合全局和局部空间特征，利用Kolmogorov Arnold表示定理的非线性可学习激活函数。

Result: 在两个基准牙科X射线数据集上，DE-KAN实现了mIoU 94.5%、Dice系数97.1%、准确率98.91%、召回率97.36%，相比现有方法Dice系数提升高达+4.7%。

Conclusion: DE-KAN通过双编码器架构和KAN瓶颈层有效提升了牙齿分割的精度和可解释性，在牙科图像分析中表现出优越性能。

Abstract: Accurate segmentation of individual teeth from panoramic radiographs remains a challenging task due to anatomical variations, irregular tooth shapes, and overlapping structures. These complexities often limit the performance of conventional deep learning models. To address this, we propose DE-KAN, a novel Dual Encoder Kolmogorov Arnold Network, which enhances feature representation and segmentation precision. The framework employs a ResNet-18 encoder for augmented inputs and a customized CNN encoder for original inputs, enabling the complementary extraction of global and local spatial features. These features are fused through KAN-based bottleneck layers, incorporating nonlinear learnable activation functions derived from the Kolmogorov Arnold representation theorem to improve learning capacity and interpretability. Extensive experiments on two benchmark dental X-ray datasets demonstrate that DE-KAN outperforms state-of-the-art segmentation models, achieving mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36%, representing up to +4.7% improvement in Dice compared to existing methods.

</details>


### [137] [HiFi-MambaV2: Hierarchical Shared-Routed MoE for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2511.18534)
*Pengcheng Fang,Hongli Chen,Guangzhen Yao,Jian Shi,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: cs.CV

TL;DR: HiFi-MambaV2是一种用于MRI重建的分层共享路由混合专家Mamba架构，通过频率分解和内容自适应计算实现高质量图像重建


<details>
  <summary>Details</summary>
Motivation: 从欠采样的k空间数据重建高保真MR图像需要在保持解剖一致性的同时恢复高频细节

Method: 采用可分离频率一致拉普拉斯金字塔(SF-Lap)提供抗混叠的稳定低频和高频流，以及分层共享路由MoE进行逐像素稀疏分发到共享专家和本地路由器

Result: 在多个数据集上评估显示，HiFi-MambaV2在PSNR、SSIM和NMSE指标上持续优于CNN、Transformer和先前基于Mamba的基线方法

Conclusion: HiFi-MambaV2能够实现可靠且鲁棒的MRI重建，在高频细节和整体结构保真度方面表现优异

Abstract: Reconstructing high-fidelity MR images from undersampled k-space data requires recovering high-frequency details while maintaining anatomical coherence. We present HiFi-MambaV2, a hierarchical shared-routed Mixture-of-Experts (MoE) Mamba architecture that couples frequency decomposition with content-adaptive computation. The model comprises two core components: (i) a separable frequency-consistent Laplacian pyramid (SF-Lap) that delivers alias-resistant, stable low- and high-frequency streams; and (ii) a hierarchical shared-routed MoE that performs per-pixel top-1 sparse dispatch to shared experts and local routers, enabling effective specialization with stable cross-depth behavior. A lightweight global context path is fused into an unrolled, data-consistency-regularized backbone to reinforce long-range reasoning and preserve anatomical coherence. Evaluated on fastMRI, CC359, ACDC, M4Raw, and Prostate158, HiFi-MambaV2 consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across single- and multi-coil settings and multiple acceleration factors, consistently surpassing consistent improvements in high-frequency detail and overall structural fidelity. These results demonstrate that HiFi-MambaV2 enables reliable and robust MRI reconstruction.

</details>


### [138] [Zero-Shot Video Deraining with Video Diffusion Models](https://arxiv.org/abs/2511.18537)
*Tuomas Varanka,Juan Luis Gonzalez,Hyeongwoo Kim,Pablo Garrido,Xu Yao*

Main category: cs.CV

TL;DR: 提出首个零样本视频去雨方法，无需合成数据或模型微调，利用预训练文本到视频扩散模型，通过负提示和注意力切换机制去除动态场景中的雨水。


<details>
  <summary>Details</summary>
Motivation: 现有视频去雨方法依赖配对数据集训练，合成数据泛化能力差，静态相机数据无法处理动态场景。扩散模型微调会削弱生成先验，限制泛化能力。

Method: 将输入视频反转到扩散模型潜在空间，通过负提示干预重建过程去除雨水概念。核心是注意力切换机制，保持动态背景和结构一致性，减少负提示引入的伪影。

Result: 在真实世界雨数据集上的广泛实验验证了方法有效性，相比先前方法有显著改进，展现出无需监督训练的鲁棒泛化能力。

Conclusion: 该方法成功实现了零样本视频去雨，无需合成数据或模型微调，在动态场景中表现出优异的去雨效果和泛化能力。

Abstract: Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion. Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases. In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities. By inverting an input video into the latent space of diffusion models, its reconstruction process can be intervened and pushed away from the model's concept of rain using negative prompting. At the core of our approach is an attention switching mechanism that we found is crucial for maintaining dynamic backgrounds as well as structural consistency between the input and the derained video, mitigating artifacts introduced by naive negative prompting. Our approach is validated through extensive experiments on real-world rain datasets, demonstrating substantial improvements over prior methods and showcasing robust generalization without the need for supervised training.

</details>


### [139] [C3Po: Cross-View Cross-Modality Correspondence by Pointmap Prediction](https://arxiv.org/abs/2511.18559)
*Kuan Wei Huang,Brandon Li,Bharath Hariharan,Noah Snavely*

Main category: cs.CV

TL;DR: 本文提出了C3数据集，用于解决地面照片与平面图之间的跨模态几何对应问题，显著提升了对应模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的几何模型在处理不同视角（如航拍与地面）或不同模态（如照片与抽象绘图）的输入时表现不佳，特别是在地面照片与平面图之间的对应预测上存在挑战。

Method: 通过从互联网照片集合中重建3D场景，然后手动将重建结果与从互联网收集的平面图进行配准，从而创建包含大量对应关系的新数据集C3。

Result: C3数据集包含90K对平面图和照片，覆盖597个场景，拥有153M像素级对应关系和85K相机姿态。在该任务上训练后，最佳方法的RMSE提升了34%。

Conclusion: C3数据集有助于解决跨模态几何推理中的开放挑战，现有最先进的对应模型在该任务上仍有困难，但通过在新数据上训练可以显著提升性能。

Abstract: Geometric models like DUSt3R have shown great advances in understanding the geometry of a scene from pairs of photos. However, they fail when the inputs are from vastly different viewpoints (e.g., aerial vs. ground) or modalities (e.g., photos vs. abstract drawings) compared to what was observed during training. This paper addresses a challenging version of this problem: predicting correspondences between ground-level photos and floor plans. Current datasets for joint photo--floor plan reasoning are limited, either lacking in varying modalities (VIGOR) or lacking in correspondences (WAFFLE). To address these limitations, we introduce a new dataset, C3, created by first reconstructing a number of scenes in 3D from Internet photo collections via structure-from-motion, then manually registering the reconstructions to floor plans gathered from the Internet, from which we can derive correspondence between images and floor plans. C3 contains 90K paired floor plans and photos across 597 scenes with 153M pixel-level correspondences and 85K camera poses. We find that state-of-the-art correspondence models struggle on this task. By training on our new data, we can improve on the best performing method by 34% in RMSE. We also identify open challenges in cross-modal geometric reasoning that our dataset aims to help address.

</details>


### [140] [Zero-Reference Joint Low-Light Enhancement and Deblurring via Visual Autoregressive Modeling with VLM-Derived Modulation](https://arxiv.org/abs/2511.18591)
*Wei Dong,Han Zhou,Junwei Lin,Jun Chen*

Main category: cs.CV

TL;DR: 提出基于视觉自回归建模的生成框架，结合视觉语言模型的感知先验，通过自适应曲线估计、动态空间频率感知旋转位置编码和递归相位域调制策略，无监督地解决暗图像恢复中的复杂噪声、模糊和动态光照问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界暗图像不仅存在低可见度和对比度问题，还包含复杂噪声和模糊，现有方法依赖配对数据或无法建模动态光照和模糊特性，导致泛化能力差。

Method: 1) 基于VLM可见性分数的自适应曲线估计调节多样光照；2) 在VAR中集成动态空间频率感知旋转位置编码增强模糊退化结构建模；3) 基于VLM模糊评分的递归相位域调制策略减轻模糊伪影。

Result: 在基准数据集上实现了最先进的性能表现。

Conclusion: 提出的无监督生成框架有效解决了暗图像恢复中的复杂退化问题，具有优异的泛化能力。

Abstract: Real-world dark images commonly exhibit not only low visibility and contrast but also complex noise and blur, posing significant restoration challenges. Existing methods often rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization. To tackle this, we propose a generative framework based on visual autoregressive (VAR) modeling, guided by perceptual priors from the vision-language model (VLM). Specifically, to supply informative conditioning cues for VAR models, we deploy an adaptive curve estimation scheme to modulate the diverse illumination based on VLM-derived visibility scores. In addition, we integrate dynamic and spatial-frequency-aware Rotary Positional Encodings (SF-RoPE) into VAR to enhance its ability to model structures degraded by blur. Furthermore, we propose a recursive phase-domain modulation strategy that mitigates blur-induced artifacts in the phase domain via bounded iterative refinement guided by VLM-assessed blur scores. Our framework is fully unsupervised and achieves state-of-the-art performance on benchmark datasets.

</details>


### [141] [Stage-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI](https://arxiv.org/abs/2511.18595)
*Wenhao Guo,Golrokh Mirzaei*

Main category: cs.CV

TL;DR: 该研究首次对胶质母细胞瘤随访MRI进行分期特异性深度学习模型基准测试，发现在第二次随访时模型性能更好，Mamba+CNN混合模型在准确性和效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 区分胶质母细胞瘤的真实进展与治疗相关假性进展在早期随访中具有挑战性，需要建立分期感知的深度学习模型基准。

Method: 使用Burdenko GBM进展队列(n=180)，在统一的质量控制驱动流程下训练11个代表性深度学习家族(CNN、LSTM、混合模型、Transformer等)，采用患者级交叉验证独立分析不同放疗后扫描时间点。

Result: 两个随访阶段的准确率相当(约0.70-0.74)，但第二次随访时的区分能力更好，F1和AUC值提高；Mamba+CNN混合模型提供最佳准确率-效率权衡，Transformer变体AUC竞争力强但计算成本高。

Conclusion: 研究建立了分期感知基准，表明绝对区分能力总体有限，反映了TP与PsP区分的固有难度，建议未来工作应纳入纵向建模、多序列MRI和更大的多中心队列。

Abstract: Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset's size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts.

</details>


### [142] [Functional Localization Enforced Deep Anomaly Detection Using Fundus Images](https://arxiv.org/abs/2511.18627)
*Jan Benedikt Ruhland,Thorsten Papenbrock,Jan-Peter Sowa,Ali Canbay,Nicole Eter,Bernd Freisleben,Dominik Heider*

Main category: cs.CV

TL;DR: 本研究系统评估了Vision Transformer在多种增强策略下对多种视网膜疾病的检测性能，在多个数据集上表现稳定，准确率0.789-0.843。几何和颜色增强效果最佳，同时开发了GANomaly异常检测器提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决视网膜疾病检测中面临的成像质量变异性、早期症状细微性和数据集间域偏移等挑战，提高检测可靠性。

Method: 使用Vision Transformer分类器，结合多种数据增强和图像增强策略，在多个异构公共数据集和自建AEyeDB数据集上进行系统评估。同时开发GANomaly异常检测器，并使用GUESS进行概率校准。

Result: ViT在不同数据集和疾病上表现稳定，准确率0.789-0.843。几何和颜色增强效果最佳，Laplacian增强降低性能。在Papila数据集上AUC达0.91，优于卷积集成基线。GANomaly异常检测器AUC为0.76，具有良好的可解释性。

Conclusion: Transformer架构和多数据集训练具有优势，几何和颜色增强是最有效的策略。结合分类器和异常检测器，通过概率校准可为临床实施提供阈值独立的决策支持。

Abstract: Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.
  On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.

</details>


### [143] [Health system learning achieves generalist neuroimaging models](https://arxiv.org/abs/2511.18640)
*Akhil Kondepudi,Akshay Rao,Chenhui Zhao,Yiwei Lyu,Samir Harake,Soumyanil Banerjee,Rushikesh Joshi,Anna-Katharina Meissner,Renly Hou,Cheng Jiang,Asadur Chowdury,Ashok Srinivasan,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 本文介绍了NeuroVFM，一个基于健康系统学习范式训练的视觉基础模型，在神经影像任务上超越了前沿AI模型，通过从临床数据中学习实现了高性能的通用神经影像分析。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型在公共互联网数据上训练，但缺乏私有临床数据访问，特别是神经影像数据因包含可识别面部特征而在公共领域代表性不足，限制了模型在临床医学中的性能。

Method: 采用健康系统学习范式，使用可扩展的体积联合嵌入预测架构，在524万临床MRI和CT体积数据上训练NeuroVFM视觉基础模型。

Result: NeuroVFM在多个临床任务上达到最先进性能，包括放射学诊断和报告生成，展现出新兴的神经解剖学理解和可解释的视觉基础。与开源语言模型结合时，在准确性、临床分诊和专家偏好方面超越前沿模型。

Conclusion: 健康系统学习是构建通用医学AI的有效范式，NeuroVFM通过临床基础的视觉理解减少了幻觉发现和关键错误，为临床基础模型提供了可扩展框架。

Abstract: Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.

</details>


### [144] [From Healthy Scans to Annotated Tumors: A Tumor Fabrication Framework for 3D Brain MRI Synthesis](https://arxiv.org/abs/2511.18654)
*Nayu Dong,Townim Chowdhury,Hieu Phan,Mark Jenkinson,Johan Verjans,Zhibin Liao*

Main category: cs.CV

TL;DR: 提出Tumor Fabrication (TF)框架，通过两阶段方法合成3D脑肿瘤数据，仅需健康扫描图像和少量真实标注数据，用于增强下游肿瘤分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决MRI肿瘤数据稀缺问题，现有方法需要大量训练数据或专家手动建模，不适用于数据有限的临床环境。

Method: 两阶段框架：粗粒度肿瘤合成和生成模型驱动的精炼过程，完全自动化，利用健康图像和少量真实标注数据生成大量配对的合成数据。

Result: 在低数据情况下，使用合成图像-标签对作为数据增强能显著提高下游肿瘤分割任务的性能。

Conclusion: TF提供了一个可扩展且可靠的医学图像增强解决方案，解决了临床AI应用中数据稀缺的关键挑战。

Abstract: The scarcity of annotated Magnetic Resonance Imaging (MRI) tumor data presents a major obstacle to accurate and automated tumor segmentation. While existing data synthesis methods offer promising solutions, they often suffer from key limitations: manual modeling is labor intensive and requires expert knowledge. Deep generative models may be used to augment data and annotation, but they typically demand large amounts of training pairs in the first place, which is impractical in data limited clinical settings. In this work, we propose Tumor Fabrication (TF), a novel two-stage framework for unpaired 3D brain tumor synthesis. The framework comprises a coarse tumor synthesis process followed by a refinement process powered by a generative model. TF is fully automated and leverages only healthy image scans along with a limited amount of real annotated data to synthesize large volumes of paired synthetic data for enriching downstream supervised segmentation training. We demonstrate that our synthetic image-label pairs used as data enrichment can significantly improve performance on downstream tumor segmentation tasks in low-data regimes, offering a scalable and reliable solution for medical image enrichment and addressing critical challenges in data scarcity for clinical AI applications.

</details>


### [145] [Robust Physical Adversarial Patches Using Dynamically Optimized Clusters](https://arxiv.org/abs/2511.18656)
*Harrison Bagley,Will Meakin,Simon Lucey,Yee Wei Law,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种基于超像素的正则化方法，通过SLIC算法在对抗性补丁优化过程中动态聚类像素，利用隐函数定理反向传播梯度，使补丁在尺度变化时保持结构稳定性，减少插值损失。


<details>
  <summary>Details</summary>
Motivation: 物理对抗性攻击在深度学习系统中令人担忧，但现有方法很少关注尺度变化问题。当补丁被重新缩放时，插值引起的颜色混合会平滑像素值，导致高频模式丢失和对抗性信号衰减。

Method: 使用SLIC算法在对抗性补丁优化过程中动态聚类像素，应用隐函数定理反向传播梯度以更新超像素边界和颜色，从而生成对尺度变化具有弹性的补丁结构。

Result: 该方法在数字域实现了更好的性能，当物理实现时这些性能提升得以保持，导致改进的物理性能。使用新的物理评估协议客观评估了真实世界性能。

Conclusion: 提出的超像素正则化方法能够产生对尺度变化具有弹性的对抗性补丁，在数字和物理域都表现出优越的性能，有效解决了插值损失问题。

Abstract: Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.

</details>


### [146] [Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement](https://arxiv.org/abs/2511.18672)
*Yuchen Xia,Souvik Kundu,Mosharaf Chowdhury,Nishil Talati*

Main category: cs.CV

TL;DR: Sphinx是一个无需训练的混合推理框架，通过回归式快速初始化引导扩散模型，结合选择性精炼和自适应噪声调度，在保持扩散级质量的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散式NVS生成质量高但计算成本大，回归式NVS计算效率高但质量不足，需要设计一个高质量且推理高效的NVS框架。

Method: 使用回归式快速初始化来引导和减少扩散模型去噪工作量，集成选择性精炼与自适应噪声调度，将更多计算资源分配给不确定区域和帧。

Result: Sphinx平均比扩散模型推理快1.8倍，感知退化小于5%，在NVS服务中建立了质量和延迟之间的新帕累托边界。

Conclusion: Sphinx框架成功实现了扩散级保真度与显著降低计算成本的平衡，为动态变化的推理场景提供了灵活的性能-质量权衡导航。

Abstract: Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.

</details>


### [147] [Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers](https://arxiv.org/abs/2511.18673)
*Yiqing Shi,Yiren Song,Mike Zheng Shou*

Main category: cs.CV

TL;DR: Edit2Perceive是一个统一的扩散框架，将图像编辑扩散模型适配用于深度、法线和抠图等密集感知任务，通过全参数微调和像素空间一致性损失实现结构保持优化，在单步确定性推理下实现更快运行速度。


<details>
  <summary>Details</summary>
Motivation: 现有密集感知方法大多依赖为随机生成设计的文本到图像生成器，而图像编辑扩散模型具有固有的图像到图像一致性，更适合作为密集感知任务的基础。

Method: 基于FLUX.1 Kontext架构，采用全参数微调和像素空间一致性损失来强制中间去噪状态间的结构保持优化，使用单步确定性推理。

Result: 在深度、法线和抠图三个任务上均实现了全面的最先进结果，相比传统方法运行速度提升高达8倍。

Conclusion: 编辑导向的扩散变换器在几何感知任务中展现出强大潜力，为密集感知提供了更合适的基础模型。

Abstract: Recent advances in diffusion transformers have shown remarkable generalization in visual synthesis, yet most dense perception methods still rely on text-to-image (T2I) generators designed for stochastic generation. We revisit this paradigm and show that image editing diffusion models are inherently image-to-image consistent, providing a more suitable foundation for dense perception task. We introduce Edit2Perceive, a unified diffusion framework that adapts editing models for depth, normal, and matting. Built upon the FLUX.1 Kontext architecture, our approach employs full-parameter fine-tuning and a pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states. Moreover, our single-step deterministic inference yields up to faster runtime while training on relatively small datasets. Extensive experiments demonstrate comprehensive state-of-the-art results across all three tasks, revealing the strong potential of editing-oriented diffusion transformers for geometry-aware perception.

</details>


### [148] [MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis](https://arxiv.org/abs/2511.18676)
*Yongcheng Yao,Yongshuo Zong,Raman Dutt,Yongxin Yang,Sotirios A Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: 本文介绍了MedVision，一个专门用于评估和改进视觉语言模型在医学图像定量分析能力的大规模数据集和基准测试，涵盖检测、尺寸估计和角度/距离测量等任务。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型主要专注于分类问答和定性描述任务，但临床决策往往依赖于定量评估（如肿瘤大小测量、关节角度测量），这种定量推理能力在现有模型中尚未得到充分探索和支持。

Method: 构建MedVision大规模数据集，涵盖22个公共数据集、3080万图像-标注对，专注于三个代表性定量任务：解剖结构和异常检测、肿瘤/病变尺寸估计、角度/距离测量，并通过监督微调提升模型性能。

Result: 现有现成视觉语言模型在这些定量任务上表现不佳，但经过MedVision的监督微调后，在检测、肿瘤/病变估计和角度/距离测量任务上的性能显著提升，错误率降低且精度提高。

Conclusion: 这项工作为开发具有强大定量推理能力的医学成像视觉语言模型奠定了基础，推动了医学人工智能向更实用的临床决策支持方向发展。

Abstract: Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., "Is this normal or abnormal?") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.

</details>


### [149] [A Theory-Inspired Framework for Few-Shot Cross-Modal Sketch Person Re-Identification](https://arxiv.org/abs/2511.18677)
*Yunpeng Gong,Yongjie Hou,Jiangming Shi,Kim Long Diep,Min Jiang*

Main category: cs.CV

TL;DR: KTCAA是一个基于泛化理论的少样本跨模态框架，通过对齐增强和知识转移催化剂解决素描-图像匹配中的模态差距和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于素描的行人重识别任务中存在的显著模态差距和标注数据有限的问题，提升模型在数据稀缺条件下的跨模态泛化能力。

Method: 提出对齐增强模块应用局部素描风格变换模拟目标分布，知识转移催化剂引入最坏情况扰动增强不变性，在元学习范式下联合优化这两个组件。

Result: 在多个基准测试中，KTCAA实现了最先进的性能，特别是在数据稀缺条件下表现优异。

Conclusion: KTCAA通过理论指导的模块设计有效解决了跨模态行人重识别中的泛化挑战，为数据稀缺场景提供了可行的解决方案。

Abstract: Sketch based person re-identification aims to match hand-drawn sketches with RGB surveillance images, but remains challenging due to significant modality gaps and limited annotated data. To address this, we introduce KTCAA, a theoretically grounded framework for few-shot cross-modal generalization. Motivated by generalization theory, we identify two key factors influencing target domain risk: (1) domain discrepancy, which quantifies the alignment difficulty between source and target distributions; and (2) perturbation invariance, which evaluates the model's robustness to modality shifts. Based on these insights, we propose two components: (1) Alignment Augmentation (AA), which applies localized sketch-style transformations to simulate target distributions and facilitate progressive alignment; and (2) Knowledge Transfer Catalyst (KTC), which enhances invariance by introducing worst-case perturbations and enforcing consistency. These modules are jointly optimized under a meta-learning paradigm that transfers alignment knowledge from data-rich RGB domains to sketch-based scenarios. Experiments on multiple benchmarks demonstrate that KTCAA achieves state-of-the-art performance, particularly in data-scarce conditions.

</details>


### [150] [Neural Geometry Image-Based Representations with Optimal Transport (OT)](https://arxiv.org/abs/2511.18679)
*Xiang Gao,Yuanpeng Liu,Xinmu Wang,Jiazhi Li,Minghao Guo,Yu Guo,Xiyun Song,Heather Yu,Zhiqiang Lao,Xianfeng David Gu*

Main category: cs.CV

TL;DR: 提出了一种基于几何图像的神经表示方法，通过将不规则网格转换为规则图像网格，实现高效的图像式神经处理，具有解码器无关、存储高效的特点。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格神经表示方法依赖神经过拟合，计算成本高且处理不规则网格结构复杂。而图像具有规则结构便于高效处理，但难以直接应用于网格。

Method: 使用几何图像表示将不规则网格转换为规则图像网格，利用最优传输(OT)解决平坦区域过采样和特征区域欠采样问题，支持几何图像mipmapping实现连续细节层次。

Result: 实验结果显示在压缩比(CR)、Chamfer距离(CD)和Hausdorff距离(HD)等指标上达到最先进的存储效率和恢复精度。

Conclusion: 基于几何图像的神经表示方法能够有效结合图像处理的高效性和网格表示的准确性，为3D网格处理提供了新的解决方案。

Abstract: Neural representations for 3D meshes are emerging as an effective solution for compact storage and efficient processing. Existing methods often rely on neural overfitting, where a coarse mesh is stored and progressively refined through multiple decoder networks. While this can restore high-quality surfaces, it is computationally expensive due to successive decoding passes and the irregular structure of mesh data. In contrast, images have a regular structure that enables powerful super-resolution and restoration frameworks, but applying these advantages to meshes is difficult because their irregular connectivity demands complex encoder-decoder architectures. Our key insight is that a geometry image-based representation transforms irregular meshes into a regular image grid, making efficient image-based neural processing directly applicable. Building on this idea, we introduce our neural geometry image-based representation, which is decoder-free, storage-efficient, and naturally suited for neural processing. It stores a low-resolution geometry-image mipmap of the surface, from which high-quality meshes are restored in a single forward pass. To construct geometry images, we leverage Optimal Transport (OT), which resolves oversampling in flat regions and undersampling in feature-rich regions, and enables continuous levels of detail (LoD) through geometry-image mipmapping. Experimental results demonstrate state-of-the-art storage efficiency and restoration accuracy, measured by compression ratio (CR), Chamfer distance (CD), and Hausdorff distance (HD).

</details>


### [151] [Hierarchical GraphCut Phase Unwrapping based on Invariance of Diffeomorphisms Framework](https://arxiv.org/abs/2511.18682)
*Xiang Gao,Xinmu Wang,Zhou Zhao,Junqi Huang,Xianfeng David Gu*

Main category: cs.CV

TL;DR: 本文提出了一种基于图割和微分同胚的相位展开框架，通过将相位展开重新表述为像素标记问题，利用共形映射和最优传输映射在图像空间中应用微分同胚的不变性特性，实现了45.5倍的加速和更低的L2误差。


<details>
  <summary>Details</summary>
Motivation: 现有的相位展开方法在速度和精度之间存在权衡：快速方法精度不足，而精确算法速度太慢无法实时使用。结构光扫描中的相位展开对于4D面部动态捕捉等应用至关重要，但噪声、遮挡和复杂3D几何使得恢复真实相位具有挑战性。

Method: 将基于图割的相位展开重新表述为像素标记问题，利用微分同胚在图像空间中的不变性特性。预计算输入相位数据的奇数个微分同胚（通过共形映射和最优传输映射），在每个域中应用分层图割算法，通过多数投票融合得到的标签图来稳健估计每个像素的k值。

Result: 实验结果显示在真实实验和模拟中实现了45.5倍的加速和更低的L2误差，展示了实时应用的潜力。

Conclusion: 提出的相位展开框架通过结合图割和微分同胚技术，在保持高精度的同时显著提高了计算效率，为实时3D扫描应用提供了可行的解决方案。

Abstract: Recent years have witnessed rapid advancements in 3D scanning technologies, with applications spanning VR/AR, digital human creation, and medical imaging. Structured-light scanning with phase-shifting techniques is preferred for its use of low-intensity visible light and high accuracy, making it well suited for capturing 4D facial dynamics. A key step is phase unwrapping, which recovers continuous phase values from measurements wrapped modulo 2pi. The goal is to estimate the unwrapped phase count k in the equation Phi = phi + 2pi k, where phi is the wrapped phase and Phi is the true phase. Noise, occlusions, and complex 3D geometry make recovering the true phase challenging because phase unwrapping is ill-posed: measurements only provide modulo 2pi values, and estimating k requires assumptions about surface continuity. Existing methods trade speed for accuracy: fast approaches lack precision, while accurate algorithms are too slow for real-time use. To overcome these limitations, this work proposes a phase unwrapping framework that reformulates GraphCut-based unwrapping as a pixel-labeling problem. This framework improves the estimation of the unwrapped phase count k through the invariance property of diffeomorphisms applied in image space via conformal and optimal transport (OT) maps. An odd number of diffeomorphisms are precomputed from the input phase data, and a hierarchical GraphCut algorithm is applied in each domain. The resulting label maps are fused via majority voting to robustly estimate k at each pixel. Experimental results demonstrate a 45.5x speedup and lower L2 error in real experiments and simulations, showing potential for real-time applications.

</details>


### [152] [Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation](https://arxiv.org/abs/2511.18684)
*Shristi Das Biswas,Arani Roy,Kaushik Roy*

Main category: cs.CV

TL;DR: ICE是一种无需训练、模态无关的单次权重修改方法，用于文本到图像和文本到视频模型的概念擦除，通过各向异性能量加权缩放定义擦除和保留子空间，并使用独特闭式重叠投影器进行正则化。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法存在重新训练成本高、推理开销大或易受对抗攻击的问题，且很少建模目标擦除概念与周围内容的潜在语义重叠，导致擦除后产生附带损害，同时很少有方法能同时在T2I和T2V领域可靠工作。

Method: ICE使用各向异性能量加权缩放定义擦除和保留子空间，通过独特闭式重叠投影器显式正则化其交集，提出凸且Lipschitz有界的谱遗忘目标，平衡擦除保真度和交集保留，获得稳定唯一的解析解。

Result: ICE在艺术风格、物体、身份和显式内容的目标移除中，高效实现强擦除效果，提高对红队测试的鲁棒性，同时在T2I和T2V模型中仅对原始生成能力造成最小退化。

Conclusion: ICE是一种训练免费、模态无关的单次权重修改方法，能够实现精确、持久的遗忘，且无运行时开销，在T2I和T2V模型中均表现出色。

Abstract: Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.

</details>


### [153] [EVCC: Enhanced Vision Transformer-ConvNeXt-CoAtNet Fusion for Classification](https://arxiv.org/abs/2511.18691)
*Kazi Reyazul Hasan,Md Nafiu Rahman,Wasif Jalal,Sadif Ahmed,Shahriar Raj,Mubasshira Musarrat,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: EVCC是一种结合Vision Transformer、轻量级ConvNeXt和CoAtNet的多分支混合视觉架构，通过自适应token剪枝、双向交叉注意力、多任务学习和动态路由门等创新技术，在保持高精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的混合视觉架构虽然提升了图像分类性能，但计算成本过高。需要开发一种既能保持高精度又能显著降低计算复杂度的新型架构。

Method: 1) 自适应token剪枝与信息保留；2) 门控双向交叉注意力增强特征细化；3) 辅助分类头实现多任务学习；4) 上下文感知的置信度驱动动态路由门。

Result: 在CIFAR-100、Tobacco3482、CelebA和Brain Cancer数据集上，EVCC相比DeiT-Base、MaxViT-Base和CrossViT-Base等模型，准确率提升最高达2个百分点，同时FLOPs减少25-35%。

Conclusion: EVCC通过动态调整计算需求，有效平衡了精度与效率的权衡，结合全局上下文、局部细节和层次特征，适用于实际应用场景。

Abstract: Hybrid vision architectures combining Transformers and CNNs have significantly advanced image classification, but they usually do so at significant computational cost. We introduce EVCC (Enhanced Vision Transformer-ConvNeXt-CoAtNet), a novel multi-branch architecture integrating the Vision Transformer, lightweight ConvNeXt, and CoAtNet through key innovations: (1) adaptive token pruning with information preservation, (2) gated bidirectional cross-attention for enhanced feature refinement, (3) auxiliary classification heads for multi-task learning, and (4) a dynamic router gate employing context-aware confidence-driven weighting. Experiments across the CIFAR-100, Tobacco3482, CelebA, and Brain Cancer datasets demonstrate EVCC's superiority over powerful models like DeiT-Base, MaxViT-Base, and CrossViT-Base by consistently achieving state-of-the-art accuracy with improvements of up to 2 percentage points, while reducing FLOPs by 25 to 35%. Our adaptive architecture adjusts computational demands to deployment needs by dynamically reducing token count, efficiently balancing the accuracy-efficiency trade-off while combining global context, local details, and hierarchical features for real-world applications. The source code of our implementation is available at https://anonymous.4open.science/r/EVCC.

</details>


### [154] [Exploring Surround-View Fisheye Camera 3D Object Detection](https://arxiv.org/abs/2511.18695)
*Changcai Li,Wenwei Lin,Zuoxun Hou,Gang Chen,Wei Zhang,Huihui Zhou,Weishi Zheng*

Main category: cs.CV

TL;DR: 本文研究了在环视鱼眼相机系统中实现端到端3D目标检测的技术可行性，开发了两种融合鱼眼图像几何特性的检测方法，并发布了首个鱼眼3D检测数据集Fisheye3DOD。


<details>
  <summary>Details</summary>
Motivation: 探索经典针孔相机3D目标检测器在鱼眼图像上的性能下降问题，并开发专门针对鱼眼图像几何特性的3D检测方法。

Method: 开发了两种方法：基于鸟瞰图范式的FisheyeBEVDet和基于查询范式的FisheyePETR，均采用球面空间表示来有效捕捉鱼眼几何特性。

Result: 在Fisheye3DOD数据集上的实验表明，鱼眼兼容建模相比基线方法准确率提升高达6.2%。

Conclusion: 通过专门设计的球面空间表示和鱼眼几何建模，可以有效提升环视鱼眼相机系统的3D目标检测性能。

Abstract: In this work, we explore the technical feasibility of implementing end-to-end 3D object detection (3DOD) with surround-view fisheye camera system. Specifically, we first investigate the performance drop incurred when transferring classic pinhole-based 3D object detectors to fisheye imagery. To mitigate this, we then develop two methods that incorporate the unique geometry of fisheye images into mainstream detection frameworks: one based on the bird's-eye-view (BEV) paradigm, named FisheyeBEVDet, and the other on the query-based paradigm, named FisheyePETR. Both methods adopt spherical spatial representations to effectively capture fisheye geometry. In light of the lack of dedicated evaluation benchmarks, we release Fisheye3DOD, a new open dataset synthesized using CARLA and featuring both standard pinhole and fisheye camera arrays. Experiments on Fisheye3DOD show that our fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods.

</details>


### [155] [CoD: A Diffusion Foundation Model for Image Compression](https://arxiv.org/abs/2511.18706)
*Zhaoyang Jia,Zihan Zheng,Naifu Xue,Jiahao Li,Bin Li,Zongyu Guo,Xiaoyi Zhang,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: CoD是首个专为压缩设计的扩散基础模型，相比基于文本到图像扩散模型（如Stable Diffusion）的现有编解码器，在超低比特率下具有更高的压缩效率，训练成本降低300倍，并在像素空间扩散中实现了VTM级别的PSNR和良好感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本到图像扩散模型的编解码器在压缩视角下存在不足，特别是在超低比特率时性能受限。需要专门为压缩优化的扩散基础模型来提升下游扩散编解码器的潜力。

Method: 从零开始训练CoD压缩导向扩散基础模型，实现压缩和生成的端到端优化。该模型不是固定编解码器，而是为各种基于扩散的编解码器设计的通用基础模型。

Result: 在DiffC等下游编解码器中用CoD替换Stable Diffusion实现了SOTA结果，特别是在0.0039 bpp等超低比特率下；训练速度比Stable Diffusion快300倍（约20 vs. 6,250 A100 GPU天）；像素空间扩散可达到VTM级别的PSNR并保持高感知质量；使用更少参数即可超越GAN基编解码器。

Conclusion: CoD为未来扩散编解码器研究奠定了基础，提供了高压缩效率、低训练成本和新的技术见解。

Abstract: Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \textbf{CoD}, the first \textbf{Co}mpression-oriented \textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \textbf{Low-cost and reproducible training}, 300$\times$ faster training than Stable Diffusion ($\sim$ 20 vs. $\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.

</details>


### [156] [Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation](https://arxiv.org/abs/2511.18711)
*Yuyang Wanyan,Xiaoshan Yang,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: 本文提出了一种用于少样本视频域适应（FSVDA）的新框架MC-LRD，通过多模态分解来解决视频中不同模态特征域偏移差异的问题，实现了更好的域对齐和模态协作。


<details>
  <summary>Details</summary>
Motivation: 视频的多模态特性在少样本域适应场景下面临独特挑战，需要同时考虑域对齐和模态协作。现有方法忽视了不同模态特征组件具有不同域偏移水平的问题，这限制了单模态和多模态特征的泛化性能。

Method: 提出MC-LRD框架，包含每个模态的多个分解器和多模态分解路由器（MDR）。分解器在不同模态间渐进共享参数，MDR选择性激活分解器产生模态独特和模态共享特征。应用正交去相关约束和跨域激活一致性损失来确保分解效果和域对齐。

Result: 在三个公共基准测试上的广泛实验结果表明，该模型相比现有方法取得了显著改进。

Conclusion: MC-LRD框架通过有效分解不同域偏移水平的模态特征，成功解决了少样本视频域适应中的挑战，为多模态域适应提供了新的解决方案。

Abstract: In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative LowRank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and subrouters, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.

</details>


### [157] [DriveFlow: Rectified Flow Adaptation for Robust 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2511.18713)
*Hongbin Lin,Yiming Yang,Chaoda Zheng,Yifan Zhang,Shuaicheng Niu,Zilu Guo,Yafeng Li,Gui Gui,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: DriveFlow是一种基于预训练文本到图像流模型的修正流适应方法，用于自动驾驶中的训练数据增强，通过高频前景保护和双频背景优化来解决OOD问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中基于视觉的3D物体检测面临标注成本高和室外场景多样性的挑战，导致训练数据无法覆盖所有测试场景（OOD问题）。训练免费的图像编辑通过数据增强提高模型鲁棒性，但现有方法存在效果有限或无法保持准确3D几何结构的问题。

Method: 基于频率分解，DriveFlow提出两种策略来适应从文本条件速度导出的无噪声编辑路径：1）高频前景保护：引入高频对齐损失以保持精确的3D物体几何结构；2）双频背景优化：进行双频优化以平衡编辑灵活性和语义一致性。

Result: 综合实验验证了DriveFlow的有效性和效率，在OOD场景下所有类别上都表现出全面的性能提升。

Conclusion: DriveFlow通过修正流适应方法成功解决了自动驾驶数据增强中的3D几何保持问题，为OOD场景下的模型鲁棒性提供了有效解决方案。

Abstract: In autonomous driving, vision-centric 3D object detection recognizes and localizes 3D objects from RGB images. However, due to high annotation costs and diverse outdoor scenes, training data often fails to cover all possible test scenarios, known as the out-of-distribution (OOD) issue. Training-free image editing offers a promising solution for improving model robustness by training data enhancement without any modifications to pre-trained diffusion models. Nevertheless, inversion-based methods often suffer from limited effectiveness and inherent inaccuracies, while recent rectified-flow-based approaches struggle to preserve objects with accurate 3D geometry. In this paper, we propose DriveFlow, a Rectified Flow Adaptation method for training data enhancement in autonomous driving based on pre-trained Text-to-Image flow models. Based on frequency decomposition, DriveFlow introduces two strategies to adapt noise-free editing paths derived from text-conditioned velocities. 1) High-Frequency Foreground Preservation: DriveFlow incorporates a high-frequency alignment loss for foreground to maintain precise 3D object geometry. 2) Dual-Frequency Background Optimization: DriveFlow also conducts dual-frequency optimization for background, balancing editing flexibility and semantic consistency. Comprehensive experiments validate the effectiveness and efficiency of DriveFlow, demonstrating comprehensive performance improvements on all categories across OOD scenarios. Code is available at https://github.com/Hongbin98/DriveFlow.

</details>


### [158] [GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.18729)
*Lin Liu,Caiyan Jia,Guanyi Yu,Ziying Song,JunQiao Li,Feiyang Jia,Peiliang Wu,Xiaoshuai Hao,Yandan Luo*

Main category: cs.CV

TL;DR: GuideFlow是一个基于约束流匹配的新型自动驾驶规划框架，解决了现有模仿式端到端规划器的多模态轨迹模式崩溃问题，并能在生成过程中直接强制执行显式约束。


<details>
  <summary>Details</summary>
Motivation: 现有模仿式端到端规划器存在多模态轨迹模式崩溃问题，无法产生多样化的轨迹提案；而生成式端到端规划器难以在生成过程中直接融入安全性和物理约束，需要额外的优化阶段来精炼输出。

Method: 提出GuideFlow框架，利用约束流匹配技术：1）显式建模流匹配过程以缓解模式崩溃；2）在流匹配生成过程中直接强制执行显式约束；3）将流匹配与基于能量的模型统一训练以增强自主优化能力；4）参数化驾驶攻击性作为生成过程中的控制信号。

Result: 在主要驾驶基准测试（Bench2Drive、NuScenes、NavSim和ADV-NuScenes）上进行了广泛评估。在NavSim测试困难分割（Navhard）上达到了最先进水平，EPDMS得分为43.0。

Conclusion: GuideFlow通过约束流匹配有效解决了现有规划器的局限性，实现了多样化的轨迹生成和直接约束执行，在多个基准测试中表现出色。

Abstract: Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \textit{\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \textit{\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \textit{\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \textit{\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \textit{\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \textit{\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be released.

</details>


### [159] [Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion](https://arxiv.org/abs/2511.18734)
*Keyang Lu,Sifan Zhou,Hongbin Xu,Gang Xu,Zhifei Yang,Yikai Wang,Zhen Xiao,Jieyi Long,Ming Li*

Main category: cs.CV

TL;DR: Yo'City是一个基于智能体框架的3D城市生成系统，通过分层规划和迭代优化实现用户定制化和无限扩展的城市生成，在语义、几何、纹理和布局等多个维度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D城市生成方法主要依赖单一扩散模型，无法实现个性化定制和无限扩展的城市规模场景生成。

Method: 采用分层"城市-区域-网格"规划策略，结合全局规划器和局部设计器，通过"生成-优化-评估"等距图像合成循环和图像到3D生成，并引入基于场景图的距离和语义感知布局优化机制。

Result: 在构建的多样化基准数据集上，Yo'City在所有评估维度（语义、几何、纹理、布局）上均优于现有最先进方法。

Conclusion: Yo'City框架通过利用大型模型的推理和组合能力，成功实现了用户定制化和无限扩展的3D城市生成，为虚拟现实和数字孪生应用提供了有效的解决方案。

Abstract: Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.

</details>


### [160] [Thinking Ahead: Foresight Intelligence in MLLMs and World Models](https://arxiv.org/abs/2511.18735)
*Zhantao Gong,Liaoyuan Fan,Qing Guo,Xun Xu,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: 本文提出了Foresight Intelligence概念，并创建了FSU-QA数据集来评估和增强视觉语言模型的前瞻推理能力。实验表明当前模型在预测未来事件方面仍有困难，但通过FSU-QA微调可以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了预测和解释未来事件的能力（Foresight Intelligence），这种能力在自动驾驶等应用中至关重要。为了填补这一空白，需要专门的数据集和评估方法。

Method: 引入FSU-QA视觉问答数据集，用于引发和评估前瞻智能；对最先进的视觉语言模型进行前瞻导向任务的全面研究；通过世界模型生成的预测语义一致性来评估世界模型；在FSU-QA上微调小型视觉语言模型。

Result: 当前模型在推理未来情境方面仍有困难；FSU-QA可以有效评估世界模型；小型模型在FSU-QA上微调后，性能大幅超越更大型的先进模型。

Conclusion: FSU-QA为开发能够真正预测和理解未来事件的下一代模型提供了原则性基础，证明了专门数据集在增强前瞻推理能力方面的有效性。

Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.

</details>


### [161] [ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion](https://arxiv.org/abs/2511.18742)
*Zhenghan Fang,Jian Zheng,Qiaozi Gao,Xiaofeng Gao,Jeremias Sulam*

Main category: cs.CV

TL;DR: 提出了一种基于反向离散化的文本到图像扩散模型ProxT2I，使用学习的条件近端算子替代分数函数，结合强化学习优化采样器，并构建了大规模人脸图像数据集LAION-Face-T2I-15M。


<details>
  <summary>Details</summary>
Motivation: 传统基于前向离散化的扩散模型采样效率低且不稳定，需要大量采样步骤才能生成高质量样本。

Method: 开发基于反向离散化的文本到图像扩散模型，使用条件近端算子替代分数函数，结合强化学习优化采样器，构建大规模人脸图像数据集。

Result: 相比基于分数的基线方法，显著提升了采样效率和人类偏好对齐度，在计算资源和模型规模更小的情况下达到与现有最先进开源模型相当的性能。

Conclusion: ProxT2I为人类文本到图像生成提供了一个轻量级但高性能的解决方案，在保持性能的同时降低了计算需求和模型大小。

Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.

</details>


### [162] [Any4D: Open-Prompt 4D Generation from Natural Language and Images](https://arxiv.org/abs/2511.18746)
*Hao Li,Qiao Sun*

Main category: cs.CV

TL;DR: 提出了Primitive Embodied World Models (PEWM)，通过限制视频生成到固定较短时间范围，解决了基于视频生成的具身世界模型对大规模交互数据的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 基于视频生成的具身世界模型依赖大规模具身交互数据，但这类数据稀缺、收集困难且维度高，限制了语言与动作的细粒度对齐，并加剧了长时视频生成的挑战。

Method: 将视频生成限制在固定较短时间范围，配备模块化视觉语言模型规划器和起始-目标热图引导机制，利用视频模型的时空视觉先验和视觉语言模型的语义感知能力。

Result: 实现了语言概念与机器人动作视觉表示的细粒度对齐，降低了学习复杂度，提高了数据收集效率，减少了推理延迟，支持原始级别策略在复杂任务上的组合泛化。

Conclusion: PEWM通过桥接细粒度物理交互与高层推理，为可扩展、可解释和通用目的的具身智能铺平了道路。

Abstract: While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \textit{"GPT moment"} in the embodied domain. There is a naive observation: \textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \textit{2) reduces} learning complexity, \textit{3) improves} data efficiency in embodied data collection, and \textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.

</details>


### [163] [VAOT: Vessel-Aware Optimal Transport for Retinal Fundus Enhancement](https://arxiv.org/abs/2511.18763)
*Xuanzhao Dong,Wenhui Zhu,Yujian Xiong,Xiwen Chen,Hao Wang,Xin Li,Jiajun Cheng,Zhipeng Wang,Shao Tang,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: VAOT是一种基于最优传输的眼底图像增强框架，通过骨架损失和端点感知损失来保持血管结构完整性，在无配对数据设置下减少噪声同时保护血管拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 传统GAN-based无配对增强方法会扭曲临床关键的血管结构，改变血管拓扑和端点完整性，这促使开发能够保持血管结构完整性的增强方法。

Method: 提出Vessel-Aware Optimal Transport (VAOT)框架，结合最优传输目标与两个结构保持正则化器：骨架损失（保持全局血管连通性）和端点感知损失（稳定局部端点）。

Result: 在合成退化基准测试和下游血管与病变分割评估中，该方法优于多个最先进的基线方法。

Conclusion: VAOT框架在无配对设置下有效减少噪声同时保持血管结构，在眼底图像增强方面表现出优越性能。

Abstract: Color fundus photography (CFP) is central to diagnosing and monitoring retinal disease, yet its acquisition variability (e.g., illumination changes) often degrades image quality, which motivates robust enhancement methods. Unpaired enhancement pipelines are typically GAN-based, however, they can distort clinically critical vasculature, altering vessel topology and endpoint integrity. Motivated by these structural alterations, we propose Vessel-Aware Optimal Transport (\textbf{VAOT}), a framework that combines an optimal-transport objective with two structure-preserving regularizers: (i) a skeleton-based loss to maintain global vascular connectivity and (ii) an endpoint-aware loss to stabilize local termini. These constraints guide learning in the unpaired setting, reducing noise while preserving vessel structure. Experimental results on synthetic degradation benchmark and downstream evaluations in vessel and lesion segmentation demonstrate the superiority of the proposed methods against several state-of-the art baselines. The code is available at https://github.com/Retinal-Research/VAOT

</details>


### [164] [NI-Tex: Non-isometric Image-based Garment Texture Generation](https://arxiv.org/abs/2511.18765)
*Hui Shan,Ming Li,Haitao Yang,Kai Zheng,Sizhe Zheng,Yanwei Fu,Xiangru Huang*

Main category: cs.CV

TL;DR: 本文提出了一种非等距图像驱动的3D服装纹理生成方法，通过构建3D服装视频数据集和使用Nano Banana进行高质量图像编辑，解决了现有方法对拓扑一致性和精确网格变形的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有工业3D服装网格已覆盖大多数真实世界服装几何形状，但其纹理多样性仍然有限。传统生成方法需要输入图像与3D网格之间的严格拓扑一致性或准确的网格变形匹配，这严重限制了纹理生成的质量和灵活性。

Method: 构建3D服装视频数据集提供跨不同变形的几何和材质监督；使用Nano Banana进行高质量非等距图像编辑；提出基于不确定性引导视图选择和重新加权的迭代烘焙方法，将多视图预测融合为无缝的PBR纹理。

Result: 通过广泛实验证明，前馈双分支架构能够生成适用于工业级3D服装设计的多样化且空间对齐的PBR材质。

Conclusion: 该方法成功解决了非等距图像驱动的服装纹理生成挑战，实现了可靠的跨拓扑纹理生成，为工业级3D服装设计提供了有效的解决方案。

Abstract: Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited. To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes. However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility. To address the challenging problem of non-isometric image-based garment texture generation, we construct 3D Garment Videos, a physically simulated, garment-centric dataset that provides consistent geometry and material supervision across diverse deformations, enabling robust cross-pose texture learning. We further employ Nano Banana for high-quality non-isometric image editing, achieving reliable cross-topology texture generation between non-isometric image-geometry pairs. Finally, we propose an iterative baking method via uncertainty-guided view selection and reweighting that fuses multi-view predictions into seamless, production-ready PBR textures. Through extensive experiments, we demonstrate that our feedforward dual-branch architecture generates versatile and spatially aligned PBR materials suitable for industry-level 3D garment design.

</details>


### [165] [Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment](https://arxiv.org/abs/2511.18766)
*Xintao Chen,Xiaohao Xu,Bozhong Zheng,Yun Liu,Yingna Wu*

Main category: cs.CV

TL;DR: VSAD是一个新颖的多视角异常检测框架，通过显式建模跨视角的几何一致性来学习视角不变表示，解决了现有方法在多视角图像中误报率高的问题。


<details>
  <summary>Details</summary>
Motivation: 解决多视角图像中真实缺陷与由视角变化引起的良性外观变化难以区分的问题，现有单视角方法处理多视角图像时特征表示不一致且误报率高。

Method: 提出ViewSense-AD框架，包含多视角对齐模块(MVAM)利用单应性投影对齐相邻视角的特征区域，集成到View-Align潜在扩散模型(VALDM)中实现渐进式多阶段对齐，并使用轻量级融合精炼模块(FRM)增强全局一致性。

Result: 在RealIAD和MANTA数据集上的广泛实验表明，VSAD在像素、视角和样本级别的视觉异常检测中均达到新的最先进水平，显著优于现有方法。

Conclusion: VSAD通过几何一致性建模和多阶段对齐策略，能够在大视角偏移和复杂纹理情况下实现鲁棒的异常检测，证明了其有效性。

Abstract: Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.

</details>


### [166] [Rethinking Garment Conditioning in Diffusion-based Virtual Try-On](https://arxiv.org/abs/2511.18775)
*Kihyun Na,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 本文提出了Re-CatVTON，一种高效的虚拟试穿模型，通过改进的单UNet架构实现了高性能，同时减少了计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的虚拟试穿模型虽然性能优越，但双UNet架构带来了显著的计算和内存开销，需要开发更高效的模型。

Method: 通过可视化分析和理论分析提出三个关于上下文特征学习的假设，开发了Re-CatVTON单UNet模型，采用改进的Classifier-Free Guidance策略和直接注入真实服装潜在特征的方法。

Result: Re-CatVTON相比前身CatVTON显著提升性能，在FID、KID和LPIPS指标上表现更好，仅SSIM略有下降，计算和内存需求低于高性能双UNet模型Leffa。

Conclusion: Re-CatVTON为单UNet虚拟试穿模型建立了新的效率-性能平衡点，在保持高质量的同时显著降低了计算成本。

Abstract: Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.

</details>


### [167] [ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection](https://arxiv.org/abs/2511.18780)
*Ruize Ma,Minghong Cai,Yilei Jiang,Jiaming Han,Yi Feng,Yingshui Tan,Xiaoyong Zhu,Bo Zhang,Bo Zheng,Xiangyu Yue*

Main category: cs.CV

TL;DR: ConceptGuard是一个统一的安全防护框架，用于主动检测和缓解多模态视频生成中的不安全语义，通过对比检测和语义抑制两阶段方法，在ConceptRisk和T2VSafetyBench-TI2V基准测试中取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: 多模态视频生成系统虽然增强了可控性，但也引入了新的安全风险，现有安全方法多为纯文本、需要预知风险类别或作为后生成审计器，难以主动缓解这种组合性多模态风险。

Method: ConceptGuard采用两阶段方法：对比检测模块将融合的图像-文本输入投影到结构化概念空间识别潜在安全风险；语义抑制机制通过干预提示的多模态条件来引导生成过程远离不安全概念。

Result: 在两个新基准上的综合实验表明，ConceptGuard在风险检测和安全视频生成方面始终优于现有基线方法，实现了最先进的结果。

Conclusion: ConceptGuard提供了一个有效的统一框架来主动应对多模态视频生成中的安全挑战，通过结构化概念空间和语义抑制机制成功缓解了组合性多模态风险。

Abstract: Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.

</details>


### [168] [STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution](https://arxiv.org/abs/2511.18786)
*Junyang Chen,Jiangxin Dong,Long Sun,Yixin Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: STCDiT是一个基于预训练视频扩散模型的视频超分辨率框架，通过运动感知VAE重建和锚帧引导方法，在复杂相机运动下恢复结构保真和时间稳定的视频。


<details>
  <summary>Details</summary>
Motivation: 解决视频超分辨率中保持时间稳定性和结构保真度的挑战，特别是在复杂相机运动场景下。

Method: 1. 运动感知VAE重建：按运动特征分段重建视频；2. 锚帧引导：利用第一帧（锚帧）的丰富空间结构信息来约束生成过程。

Result: 在结构保真度和时间一致性方面优于现有最先进方法。

Conclusion: 结合运动感知重建和锚帧引导的视频扩散模型能够实现高质量的视频超分辨率。

Abstract: We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.

</details>


### [169] [Understanding Task Transfer in Vision-Language Models](https://arxiv.org/abs/2511.18787)
*Bhuvan Sachdeva,Karan Uppal,Abhinav Java,Vineeth N. Balasubramanian*

Main category: cs.CV

TL;DR: 本文系统研究了视觉语言模型（VLMs）在感知任务间的迁移性，提出了Perfection Gap Factor（PGF）指标来衡量任务迁移的广度和幅度，通过实验揭示了感知任务间的正负迁移模式。


<details>
  <summary>Details</summary>
Motivation: VLMs在多模态基准测试中表现良好，但在深度估计、物体计数等视觉感知任务上落后于人类和专用模型。针对单一任务的微调可能对其他任务产生不可预测的影响，这使得任务特定微调具有挑战性。

Method: 通过系统研究任务迁移性，考察在某一感知任务上微调VLM对其在其他任务上零样本性能的影响。引入PGF指标量化迁移效果，使用三个开源VLM在13个感知任务上进行评估，构建任务迁移图。

Result: 揭示了感知任务间先前未观察到的关系，发现了正负迁移模式，识别了相互影响的任务组，根据迁移行为将任务组织为不同的角色，并展示了PGF如何指导数据选择以提高训练效率。

Conclusion: 这些发现既突出了正迁移的机会，也揭示了负干扰的风险，为推进VLMs的发展提供了可操作的指导。

Abstract: Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.

</details>


### [170] [StereoDETR: Stereo-based Transformer for 3D Object Detection](https://arxiv.org/abs/2511.18788)
*Shiyi Mu,Zichong Gu,Zhiqi Ai,Anqi Liu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: StereoDETR是一个基于DETR的高效立体3D目标检测框架，通过单目DETR分支和立体分支结合，实现实时推理并超越单目方法的速度，在KITTI基准测试中达到竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 立体3D检测方法相比单目方法精度更高，但计算开销和延迟较大。现有最优立体方法精度是单目方法的两倍，但推理速度只有单目方法的一半。

Method: StereoDETR包含两个分支：单目DETR分支（基于2D DETR，增加预测物体尺度、朝向和采样点的通道）和立体分支（利用低成本多尺度视差特征预测物体级深度图）。两个分支通过可微深度采样策略耦合，并引入约束监督策略处理遮挡问题。

Result: StereoDETR实现实时推理，是首个在速度上超越单目方法的立体方法。在KITTI基准测试中达到竞争性精度，在行人和骑行者子集上创下新的最优结果。

Conclusion: StereoDETR成功解决了立体3D检测的高计算开销问题，实现了速度与精度的平衡，为实时立体3D目标检测提供了有效解决方案。

Abstract: Compared to monocular 3D object detection, stereo-based 3D methods offer significantly higher accuracy but still suffer from high computational overhead and latency. The state-of-the-art stereo 3D detection method achieves twice the accuracy of monocular approaches, yet its inference speed is only half as fast. In this paper, we propose StereoDETR, an efficient stereo 3D object detection framework based on DETR. StereoDETR consists of two branches: a monocular DETR branch and a stereo branch. The DETR branch is built upon 2D DETR with additional channels for predicting object scale, orientation, and sampling points. The stereo branch leverages low-cost multi-scale disparity features to predict object-level depth maps. These two branches are coupled solely through a differentiable depth sampling strategy. To handle occlusion, we introduce a constrained supervision strategy for sampling points without requiring extra annotations. StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It also achieves competitive accuracy on the public KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets. The code is available at https://github.com/shiyi-mu/StereoDETR-OPEN.

</details>


### [171] [Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing](https://arxiv.org/abs/2511.18792)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Chun Tung Chou,Wen Hu*

Main category: cs.CV

TL;DR: 该论文通过大规模MAE预训练方法，在14个数据集、130万样本上验证了Wi-Fi CSI感知的跨域泛化能力，发现数据规模和多样性是提升泛化性能的关键，而模型容量在当前数据量下增益有限。


<details>
  <summary>Details</summary>
Motivation: 解决Wi-Fi感知在实际应用中因环境、硬件和用户变化导致的跨域泛化问题，传统方法在有限数据集上训练难以适应新场景。

Method: 采用基础模型方法，使用掩码自编码器(MAE)在14个异构Wi-Fi CSI数据集上进行大规模预训练，涵盖4种设备、2.4/5/6 GHz频段和20-160 MHz带宽。

Result: 实验表明：1) 预训练数据量增加带来对数线性性能提升；2) 当前数据量下大模型仅提供边际增益；3) 在人类活动识别、手势识别和用户识别任务上，跨域准确率比监督学习基线提升2.2%-15.7%。

Conclusion: 数据而非模型容量是Wi-Fi感知泛化的当前瓶颈，大规模预训练为设计实际可部署的鲁棒Wi-Fi感知系统提供了方向。

Abstract: While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical "domain shift" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.

</details>


### [172] [PartDiffuser: Part-wise 3D Mesh Generation via Discrete Diffusion](https://arxiv.org/abs/2511.18801)
*Yichen Yang,Hong Li,Haodong Zhu,Linin Yang,Guojun Lei,Sheng Xu,Baochang Zhang*

Main category: cs.CV

TL;DR: PartDiffuser是一种半自回归扩散框架，用于点云到网格生成，通过分部分处理平衡全局结构一致性和局部细节，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自回归方法在生成艺术家设计的网格时难以平衡全局结构一致性与高保真局部细节，且容易产生误差累积。

Method: 首先对网格进行语义分割，然后采用"分部分"方式：在部分之间使用自回归确保全局拓扑，在每个语义部分内使用并行离散扩散过程精确重建高频几何特征。基于DiT架构，引入部分感知交叉注意力机制，使用点云作为分层几何条件动态控制生成过程。

Result: 实验表明该方法在生成具有丰富细节的3D网格方面显著优于现有最先进模型，展现出适用于实际应用的卓越细节表示能力。

Conclusion: PartDiffuser通过有效解耦全局和局部生成任务，成功解决了网格生成中全局一致性与局部细节的平衡问题。

Abstract: Existing autoregressive (AR) methods for generating artist-designed meshes struggle to balance global structural consistency with high-fidelity local details, and are susceptible to error accumulation. To address this, we propose PartDiffuser, a novel semi-autoregressive diffusion framework for point-cloud-to-mesh generation. The method first performs semantic segmentation on the mesh and then operates in a "part-wise" manner: it employs autoregression between parts to ensure global topology, while utilizing a parallel discrete diffusion process within each semantic part to precisely reconstruct high-frequency geometric features. PartDiffuser is based on the DiT architecture and introduces a part-aware cross-attention mechanism, using point clouds as hierarchical geometric conditioning to dynamically control the generation process, thereby effectively decoupling the global and local generation tasks. Experiments demonstrate that this method significantly outperforms state-of-the-art (SOTA) models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.

</details>


### [173] [TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging](https://arxiv.org/abs/2511.18806)
*Qinglei Cao,Ziyao Tang,Xiaoqin Tang*

Main category: cs.CV

TL;DR: 提出了一种新颖的3D CT重建框架，利用目标先验增强隐式学习，在超稀疏视图场景下显著提升重建精度和学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF的隐式3D重建方法往往忽略物体解剖先验的重要性，限制了重建精度和学习效率，特别是在超稀疏视图场景下。

Method: 采用目标先验指导体素采样和丰富结构编码，结合位置和结构编码实现体素级隐式重建，并引入CUDA算法从稀疏投影快速估计高质量3D目标先验。

Result: 在复杂腹部数据集上，学习效率比当前领先模型NAF提升10倍，重建质量超过最准确模型NeRP，在10、20、30个投影下PSNR分别提升3.57 dB、5.42 dB和5.70 dB。

Conclusion: 提出的目标先验引导隐式学习框架有效解决了稀疏视图CT重建的挑战，显著提升了重建质量和学习效率。

Abstract: X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.

</details>


### [174] [Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache](https://arxiv.org/abs/2511.18811)
*Yuqiu Jiang,Xiaozhen Qiao,Tianyu Mei,Haojian Huang,Yifan Chen,Ye Zheng,Zhe Sun*

Main category: cs.CV

TL;DR: 提出自适应多样性缓存（ADC）模块，一种无需训练即插即用的机制，用于缓解HOI检测中的长尾偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的HOI检测方法依赖额外训练或提示调优，计算开销大且可扩展性有限，特别是在长尾场景中罕见交互严重不足。

Method: ADC构建类别特定缓存，在推理过程中积累高置信度和多样化的特征表示，包含频率感知缓存适配机制，偏向罕见类别。

Result: 在HICO-DET和V-COCO数据集上的实验显示，ADC持续改进现有HOI检测器，在罕见类别上获得最高+8.57% mAP提升，完整数据集上+4.39%。

Conclusion: ADC有效缓解长尾偏差，同时保持整体性能，无需额外训练或微调。

Abstract: Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\% mAP gain on rare categories and +4.39\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.

</details>


### [175] [DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video](https://arxiv.org/abs/2511.18814)
*Jiawei Hou,Shenghao Zhang,Can Wang,Zheng Gu,Yonggen Ling,Taiping Zeng,Xiangyang Xue,Jingbo Zhang*

Main category: cs.CV

TL;DR: 本文提出了DetAny4D，一个开放集端到端4D物体检测框架，通过融合多模态特征和几何感知时空解码器，直接从序列输入预测3D边界框，显著提升了检测精度和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有4D物体检测方法存在时间一致性建模不足或复杂多阶段流程导致错误传播的问题，且缺乏大规模连续可靠3D边界框标注数据集。

Method: 首先构建DA4D大规模4D检测数据集，然后提出DetAny4D框架，融合预训练基础模型的多模态特征，设计几何感知时空解码器捕获时空动态，采用多任务学习架构和专门训练策略保持序列一致性。

Result: 实验表明DetAny4D在检测精度上具有竞争力，并显著改善时间稳定性，有效解决了4D物体检测中长期存在的抖动和不一致问题。

Conclusion: DetAny4D为4D物体检测提供了一个有效的端到端解决方案，通过大规模数据集和创新的框架设计，在精度和稳定性方面都取得了显著进展。

Abstract: Reliable 4D object detection, which refers to 3D object detection in streaming video, is crucial for perceiving and understanding the real world. Existing open-set 4D object detection methods typically make predictions on a frame-by-frame basis without modeling temporal consistency, or rely on complex multi-stage pipelines that are prone to error propagation across cascaded stages. Progress in this area has been hindered by the lack of large-scale datasets that capture continuous reliable 3D bounding box (b-box) annotations. To overcome these challenges, we first introduce DA4D, a large-scale 4D detection dataset containing over 280k sequences with high-quality b-box annotations collected under diverse conditions. Building on DA4D, we propose DetAny4D, an open-set end-to-end framework that predicts 3D b-boxes directly from sequential inputs. DetAny4D fuses multi-modal features from pre-trained foundational models and designs a geometry-aware spatiotemporal decoder to effectively capture both spatial and temporal dynamics. Furthermore, it adopts a multi-task learning architecture coupled with a dedicated training strategy to maintain global consistency across sequences of varying lengths. Extensive experiments show that DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing long-standing issues of jitter and inconsistency in 4D object detection. Data and code will be released upon acceptance.

</details>


### [176] [SupLID: Geometrical Guidance for Out-of-Distribution Detection in Semantic Segmentation](https://arxiv.org/abs/2511.18816)
*Nimeshika Udayangani,Sarah Erfani,Christopher Leckie*

Main category: cs.CV

TL;DR: SupLID是一个新颖的语义分割OOD检测框架，通过利用语义空间的几何结构（特别是线性内在维度LID）来指导基于分类器的OOD评分，解决了现有方法对过度自信的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于分类器置信度（如能量或熵）的像素级OOD检测方法存在局限性，特别是容易受到过度自信的影响。需要利用语义空间的几何结构来增强OOD检测能力。

Method: SupLID构建几何核心集来捕捉ID子空间的内在结构，在超像素级别计算OOD分数，实现高效实时推理和改善空间平滑性。作为后处理评分方法，可与任何语义分割分类器无缝集成。

Result: SupLID显著增强了现有基于分类器的OOD评分，在AUR、FPR和AUP等关键评估指标上达到了最先进的性能。

Conclusion: SupLID通过几何线索与传统分类器置信度的互补，有效提升了模型检测多样化OOD场景的能力，为语义分割中的OOD检测提供了新的解决方案。

Abstract: Out-of-Distribution (OOD) detection in semantic segmentation aims to localize anomalous regions at the pixel level, advancing beyond traditional image-level OOD techniques to better suit real-world applications such as autonomous driving. Recent literature has successfully explored the adaptation of commonly used image-level OOD methods--primarily based on classifier-derived confidence scores (e.g., energy or entropy)--for this pixel-precise task. However, these methods inherit a set of limitations, including vulnerability to overconfidence. In this work, we introduce SupLID, a novel framework that effectively guides classifier-derived OOD scores by exploiting the geometrical structure of the underlying semantic space, particularly using Linear Intrinsic Dimensionality (LID). While LID effectively characterizes the local structure of high-dimensional data by analyzing distance distributions, its direct application at the pixel level remains challenging. To overcome this, SupLID constructs a geometrical coreset that captures the intrinsic structure of the in-distribution (ID) subspace. It then computes OOD scores at the superpixel level, enabling both efficient real-time inference and improved spatial smoothness. We demonstrate that geometrical cues derived from SupLID serve as a complementary signal to traditional classifier confidence, enhancing the model's ability to detect diverse OOD scenarios. Designed as a post-hoc scoring method, SupLID can be seamlessly integrated with any semantic segmentation classifier at deployment time. Our results demonstrate that SupLID significantly enhances existing classifier-based OOD scores, achieving state-of-the-art performance across key evaluation metrics, including AUR, FPR, and AUP. Code is available at https://github.com/hdnugit/SupLID.

</details>


### [177] [Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring](https://arxiv.org/abs/2511.18817)
*Siyuan Wei,Chunjie Wang,Xiao Liu,Xiaosheng Yan,Zhishan Zhou,Rui Huang*

Main category: cs.CV

TL;DR: 本文提出了一个完全自动化的流水线，将原始3D扫描转换为无歧义的高质量对话数据，解决了3D多模态大语言模型数据稀缺问题。该流水线通过结合规则约束与2D MLLMs和LLMs，以低成本生成可控、可扩展的数据，最终产生包含200多万样本的Disc3D数据集。


<details>
  <summary>Details</summary>
Motivation: 3D多模态大语言模型落后于2D同行，主要因为缺乏大规模、高质量的3D场景对话数据集。现有方法依赖昂贵的人工标注，且存在视角歧义和对象指代歧义两个关键问题。

Method: 提出四阶段自动化流水线：(1)元标注收集，获取对象、帧和场景级描述；(2)带关系校正的场景图构建，捕捉邻近对象关系；(3)判别性对象指代，生成排他性和紧凑描述；(4)多任务数据生成，合成多样化对话。

Result: 流水线系统性地缓解了源数据集的内在缺陷，产生了包含200多万样本的Disc3D数据集，涵盖场景、视图和对象描述、视觉定位以及五个对象中心问答任务。实验显示使用Disc3D训练在公共基准和Disc3D-QA任务上均取得显著改进。

Conclusion: 该自动化流水线能够以低成本生成高质量3D对话数据，有效解决了3D MLLMs的数据瓶颈问题，为3D多模态理解提供了有力支持。

Abstract: 3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.

</details>


### [178] [DiP: Taming Diffusion Models in Pixel Space](https://arxiv.org/abs/2511.18822)
*Zhennan Chen,Junwei Zhu,Xu Chen,Jiangning Zhang,Xiaobin Hu,Hanzhen Zhao,Chengjie Wang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DiP是一种高效的像素空间扩散框架，通过将生成过程解耦为全局和局部两个阶段来解决扩散模型在生成质量与计算效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在生成质量与计算效率之间的基本权衡问题，避免潜在扩散模型的信息损失和非端到端训练问题，同时克服现有像素空间模型在高分辨率合成中的计算瓶颈。

Method: DiP将生成过程解耦为全局和局部两个阶段：使用扩散变换器（DiT）主干在大块上操作以高效构建全局结构，同时通过协同训练的轻量级补丁细节器头利用上下文特征恢复细粒度局部细节。

Result: DiP实现了与潜在扩散模型相当的计算效率，无需依赖VAE，推理速度比先前方法快10倍，仅增加0.3%的总参数数量，在ImageNet 256×256上达到1.90 FID分数。

Conclusion: DiP通过全局-局部解耦的协同设计，成功解决了扩散模型在质量与效率之间的权衡问题，提供了一种高效且无需VAE的像素空间扩散框架。

Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\times$256.

</details>


### [179] [VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models](https://arxiv.org/abs/2511.18823)
*Fufangchen Zhao,Liao Zhang,Daiqi Shi,Yuanjun Gao,Chen Ye,Yang Cai,Jian Gao,Danfeng Yan*

Main category: cs.CV

TL;DR: VideoPerceiver是一个新颖的视频多模态大语言模型，通过两阶段训练框架增强视频理解中的细粒度感知能力，专门解决现有模型在短片段中推理短暂动作和长视频中罕见瞬态事件的能力限制。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态大语言模型在细粒度感知方面存在局限，难以准确理解短片段中的短暂动作和长视频中的罕见瞬态事件，需要开发能够更好捕捉精细运动线索的模型。

Method: 采用两阶段训练框架：1）监督微调阶段构建"关键信息缺失"视频，通过替换关键帧并引入辅助对比损失来增强对细粒度运动线索的敏感性；2）强化学习阶段使用相对奖励机制，确保完整视频生成的描述优于降级输入，显式训练模型恢复时间精确的动作细节。

Result: 实验表明VideoPerceiver在细粒度动作理解和罕见事件描述基准上显著优于最先进的视频多模态大语言模型，同时在标准任务上保持强大性能。

Conclusion: 通过优先处理任务相关的视觉特征，VideoPerceiver重新定义了视频语言模型的细粒度感知训练方法，为视频理解提供了更精确的细粒度分析能力。

Abstract: We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct "key-information-missing" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.

</details>


### [180] [Assessing the alignment between infants' visual and linguistic experience using multimodal language models](https://arxiv.org/abs/2511.18824)
*Alvin Wei Ming Tan,Jane Yang,Tarun Sepuri,Khai Loong Aw,Robert Z. Sparks,Zi Yin,Virginia A. Marchman,Michael C. Frank,Bria Long*

Main category: cs.CV

TL;DR: 本研究使用CLIP模型自动分析婴儿视角视频中的视觉-语言对齐情况，发现理想化的学习对齐时刻在儿童日常经验中相对罕见，远少于现代机器学习数据集中的对齐频率。


<details>
  <summary>Details</summary>
Motivation: 理解儿童如何学习语言需要了解其视觉和语言经验的时序对齐程度，但传统人工标注方法效率低下，需要自动化分析工具。

Method: 使用对比语言-图像预训练(CLIP)模型自动评估婴儿视角视频中的视觉-语言对齐情况，并通过人工判断验证CLIP对齐分数的有效性。

Result: 验证了CLIP对齐分数的可靠性，并发现儿童日常经验中理想对齐时刻（如"看球"时球确实在视野中）相对稀少，且存在个体内和个体间的变异性。

Conclusion: 稀疏的对齐是早期词汇学习模型的一个约束条件，本研究为调查儿童多模态环境提供了新方法。

Abstract: Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., "look at the ball" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.

</details>


### [181] [Q-Save: Towards Scoring and Attribution for Generated Video Evaluation](https://arxiv.org/abs/2511.18825)
*Xiele Wu,Zicheng Zhang,Mingtao Chen,Yixian Liu,Yiming Liu,Shushi Wang,Zhichao Hu,Yuhong Liu,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: Q-Save是一个用于AI生成视频质量评估的新基准数据集和模型，包含近10000个视频，提供MOS评分和三个维度的细粒度标注，支持可解释的质量评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够同时进行准确质量评估和提供可解释推理的AI生成视频评估方法，需要建立更全面和可解释的评估体系。

Method: 采用SlowFast框架区分快慢帧处理，使用多阶段训练策略：先进行监督微调，然后使用分组相对策略优化增强，最后再次微调以提高稳定性。

Result: 实验结果表明该模型在视频质量预测方面达到最先进性能，同时提供与人类对齐的可解释性说明。

Conclusion: Q-Save为生成视频研究中的可解释评估建立了坚实基础，有助于多模态生成和可信AI的发展。

Abstract: We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.

</details>


### [182] [FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories](https://arxiv.org/abs/2511.18834)
*Lei Ke,Hubery Yin,Gongye Liu,Zhengyao Lv,Jingcai Guo,Chen Li,Wenhan Luo,Yujiu Yang,Jing Lyu*

Main category: cs.CV

TL;DR: FlowSteer方法解决了ReFlow框架在流匹配中的采样效率问题，通过在线轨迹对齐和对抗蒸馏目标，提升了学生模型对教师生成轨迹的跟随能力，并在SD3上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 流匹配在视觉生成中取得成功，但采样效率仍是实际应用的关键瓶颈。ReFlow方法虽然与流匹配理论一致，但在实际场景中性能不如一致性蒸馏和分数蒸馏，因此需要改进。

Method: 提出了FlowSteer方法：1）通过在线轨迹对齐(OTA)解决分段ReFlow训练中的分布不匹配问题；2）在ODE轨迹上应用对抗蒸馏目标，增强学生对教师生成轨迹的跟随；3）修复了FlowMatchEulerDiscreteScheduler中影响少步推理质量的缺陷。

Result: 在SD3上的实验结果表明，该方法有效提升了ReFlow蒸馏的性能。

Conclusion: FlowSteer方法成功释放了基于ReFlow的蒸馏潜力，通过引导学生沿着教师的真实生成轨迹，显著提升了采样效率。

Abstract: With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.

</details>


### [183] [Enhancing Multi-Label Thoracic Disease Diagnosis with Deep Ensemble-Based Uncertainty Quantification](https://arxiv.org/abs/2511.18839)
*Yasiru Laksara,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 该研究通过集成深度集成方法解决了CheXNet等深度学习模型在临床应用中缺乏可靠置信度的问题，在NIH ChestX-ray14数据集上实现了优异的校准性能和不确定性分解。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在临床高风险环境中的应用受到其确定性性质的限制，无法提供可靠的预测置信度度量，这阻碍了其在临床决策支持系统中的可信应用。

Method: 从蒙特卡洛Dropout方法转向高多样性的9成员深度集成方法，以稳定性能并改善校准。

Result: 深度集成方法实现了最先进的平均AUROC 0.8559和平均F1分数0.3857，显著改善了校准性能（平均ECE 0.0728，NLL 0.1916），并能可靠地将总不确定性分解为偶然不确定性和认知不确定性。

Conclusion: 深度集成方法将模型从概率工具转变为可靠的临床决策支持系统，建立了可信且可解释的诊断平台。

Abstract: The utility of deep learning models, such as CheXNet, in high stakes clinical settings is fundamentally constrained by their purely deterministic nature, failing to provide reliable measures of predictive confidence. This project addresses this critical gap by integrating robust Uncertainty Quantification (UQ) into a high performance diagnostic platform for 14 common thoracic diseases on the NIH ChestX-ray14 dataset. Initial architectural development failed to stabilize performance and calibration using Monte Carlo Dropout (MCD), yielding an unacceptable Expected Calibration Error (ECE) of 0.7588. This technical failure necessitated a rigorous architectural pivot to a high diversity, 9-member Deep Ensemble (DE). This resulting DE successfully stabilized performance and delivered superior reliability, achieving a State-of-the-Art (SOTA) average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.8559 and an average F1 Score of 0.3857. Crucially, the DE demonstrated superior calibration (Mean ECE of 0.0728 and Negative Log-Likelihood (NLL) of 0.1916) and enabled the reliable decomposition of total uncertainty into its Aleatoric (irreducible data noise) and Epistemic (reducible model knowledge) components, with a mean Epistemic Uncertainty (EU) of 0.0240. These results establish the Deep Ensemble as a trustworthy and explainable platform, transforming the model from a probabilistic tool into a reliable clinical decision support system.

</details>


### [184] [Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration](https://arxiv.org/abs/2511.18847)
*Ishmam Tashdeed,Md. Atiqur Rahman,Sabrina Islam,Md. Azam Hossain*

Main category: cs.CV

TL;DR: FedOAP是一种新颖的个性化联邦学习方法，通过解耦交叉注意力和扰动边界损失来解决多器官肿瘤分割中的数据异构性问题，在保持客户端数据机密性的同时利用共享特征提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习方法大多忽视了利用客户端间共享特征的潜在好处，特别是在每个客户端包含不同器官分割数据的情况下。

Method: 提出FedOAP方法，包含解耦交叉注意力（DCA）机制和扰动边界损失（PBL）。DCA让每个客户端保留本地查询，同时关注来自所有客户端的全局共享键值对；PBL关注预测掩码边界的不一致性，迫使模型更精确地定位边缘。

Result: 在跨越不同器官的多样化肿瘤分割任务上评估FedOAP，大量实验表明FedOAP始终优于现有的最先进联邦和个性化分割方法。

Conclusion: FedOAP通过建模客户端间共享特征的远程依赖关系和改进分割一致性，在个性化联邦学习框架下有效提升了多器官肿瘤分割性能。

Abstract: Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.

</details>


### [185] [Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization](https://arxiv.org/abs/2511.18851)
*Yilin Wen,Kechuan Dong,Yusuke Sugano*

Main category: cs.CV

TL;DR: 本文提出了一种基于运动离散化的在线测试时适应方法，通过无监督聚类获得锚点运动，结合软重置机制来缓解3D人体姿态估计中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 在线测试时适应在3D人体姿态估计中面临误差累积的挑战，当依赖不完美预测进行自监督时，会导致性能随时间下降。

Method: 使用潜在运动表示空间的无监督聚类获得锚点运动，利用其规律性监督姿态估计器并实现高效自回放；引入软重置机制，在连续适应过程中将姿态估计器恢复到其指数移动平均值。

Result: 实验表明，该方法在长时间在线适应中优于之前的在线测试时适应方法，能够有效利用个人形状和运动特征提高准确性。

Conclusion: 通过缓解误差累积，该方法能够稳健地利用个人特征进行增强的3D人体姿态估计。

Abstract: Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.

</details>


### [186] [Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos](https://arxiv.org/abs/2511.18856)
*Sana Alamgeer*

Main category: cs.CV

TL;DR: 设计一个混合显著性模型来预测360度视频中的兴趣区域，以优化视频流传输和提升观看体验。


<details>
  <summary>Details</summary>
Motivation: 兴趣区域在360度视频流中至关重要，可用于预测视口、智能裁剪视频以节省带宽，减少头戴设备观看时的头部移动，提高流媒体效率和观看质量。

Method: 预处理视频获取帧，开发混合显著性模型预测兴趣区域，后处理模型输出得到每帧的兴趣区域。

Result: 将所提方法与360RAT数据集的主观标注进行性能比较。

Conclusion: 通过混合显著性模型有效识别360度视频中的兴趣区域，为视频流优化提供技术支持。

Abstract: The main goal of the project is to design a new model that predicts regions of interest in 360$^{\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.

</details>


### [187] [Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling](https://arxiv.org/abs/2511.18858)
*Xiao Cui,Yulei Qin,Xinyue Li,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种针对长尾数据集蒸馏的新方法，通过统计对齐视角解决模型偏差和统计估计问题，显著提升了在类别不平衡数据集上的蒸馏性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法在平衡数据集上表现良好，但在长尾分布下表现不佳，因为类别不平衡会导致模型表示偏差和批归一化统计估计失真。

Method: 提出三个核心组件：增强专家模型用于可靠统计估计和软标签生成；通过动态调整动量的全前向传播重新校准BN统计以减少表示偏差；通过多轮机制增量选择高置信度和多样化的增强来初始化合成图像。

Result: 在四个长尾基准测试上均优于现有方法，在CIFAR-100-LT和Tiny-ImageNet-LT上分别提升15.6%和11.8%的top-1准确率（IPC=10，IF=10）。

Conclusion: 该方法通过统计对齐有效解决了长尾数据集蒸馏中的模型偏差和统计估计问题，为长尾学习提供了新的解决方案。

Abstract: Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.

</details>


### [188] [DualGazeNet: A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection](https://arxiv.org/abs/2511.18865)
*Yu Zhang,Haoan Ping,Yuchen Li,Zhenshan Bing,Fuchun Sun,Alois Knoll*

Main category: cs.CV

TL;DR: DualGazeNet是一个受生物视觉启发的纯Transformer框架，通过模拟人类视觉系统的双通路处理机制，在显著目标检测任务中实现了高性能、高效率和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测方法架构复杂，引入特征冗余和跨组件干扰，而人类视觉系统能够高效识别显著目标。研究旨在设计一个生物启发但架构简单的框架，在保持高性能的同时提升计算效率和可解释性。

Method: 提出DualGazeNet，基于纯Transformer架构，模拟人类视觉系统的稳健表示学习和双通路处理（大细胞-小细胞通路）原理，结合皮层注意力调制机制。

Result: 在5个RGB显著目标检测基准测试中，DualGazeNet持续超越25个最先进的CNN和Transformer方法，推理速度比类似容量的Transformer基线快约60%，FLOPs减少53.4%，并在伪装和水下显著目标检测任务中表现出强大的跨域泛化能力。

Conclusion: DualGazeNet证明了通过模拟生物视觉原理可以设计出架构简单但性能优越的显著目标检测框架，在准确性、计算效率和泛化能力方面均达到领先水平。

Abstract: Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60\% higher inference speed and 53.4\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.

</details>


### [189] [Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction](https://arxiv.org/abs/2511.18873)
*Yiming Wang,Shaofei Wang,Marko Mihajlovic,Siyu Tang*

Main category: cs.CV

TL;DR: 本文提出了Neural Texture Splatting (NTS)方法，通过全局神经场为每个高斯基元预测局部外观和几何场，显著提升了3D高斯泼溅在多种重建任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法使用3D高斯核建模局部变化，表达能力有限。虽然已有工作尝试通过每个基元的纹理增强表达能力，但这些方法主要针对密集新视角合成，在更一般的重建场景中效果有限。

Method: 引入Neural Texture Splatting (NTS)，核心是使用三平面和神经解码器的混合表示构建全局神经场，为每个基元预测局部外观和几何场。通过共享全局表示建模跨基元的局部纹理场，减少模型大小并促进全局信息交换。

Result: 大量实验表明，Neural Texture Splatting在多个基准测试中持续改进模型性能，并取得了最先进的结果。

Conclusion: NTS通过神经建模局部纹理场，引入了表达性的视角和时间依赖效应，在稀疏和密集输入设置下，在多种重建任务中实现了对现有3DGS变体的显著性能提升。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.

</details>


### [190] [Facade Segmentation for Solar Photovoltaic Suitability](https://arxiv.org/abs/2511.18882)
*Ayca Duran,Christoph Waibel,Bernd Bickel,Iro Armeni,Arno Schlueter*

Main category: cs.CV

TL;DR: 本文提出了一种自动化管道，用于识别建筑立面光伏应用合适表面并估算太阳能潜力，通过微调SegFormer-B5模型，考虑模块尺寸和间隙，将语义预测转换为光伏适用性掩码和面板布局。


<details>
  <summary>Details</summary>
Motivation: 建筑一体化光伏立面是实现城市脱碳的有前景途径，特别是当屋顶面积不足且地面安装不可行时。目前基于机器学习的屋顶光伏规划方法已有研究，但立面自动化方法仍然稀缺且过于简化。

Method: 提出一个管道，集成立面建筑组成的详细信息，自动识别光伏应用合适表面并估算太阳能潜力。在CMP Facades数据集上微调SegFormer-B5，将语义预测转换为考虑模块尺寸和间隙的立面级光伏适用性掩码和光伏面板布局。

Result: 应用于来自10个城市的373个已知尺寸立面数据集，结果显示可安装BIPV潜力显著低于理论潜力，为可靠的城市能源规划提供了宝贵见解。

Conclusion: 随着立面图像的日益可用性，所提出的管道可以扩展到支持全球城市的BIPV规划。

Abstract: Building integrated photovoltaic (BIPV) facades represent a promising pathway towards urban decarbonization, especially where roof areas are insufficient and ground-mounted arrays are infeasible. Although machine learning-based approaches to support photovoltaic (PV) planning on rooftops are well researched, automated approaches for facades still remain scarce and oversimplified. This paper therefore presents a pipeline that integrates detailed information on the architectural composition of the facade to automatically identify suitable surfaces for PV application and estimate the solar energy potential. The pipeline fine-tunes SegFormer-B5 on the CMP Facades dataset and converts semantic predictions into facade-level PV suitability masks and PV panel layouts considering module sizes and clearances. Applied to a dataset of 373 facades with known dimensions from ten cities, the results show that installable BIPV potential is significantly lower than theoretical potential, thus providing valuable insights for reliable urban energy planning. With the growing availability of facade imagery, the proposed pipeline can be scaled to support BIPV planning in cities worldwide.

</details>


### [191] [MagicWorld: Interactive Geometry-driven Video World Exploration](https://arxiv.org/abs/2511.18886)
*Guangyuan Li,Siming Zheng,Shuolin Xu,Jinwei Chen,Bo Li,Xiaobin Hu,Lei Zhao,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: MagicWorld是一个交互式视频世界模型，通过整合3D几何先验和历史检索机制，解决了现有方法在视点变化下的结构不稳定性和多步交互中的历史信息遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有交互式视频世界模型存在两个关键限制：1）未能充分利用指令驱动场景运动与底层3D几何的对应关系，导致视点变化时结构不稳定；2）多步交互中容易遗忘历史信息，导致错误累积和场景语义结构漂移。

Method: 提出MagicWorld模型，包含两个核心模块：1）动作引导3D几何模块（AG3D），从每个交互的第一帧构建点云，为视点转换提供显式几何约束；2）历史缓存检索机制（HCR），在生成过程中检索相关历史帧作为条件信号，帮助模型利用过去场景信息。

Result: 实验结果表明，MagicWorld在交互迭代过程中显著提升了场景稳定性和连续性。

Conclusion: MagicWorld通过整合3D几何先验和历史检索机制，有效解决了交互式视频生成中的结构一致性和历史信息保持问题，为交互式场景演化提供了更稳定和连续的解决方案。

Abstract: Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.

</details>


### [192] [MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting](https://arxiv.org/abs/2511.18894)
*Chenyu Mu,Guihai Chen,Xun Yang,Erkun Yang,Cheng Deng*

Main category: cs.CV

TL;DR: MetaDCSeg是一个用于医学图像分割的鲁棒框架，通过动态学习像素级权重来抑制噪声标注的影响，特别关注模糊边界区域，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割常受噪声标注和模糊解剖边界干扰，导致模型训练不稳定。现有方法依赖全局噪声假设或基于置信度的样本选择，无法有效处理边界区域的噪声问题。

Method: 提出MetaDCSeg框架，通过动态中心距离机制显式建模边界不确定性，利用加权特征距离计算前景、背景和边界中心，引导模型关注模糊边界附近的难分割像素。

Result: 在四个基准数据集上的广泛实验表明，MetaDCSeg在不同噪声水平下始终优于现有最先进方法。

Conclusion: 该框架通过动态像素权重学习和边界不确定性建模，有效提升了医学图像分割在噪声标注环境下的鲁棒性和性能。

Abstract: Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.

</details>


### [193] [Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation](https://arxiv.org/abs/2511.18919)
*Ruiying Liu,Yuanzhi Liang,Haibin Huang,Tianshu Yu,Chi Zhang*

Main category: cs.CV

TL;DR: BPGO是一种改进GRPO框架的新方法，通过引入贝叶斯先验锚点来建模奖励不确定性，在图像和视频生成任务中实现了更好的语义对齐和感知保真度。


<details>
  <summary>Details</summary>
Motivation: 传统GRPO框架的性能受到文本-视觉对应关系模糊性的限制，即单一提示可以描述多种视觉输出，单一图像/视频可以有多种正确解释，这种多对多关系导致奖励模型产生不确定和弱区分性的信号。

Method: BPGO通过语义先验锚点显式建模奖励不确定性，在两层进行自适应优化信任调节：组间贝叶斯信任分配强调与先验一致的组更新，组内先验锚定重归一化通过扩展自信偏差和压缩不确定分数来锐化样本区分。

Result: 在图像和视频生成任务中，BPGO相比标准GRPO及其变体，实现了持续更强的语义对齐、增强的感知保真度和更快的收敛速度。

Conclusion: BPGO通过显式建模奖励不确定性，有效解决了GRPO框架中文本-视觉对应模糊性问题，显著提升了视觉生成模型的性能。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.

</details>


### [194] [EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models](https://arxiv.org/abs/2511.18920)
*Wenhao Xu,Xin Dong,Yue Li,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 提出EventSTU框架，利用事件相机特性实现高效视频理解，通过时间域关键帧采样和空间域token剪枝，大幅降低计算成本同时提升性能


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在处理长视频时推理成本过高，受事件相机启发，希望利用事件触发特性消除冗余信息

Method: 提出训练无关的事件引导框架：时间域采用粗到细关键帧采样算法，空间域设计自适应token剪枝算法，结合问题相关性自适应分配token剪枝预算

Result: 在EventBench基准测试中，相比最强基线实现3.01倍FLOPs减少和3.10倍预填充加速，同时性能仍有提升

Conclusion: EventSTU框架通过事件引导的时空理解，在显著降低计算成本的同时保持甚至提升视频理解性能，支持真实事件相机和模拟事件

Abstract: Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.

</details>


### [195] [BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models](https://arxiv.org/abs/2511.18921)
*Juncheng Li,Yige Li,Hanxun Huang,Yunhao Chen,Xin Wang,Yixu Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: BackdoorVLM是首个针对视觉语言模型（VLMs）的全面后门攻击基准，系统评估了5类多模态后门威胁在图像描述和视觉问答等任务中的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型（特别是视觉语言模型）中的后门攻击威胁尚未得到充分研究，而这类攻击可能严重损害机器学习系统的可靠性和可信度。

Method: 采用统一视角，在核心视觉语言任务中注入和分析后门，将多模态后门威胁组织为5个代表性类别：目标拒绝、恶意注入、越狱、概念替换和感知劫持。使用12种代表性攻击方法，涵盖文本、图像和双模态触发器，在2个开源VLMs和3个多模态数据集上进行测试。

Result: 研究发现VLMs对文本指令表现出强烈敏感性，在双模态后门中文本触发器通常压倒图像触发器。涉及文本模态的后门攻击效果显著，仅需1%的投毒率即可在大多数任务中实现超过90%的成功率。

Conclusion: 当前VLMs存在显著且先前未充分探索的漏洞，BackdoorVLM可作为分析和缓解多模态后门威胁的有用基准。

Abstract: Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\% yielding over 90\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .

</details>


### [196] [One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control](https://arxiv.org/abs/2511.18922)
*Zhenxing Mi,Yuxin Wang,Dan Xu*

Main category: cs.CV

TL;DR: One4D是一个统一的4D生成和重建框架，通过统一掩码条件机制处理不同稀疏度的输入帧，能够在单图像4D生成、完整视频4D重建和稀疏帧混合生成重建之间无缝切换。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不同稀疏度输入帧时缺乏一致性，且联合RGB和点云图生成时传统扩散微调策略容易导致基础视频模型性能退化。

Method: 采用统一掩码条件机制处理不同输入稀疏度，设计解耦LoRA控制，使用两个模态特定的LoRA适配器形成RGB帧和点云图的解耦计算分支，通过轻量级零初始化控制链接学习像素级一致性。

Result: 在合成和真实4D数据集上训练后，One4D在生成和重建任务中均能产生高质量RGB帧和准确点云图。

Conclusion: 这项工作代表了使用视频扩散模型实现通用、高质量基于几何的4D世界建模的重要一步。

Abstract: We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D

</details>


### [197] [AttenDence: Maximizing Attention Confidence for Test Time Adaptation](https://arxiv.org/abs/2511.18925)
*Yash Mali*

Main category: cs.CV

TL;DR: 提出一种基于注意力熵最小化的测试时自适应方法，通过最小化CLS令牌到图像补丁的注意力分布熵来增强模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的测试时自适应方法主要依赖输出分布的熵最小化，但transformer模型提供了额外的无监督学习信号——注意力机制，可以用来提升模型在分布偏移下的适应能力。

Method: 提出最小化CLS令牌到图像补丁的注意力分布熵作为新的测试时自适应目标，鼓励模型在分布偏移下更自信地关注相关图像区域，即使在单个测试图像下也有效。

Result: 该方法在各种损坏类型下都提高了鲁棒性，同时在干净数据上不会降低性能，适用于测试时的单样本图像流。

Conclusion: 注意力熵最小化是一种有效的测试时自适应方法，能够利用transformer的注意力机制来提升模型在分布偏移下的性能。

Abstract: Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.

</details>


### [198] [FineXtrol: Controllable Motion Generation via Fine-Grained Text](https://arxiv.org/abs/2511.18927)
*Keming Shen,Bizhu Wu,Junliang Chen,Xiaoqin Wang,Linlin Shen*

Main category: cs.CV

TL;DR: FineXtrol是一个新颖的运动生成控制框架，通过时间感知、精确、用户友好的细粒度文本控制信号来指导特定身体部位随时间移动，解决了现有方法中细节不对齐和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动运动生成方法存在两个主要问题：使用大语言模型生成详细文本时引入不对齐细节且缺乏明确时间线索；使用全局3D坐标序列作为控制信号时计算成本高。需要一种更高效、精确的控制方法。

Method: 提出FineXtrol控制框架，使用时间感知的细粒度文本控制信号描述特定身体部位移动；设计了分层对比学习模块，鼓励文本编码器为新型控制信号生成更具区分性的嵌入，从而提高运动可控性。

Result: 定量结果显示FineXtrol在可控运动生成方面表现强劲；定性分析证明了其在指导特定身体部位移动方面的灵活性。

Conclusion: FineXtrol框架有效解决了现有方法的局限性，通过细粒度文本控制实现了高效、精确的运动生成，在可控性和灵活性方面都表现出色。

Abstract: Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.

</details>


### [199] [Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search](https://arxiv.org/abs/2511.18929)
*Zijian Song,Xiaoxin Lin,Tao Pu,Zhenlong Yuan,Guangrun Wang,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了人类中心开放未来任务发现（HOTD）问题，旨在在开放未来场景中发现减少人类努力的任务，并开发了HOTD-Bench基准和协作多智能体搜索树（CMAST）框架。


<details>
  <summary>Details</summary>
Motivation: 当前机器人和具身AI主要依赖大型多模态模型，但在开放未来场景中如何发现直接辅助人类的任务仍是一个未充分探索的挑战，特别是当人类意图高度并发和动态变化时。

Method: 提出了协作多智能体搜索树（CMAST）框架，通过多智能体系统分解复杂推理，并使用可扩展的搜索树模块结构化推理过程。

Result: CMAST在HOTD-Bench上取得了最佳性能，显著超越了现有的大型多模态模型，并能与现有LMMs良好集成，持续提升性能。

Conclusion: HOTD问题对于推进LMMs在开放未来场景中的应用具有重要意义，CMAST框架为解决这一问题提供了有效方案。

Abstract: Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.

</details>


### [200] [Eevee: Towards Close-up High-resolution Video-based Virtual Try-on](https://arxiv.org/abs/2511.18957)
*Jianhao Zeng,Yancheng Bai,Ruidong Chen,Xuanpu Zhang,Lei Sun,Dongyang Jin,Ryan Xu,Nannan Zhang,Dan Song,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出了一个高分辨率视频虚拟试穿数据集，解决了现有方法依赖单一服装图像和缺乏特写镜头的局限性，并提出了新的服装一致性评估指标VGID。


<details>
  <summary>Details</summary>
Motivation: 当前视频虚拟试穿技术存在两个关键限制：1) 依赖单一服装图像输入无法准确捕捉真实纹理细节；2) 现有方法只生成全景试穿视频，忽略了商业对特写镜头的需求。

Method: 构建高分辨率视频虚拟试穿数据集，包含高保真服装图像、文本描述以及真实模特的全景和特写试穿视频；提出VGID指标量化服装纹理和结构的一致性保持。

Result: 实验验证了通过使用数据集中的详细图像，现有视频生成模型能够提取并整合纹理特征，显著提升虚拟试穿结果的真实感和细节保真度；基准测试有效识别了当前方法的纹理和结构保持问题。

Conclusion: 该数据集和评估指标为视频虚拟试穿技术提供了更全面的解决方案，满足了电商营销对高质量全景和特写视频的需求，推动了该技术的实际应用。

Abstract: Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.

</details>


### [201] [CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery](https://arxiv.org/abs/2511.18968)
*Bhuvan Sachdeva,Sneha Kumari,Rudransh Agarwal,Shalaka Kumaraswamy,Niharika Singri Prasad,Simon Mueller,Raphael Lechtenboehmer,Maximilian W. M. Wintergerst,Thomas Schultz,Kaushik Murali,Mohit Jain*

Main category: cs.CV

TL;DR: 提出了CataractCompDetect框架，结合相位感知定位、SAM 2跟踪、并发症特定风险评分和视觉语言推理，用于白内障手术中并发症的自动检测。


<details>
  <summary>Details</summary>
Motivation: 白内障手术是全球最常见的手术之一，但术中并发症如虹膜脱垂、后囊破裂和玻璃体丢失仍是导致不良结果的主要原因。自动检测这些事件可以实现早期预警系统和客观培训反馈。

Method: 开发了CataractCompDetect框架，包含四个关键组件：相位感知定位、基于SAM 2的跟踪、并发症特定风险评分和视觉语言推理用于最终分类。

Result: 在CataComp数据集上，CataractCompDetect平均F1得分为70.63%，各并发症检测性能分别为：虹膜脱垂81.8%、后囊破裂60.87%、玻璃体丢失69.23%。

Conclusion: 结果表明，将结构化手术先验知识与视觉语言推理相结合，对于识别罕见但高影响的术中事件具有重要价值。

Abstract: Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.

</details>


### [202] [Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs](https://arxiv.org/abs/2511.18976)
*Huaming Ling,Ying Wang,Si Chen,Junfeng Fan*

Main category: cs.CV

TL;DR: 提出单阶段微调策略和广义交错打包方案，解决全同态加密CNN推理中的非线性激活函数近似和密文容量限制问题，实现高效端到端FHE推理。


<details>
  <summary>Details</summary>
Motivation: 解决深度CNN在全同态加密推理中的两个核心挑战：用低阶多项式近似非线性激活函数（如ReLU）并最小化精度损失，以及突破密文容量限制以支持高分辨率图像处理。

Method: 1. 单阶段微调策略直接转换预训练CNN为FHE友好形式；2. 广义交错打包方案兼容任意空间分辨率的特征图，配合精心设计的同态运算符保持GIP形式加密。

Result: 在CIFAR-10、ImageNet和MS COCO上的实验表明，通过SFT策略获得的FHE友好CNN达到与使用ReLU或SiLU激活函数的基线相当的精度，并首次展示了基于FHE的YOLO架构目标检测推理。

Conclusion: 该工作实现了跨不同CNN架构的高效端到端FHE推理，为全同态加密在深度学习中的实际应用提供了重要进展。

Abstract: We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.

</details>


### [203] [Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models](https://arxiv.org/abs/2511.18978)
*Santiago Moreno,Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: ZEUS是一个零样本视觉语言分割框架，利用类别特定的文本提示集合和冻结的VLM编码器，在无需像素级标注的情况下自动生成全玻片图像中的高分辨率肿瘤分割掩码。


<details>
  <summary>Details</summary>
Motivation: 皮肤肿瘤活检的准确注释面临巨大挑战，包括形态变异大、组织学模式重叠以及良恶性病变的细微区别。现有的视觉语言模型在病理学中主要局限于玻片级任务或依赖粗糙的交互提示，难以在千兆像素全玻片图像上产生细粒度分割。

Method: 将每个WSI划分为重叠的图块，提取视觉嵌入，通过计算与文本提示的余弦相似度生成最终分割掩码。使用类别特定的文本提示集合和冻结的VLM编码器构建零样本分割流程。

Result: 在两个内部数据集（原发性梭形细胞肿瘤和皮肤转移瘤）上展示了具有竞争力的性能，突出了提示设计、领域偏移和机构变异性对病理学中VLM的影响。

Conclusion: ZEUS显著减少了标注负担，同时为下游诊断工作流程提供了可扩展、可解释的肿瘤划分方法。

Abstract: Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.

</details>


### [204] [UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection](https://arxiv.org/abs/2511.18983)
*Ching-Yi Lai,Chih-Yu Jian,Pei-Cheng Chuang,Chia-Ming Lee,Chih-Chung Hsu,Chiou-Ting Hsu,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 提出了一种新颖的单模态生成多模态对比学习框架，通过将单一视觉模态转换为三个互补特征来应对社交媒体压缩带来的深度伪造检测挑战。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体平台不同压缩程度对深度伪造检测模型泛化性和可靠性的挑战，克服单模态方法在数据压缩下特征退化以及多模态方法数据收集成本高、模态质量不一致的问题。

Method: UMCL框架：将单一视觉模态转换为压缩稳健的rPPG信号、时间地标动态和预训练视觉语言模型的语义嵌入；通过亲和力驱动的语义对齐策略显式对齐特征；采用跨质量相似性学习增强跨压缩率的特征鲁棒性。

Result: 在各种压缩率和操纵类型下实现卓越性能，为稳健深度伪造检测建立新基准；即使在单个特征退化时仍保持高检测精度，通过显式对齐提供特征关系的可解释性洞察。

Conclusion: 该方法有效解决了社交媒体压缩环境下的深度伪造检测挑战，提供了一种高效且鲁棒的解决方案，在保持检测精度的同时增强了模型的可解释性。

Abstract: In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.

</details>


### [205] [View-Consistent Diffusion Representations for 3D-Consistent Video Generation](https://arxiv.org/abs/2511.18991)
*Duolikun Danier,Ge Gao,Steven McDonagh,Changjian Li,Hakan Bilen,Oisin Mac Aodha*

Main category: cs.CV

TL;DR: ViCoDR是一种改进视频生成模型3D一致性的新方法，通过学习多视角一致的扩散表示来减少视觉伪影。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型存在3D不一致问题，如物体和结构在相机姿态变化时发生变形，这影响了用户体验和模拟保真度。

Method: 通过分析多个相机控制视频扩散模型，提出ViCoDR方法学习多视角一致的扩散表示，应用于相机控制的图像到视频、文本到视频和多视角生成模型。

Result: 在多个视频生成任务上显著提高了生成视频的3D一致性。

Conclusion: 改进视频扩散模型的多视角一致性表示能够有效提升生成视频的3D一致性质量。

Abstract: Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.

</details>


### [206] [AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization](https://arxiv.org/abs/2511.18993)
*Christos Koutlis,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 提出了一种基于音频-视觉语音表示重建（AuViRe）的新方法，用于时间定位深度伪造视频，通过跨模态重建在伪造片段中产生显著差异来检测篡改。


<details>
  <summary>Details</summary>
Motivation: 随着合成音视频内容的快速发展，特别是恶意操纵的出现，确保数字媒体完整性变得至关重要。

Method: 利用音频-视觉语音表示重建，从一个模态（如唇部运动）重建另一个模态（如音频波形）的语音表示，在篡改视频片段中跨模态重建更具挑战性，从而产生放大差异。

Result: 在LAV-DF数据集上AP@0.95提升8.9，在AV-Deepfake1M数据集上AP@0.5提升9.6，在真实场景实验中AUC提升5.1。

Conclusion: AuViRe方法通过跨模态重建差异提供了强大的判别线索，能够精确时间定位深度伪造，性能显著优于现有技术。

Abstract: With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.

</details>


### [207] [A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation](https://arxiv.org/abs/2511.19004)
*Wentao Qu,Guofeng Mei,Yang Wu,Yongshun Gong,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: 本文提出T2LDM，一种文本到LiDAR的扩散模型，通过自条件表示引导（SCRG）生成高质量的3D场景，解决了文本-LiDAR数据对稀缺和文本描述质量低的问题。


<details>
  <summary>Details</summary>
Motivation: 解决文本到LiDAR生成中由于文本-LiDAR数据对稀缺导致的训练先验不足，以及低质量文本描述降低生成质量和可控性的问题。

Method: 提出T2LDM模型，采用自条件表示引导（SCRG）在训练时提供软监督，推理时解耦；构建T2nuScenes基准和可控性指标；设计方向位置先验缓解街道失真；通过条件编码器支持多种条件生成任务。

Result: 在无条件和条件生成任务中，T2LDM优于现有方法，实现了最先进的场景生成效果。

Conclusion: T2LDM通过SCRG机制和多种创新设计，有效提升了文本到LiDAR生成的细节质量和可控性，支持多种条件生成任务。

Abstract: Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.

</details>


### [208] [Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting](https://arxiv.org/abs/2511.19021)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min*

Main category: cs.CV

TL;DR: 提出了Grc-ViT，一种动态粗到细的视觉Transformer框架，通过自适应调整视觉粒度来解决ViT在细粒度局部细节表示上的不足，在保持计算效率的同时提升细粒度识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度方法虽然缓解了ViT在局部细节表示上的问题，但依赖固定补丁大小并引入冗余计算。需要一种能够根据图像复杂度自适应调整粒度的动态框架。

Method: 包含两个关键模块：粗粒度评估模块（使用边缘密度、熵和频域线索评估视觉复杂度）和细粒度精炼模块（根据选定粒度优化注意力计算）。通过两个可学习参数α和β平衡全局推理和局部感知。

Result: 综合评估表明，Grc-ViT在提升细粒度区分能力的同时，实现了准确性和计算效率之间的优越平衡。

Conclusion: Grc-ViT通过动态粒度调整机制，有效解决了ViT在局部细节表示上的局限性，为视觉Transformer提供了更高效的细粒度特征学习框架。

Abstract: Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, α and \b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.

</details>


### [209] [Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric](https://arxiv.org/abs/2511.19032)
*Xiangjie Sui,Songyang Li,Hanwei Zhu,Baoliang Chen,Yuming Fang,Xin Sun*

Main category: cs.CV

TL;DR: 本文提出Bench-C基准测试和RAS指标，用于评估大型视觉语言模型在视觉损坏下的鲁棒性，解决了现有评估方法中样本区分度不足和准确率指标无法捕捉预测结构退化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在两个主要局限：1）当前数据集中低区分度样本占主导，掩盖了模型间的真实鲁棒性差距；2）传统基于准确率的指标无法捕捉底层预测结构的退化。

Method: 提出Bench-C基准测试，通过考虑损坏下的预测不一致性和语义多样性来选择区分性样本；提出RAS统一指标，通过考虑预测不确定性和校准对齐的变化来测量logit级预测结构的退化。

Result: 实验发现：1）模型在损坏下表现出不同的行为模式，如错误置信和犹豫；2）轻微损坏可能导致准确率略有提升，但整体预测结构仍会退化；3）通过将鲁棒性分解为破坏性和纠正性组件，可以揭示不同模型的失败和恢复模式。

Conclusion: Bench-C和RAS为评估LVLMs在视觉损坏下的鲁棒性提供了更全面的框架，揭示了传统准确率指标无法捕捉的重要鲁棒性特征。

Abstract: Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.

</details>


### [210] [Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation](https://arxiv.org/abs/2511.19049)
*Ruojun Xu,Yu Kai,Xuhua Ren,Jiaxiang Cheng,Bing Ma,Tianxiang Zheng,Qinhlin Lu*

Main category: cs.CV

TL;DR: 本文分析了DPO在扩散模型中存在的似然位移问题，提出了PG-DPO方法，通过自适应拒绝缩放和隐式偏好正则化来改善视频生成任务中的偏好对齐效果。


<details>
  <summary>Details</summary>
Motivation: DPO在生成模型中对齐人类偏好时存在似然位移问题，即选择样本的概率在训练中反而下降，这一问题在扩散模型中的影响尚未充分研究，导致视频生成任务性能不佳。

Method: 在扩散框架下对DPO损失进行形式化分析，识别出优化冲突和次优最大化两种失效模式，提出PG-DPO方法，结合自适应拒绝缩放和隐式偏好正则化。

Result: 实验表明PG-DPO在定量指标和定性评估上都优于现有方法，为视频生成任务中的偏好对齐提供了稳健解决方案。

Conclusion: PG-DPO有效缓解了DPO中的似然位移问题，显著提升了扩散模型在视频生成任务中的偏好对齐性能。

Abstract: Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.

</details>


### [211] [LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space](https://arxiv.org/abs/2511.19057)
*Hai Wu,Shuai Tang,Jiale Wang,Longkun Zou,Mingyue Guo,Rongqin Liang,Ke Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: LAA3D是一个大规模低空飞行器3D感知数据集，包含15,000张真实图像和600,000帧合成数据，支持3D检测、多目标跟踪和6自由度姿态估计任务。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对低空飞行器3D感知的数据集，限制了该领域的研究进展。

Method: 构建包含真实和合成图像的大规模数据集，涵盖多种飞行器类别，并提供统一的评估基准和单目3D检测基线方法MonoLAA。

Result: 数据集支持多种3D感知任务，合成数据预训练模型能有效迁移到真实数据，展示了良好的仿真到真实泛化能力。

Conclusion: LAA3D为低空3D物体感知研究提供了全面的基础，推动了该领域的发展。

Abstract: Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.

</details>


### [212] [Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation](https://arxiv.org/abs/2511.19062)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min,Yi Zhang*

Main category: cs.CV

TL;DR: Grc-SAM是一个基于粒度计算的从粗到细的图像分割框架，通过自适应提取高响应区域实现前景定位，使用细粒度补丁分区增强细节建模，并用潜在提示嵌入替代手工提示，在无提示图像分割中表现出优越的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决现有无提示图像分割方法（如SAM）的两个局限性：缺乏自主区域定位机制（局部化能力不足）和在高分辨率下细粒度建模受限（可扩展性不足）。

Method: 提出Grc-SAM框架：1）粗粒度阶段自适应提取高响应区域实现前景定位；2）细粒度阶段使用稀疏局部Swin注意力进行细粒度补丁分区；3）将精炼掩码编码为潜在提示嵌入，替代手工提示。

Result: 广泛的实验结果表明，Grc-SAM在准确性和可扩展性方面均优于基线方法。

Conclusion: Grc-SAM通过集成多粒度注意力，将粒度计算与视觉变换器相结合，为无提示分割提供了独特的粒度计算视角。

Abstract: Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.

</details>


### [213] [Understanding, Accelerating, and Improving MeanFlow Training](https://arxiv.org/abs/2511.19065)
*Jin-Young Kim,Hyojun Go,Lea Bogensperger,Julius Erbach,Nikolai Kalischek,Federico Tombari,Konrad Schindler,Dominik Narnhofer*

Main category: cs.CV

TL;DR: 本文分析了MeanFlow训练动态，发现瞬时速度场是学习平均速度场的前提，并设计了新的训练方案来加速瞬时速度形成，然后转向长间隔平均速度学习，显著提升了少步生成质量。


<details>
  <summary>Details</summary>
Motivation: MeanFlow通过联合学习瞬时和平均速度场实现高质量少步生成，但其训练动态机制尚不明确，需要深入分析两种速度场的相互作用以优化训练过程。

Method: 分析两种速度场的相互作用机制，设计分阶段训练方案：先加速瞬时速度场形成，然后从短间隔转向长间隔平均速度场学习。

Result: 增强的MeanFlow训练实现了更快的收敛速度和显著更好的少步生成质量：在DiT-XL骨干上，1步生成ImageNet 256x256的FID达到2.87，相比基线3.43有明显提升。

Conclusion: 瞬时速度场是学习平均速度场的基础，分阶段的训练策略能有效提升MeanFlow的性能，实现更快的训练速度和更好的少步生成质量。

Abstract: MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.

</details>


### [214] [DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling](https://arxiv.org/abs/2511.19067)
*Timur Mamedov,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: DynaMix是一种新颖的可泛化行人重识别方法，通过动态结合手动标记的多摄像头数据和伪标记的单摄像头数据，在三个核心模块的协同作用下实现高效的大规模训练，在未见过的摄像头和环境上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法严重依赖有限的多摄像头标记数据，而实际应用中存在大量单摄像头数据未被充分利用。DynaMix旨在有效结合手动标记的多摄像头数据和大规模伪标记的单摄像头数据，提升模型的泛化能力。

Method: DynaMix包含三个核心模块：1) 动态重标记模块，实时优化单摄像头身份的伪标签；2) 高效质心模块，在大规模身份空间下保持稳健的身份表示；3) 数据采样模块，精心组合混合数据小批量以平衡学习复杂度和批内多样性。

Result: 大量实验表明，DynaMix在可泛化行人重识别任务中持续优于现有最先进方法，能够有效处理数百万图像和数十万身份的大规模训练。

Conclusion: DynaMix通过动态适应训练数据的结构和噪声，成功结合了不同类型的数据源，为可泛化行人重识别提供了一种高效且有效的解决方案。

Abstract: Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.

</details>


### [215] [DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation](https://arxiv.org/abs/2511.19071)
*Fangda Chen,Jintao Tang,Pancheng Wang,Ting Wang,Shasha Li,Ting Deng*

Main category: cs.CV

TL;DR: 本文提出了DEAP-3DSAM模型，通过特征增强解码器和双注意力提示器解决了SAM在3D医学图像分割中的空间特征丢失和手动提示依赖问题。


<details>
  <summary>Details</summary>
Motivation: SAM在医学图像分割中表现潜力，但应用于3D图像时存在空间特征丢失问题，且大多数方法依赖手动提示，难以在实际场景中应用。

Method: 提出特征增强解码器融合原始图像特征与空间信息，设计双注意力提示器通过空间注意力和通道注意力自动获取提示信息。

Result: 在四个公共腹部肿瘤分割数据集上的实验表明，DEAP-3DSAM在3D图像分割中达到最先进性能，优于或匹配现有手动提示方法。

Conclusion: 定量和定性消融研究证实了所提出模块的有效性，DEAP-3DSAM成功解决了SAM在3D医学图像分割中的关键限制。

Abstract: The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.

</details>


### [216] [DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection](https://arxiv.org/abs/2511.19111)
*Hai Ci,Ziheng Peng,Pei Yang,Yingxin Xuan,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文介绍了DiffSeg30k数据集，这是一个包含3万张扩散编辑图像的数据集，具有像素级标注，用于支持细粒度的AI生成内容检测。该数据集包含真实世界图像、多种扩散模型、多轮编辑和真实编辑场景，将AIGC检测从二元分类转向语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有的AIGC检测基准主要关注整个图像的分类，忽视了扩散编辑的定位问题。扩散编辑能够对局部图像区域进行逼真修改，使得AI生成内容更难检测。

Method: 构建DiffSeg30k数据集，包含：1）真实世界图像，从COCO收集图像或图像提示；2）使用8种最先进的扩散模型进行局部编辑；3）每张图像最多进行三轮顺序编辑；4）基于视觉语言模型的流程自动识别有意义区域并生成上下文感知的提示。

Result: 基准测试显示语义分割任务面临显著挑战，特别是在图像失真鲁棒性方面。分割模型在识别扩散编辑方面表现出色，优于现有的伪造分类器，并在跨生成器泛化方面显示出巨大潜力。

Conclusion: DiffSeg30k通过展示基于分割方法的前景和局限性，将推动AI生成内容细粒度定位的研究。该数据集将AIGC检测从二元分类转向语义分割，能够同时定位编辑并识别编辑模型。

Abstract: Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k

</details>


### [217] [MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images](https://arxiv.org/abs/2511.19119)
*Qirui Wang,Jingyi He,Yining Pan,Si Yong Yeo,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: MonoSR是一个大规模单目空间推理数据集，涵盖室内、室外和物体中心场景，支持多种问题类型，旨在推动开放世界单目空间推理研究。


<details>
  <summary>Details</summary>
Motivation: 现有空间推理研究主要关注室内环境并依赖多视角观测，限制了其在室外场景的泛化能力和在单目图像（最常见真实世界设置）中的应用。

Method: 提出MonoSR数据集，包含多样化场景和问题类型；评估先进视觉语言模型在该任务上的表现；分析辅助信息对单目空间推理的重要性。

Result: 建立了大规模单目空间推理基准，揭示了现有模型在这一挑战性任务上的局限性，并提供了辅助信息重要性的分析。

Conclusion: 该工作为在真实世界开放环境中推进单目空间推理奠定了基础，为未来模型设计提供了实用指导。

Abstract: Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.

</details>


### [218] [When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP](https://arxiv.org/abs/2511.19126)
*Beilin Chu,Weike You,Mengtao Li,Tingting Zheng,Kehan Zhao,Xuan Xu,Zhigao Lu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CV

TL;DR: 本文提出SemAnti方法，通过冻结CLIP的语义子空间并在打乱语义下仅调整对生成痕迹敏感的层，显著提升了AI生成图像检测的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CLIP的AI生成图像检测器过度依赖语义线索而非生成器痕迹，导致在分布偏移下性能脆弱。需要解决语义偏差问题以实现更鲁棒的检测。

Method: 提出SemAnti语义对抗微调范式：1）使用Patch Shuffle破坏全局语义连续性但保留局部痕迹线索；2）冻结CLIP的语义子空间；3）仅在打乱语义下调整对生成痕迹敏感的层。

Result: 在AIGCDetectBenchmark和GenImage基准测试中实现了最先进的跨域泛化性能，证明了调节语义是释放CLIP在AI生成图像检测中全部潜力的关键。

Conclusion: 通过抑制语义偏差并专注于生成器痕迹，CLIP可以成为更鲁棒的AI生成图像检测器，SemAnti方法为这一目标提供了有效的实现路径。

Abstract: The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.

</details>


### [219] [MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery](https://arxiv.org/abs/2511.19134)
*Shuyu Cao,Minxin Chen,Yucheng Song,Zhaozhong Chen,Xinyou Zhang*

Main category: cs.CV

TL;DR: MambaRefine-YOLO提出了一种用于无人机图像中小目标检测的双模态融合方法，通过双门控互补Mamba融合模块和分层特征聚合颈部，在精度和速度之间取得了优越的平衡。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的小目标检测面临低分辨率和背景杂波的挑战，现有的RGB和红外数据融合方法在跨模态交互和计算效率之间存在权衡困难。

Method: 提出了Dual-Gated Complementary Mamba融合模块（DGC-MFM），通过光照感知和差异感知门控机制自适应平衡RGB和红外模态；以及分层特征聚合颈部（HFAN），采用“先精炼后融合”策略增强多尺度特征。

Result: 在双模态DroneVehicle数据集上达到83.2%的最优mAP，比基线提高7.9%；在单模态VisDrone数据集上，仅使用HFAN的变体也显示出显著增益。

Conclusion: 该方法在精度和速度之间实现了优越的平衡，非常适合实际无人机应用。

Abstract: Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse'' strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.

</details>


### [220] [ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation](https://arxiv.org/abs/2511.19145)
*Dongha Lee,Jinhee Park,Minjun Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: ABM-LoRA是一种通过对齐预训练模型和适配器激活边界来加速低秩适配器收敛的初始化策略，显著减少信息损失并提高训练效率。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然参数效率高，但其随机初始化导致梯度更新在失配的切空间中进行，造成显著信息损失并阻碍早期收敛。

Method: 提出激活边界匹配方法，在下游训练前将适配器的激活边界与预训练模型对齐，最大化全参数梯度在适配器子空间中的投影。

Result: 在语言理解、对话生成和视觉识别等多种架构和任务上验证有效性，在VTAB-1K上达到所有方法中最高的准确率，在需要几何理解的结构化推理任务上表现突出。

Conclusion: ABM-LoRA通过改进初始化策略显著加速了低秩适配器的收敛，减少了信息损失，在多种任务上取得了优异的性能。

Abstract: We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.

</details>


### [221] [Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation](https://arxiv.org/abs/2511.19147)
*Huisoo Lee,Jisu Han,Hyunsouk Cho,Wonjun Hwang*

Main category: cs.CV

TL;DR: 提出CoMA框架，通过协同使用两种互补的基础模型（如CLIP和BLIP）来解决无源域自适应问题，利用双向适应机制和分解互信息实现稳定高效的领域适应。


<details>
  <summary>Details</summary>
Motivation: 现有基于单一基础模型的SFDA方法语义覆盖范围有限，难以捕捉领域偏移下的多样化上下文线索，需要多基础模型协同来提升适应性能。

Method: CoMA框架联合利用两种互补的基础模型，采用双向适应机制：对齐不同基础模型与目标模型进行任务适应，同时保持语义独特性；将互补知识从基础模型转移到目标模型。引入分解互信息来增强真实依赖关系，抑制由不完整类别覆盖引起的虚假依赖。

Result: 在四个基准测试（Office-31、Office-Home、DomainNet-126、VisDA）上均优于现有最先进的SFDA方法，在闭集设置下表现最佳，同时在部分集和开集变体上也取得最优结果。

Conclusion: CoMA框架通过多基础模型协同和双向适应机制，有效解决了SFDA中语义覆盖不足的问题，在各种设置下均实现了优异的性能表现。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.

</details>


### [222] [From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation](https://arxiv.org/abs/2511.19149)
*Moazzam Umer Gondal,Hamad Ul Qudous,Daniya Siddiqui,Asma Ahmad Farhan*

Main category: cs.CV

TL;DR: 本文提出了一个检索增强的自动时尚图像描述和标签生成框架，结合多服装检测、属性推理和大型语言模型提示，旨在生成视觉基础扎实、描述性强且风格有趣的时尚文本内容。


<details>
  <summary>Details</summary>
Motivation: 解决端到端图像描述生成器在属性保真度和领域泛化方面的问题，为时尚图像生成更具视觉基础、描述性和风格化的文本内容。

Method: 采用YOLO-based检测器进行多服装定位，k-means聚类提取主色调，CLIP-FAISS检索模块基于结构化产品索引推断面料和性别属性，结合检索到的风格示例构建事实证据包，指导LLM生成类人描述和上下文丰富的标签。

Result: YOLO检测器在9个服装类别上获得0.71的mAP@0.5；RAG-LLM流水线生成富有表现力的属性对齐描述，在标签生成中实现0.80的平均属性覆盖率和50%阈值下的完全覆盖；相比BLIP基线模型具有更好的事实基础和更少的幻觉。

Conclusion: 检索增强生成方法展示了作为自动化和视觉基础时尚内容生成的有效且可解释的范式，具有更好的事实基础、更少的幻觉以及在各种服装领域可扩展部署的巨大潜力。

Abstract: This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.

</details>


### [223] [MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes](https://arxiv.org/abs/2511.19172)
*Kehua Chen,Tianlu Mao,Zhuxin Ma,Hao Jiang,Zehao Li,Zihan Liu,Shuqi Gao,Honglong Zhao,Feng Dai,Yucheng Zhang,Zhaoqi Wang*

Main category: cs.CV

TL;DR: MetroGS是一个用于复杂城市场景高效稳健重建的新型高斯溅射框架，通过分布式2D高斯溅射表示、结构化密集增强、渐进混合几何优化和深度引导外观建模，实现了高质量的几何保真度和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射及其衍生方法在大规模场景重建中如何高效稳定地实现高质量几何保真度的核心挑战，特别是在复杂城市场景中。

Method: 基于分布式2D高斯溅射表示作为核心基础；提出结构化密集增强方案，利用SfM先验和点图模型实现更密集初始化；设计渐进混合几何优化策略，整合单目和多视图优化；引入深度引导外观建模方法学习具有3D一致性的空间特征。

Result: 在大规模城市场景数据集上的实验表明，MetroGS实现了优越的几何精度和渲染质量。

Conclusion: MetroGS为高保真度大规模场景重建提供了一个统一的解决方案，能够有效处理复杂城市场景中的稀疏区域和外观不一致问题。

Abstract: Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.

</details>


### [224] [nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation](https://arxiv.org/abs/2511.19183)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jaeger,Fabian Isensee,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: nnActive是一个开源主动学习框架，通过大规模研究、3D补丁查询选择、前景感知随机采样策略和前景效率指标，解决了3D生物医学图像分割中主动学习评估的四个陷阱。研究发现主动学习方法都优于标准随机采样，但未能可靠超越改进的前景感知随机采样。


<details>
  <summary>Details</summary>
Motivation: 生物医学图像语义分割依赖大量标注数据，但手动标注成本高且需要专业知识。主动学习旨在通过选择信息量最大的样本来减少标注工作量，但在3D生物医学成像领域，主动学习是否始终优于随机采样尚无共识。

Method: 开发nnActive框架，包括：(1)在四个生物医学图像数据集和三种标注机制下进行大规模研究；(2)扩展nnU-Net，使用部分标注进行3D补丁查询选择训练；(3)提出处理医学图像前景-背景类别不平衡的前景感知随机采样策略；(4)提出前景效率指标来捕捉背景区域低标注成本。

Result: (A)所有主动学习方法都优于标准随机采样，但未能可靠超越改进的前景感知随机采样；(B)主动学习的优势取决于任务特定参数；(C)预测熵是整体表现最好的主动学习方法，但可能需要最多的标注工作量；(D)通过更密集的计算设计选择可以提升主动学习性能。

Conclusion: nnActive作为一个全面的开源框架，可以作为3D生物医学成像中主动学习研究和应用的催化剂。预测熵是表现最好的主动学习方法，但前景感知随机采样是一个强有力的基准。

Abstract: Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive

</details>


### [225] [SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection](https://arxiv.org/abs/2511.19187)
*Nithira Jayarathne,Naveen Basnayake,Keshawa Jayasundara,Pasindu Dodampegama,Praveen Wijesinghe,Hirushika Pelagewatta,Kavishka Abeywardana,Sandushan Ranaweera,Chamira Edussooriya*

Main category: cs.CV

TL;DR: 提出基于EfficientNet-B6的轻量级深度伪造图像检测模型，通过变换技术和优化策略解决类别不平衡问题，实现高精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 检测深度伪造图像对于打击虚假信息至关重要，需要开发轻量级且可泛化的模型来帮助非专家有效识别伪造图像。

Method: 使用EfficientNet-B6架构，结合变换技术进行微调，采用鲁棒预处理、过采样和优化策略，并探索傅里叶变换的相位和幅度特征。

Result: 模型实现了高准确率、稳定性和泛化能力，但傅里叶变换特征的加入影响有限。

Conclusion: 该框架为非专家提供了有效的深度伪造图像检测工具，在可访问和可靠的检测方面取得了重要进展。

Abstract: Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.

</details>


### [226] [CLASH: A Benchmark for Cross-Modal Contradiction Detection](https://arxiv.org/abs/2511.19199)
*Teodora Popordanoska,Jiameng Li,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: CLASH是一个用于多模态矛盾检测的新基准，包含COCO图像与包含对象级或属性级矛盾的矛盾标题配对，通过多项选择和开放式问题评估模型识别跨模态冲突的能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中存在大量矛盾的多模态输入，但现有基准通常假设输入一致性，无法评估跨模态矛盾检测这一防止幻觉和确保可靠性的基本能力。

Method: 使用COCO图像与包含对象级或属性级矛盾的标题配对，构建包含大量自动质量检查过滤的微调集和较小的人工验证诊断集，在多项选择和开放式格式下评估模型。

Result: 对最先进模型的分析显示它们在识别跨模态冲突方面存在显著局限性，暴露出系统性模态偏差和类别特定弱点。针对CLASH的有针对性微调显著增强了冲突检测能力。

Conclusion: CLASH基准揭示了当前多模态模型在矛盾检测方面的不足，并证明通过针对性训练可以有效提升这一关键能力。

Abstract: Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.

</details>


### [227] [Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?](https://arxiv.org/abs/2511.19200)
*Itay Cohen,Ethan Fetaya,Amir Rosenfeld*

Main category: cs.CV

TL;DR: 该论文研究了CLIP等视觉语言模型是否能区分真实物体与其外观相似物，创建了RoLA数据集，并提出了在嵌入空间中估计真实与相似物方向的方法来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管计算机视觉模型在识别基准上表现良好，但与人类感知相比仍存在差距，特别是在判断图像是否只是看起来像某个物体而非真实实例的能力上。

Method: 创建RoLA数据集包含真实和外观相似样本，评估基于提示的基线方法，并在CLIP嵌入空间中估计真实与相似物之间的方向向量。

Result: 应用该方向向量可改善CLIP在Conceptual12M上的跨模态检索性能，并提升CLIP前缀字幕生成器的字幕质量。

Conclusion: CLIP模型能够捕捉真实物体与外观相似物之间的细微区别，通过在嵌入空间中估计方向向量可以进一步提升模型的相关任务性能。

Abstract: Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired "real"/"lookalike" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.

</details>


### [228] [NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting](https://arxiv.org/abs/2511.19202)
*Brent Zoomers,Florian Hahlbohm,Joni Vanherck,Lode Jorissen,Marcus Magnor,Nick Michiels*

Main category: cs.CV

TL;DR: 提出一种新方法，通过小型共享MLP学习3D高斯模型中所有高斯的视点相关可见性函数，在渲染前剔除被遮挡的基元，结合实例化软件光栅化器实现高效渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然可以利用视锥体剔除和细节层次策略加速渲染，但由于高斯的半透明特性无法应用遮挡剔除这一高效技术，这限制了渲染性能的进一步提升。

Method: 使用小型共享MLP学习训练模型中所有高斯的视点相关可见性函数，在光栅化前查询视锥体内高斯的可见性，结合利用Tensor Core的实例化软件光栅化器实现高效计算。

Result: 在组合场景中，该方法在VRAM使用和图像质量方面优于当前最先进技术，结合实例化光栅化器和遮挡剔除MLP，并与现有LoD技术具有互补特性。

Conclusion: 提出的神经可见性查询方法成功解决了3D高斯渲染中的遮挡剔除问题，显著提升了渲染效率，为大规模场景渲染提供了有效解决方案。

Abstract: 3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.

</details>


### [229] [ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment](https://arxiv.org/abs/2511.19217)
*Wanjiang Weng,Xiaofeng Tan,Junbo Wang,Guo-Sen Xie,Pan Zhou,Hongsong Wang*

Main category: cs.CV

TL;DR: 提出ReAlign方法解决文本到运动生成中的对齐问题，通过奖励引导采样改善扩散模型中的文本-运动分布不匹配问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到运动生成中存在文本与运动分布不匹配问题，导致语义不一致或低质量运动

Method: 提出奖励引导采样对齐(ReAlign)，包含步骤感知奖励模型和奖励引导策略，集成文本对齐模块和运动对齐模块

Result: 在运动生成和检索任务上的广泛实验表明，相比现有最先进方法显著改善了文本-运动对齐和运动质量

Conclusion: ReAlign方法有效解决了扩散模型中文本-运动对齐问题，提升了生成运动的语义一致性和质量

Abstract: Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.

</details>


### [230] [Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering](https://arxiv.org/abs/2511.19220)
*Federico Felizzi,Olivia Riccomi,Michele Ferramola,Francesco Andrea Causio,Manuel Del Medico,Vittorio De Vita,Lorenzo De Mori,Alessandra Piscitelli Pietro Eric Risuleo,Bianca Destro Castaniti,Antonio Cristiano Alessia Longo,Luigi De Angelis,Mariapia Vassalli,Marcello Di Pumpo*

Main category: cs.CV

TL;DR: 研究评估了四种前沿视觉语言模型在意大利医学问答中的视觉依赖性，发现GPT-4o对视觉信息依赖最强，而其他模型更依赖文本捷径。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医学视觉问答基准上表现优异，但其对视觉信息的真实依赖程度尚不明确，需要验证模型是否真正整合视觉和文本信息。

Method: 使用欧洲医学问答意大利数据集的60个明确需要图像解释的问题，将正确的医学图像替换为空白占位符，测试四种最先进模型（Claude Sonnet 4.5、GPT-4o、GPT-5-mini和Gemini 2.0 flash exp）的性能变化。

Result: GPT-4o显示出最强的视觉依赖性，准确率下降27.9个百分点（从83.2%降至55.3%），而GPT-5-mini、Gemini和Claude的准确率下降较小，分别为8.5pp、2.4pp和5.6pp。所有模型都对虚构的视觉解释生成了自信的推理。

Conclusion: 不同模型在视觉依赖性上存在显著差异，强调了在临床部署前需要进行严格评估，以确保模型的鲁棒性和可靠性。

Abstract: Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.

</details>


### [231] [Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving](https://arxiv.org/abs/2511.19221)
*Jianhua Han,Meng Tian,Jiangtong Zhu,Fan He,Huixin Zhang,Sitong Guo,Dechang Zhu,Hao Tang,Pei Xu,Yuze Guo,Minzhe Niu,Haojie Zhu,Qichao Dong,Xuechao Yan,Siyuan Dong,Lu Hou,Qingqiu Huang,Xiaosong Jia,Hang Xu*

Main category: cs.CV

TL;DR: Percept-WAM是一个感知增强的世界感知-动作模型，首次在单一视觉语言模型中隐式整合2D/3D场景理解能力，通过World-PV和World-BEV令牌统一空间感知任务，提升自动驾驶在长尾场景和复杂交互中的稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在空间感知方面存在不足，特别是在长尾场景和复杂交互中容易产生不准确和不稳定的问题。现有的视觉语言模型在空间基础和空间理解方面较弱，限制了感知和定位能力。

Method: 提出Percept-WAM模型，采用网格条件预测机制进行密集目标感知，包含IoU感知评分和并行自回归解码。通过World-PV和World-BEV令牌编码空间坐标和置信度，统一2D/3D感知任务，同时保留预训练VLM参数以维持通用智能。

Result: 在COCO 2D检测上达到51.7/58.9 mAP，在nuScenes BEV 3D检测上表现优异。与轨迹解码器集成后，在nuScenes和NAVSIM上提升规划性能，在NAVSIM上超越DiffusionDrive 2.1 PMDS。定性结果显示具有强大的开放词汇和长尾泛化能力。

Conclusion: Percept-WAM成功实现了2D/3D感知能力的统一集成，在保持通用智能的同时显著提升了自动驾驶系统的感知稳定性和规划性能，特别是在长尾、远距离和小目标场景中表现突出。

Abstract: Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.

</details>


### [232] [Learning Plug-and-play Memory for Guiding Video Diffusion Models](https://arxiv.org/abs/2511.19229)
*Selena Song,Ziming Xu,Zijun Zhang,Kun Zhou,Jiaxian Guo,Lianhui Qin,Biwei Huang*

Main category: cs.CV

TL;DR: 本文提出DiT-Mem方法，通过可学习的记忆编码器为扩散Transformer视频生成模型注入世界知识，改善物理规律遵循和视频保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的DiT视频生成模型虽然视觉质量和时间连贯性出色，但经常违反基本物理定律和常识动态，缺乏明确的世界知识。

Method: 提出DiT-Mem记忆编码器，由堆叠的3D CNN、低通/高通滤波器和自注意力层组成，将参考视频映射为紧凑的记忆标记，在DiT自注意力层中作为记忆使用。训练时冻结扩散主干，仅优化记忆编码器。

Result: 该方法在少量训练参数（1.5亿）和1万数据样本上实现高效训练，推理时可即插即用，显著提升了物理规则遵循能力和视频保真度。

Conclusion: DiT-Mem方法有效解决了DiT视频生成模型缺乏世界知识的问题，通过记忆注入机制显著改善了生成视频的物理合理性和质量。

Abstract: Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.

</details>


### [233] [IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2511.19235)
*Carl Lindström,Mahan Rafidashti,Maryam Fatemi,Lars Hammarstrand,Martin R. Oswald,Lennart Svensson*

Main category: cs.CV

TL;DR: IDSplat是一个自监督的3D高斯泼溅框架，无需人工标注即可重建具有明确实例分解和可学习运动轨迹的动态驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 现有的动态场景重建方法要么依赖昂贵的人工标注来获取物体轨迹，要么使用没有明确对象级分解的时变表示，导致静态和动态元素交织在一起，阻碍场景分离。

Method: 将动态对象建模为经历刚性变换的连贯实例，而不是非结构化的时变基元。使用基于语言的零样本视频跟踪与激光雷达进行3D锚定，通过特征对应估计一致的姿态。引入协调转向平滑方案获得时间和物理一致的运动轨迹，联合优化对象姿态和高斯参数。

Result: 在Waymo Open Dataset上的实验表明，该方法在保持实例级分解的同时实现了有竞争力的重建质量，并且能够跨不同序列和视图密度进行泛化而无需重新训练。

Conclusion: IDSplat为大规模自动驾驶应用提供了一种实用的动态场景重建解决方案，无需人工标注即可实现实例分解和运动轨迹学习。

Abstract: Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.

</details>


### [234] [Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation](https://arxiv.org/abs/2511.19254)
*Mohamed Rissal Hedna,Sesugh Samuel Nder*

Main category: cs.CV

TL;DR: 本文研究了在物流系统中针对货物占用率分类器的物理对抗攻击可行性，通过3D模拟环境优化对抗补丁纹理，在拒绝服务攻击中达到84.94%的成功率。


<details>
  <summary>Details</summary>
Motivation: 随着计算机视觉系统在现代物流运营中的广泛应用，这些系统可能面临物理对抗攻击的威胁，特别是可打印并放置在内部表面的对抗补丁。

Method: 使用Mitsuba 3进行可微分渲染，在几何、光照和视角变化下优化补丁纹理，并与2D合成基线进行比较。

Result: 3D优化补丁在拒绝服务攻击（空到满）中达到84.94%的成功率，隐藏攻击（满到空）达到30.32%的成功率。

Conclusion: 这是首个在物理真实的完全模拟3D场景中研究货物占用率估计对抗补丁攻击的工作，强调了自动化物流管道安全的重要性并指出了增强物理鲁棒性的方向。

Abstract: Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.

</details>


### [235] [LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models](https://arxiv.org/abs/2511.19261)
*Shuai Wang,Daoan Zhang,Tianyi Bai,Shitong Shao,Jiebo Luo,Jiaheng Wei*

Main category: cs.CV

TL;DR: LAST方法通过让视觉语言模型在给出最终答案前进行空间和时间维度的视觉思考，联合提升3D空间理解和长视频理解能力，仅使用2D图像作为输入。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视觉语言模型在3D空间理解和长视频理解方面仍然表现不佳，尽管它们在典型的视觉语言任务中表现强大。现有方法通常依赖专门的架构设计来分别改进3D任务和视频理解任务的性能。

Method: 提出LAST方法，让VLMs在空间和时间维度进行思考，构建3D空间和时间维度的视觉思考轨迹，而不是仅使用文本。支持两种场景：零样本直接提示专有模型，以及使用包含时空思考轨迹的数据微调通用VLMs。

Result: LAST在各种基准测试中带来显著提升，包括3个空间理解、4个视频理解和3个图像理解任务。在零样本设置下，GPT-4o在EgoSchema上获得15.8%的提升；与Qwen2.5-VL-7B相比，在VSI-Bench上获得8.3的提升。

Conclusion: LAST方法通过让视觉语言模型在空间和时间维度进行视觉思考，有效提升了3D空间理解和长视频理解能力，为通用VLMs提供了联合改进时空理解的有效途径。

Abstract: Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.

</details>


### [236] [BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment](https://arxiv.org/abs/2511.19268)
*Dewei Zhou,Mingwei Li,Zongxin Yang,Yu Lu,Yunqiu Xu,Zhizhong Wang,Zeyi Huang,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了BideDPO框架，通过双向解耦的偏好优化方法解决条件图像生成中文本与条件图像之间的冲突问题，显著提升了文本成功率和条件对齐度。


<details>
  <summary>Details</summary>
Motivation: 当前条件图像生成方法在处理文本与条件图像之间的冲突时面临挑战，包括输入级冲突和模型偏置冲突。标准监督微调难以有效解决这些问题，而现有的偏好优化方法存在梯度纠缠和数据不足的限制。

Method: 提出双向解耦DPO框架(BideDPO)，创建两个解耦的偏好对（一个用于条件，一个用于文本）以减少梯度纠缠。采用自适应损失平衡策略管理偏好对的影响，并构建自动化数据管道采样模型输出和生成冲突感知数据。

Result: 实验表明BideDPO显著提高了文本成功率（例如+35%）和条件对齐度，在构建的DualAlign基准测试中有效解决了文本与条件之间的冲突，并在COCO数据集上验证了方法的有效性。

Conclusion: BideDPO框架通过解耦偏好优化和自适应平衡策略，成功解决了条件图像生成中的冲突问题，为多约束任务提供了有效的优化方案。

Abstract: Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.

</details>


### [237] [Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection](https://arxiv.org/abs/2511.19274)
*Mingyang Chen,Jiawei Du,Bo Huang,Yi Wang,Xiaobo Zhang,Wei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的新核心集选择方法，通过部分反向去噪的重构偏差来估计数据似然，替代传统启发式评分方法，在ImageNet上仅用50%数据即可接近全数据训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法主要依赖训练动态或模型不确定性等启发式评分信号，缺乏对数据似然的显式建模，可能无法捕捉支撑有效模型训练的关键分布结构。

Method: 利用扩散模型通过部分反向去噪诱导的重构偏差来估计数据似然，基于马尔可夫扩散过程的证据下界建立重构误差与数据似然的形式化联系，并提出信息论方法确定最优重构时间步。

Result: 在ImageNet上的广泛实验表明，重构偏差提供了有效的评分标准，在不同选择比率下持续优于现有基线方法，仅用50%数据即可接近全数据训练效果。

Conclusion: 基于似然信息的评分方法在数据选择中揭示了有价值的见解，阐明了数据分布特征与模型学习偏好之间的相互作用。

Abstract: Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.

</details>


### [238] [ReMatch: Boosting Representation through Matching for Multimodal Retrieval](https://arxiv.org/abs/2511.19278)
*Qianying Liu,Xiao Liang,Zhiqiang Zhang,Yibo Chen,Xu Tang,Zhongfei Qing,Fengfan Zhou,Yao Hu,Paul Henderson*

Main category: cs.CV

TL;DR: ReMatch是一个利用多模态大语言模型生成能力进行多模态检索的框架，通过端到端训练嵌入MLLM和生成式匹配阶段，结合对比损失和实例级判别监督，实现了在多模态嵌入基准上的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将MLLM视为简单编码器，忽略了其生成特性，未能充分利用其组合推理和世界知识。ReMatch旨在更好地利用MLLM的生成能力来提升多模态检索性能。

Method: 1. 端到端训练嵌入MLLM 2. 聊天式生成匹配阶段，使用相同MLLM自回归判断相关性 3. 多视图输入（原始数据和投影嵌入）4. 实例级判别监督补充对比损失 5. 使用多个可学习token增强输入，生成细粒度上下文嵌入

Result: 在Massive Multimodal Embedding Benchmark上达到新的最先进水平，在五个数据集上表现出特别强的零样本泛化结果，证明了ReMatch的鲁棒性和可迁移性。

Conclusion: ReMatch通过有效利用MLLM的生成能力，结合创新的训练策略，显著提升了多模态检索性能，特别是在零样本场景下表现出优异的泛化能力。

Abstract: We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.

</details>


### [239] [DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting](https://arxiv.org/abs/2511.19294)
*Phurtivilai Patt,Leyang Huang,Yinqiang Zhang,Yang Lei*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D高斯泼溅方法，通过结合稀疏LiDAR数据和单目深度估计来预先密集化3D场景，避免了传统自适应密度控制导致的浮动伪影和资源浪费问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法依赖自适应密度控制，容易产生浮动伪影和资源使用效率低下的问题，需要改进初始化过程。

Method: 采用预先密集化方法，结合稀疏LiDAR数据和RGB图像的单目深度估计，使用ROI感知采样方案优先处理语义和几何重要区域。

Result: 该方法在保持与最先进技术相当结果的同时，显著降低了资源消耗和训练时间，在四个新收集的数据集上验证了有效性。

Conclusion: 提出的预先密集化方法通过绕过自适应密度控制，提高了视觉质量并减少了重叠，在复杂场景中有效保留了感兴趣区域。

Abstract: This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.

</details>


### [240] [Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection](https://arxiv.org/abs/2511.19306)
*Zixuan Wang,Haoran Sun,Jiaming Lu,Wenxuan Wang,Zhongling Huang,Dingwen Zhang,Xuelin Qian,Junwei Han*

Main category: cs.CV

TL;DR: 提出了DGSPNet框架，通过双粒度语义提示（粗粒度文本先验和细粒度个性化语义描述）解决红外小目标检测中的特征表示有限和背景干扰问题，无需依赖人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的方法受限于不准确的文本描述和人工标注依赖，无法有效应对红外小目标检测中的特征表示有限和背景干扰严重的问题。

Method: 设计了双粒度语义提示机制，包括粗粒度文本先验和基于视觉到文本映射的细粒度个性化语义描述；引入了文本引导的通道注意力（TGCA）和空间注意力（TGSA）机制，增强模型对潜在目标的敏感性。

Result: 在三个基准数据集上的广泛实验表明，该方法显著提高了检测精度，达到了最先进的性能水平。

Conclusion: DGSPNet通过语言提示驱动的框架有效解决了红外小目标检测的挑战，实现了无需标注的高性能检测。

Abstract: Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.

</details>


### [241] [Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach](https://arxiv.org/abs/2511.19316)
*Xincheng Wang,Hanchi Sun,Wenjun Sun,Kejun Xue,Wangqiu Zhou,Jianbo Zhang,Wei Sun,Dandan Zhu,Xiongkuo Min,Jun Jia,Zhijun Fang*

Main category: cs.CV

TL;DR: 本文提出了一个用于评估扩散模型数据集水印方法的统一框架，涵盖通用性、可传递性和鲁棒性三个维度。实验表明现有方法在前两方面表现良好，但在真实威胁场景下鲁棒性不足。论文还提出了一种实用的水印去除方法，揭示了当前技术的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型微调技术虽然能复现特定图像集，但也带来了版权和安全风险。数据集水印技术被提出用于确保可追溯性，但缺乏统一的评估框架来系统评估其有效性。

Method: 建立通用威胁模型，提出包含通用性、可传递性和鲁棒性的综合评估框架。通过实验验证现有水印方法的性能，并进一步提出一种实用的水印去除方法来揭示漏洞。

Result: 实验结果显示，现有水印方法在通用性和可传递性方面表现良好，对常见图像处理操作具有一定鲁棒性，但在真实威胁场景下仍然不足。提出的水印去除方法能完全消除数据集水印而不影响微调过程。

Conclusion: 当前数据集水印技术存在显著脆弱性，特别是在真实威胁场景下。论文提出的评估框架和漏洞揭示方法为未来研究指明了关键挑战和改进方向。

Abstract: Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.

</details>


### [242] [SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis](https://arxiv.org/abs/2511.19319)
*Lingwei Dang,Zonghan Li,Juntong Li,Hongwen Zhang,Liang An,Yebin Liu,Qingyao Wu*

Main category: cs.CV

TL;DR: SyncMV4D是首个联合生成同步多视角手-物交互视频和4D运动的模型，通过统一视觉先验、运动动力学和多视角几何，解决了单视角方法的几何失真和3D方法在现实场景中泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于视频的方法主要是单视角的，阻碍了全面的3D几何感知，导致几何失真或不真实的运动模式；而3D HOI方法虽然能生成动态合理的运动，但对高质量3D数据的依赖严重限制了其在现实场景中的泛化能力。

Method: 提出两个核心创新：(1) 多视角联合扩散模型共同生成HOI视频和中间运动；(2) 扩散点对齐器将粗糙中间运动细化为全局对齐的4D度量点轨迹。通过建立闭环、相互增强的循环，在扩散去噪过程中，生成的视频条件化4D运动的细化，而对齐的4D点轨迹被重投影以指导下一步联合生成。

Result: 实验表明，该方法在视觉真实性、运动合理性和多视角一致性方面优于现有最先进方法。

Conclusion: SyncMV4D通过统一视觉、运动和几何信息，成功解决了手-物交互生成中的关键挑战，为动画和机器人应用提供了更实用的解决方案。

Abstract: Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.

</details>


### [243] [SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation](https://arxiv.org/abs/2511.19320)
*Jiaming Zhang,Shengming Cao,Rui Li,Xiaotong Zhao,Yutao Cui,Xinglin Hou,Gangshan Wu,Haolan Chen,Yu Xu,Limin Wang,Kai Ma*

Main category: cs.CV

TL;DR: SteadyDancer是一个基于图像到视频(I2V)范式的框架，通过条件协调机制、协同姿态调制模块和分阶段解耦目标训练管道，解决了人类图像动画中第一帧身份保持和精确运动控制的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决主流参考到视频(R2V)范式在图像到运动绑定过程中忽视时空不对齐问题，导致身份漂移和视觉伪影的挑战。

Method: 1. 条件协调机制协调冲突条件；2. 协同姿态调制模块生成自适应姿态表示；3. 分阶段解耦目标训练管道分层优化模型。

Result: 在外观保真度和运动控制方面达到最先进性能，且训练资源需求显著少于同类方法。

Conclusion: SteadyDancer是首个能够稳健确保第一帧保持的框架，实现了协调一致的动画效果。

Abstract: Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.

</details>


### [244] [POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse](https://arxiv.org/abs/2511.19339)
*Anjie Le,Can Peng,Yuyuan Liu,J. Alison Noble*

Main category: cs.CV

TL;DR: 本文提出POUR方法，通过几何投影在表示层面实现机器遗忘，证明正交投影保持ETF结构，提供封闭形式解和蒸馏变体，在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法主要修改分类器而保留内部表示，导致不完全遗忘。需要将遗忘概念扩展到表示层面，解决遗忘效果、保留保真度和类别分离之间的权衡。

Method: 基于神经坍缩理论，证明正交投影保持ETF结构，提出POUR方法：封闭形式的几何投影(POUR-P)和蒸馏方案下的特征级遗忘变体(POUR-D)。

Result: 在CIFAR-10/100和PathMNIST上的实验表明，POUR在分类级和表示级指标上都优于最先进的遗忘方法，有效实现遗忘同时保留知识。

Conclusion: POUR通过表示层面的几何投影提供可证明最优的遗忘操作，引入RUS量化表示级遗忘和保留保真度，为机器遗忘提供了新的理论框架和实用方法。

Abstract: In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.

</details>


### [245] [Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning](https://arxiv.org/abs/2511.19343)
*Qihan Huang,Haofei Zhang,Rong Wei,Yi Wang,Rui Tang,Mingli Song,Jie Song*

Main category: cs.CV

TL;DR: 本文提出Syn-GRPO方法，通过在线数据生成器合成高质量多样化训练数据，解决MLLM强化学习中数据质量低的问题，在三个视觉感知任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM强化学习方法面临数据质量低的问题，样本无法激发MLLM产生多样化响应，限制了强化学习的探索范围。虽然有些方法尝试通过熵约束缓解此问题，但未从根本上解决。

Method: Syn-GRPO包含数据服务器和GRPO工作流两个组件。数据服务器使用图像生成模型从现有样本合成新样本，采用解耦异步方案实现高生成效率。GRPO工作流向数据服务器提供新图像描述，并利用多样性奖励监督MLLM预测图像描述以合成多样化响应样本。

Result: 在三个视觉感知任务上的实验结果表明，Syn-GRPO大幅提高了数据质量，性能显著优于现有MLLM感知方法，并展现出长期自演化强化学习的良好潜力。

Conclusion: Syn-GRPO通过在线合成高质量多样化训练数据，有效解决了MLLM强化学习中的数据质量问题，为长期自演化强化学习提供了有前景的解决方案。

Abstract: RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.

</details>


### [246] [CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting](https://arxiv.org/abs/2511.19351)
*Abdurahman Ali Mohammed,Catherine Fonder,Ying Wei,Wallapak Tavanapong,Donald S Sakaguchi,Qi Li,Surya K. Mallapragada*

Main category: cs.CV

TL;DR: 本文介绍了一个包含3,023张免疫细胞化学实验图像的大规模细胞计数数据集，包含超过430,000个手动标注的细胞位置，并评估了多种细胞计数方法，包括基于SAM的改进方法SAM-Counter，在测试集上取得了22.12的MAE，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确的细胞计数在生物医学研究和临床应用中至关重要，但手动计数耗时且易出错，而现有数据集通常规模较小（少于500张图像），限制了深度学习模型的训练效果。

Method: 构建大规模细胞计数数据集，包含高细胞密度、重叠细胞、形态多样性和长尾分布等挑战；评估回归方法、人群计数和细胞计数三类现有方法；提出基于Segment Anything Model (SAM)的密度图适应方法SAM-Counter。

Result: SAM-Counter在测试集上取得了22.12的MAE，优于第二佳方法的27.46 MAE；数据集包含3,023张图像，覆盖细胞数量从10到2,126个的广泛范围。

Conclusion: 该数据集和基准测试框架为自动化细胞计数的进展提供了价值，并为未来研究开发奠定了坚实基础。

Abstract: Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.

</details>


### [247] [DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation](https://arxiv.org/abs/2511.19365)
*Zehong Ma,Longhui Wei,Shuai Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: DeCo提出了一种频率解耦的像素扩散框架，通过将高频细节生成和低频语义建模分离，使用轻量级像素解码器生成高频细节，让DiT专注于低频语义，实现了更高效的像素扩散。


<details>
  <summary>Details</summary>
Motivation: 现有像素扩散模型在单一扩散变换器中同时建模高频信号和低频语义，导致训练和推理速度缓慢。为了追求更高效的像素扩散范式，需要解耦高频和低频组件的生成。

Method: 提出频率解耦像素扩散框架，使用轻量级像素解码器生成高频细节，DiT专注于低频语义建模。同时引入频率感知流匹配损失，强调视觉显著频率并抑制不显著频率。

Result: 在ImageNet上达到FID 1.62(256x256)和2.22(512x512)，在像素扩散模型中表现优异，缩小了与潜在扩散方法的差距。预训练文本到图像模型在GenEval上获得0.86的领先总体分数。

Conclusion: DeCo通过频率解耦实现了高效的像素扩散，在保持高质量生成的同时显著提升了效率，为像素扩散模型提供了新的发展方向。

Abstract: Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.

</details>


### [248] [UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval](https://arxiv.org/abs/2511.19380)
*Maroun Ayli,Youssef Bakouny,Tushar Sharma,Nader Jalloul,Hani Seifeddine,Rima Kilany*

Main category: cs.CV

TL;DR: 提出一种基于图结构的UI表示方法，将UI截图转换为编码层次关系和空间布局的属性图，通过对比图自编码器学习多级相似性嵌入，并在UISearch框架中实现多模态搜索。


<details>
  <summary>Details</summary>
Motivation: 企业软件拥有数千个UI界面，面临设计一致性、模式发现和合规检查的挑战，现有方法缺乏对UI组成结构属性的显式建模。

Method: 使用图表示将UI截图转换为属性图，采用对比图自编码器学习视觉、结构和语义多级相似性的嵌入表示。

Result: 在20,396个金融软件UI上，UISearch达到0.92的Top-5准确率，中位延迟47.5ms，可扩展到2万+屏幕，结构嵌入比最先进视觉编码器具有更好区分能力。

Conclusion: 该方法在UI表示表达能力上取得根本性进展，混合索引架构支持复杂查询和细粒度UI区分，这是纯视觉方法无法实现的。

Abstract: Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.

</details>


### [249] [BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation](https://arxiv.org/abs/2511.19394)
*Rachit Saluja,Asli Cihangir,Ruining Deng,Johannes C. Paetzold,Fengbei Liu,Mert R. Sabuncu*

Main category: cs.CV

TL;DR: 提出BackSplit方法，通过细粒度标注背景类别来提升小病灶分割性能，无需增加推理成本


<details>
  <summary>Details</summary>
Motivation: 传统病灶分割将所有非病灶像素归为单一背景类别，忽略了丰富的解剖学背景信息，而背景实际上由多种组织和器官组成

Method: 使用细粒度标签对背景类别进行细分（BackSplit），通过信息论证明该方法能增加Fisher信息量，提供更紧的渐近边界和更稳定的优化

Result: 在多个数据集和架构上的实验表明，BackSplit能持续提升小病灶分割性能，即使辅助标签是通过预训练分割模型自动生成的

Conclusion: BackSplit是一种简单而强大的范式，通过细粒度背景建模显著提升小病灶分割性能，具有鲁棒性、简单性和广泛适用性

Abstract: Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single "background" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.
  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.

</details>


### [250] [In-Video Instructions: Visual Signals as Generative Control](https://arxiv.org/abs/2511.19401)
*Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为"视频内指令"的新范式，通过将用户指导直接编码到视觉域中（如叠加文本、箭头或轨迹），实现可控的图像到视频生成，相比基于文本提示的方法能提供更明确、空间感知的指令。


<details>
  <summary>Details</summary>
Motivation: 探索大规模视频生成模型是否能够利用当前观察中的逻辑和物理线索，通过视觉信号作为指令来实现可控的图像到视频生成，克服文本提示的全局性和粗糙性限制。

Method: 提出In-Video Instruction范式，将用户指导直接嵌入到视觉域中，使用叠加文本、箭头或轨迹等元素，为不同对象分配明确的指令，实现空间感知的对应关系。

Result: 在三个最先进的生成器（Veo 3.1、Kling 2.5和Wan 2.2）上的广泛实验表明，视频模型能够可靠地解释和执行这种视觉嵌入指令，特别是在复杂的多对象场景中。

Conclusion: 视频生成模型能够有效解释和执行视觉嵌入指令，In-Video Instruction范式为可控图像到视频生成提供了一种有效的方法，特别适用于多对象场景。

Abstract: Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.

</details>


### [251] [Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens](https://arxiv.org/abs/2511.19418)
*Yiming Qin,Bomin Wei,Jiaxin Ge,Konstantinos Kallidromitis,Stephanie Fu,Trevor Darrell,Xudong Wang*

Main category: cs.CV

TL;DR: Chain-of-Visual-Thought (COVT)框架让视觉语言模型能够通过连续视觉token进行推理，解决了传统VLMs在密集视觉感知方面的局限性，在多个基准测试中提升性能3%-16%。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在语言推理方面表现出色，但在需要密集视觉感知的任务（如空间推理和几何感知）上表现不佳，主要原因是缺乏捕捉空间维度密集视觉信息的机制。

Method: 提出COVT框架，使用约20个连续视觉token编码丰富的感知线索，从轻量级视觉专家中提取知识，捕捉2D外观、3D几何、空间布局和边缘结构等互补属性。训练时自回归预测视觉token来重建密集监督信号，推理时直接在连续视觉token空间中进行推理。

Result: 在超过十个不同的感知基准测试中，将COVT集成到Qwen2.5-VL和LLaVA等强VLMs中，性能一致提升3%到16%，证明紧凑的连续视觉思维能够实现更精确、更基础和更可解释的多模态智能。

Conclusion: COVT框架通过引入连续视觉token，使VLMs能够在视觉空间中进行推理，显著提升了模型的感知能力和多模态推理性能，为更精确和可解释的多模态智能提供了有效解决方案。

Abstract: Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.

</details>


### [252] [SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation](https://arxiv.org/abs/2511.19425)
*Tianrun Chen,Runlong Cao,Xinda Yu,Lanyun Zhu,Chaotao Ding,Deyi Ji,Cheng Chen,Qi Zhu,Chunyan Xu,Papa Mao,Ying Zang*

Main category: cs.CV

TL;DR: 本文提出了SAM3-Adapter，这是首个专为SAM3设计的适配器框架，通过在SAM3基础上集成适配器，显著提升了在细粒度、低层次分割任务上的性能，包括医学图像分割、伪装物体检测和阴影检测等。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM及其后继模型在图像分割领域取得了显著进展，但在细粒度、低层次分割任务上仍存在局限。为了解决这些问题，作者基于新发布的SAM3模型开发了专门的适配器框架。

Method: 提出了SAM3-Adapter适配器框架，采用模块化和可组合的设计理念，在SAM3基础上集成适配器来增强其分割能力，同时减少计算开销。

Result: SAM3-Adapter在多个下游任务上超越了SAM和SAM2解决方案，建立了新的最先进结果，特别是在医学成像、伪装物体分割和阴影检测等任务上表现出色。

Conclusion: SAM3-Adapter提供了更强的泛化能力、更丰富的任务适应性和显著改进的分割精度，有望成为未来研究和实际分割应用的基础。

Abstract: The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.

</details>


### [253] [Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction](https://arxiv.org/abs/2511.19426)
*Yun Zhou,Yaoting Wang,Guangquan Jie,Jinyu Liu,Henghui Ding*

Main category: cs.CV

TL;DR: Ref-SAM3D是SAM3D的扩展版本，通过引入文本描述作为高级先验，实现了从单张RGB图像进行文本引导的3D重建，解决了SAM3D无法根据文本描述重建特定对象的问题。


<details>
  <summary>Details</summary>
Motivation: SAM3D虽然具有强大的3D重建能力，但无法根据文本描述重建特定对象，这在3D编辑、游戏开发和虚拟环境等实际应用中是一个关键限制。

Method: 在SAM3D基础上引入文本描述作为高级先验，实现文本引导的3D重建，仅需自然语言和单张2D视图即可工作。

Result: Ref-SAM3D在零样本重建任务中表现出竞争力和高保真度，有效弥合了2D视觉线索与3D几何理解之间的差距。

Conclusion: Ref-SAM3D为参考引导的3D重建提供了一个更灵活和易用的范式，代码已开源。

Abstract: SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.

</details>


### [254] [Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution](https://arxiv.org/abs/2511.19430)
*Dingkang Liang,Cheng Zhang,Xiaopeng Xu,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: ORS3D是一个新的任务调度基准，结合语言理解、3D空间定位和效率优化，要求智能体在3D物理世界中最小化任务完成时间。研究者构建了ORS3D-60K数据集并提出GRANT模型，通过调度令牌机制生成高效的任务调度方案。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在任务规划中往往忽略运筹学知识和3D空间定位，简化了真实世界中的复杂任务调度问题。

Method: 提出ORS3D任务框架，构建包含6万个复合任务的ORS3D-60K数据集，并开发GRANT模型，采用调度令牌机制来生成任务调度和定位动作。

Result: 在ORS3D-60K数据集上的广泛实验验证了GRANT模型在语言理解、3D定位和调度效率方面的有效性。

Conclusion: ORS3D任务和GRANT模型为具身AI中的高效任务调度提供了新的基准和解决方案，强调了运筹学知识与3D空间定位的协同作用。

Abstract: Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT

</details>


### [255] [Cloud4D](https://arxiv.org/abs/2511.19431)
*Jacob Lin,Edward Gryspeerdt,Ronald Clark*

Main category: cs.CV

TL;DR: Cloud4D是一个基于学习的框架，使用同步地面相机重建物理一致的4维云状态，提供25米空间和5秒时间分辨率的3D液态水含量分布，比现有卫星测量提高一个数量级的分辨率。


<details>
  <summary>Details</summary>
Motivation: 当前全球数值天气预报和气候模型大多在千米尺度运行，难以模拟单个云层和极端天气现象，需要更高分辨率的模型和观测数据，但现有仪器难以获取高分辨率真实观测。

Method: 利用同形引导的2D到3D变换器，仅使用同步地面相机推断完整的3D液态水含量分布，通过跟踪3D液态水含量随时间变化来估计水平风矢量。

Result: 在包含六个向上拍摄相机的两个月部署中，系统相对于最先进的卫星测量提供了数量级的时空分辨率改进，同时相对于共置雷达测量保持个位数相对误差（<10%）。

Conclusion: Cloud4D是首个仅使用同步地面相机重建物理一致4维云状态的学习框架，为高分辨率天气和气候建模提供了新的观测能力。

Abstract: There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.

</details>


### [256] [Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts](https://arxiv.org/abs/2511.19434)
*Yasin Esfandiari,Stefan Bauer,Sebastian U. Stich,Andrea Dittadi*

Main category: cs.CV

TL;DR: 提出了一种简单的即插即用采样方法，通过在高噪声水平使用图像质量专家塑造全局结构，在低噪声水平切换到似然专家优化像素统计，无需重新训练即可打破扩散模型中似然与质量的权衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中存在感知样本质量与数据似然之间的权衡：强调高噪声去噪步骤的训练目标能产生真实图像但似然较差，而似然导向的训练过度加权低噪声步骤会损害视觉保真度。

Method: 结合两个预训练扩散专家，在去噪轨迹中切换使用：高噪声水平应用图像质量专家塑造全局结构，低噪声水平切换到似然专家优化像素统计，仅需选择中间切换步骤。

Result: 在CIFAR-10和ImageNet32上，合并模型始终匹配或优于其基础组件，相对于每个单独专家，改善或保持了似然和样本质量。

Conclusion: 在噪声水平间进行专家切换是打破图像扩散模型中似然-质量权衡的有效方法。

Abstract: Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.

</details>


### [257] [Are Image-to-Video Models Good Zero-Shot Image Editors?](https://arxiv.org/abs/2511.19435)
*Zechuan Zhang,Zhenyuan Chen,Zongxin Yang,Yi Yang*

Main category: cs.CV

TL;DR: IF-Edit是一个无需调优的框架，将预训练的图像到视频扩散模型重新用于指令驱动的图像编辑，解决了提示不对齐、冗余时间潜在变量和模糊后期帧三个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模视频扩散模型展现出强大的世界模拟和时间推理能力，但作为零样本图像编辑器的应用尚未充分探索。研究旨在利用这些模型进行指令驱动的图像编辑。

Method: 包括：(1)思维链提示增强模块，将静态编辑指令转换为时间基础推理提示；(2)时间潜在变量丢弃策略，在专家切换点后压缩帧潜在变量；(3)自一致后精炼步骤，使用短静态视频轨迹锐化后期帧。

Result: 在四个公共基准测试上的实验表明，IF-Edit在推理中心任务上表现强劲，同时在通用编辑任务上保持竞争力。

Conclusion: 研究提供了视频扩散模型作为图像编辑器的系统视角，并突出了统一视频-图像生成推理的简单方法。

Abstract: Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.

</details>


### [258] [LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context](https://arxiv.org/abs/2511.19437)
*Jingzhi Bao,Hongze Chen,Lingting Zhu,Chenyu Liu,Runze Zhang,Keyang Luo,Zeyu Hu,Weikai Chen,Yingda Yin,Xin Wang,Zehong Lin,Jun Zhang,Xiaoguang Han*

Main category: cs.CV

TL;DR: LumiTex是一个端到端的PBR纹理生成框架，通过多分支生成方案、光照感知材料注意力机制和几何引导修复模块，解决了材料分解和纹理补全的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有PBR纹理生成方法无法解决两个基本挑战：1)在有限光照线索下从图像提示进行材料分解；2)无缝且视角一致的纹理补全。

Method: 包含三个关键组件：1)多分支生成方案，在共享光照先验下解耦反照率和金属粗糙度；2)光照感知材料注意力机制，将光照上下文注入解码过程；3)基于大视角合成模型的几何引导修复模块，确保无缝UV补全。

Result: 大量实验表明，LumiTex在纹理质量方面达到了最先进的性能，超越了现有的开源和商业方法。

Conclusion: LumiTex通过创新的多组件框架成功解决了PBR纹理生成中的材料分解和纹理补全问题，实现了高质量的物理渲染纹理生成。

Abstract: Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [259] [AUTOSAR AP and ROS 2 Collaboration Framework](https://arxiv.org/abs/2511.17540)
*Ryudai Iwakami,Bo Peng,Hiroyuki Hanyu,Tasuku Ishigooka,Takuya Azumi*

Main category: cs.RO

TL;DR: 本文提出了一个AUTOSAR AP和ROS 2之间的协作框架，通过DDS桥接器解决两者通信协议差异，实现无缝交互，并通过自动生成配置文件提高可用性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶研究领域存在研究平台（ROS 2）与开发平台（AUTOSAR AP）之间的鸿沟，阻碍了快速商业化。AUTOSAR AP的许可限制和工具实现挑战限制了其在研究中的使用。

Method: 提出一个协作框架，使用DDS作为通信中间件，构建桥接器来解决AUTOSAR AP（使用SOME/IP）和ROS 2之间的协议差异。

Result: 通过实证分析验证了桥接器的功能和性能，证明其在转换时间和与ROS 2工具集成方面的高效性。

Conclusion: 该协作框架成功弥合了AUTOSAR AP和ROS 2之间的平台差异，为自动驾驶研究到商业化的快速转化提供了可行方案。

Abstract: The field of autonomous vehicle research is advancing rapidly, necessitating platforms that meet real-time performance, safety, and security requirements for practical deployment. AUTOSAR Adaptive Platform (AUTOSAR AP) is widely adopted in development to meet these criteria; however, licensing constraints and tool implementation challenges limit its use in research. Conversely, Robot Operating System 2 (ROS 2) is predominantly used in research within the autonomous driving domain, leading to a disparity between research and development platforms that hinders swift commercialization. This paper proposes a collaboration framework that enables AUTOSAR AP and ROS 2 to communicate with each other using a Data Distribution Service for Real-Time Systems (DDS). In contrast, AUTOSAR AP uses Scalable service-Oriented Middleware over IP (SOME/IP) for communication. The proposed framework bridges these protocol differences, ensuring seamless interaction between the two platforms. We validate the functionality and performance of our bridge converter through empirical analysis, demonstrating its efficiency in conversion time and ease of integration with ROS 2 tools. Furthermore, the availability of the proposed collaboration framework is improved by automatically generating a configuration file for the proposed bridge converter.

</details>


### [260] [Translating Cultural Choreography from Humanoid Forms to Robotic Arm](https://arxiv.org/abs/2511.17603)
*Chelsea-Xi Chen,Zhe Zhang,Aven-Le Zhou*

Main category: cs.RO

TL;DR: 该研究开发了ROPERA系统，通过符号姿势转移和关节空间兼容符号来保持机器人手臂舞蹈的文化语义保真度，并在昆曲《牡丹亭》场景中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 机器人手臂编舞常常在重现轨迹时丢失文化语义，本研究旨在探索是否可以通过符号姿势转移和关节空间兼容符号来保持文化语义保真度，并实现跨形态的可移植性。

Method: 实现ROPERA三阶段流程：编码文化编码姿势、组合符号序列、解码为伺服命令。使用昆曲《牡丹亭》场景进行评估，包括基于语料库的姿势选择、符号记谱、直接关节角度执行，以及带有光绘和服装色彩信息的视觉层。

Result: 结果表明，系统能够以预期时间执行可重现的动作，专家和观众报告显示具有文化可读性。

Conclusion: 该研究指向非人类中心主义的文化保护和可移植的创作工作流程。未来工作将设计舞蹈感知的过渡配置文件，将符号扩展到包含触觉、音乐和空间线索的移动，并测试跨平台的可移植性。

Abstract: Robotic arm choreography often reproduces trajectories while missing cultural semantics. This study examines whether symbolic posture transfer with joint space compatible notation can preserve semantic fidelity on a six-degree-of-freedom arm and remain portable across morphologies. We implement ROPERA, a three-stage pipeline for encoding culturally codified postures, composing symbolic sequences, and decoding to servo commands. A scene from Kunqu opera, \textit{The Peony Pavilion}, serves as the material for evaluation. The procedure includes corpus-based posture selection, symbolic scoring, direct joint angle execution, and a visual layer with light painting and costume-informed colors. Results indicate reproducible execution with intended timing and cultural legibility reported by experts and audiences. The study points to non-anthropocentric cultural preservation and portable authoring workflows. Future work will design dance-informed transition profiles, extend the notation to locomotion with haptic, musical, and spatial cues, and test portability across platforms.

</details>


### [261] [Robot joint characterisation and control using a magneto-optical rotary encoder](https://arxiv.org/abs/2511.17608)
*Yunlong Guo,John Canning,Zenon Chaczko,Gang-Ding Peng*

Main category: cs.RO

TL;DR: 提出了一种用于机器人旋转关节表征的紧凑型磁光旋转编码器，采用双通配置和旋转非均匀磁体，实现360°连续旋转跟踪，角分辨率0.3°，旋转速率135°/s至370°/s。


<details>
  <summary>Details</summary>
Motivation: 为机器人旋转关节提供低成本、可靠的替代方案，替代传统旋转编码器，同时保持竞争力性能。

Method: 利用磁场诱导的光学衰减效应，在双通配置中使用旋转非均匀磁体和光学环行器，工作在反射模式下。

Result: 编码器能够跟踪360°连续旋转，旋转速率范围135°/s至370°/s，角分辨率达到0.3°。

Conclusion: 该系统为传统机器人旋转编码器提供了低成本、可靠的替代方案，同时保持了竞争性的性能表现。

Abstract: A robust and compact magneto-optical rotary encoder for the characterisation of robotic rotary joints is demonstrated. The system employs magnetic field-induced optical attenuation in a double-pass configuration using rotating nonuniform magnets around an optical circulator operating in reflection. The encoder tracks continuous 360° rotation with rotation sweep rates from ν = 135 °/s to ν = 370 °/s, and an angular resolution of Δθ = 0.3°. This offers a low-cost and reliable alternative to conventional robot rotation encoders while maintaining competitive performance.

</details>


### [262] [Vision-Guided Optic Flow Navigation for Small Lunar Missions](https://arxiv.org/abs/2511.17720)
*Sean Cowan,Pietro Fanti,Leon B. S. Williams,Chit Hong Yam,Kaneyasu Asakuma,Yuichiro Nada,Dario Izzo*

Main category: cs.RO

TL;DR: 提出一种基于光流和测距仪深度估计的运动场反演框架，用于月球着陆过程中的自主导航，适用于计算资源受限的小型月球着陆器。


<details>
  <summary>Details</summary>
Motivation: 解决私人月球任务在严格的质量、功率和计算资源限制下实现鲁棒自主导航的挑战。

Method: 扩展经典光流公式，结合针对月球/行星接近、下降和着陆几何形状的深度建模策略（平面和球形地形近似），使用激光测距仪参数化，通过最小二乘法框架进行运动场反演，采用金字塔Lucas-Kanade算法提取稀疏光流特征。

Result: 在月球南极复杂地形上使用合成图像验证，速度估计准确，复杂地形误差低于10%，典型地形误差约为1%，性能适合实时应用。

Conclusion: 该框架有望为小型月球任务实现鲁棒、轻量级的机载导航。

Abstract: Private lunar missions are faced with the challenge of robust autonomous navigation while operating under stringent constraints on mass, power, and computational resources. This work proposes a motion-field inversion framework that uses optical flow and rangefinder-based depth estimation as a lightweight CPU-based solution for egomotion estimation during lunar descent. We extend classical optical flow formulations by integrating them with depth modeling strategies tailored to the geometry for lunar/planetary approach, descent, and landing, specifically, planar and spherical terrain approximations parameterized by a laser rangefinder. Motion field inversion is performed through a least-squares framework, using sparse optical flow features extracted via the pyramidal Lucas-Kanade algorithm. We verify our approach using synthetically generated lunar images over the challenging terrain of the lunar south pole, using CPU budgets compatible with small lunar landers. The results demonstrate accurate velocity estimation from approach to landing, with sub-10% error for complex terrain and on the order of 1% for more typical terrain, as well as performances suitable for real-time applications. This framework shows promise for enabling robust, lightweight on-board navigation for small lunar missions.

</details>


### [263] [LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation](https://arxiv.org/abs/2511.17765)
*Darren Chiu,Zhehui Huang,Ruohai Ge,Gaurav S. Sukhatme*

Main category: cs.RO

TL;DR: LEARN是一个轻量级的两阶段安全引导强化学习框架，用于多无人机在复杂空间中的导航，结合低分辨率ToF传感器和紧凑的注意力RL策略，在资源受限的纳米无人机平台上实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 纳米无人机团队具有高灵活性，但受限于机载传感、通信和计算能力，现有基于高分辨率视觉或计算密集型规划器的方法不适用于这些平台。

Method: 采用两阶段安全引导强化学习框架，结合低分辨率ToF传感器、简单运动规划器和紧凑的注意力RL策略。

Result: 在仿真中性能优于两种最先进规划器10%，资源消耗显著减少；在6架Crazyflie四旋翼上实现全机载飞行，在室内外环境中速度达2.0 m/s，可穿越0.2米间隙。

Conclusion: LEARN框架证明了在资源受限的纳米无人机平台上实现高效多无人机导航的可行性，为实际应用提供了轻量级解决方案。

Abstract: Nano-UAV teams offer great agility yet face severe navigation challenges due to constrained onboard sensing, communication, and computation. Existing approaches rely on high-resolution vision or compute-intensive planners, rendering them infeasible for these platforms. We introduce LEARN, a lightweight, two-stage safety-guided reinforcement learning (RL) framework for multi-UAV navigation in cluttered spaces. Our system combines low-resolution Time-of-Flight (ToF) sensors and a simple motion planner with a compact, attention-based RL policy. In simulation, LEARN outperforms two state-of-the-art planners by $10\%$ while using substantially fewer resources. We demonstrate LEARN's viability on six Crazyflie quadrotors, achieving fully onboard flight in diverse indoor and outdoor environments at speeds up to $2.0 m/s$ and traversing $0.2 m$ gaps.

</details>


### [264] [Learning Diffusion Policies for Robotic Manipulation of Timber Joinery under Fabrication Uncertainty](https://arxiv.org/abs/2511.17774)
*Salma Mozaffari,Daniel Ruan,William van den Bogert,Nima Fazeli,Sigrid Adriaenssens,Arash Adel*

Main category: cs.RO

TL;DR: 本文研究了扩散策略学习在建筑规模接触敏感机器人装配中的性能和鲁棒性，以木工榫卯接头为案例，展示了在高达10毫米扰动下75%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 建筑不确定性（如制造误差和材料缺陷）对接触密集型机器人操作构成重大挑战，阻碍精确和稳健的装配。

Method: 采用两阶段研究：首先评估策略性能和适用性，其次评估处理制造不确定性的鲁棒性，通过随机扰动榫眼位置来模拟不确定性。

Result: 最佳策略在高达10毫米扰动下实现了75%的总平均成功率，在无扰动情况下达到100%成功率。

Conclusion: 结果表明感觉运动扩散策略有潜力推广到建筑和制造业中各种复杂、接触密集的装配任务，推进不确定性下的机器人建造，促进更安全、更高效的建筑实践。

Abstract: Construction uncertainties such as fabrication inaccuracies and material imperfections pose a significant challenge to contact-rich robotic manipulation by hindering precise and robust assembly. In this paper, we explore the performance and robustness of diffusion policy learning as a promising solution for contact-sensitive robotic assembly at construction scale, using timber mortise and tenon joints as a case study. A two-phase study is conducted: first, to evaluate policy performance and applicability; second, to assess robustness in handling fabrication uncertainties simulated as randomized perturbations to the mortise position. The best-performing policy achieved a total average success rate of 75% with perturbations up to 10 mm, including 100% success in unperturbed cases. The results demonstrate the potential of sensory-motor diffusion policies to generalize to a wide range of complex, contact-rich assembly tasks across construction and manufacturing, advancing robotic construction under uncertainty and contributing to safer, more efficient building practices.

</details>


### [265] [See, Plan, Cut: MPC-Based Autonomous Volumetric Robotic Laser Surgery with OCT Guidance](https://arxiv.org/abs/2511.17777)
*Ravi Prakash,Vincent Y. Wang,Arpit Mishra,Devi Yuliarti,Pei Zhong,Ryan P. McNabb,Patrick J. Codd,Leila J. Bridgeman*

Main category: cs.RO

TL;DR: RATS是一个智能光学机械平台，整合了RGB-D成像、OCT和手术激光，通过多阶段校准实现高精度激光校准，使用超高斯激光-组织交互模型和基于采样的模型预测控制框架，在离体猪组织上实现了0.842mm的均方根误差和64.8%的交并比提升。


<details>
  <summary>Details</summary>
Motivation: 现有机器人激光系统缺乏体积规划和术中反馈能力，限制了其在软组织切除手术中的精确应用。

Method: 整合宏观RGB-D成像、微观OCT和光纤耦合手术激光，采用多阶段校准管道实现OCT到激光校准；使用超高斯激光-组织交互模型表征消融坑形态；基于采样的模型预测控制框架直接在OCT体素数据上操作。

Result: OCT到激光校准精度达0.161±0.031mm；激光-组织交互模型平均RMSE为0.231±0.121mm；模型预测控制实现0.842mm RMSE，比前馈执行提高64.8%的交并比一致性。

Conclusion: RATS平台能够检测亚表面结构并修改规划目标以保护这些结构，展示了临床可行性，为自主体积软组织切除提供了有效解决方案。

Abstract: Robotic laser systems offer the potential for sub-millimeter, non-contact, high-precision tissue resection, yet existing platforms lack volumetric planning and intraoperative feedback. We present RATS (Robot-Assisted Tissue Surgery), an intelligent opto-mechanical, optical coherence tomography (OCT)-guided robotic platform designed for autonomous volumetric soft tissue resection in surgical applications. RATS integrates macro-scale RGB-D imaging, micro-scale OCT, and a fiber-coupled surgical laser, calibrated through a novel multistage alignment pipeline that achieves OCT-to-laser calibration accuracy of 0.161+-0.031mm on tissue phantoms and ex vivo porcine tissue. A super-Gaussian laser-tissue interaction (LTI) model characterizes ablation crater morphology with an average RMSE of 0.231+-0.121mm, outperforming Gaussian baselines. A sampling-based model predictive control (MPC) framework operates directly on OCT voxel data to generate constraint-aware resection trajectories with closed-loop feedback, achieving 0.842mm RMSE and improving intersection-over-union agreement by 64.8% compared to feedforward execution. With OCT, RATS detects subsurface structures and modifies the planner's objective to preserve them, demonstrating clinical feasibility.

</details>


### [266] [SAFE-SMART: Safety Analysis and Formal Evaluation using STL Metrics for Autonomous RoboTs](https://arxiv.org/abs/2511.17781)
*Kristy Sakano,Jianyu An,Dinesh Manocha,Huan Xu*

Main category: cs.RO

TL;DR: 提出了一种基于监管驱动的后验安全评估方法，用于评估基于学习的黑盒自主移动机器人，确保其符合人类定义的安全规则。该方法通过信号时序逻辑规范验证机器人轨迹，使用TRV和LRV量化安全指标，并指导针对性重新训练。


<details>
  <summary>Details</summary>
Motivation: 确保学习型黑盒自主移动机器人能够持续符合不断演进的人类安全规则，解决传统方法难以评估黑盒系统安全性的问题。

Method: 采用迭代工作流程：将人类安全需求转换为STL规范，外部验证黑盒模型的轨迹合规性，计算TRV和LRV安全指标，基于指标进行针对性重新训练。

Result: 在虚拟驾驶场景中：遵守速度限制的轨迹增加177%，减少越野驾驶的轨迹增加1138%，按时到达目标的轨迹增加16%。在自主导航场景中：避免急转弯的轨迹增加300%，按时到达目标的轨迹增加200%，减少接近障碍物时间的轨迹增加49%。在真实TurtleBot3机器人上验证了改进的障碍物导航能力。

Conclusion: 该方法能够有效评估和改善黑盒自主机器人的安全性，在不同应用场景中均实现了统计显著的性能提升，并在真实环境中验证了有效性。

Abstract: We present a novel, regulator-driven approach for post hoc safety evaluation of learning-based, black-box autonomous mobile robots, ensuring ongoing compliance with evolving, human-defined safety rules. In our iterative workflow, human safety requirements are translated by regulators into Signal Temporal Logic (STL) specifications. Rollout traces from the black-box model are externally verified for compliance, yielding quantitative safety metrics, Total Robustness Value (TRV) and Largest Robustness Value (LRV), which measure average and worst-case specification adherence. These metrics inform targeted retraining and iterative improvement by model designers. We apply our method across two different applications: a virtual driving scenario and an autonomous mobile robot navigating a complex environment, and observe statistically significant improvements across both scenarios. In the virtual driving scenario, we see a 177% increase in traces adhering to the simulation speed limit, a 1138% increase in traces minimizing off-road driving, and a 16% increase in traces successfully reaching the goal within the time limit. In the autonomous navigation scenario, there is a 300% increase in traces avoiding sharp turns, a 200% increase in traces reaching the goal within the time limit, and a 49% increase in traces minimizing time spent near obstacles. Finally, we validate our approach on a TurtleBot3 robot in the real world, and demonstrate improved obstacle navigation with safety buffers.

</details>


### [267] [MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/abs/2511.17889)
*Ting Huang,Dongjian Li,Rui Yang,Zeyu Zhang,Zida Yang,Hao Tang*

Main category: cs.RO

TL;DR: MobileVLA-R1是一个统一的视觉-语言-动作框架，用于四足机器人的显式推理和连续控制，通过构建大规模思维链数据集和两阶段训练范式，显著提升了推理一致性和控制稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将高级语义推理与低级驱动连接起来，导致在真实世界中存在不稳定的接地和弱泛化问题。

Method: 构建MobileVLA-CoT大规模多粒度思维链数据集，采用监督CoT对齐与GRPO强化学习的两阶段训练范式。

Result: 在VLN和VLA任务上表现优于强基线，提升约5%，在复杂环境中的真实四足机器人部署验证了鲁棒性能。

Conclusion: MobileVLA-R1通过显式推理和连续控制的统一框架，有效解决了四足机器人自然语言指令接地问题，在真实世界中展现出优越性能。

Abstract: Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.

</details>


### [268] [L1 Sample Flow for Efficient Visuomotor Learning](https://arxiv.org/abs/2511.17898)
*Weixi Song,Zhetao Chen,Tao Xu,Xianchao Zeng,Xinyu Zhou,Lixin Yang,Donglin Wang,Cewu Lu,Yong-Lu Li*

Main category: cs.RO

TL;DR: 本文提出L1 Flow方法，将v预测流匹配转换为样本预测的L1训练目标，通过两步采样实现多模态动作生成，将迭代神经函数评估减少到仅两次，同时保持流匹配的优势。


<details>
  <summary>Details</summary>
Motivation: 结合去噪模型的多模态分布捕获能力和L1回归目标的高效性，避免模式崩溃同时实现快速收敛和推理。

Method: 将原始v预测流匹配重新表述为样本预测的L1训练目标，提出两步采样方案：通过单次积分步骤生成次优动作序列，然后通过单次预测重构精确动作序列。

Result: 在MimicGen的8个任务、RoboMimic和PushT Bench的5个任务以及真实场景任务中评估，结果显示该方法在训练效率、推理速度和整体性能方面具有优势。

Conclusion: L1 Flow方法在保持流匹配多模态分布捕获能力的同时，显著提高了训练和推理效率，实现了两种范式的优势结合。

Abstract: Denoising-based models, such as diffusion and flow matching, have been a critical component of robotic manipulation for their strong distribution-fitting and scaling capacity. Concurrently, several works have demonstrated that simple learning objectives, such as L1 regression, can achieve performance comparable to denoising-based methods on certain tasks, while offering faster convergence and inference. In this paper, we focus on how to combine the advantages of these two paradigms: retaining the ability of denoising models to capture multi-modal distributions and avoid mode collapse while achieving the efficiency of the L1 regression objective. To achieve this vision, we reformulate the original v-prediction flow matching and transform it into sample-prediction with the L1 training objective. We empirically show that the multi-modality can be expressed via a single ODE step. Thus, we propose \textbf{L1 Flow}, a two-step sampling schedule that generates a suboptimal action sequence via a single integration step and then reconstructs the precise action sequence through a single prediction. The proposed method largely retains the advantages of flow matching while reducing the iterative neural function evaluations to merely two and mitigating the potential performance degradation associated with direct sample regression. We evaluate our method with varying baselines and benchmarks, including 8 tasks in MimicGen, 5 tasks in RoboMimic \& PushT Bench, and one task in the real-world scenario. The results show the advantages of the proposed method with regard to training efficiency, inference speed, and overall performance. \href{https://song-wx.github.io/l1flow.github.io/}{Project Website.}

</details>


### [269] [Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game](https://arxiv.org/abs/2511.17925)
*Jeonghwan Kim,Wontaek Kim,Yidan Lu,Jin Cheng,Fatemeh Zargarbashi,Zicheng Zeng,Zekun Qi,Zhiyang Dou,Nitish Sontakke,Donghoon Baek,Sehoon Ha,Tianyu Li*

Main category: cs.RO

TL;DR: Switch-JustDance是一个低成本、可复现的机器人全身控制基准测试平台，利用任天堂Switch的Just Dance游戏来评估机器人性能，通过游戏内置评分系统进行控制器性能评估。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人全身控制评估方法依赖预收集的人类运动数据集或基于仿真的实验，存在可复现性差、忽略硬件因素、难以进行公平的人机比较等问题。

Method: 使用Just Dance游戏作为平台，通过流媒体、运动重建和运动重定向模块将游戏中的编舞转换为机器人可执行的运动，并利用游戏内置评分系统评估控制器性能。

Result: 验证了Just Dance平台的评估特性，证明其提供一致且可解释的性能度量；在硬件上对三种最先进的人形机器人全身控制器进行了基准测试。

Conclusion: Switch-JustDance是一个适合评估具身AI的基准测试工具，能够提供对机器人控制器相对优势和局限性的深入见解。

Abstract: Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.

</details>


### [270] [RoboArmGS: High-Quality Robotic Arm Splatting via Bézier Curve Refinement](https://arxiv.org/abs/2511.17961)
*Hao Wang,Xiaobao Wei,Ying Li,Qingpo Wuwu,Dongli Wu,Jiajun Cao,Ming Lu,Wenzhao Zheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出RoboArmGS方法，通过可学习的贝塞尔曲线修正URDF绑定运动中的误差，提高机械臂真实运动建模和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法将静态3D高斯朴素绑定到URDF链接上，强制其被动跟随URDF绑定运动，但真实机械臂运动存在噪声，理想化的URDF运动无法准确建模，导致3D高斯渲染出现严重伪影。

Method: 提出RoboArmGS混合表示方法，使用可学习的贝塞尔曲线运动修正器来细化URDF绑定运动，通过修正每个关节的残差来解决真实运动与URDF运动之间的不匹配问题。

Result: 在RoboArm4D数据集上评估，RoboArmGS在真实运动建模和渲染质量方面达到最先进性能。

Conclusion: RoboArmGS能够学习更准确的真实世界运动，同时实现跨机械臂部件的3D高斯连贯绑定，为构建高质量数字资产提供有效解决方案。

Abstract: Building high-quality digital assets of robotic arms is crucial yet challenging for the Real2Sim2Real pipeline. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, real-world arm motion is noisy, and the idealized URDF-rigged motion cannot accurately model it, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable Bézier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable Bézier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.

</details>


### [271] [Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation](https://arxiv.org/abs/2511.17992)
*Chungeng Tian,Fenghua He,Ning Hao*

Main category: cs.RO

TL;DR: 本文提出了一个名为不可观测子空间演化(USE)的新分析框架，系统性地描述了不可观测子空间在整个估计流程中的演化过程，并通过不可观测子空间对齐(USA)方法解决了VINS中的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 视觉惯性导航系统(VINS)中的不一致性是一个长期存在的基本挑战。现有研究主要将不一致性归因于可观测性不匹配，但这些分析基于简化的理论框架，未能涵盖实际VINS估计器中的非标准估计步骤，如MSCKF校正和延迟初始化。

Method: 提出了不可观测子空间演化(USE)分析框架，通过显式跟踪不可观测子空间评估点的变化来系统表征其在整个估计流程中的演化。基于此提出了不可观测子空间对齐(USA)解决方案，包括基于变换和基于重评估的两种方法。

Result: 分析揭示了某些估计步骤引起的可观测性错位是可观测性不匹配的前因。提出的USA方法通过选择性地干预那些引起错位的估计步骤来消除不一致性，提供了准确且计算轻量的解决方案。

Conclusion: 广泛的仿真和真实世界实验验证了所提方法的有效性，解决了VINS估计器在精度、一致性和实现复杂度之间的权衡问题。

Abstract: The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge. While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction. Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators. Furthermore, the lack of a comprehensive understanding of how inconsistency dynamically emerges across estimation steps has hindered the development of precise and efficient solutions. As a result, current approaches often face a trade-off between estimator accuracy, consistency, and implementation complexity. To address these limitations, this paper proposes a novel analysis framework termed Unobservable Subspace Evolution (USE), which systematically characterizes how the unobservable subspace evolves throughout the entire estimation pipeline by explicitly tracking changes in its evaluation points. This perspective sheds new light on how individual estimation steps contribute to inconsistency. Our analysis reveals that observability misalignment induced by certain steps is the antecedent of observability mismatch. Guided by this insight, we propose a simple yet effective solution paradigm, Unobservable Subspace Alignment (USA), which eliminates inconsistency by selectively intervening only in those estimation steps that induce misalignment. We design two USA methods: transformation-based and re-evaluation-based, both offering accurate and computationally lightweight solutions. Extensive simulations and real-world experiments validate the effectiveness of the proposed methods.

</details>


### [272] [Anti-Jamming based on Null-Steering Antennas and Intelligent UAV Swarm Behavior](https://arxiv.org/abs/2511.18086)
*Miguel Lourenço,António Grilo*

Main category: cs.RO

TL;DR: 本文研究了无人机群在干扰环境下如何保持通信和任务效率，提出了结合遗传算法、监督学习和强化学习的统一优化框架，使用零陷天线技术增强抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 无人机群依赖无线通信，容易受到干扰攻击，这会破坏协调和任务成功。本文旨在探索无人机群是否能在干扰环境下有效保持通信和任务效率。

Method: 提出了统一的优化框架，结合遗传算法、监督学习和强化学习。采用分时段任务模型，支持动态路径规划、天线定向和编队调整。使用零陷天线技术将天线零点指向干扰源。

Result: 遗传算法实现了稳定无碰撞轨迹但计算成本高；监督学习能复制遗传算法配置但在动态约束下泛化能力不足；强化学习表现出适应性和实时决策能力，通信稳定且计算需求低。自适应运动模型通过旋转机制实现了任意方向的无人机运动。

Conclusion: 配备零陷天线并由智能优化算法引导的无人机群能有效缓解干扰，保持通信稳定性、编队凝聚力和碰撞安全性。该框架为弹性群通信系统的未来研究建立了统一、灵活和可重复的基础。

Abstract: Unmanned Aerial Vehicle (UAV) swarms represent a key advancement in autonomous systems, enabling coordinated missions through inter-UAV communication. However, their reliance on wireless links makes them vulnerable to jamming, which can disrupt coordination and mission success. This work investigates whether a UAV swarm can effectively overcome jamming while maintaining communication and mission efficiency.
  To address this, a unified optimization framework combining Genetic Algorithms (GA), Supervised Learning (SL), and Reinforcement Learning (RL) is proposed. The mission model, structured into epochs and timeslots, allows dynamic path planning, antenna orientation, and swarm formation while progressively enforcing collision rules. Null-steering antennas enhance resilience by directing antenna nulls toward interference sources.
  Results show that the GA achieved stable, collision-free trajectories but with high computational cost. SL models replicated GA-based configurations but struggled to generalize under dynamic or constrained settings. RL, trained via Proximal Policy Optimization (PPO), demonstrated adaptability and real-time decision-making with consistent communication and lower computational demand. Additionally, the Adaptive Movement Model generalized UAV motion to arbitrary directions through a rotation-based mechanism, validating the scalability of the proposed system.
  Overall, UAV swarms equipped with null-steering antennas and guided by intelligent optimization algorithms effectively mitigate jamming while maintaining communication stability, formation cohesion, and collision safety. The proposed framework establishes a unified, flexible, and reproducible basis for future research on resilient swarm communication systems.

</details>


### [273] [A Unified Multi-Dynamics Framework for Perception-Oriented Modeling in Tendon-Driven Continuum Robots](https://arxiv.org/abs/2511.18088)
*Ibrahim Alsarraj,Yuhao Wang,Abdalla Swikir,Cesare Stefanini,Dezhen Song,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: 提出了一个统一的多动力学建模框架，用于肌腱驱动连续体机器人系统，通过电机信号揭示外部交互的机电特征，实现基于内在动力学的感知。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动连续体机器人具有运动冗余和结构顺应性，但感知通常依赖外部传感器，增加了硬件复杂性并限制了可扩展性。

Method: 开发了统一的多动力学建模框架，集成电机电气动力学、电机-卷筒动力学和连续体机器人动力学，通过电机电流和角位移等信号建模外部交互的机电特征。

Result: 模型成功捕获并验证了真实系统的关键物理行为，包括驱动迟滞和运动极限处的自接触。框架应用于环境交互感知，在被动接触检测、主动接触感知和物体尺寸估计等方面都取得了成功。

Conclusion: 该框架为肌腱驱动连续体机器人提供了一种基于物理的方式来解释来自内在电机信号的交互特征，实现了无需外部传感器的感知能力。

Abstract: Tendon-driven continuum robots offer intrinsically safe and contact-rich interactions owing to their kinematic redundancy and structural compliance. However, their perception often depends on external sensors, which increase hardware complexity and limit scalability. This work introduces a unified multi-dynamics modeling framework for tendon-driven continuum robotic systems, exemplified by a spiral-inspired robot named Spirob. The framework integrates motor electrical dynamics, motor-winch dynamics, and continuum robot dynamics into a coherent system model. Within this framework, motor signals such as current and angular displacement are modeled to expose the electromechanical signatures of external interactions, enabling perception grounded in intrinsic dynamics. The model captures and validates key physical behaviors of the real system, including actuation hysteresis and self-contact at motion limits. Building on this foundation, the framework is applied to environmental interaction: first for passive contact detection, verified experimentally against simulation data; then for active contact sensing, where control and perception strategies from simulation are successfully applied to the real robot; and finally for object size estimation, where a policy learned in simulation is directly deployed on hardware. The results demonstrate that the proposed framework provides a physically grounded way to interpret interaction signatures from intrinsic motor signals in tendon-driven continuum robots.

</details>


### [274] [EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation](https://arxiv.org/abs/2511.18112)
*Min Lin,Xiwen Liang,Bingqian Lin,Liu Jingzhi,Zijian Jiao,Kehan Li,Yuhan Ma,Yuecheng Liu,Shen Zhao,Yuzheng Zhuang,Xiaodan Liang*

Main category: cs.RO

TL;DR: EchoVLA是一个具有记忆能力的视觉-语言-动作模型，专门用于长视野移动操作任务，通过场景记忆和情景记忆的协同工作来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型主要局限于短视野、桌面操作任务，缺乏处理长视野移动操作所需的内存和推理能力，其中智能体需要在变化的空间环境中协调导航和操作。

Method: EchoVLA引入了受人类大脑启发的声明性记忆系统，包括维护空间-语义地图的场景记忆和存储任务级经验的情景记忆。通过粗粒度和细粒度注意力融合两种记忆的检索表示，指导移动臂扩散策略。

Result: 在模拟和真实环境实验中，EchoVLA在长视野任务上表现优异，在操作/导航任务上达到0.52成功率，在移动操作任务上达到0.31成功率，相比基线模型分别提升了0.08和0.11。

Conclusion: EchoVLA通过记忆增强的架构有效解决了长视野移动操作的挑战，在复杂任务中展现出显著性能提升。

Abstract: Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $π_{0.5}$ by +0.08 and +0.11.

</details>


### [275] [Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting](https://arxiv.org/abs/2511.18140)
*Yilong Wang,Cheng Qian,Ruomeng Fan,Edward Johns*

Main category: cs.RO

TL;DR: ObAct是一个新颖的主动视觉模仿学习框架，通过动态分配观察者和执行者角色，让观察者移动到最优视觉观察位置，从而提升策略的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决静态相机设置中由于遮挡和视角不佳导致的视觉观察质量下降问题，提升模仿学习策略在真实机器人系统中的性能。

Method: 使用双臂机器人系统，观察者臂构建3D高斯溅射表示，虚拟探索找到最优相机姿态并移动到该位置，执行者臂使用观察者的观测执行策略。

Result: 相比静态相机设置，轨迹传递方法在无遮挡和有遮挡情况下分别提升145%和233%，行为克隆方法分别提升75%和143%。

Conclusion: ObAct框架通过主动视觉观察显著提升了模仿学习策略的性能和鲁棒性，使观测更接近无遮挡训练分布。

Abstract: We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.

</details>


### [276] [Time-aware Motion Planning in Dynamic Environments with Conformal Prediction](https://arxiv.org/abs/2511.18170)
*Kaier Liang,Licheng Luo,Yixuan Wang,Mingyu Cai,Cristian Ioan Vasile*

Main category: cs.RO

TL;DR: 提出了两种基于共形预测的运动规划框架：全局规划器集成SIPP进行不确定性感知轨迹生成，局部规划器进行在线反应式规划，通过自适应共形预测增强动态环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 动态环境中的安全导航面临障碍物行为不确定性和缺乏形式化预测保证的挑战，需要开发能够提供安全保证的规划方法。

Method: 使用共形预测方法，全局规划器结合安全间隔路径规划(SIPP)，局部规划器采用自适应共形预测；引入自适应分位数机制优化不确定性量化，自动调整置信水平以保持轨迹可行性。

Result: 在动态和拥挤环境中进行了数值实验验证，框架能够提供分布无关的安全保证，并在不确定性较高区域自适应收紧安全边界。

Conclusion: 所提出的框架通过共形预测和自适应分位数机制，实现了在动态环境中具有安全保证的鲁棒运动规划，能够有效处理障碍物轨迹预测的不准确性。

Abstract: Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io

</details>


### [277] [Off-Road Navigation via Implicit Neural Representation of Terrain Traversability](https://arxiv.org/abs/2511.18183)
*Yixuan Jia,Qingyuan Li,Jonathan P. How*

Main category: cs.RO

TL;DR: TRAIL是一个越野导航框架，使用隐式神经表示连续参数化地形属性，结合梯度轨迹优化方法自适应调整路径几何和速度分布。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的规划器只能优化短视窗，无法推理完整路径几何，且缺乏根据地形颠簸度调整速度的能力。

Method: 利用隐式神经表示连续参数化地形属性，产生空间梯度，与新型梯度轨迹优化方法集成。

Result: 能够自适应调整路径几何和速度分布，基于地形可通行性进行优化。

Conclusion: TRAIL框架通过隐式学习和梯度优化，提升了越野导航中路径规划和速度适应的能力。

Abstract: Autonomous off-road navigation requires robots to estimate terrain traversability from onboard sensors and plan accordingly. Conventional approaches typically rely on sampling-based planners such as MPPI to generate short-term control actions that aim to minimize traversal time and risk measures derived from the traversability estimates. These planners can react quickly but optimize only over a short look-ahead window, limiting their ability to reason about the full path geometry, which is important for navigating in challenging off-road environments. Moreover, they lack the ability to adjust speed based on the terrain bumpiness, which is important for smooth navigation on challenging terrains. In this paper, we introduce TRAIL (Traversability with an Implicit Learned Representation), an off-road navigation framework that leverages an implicit neural representation to continuously parameterize terrain properties. This representation yields spatial gradients that enable integration with a novel gradient-based trajectory optimization method that adapts the path geometry and speed profile based on terrain traversability.

</details>


### [278] [SkillWrapper: Generative Predicate Invention for Skill Abstraction](https://arxiv.org/abs/2511.18203)
*Ziyi Yang,Benned Hedegaard,Ahmed Jaafar,Yichen Wei,Skye Thompson,Shreyas S. Raman,Haotian Fu,Stefanie Tellex,George Konidaris,David Paulius,Naman Shah*

Main category: cs.RO

TL;DR: 本文提出了SkillWrapper方法，利用基础模型主动收集机器人数据，学习可解释、可规划的技能抽象表示，以解决长时程任务规划问题。


<details>
  <summary>Details</summary>
Motivation: 解决从个体技能执行泛化到长时程任务规划的核心挑战，通过学习高层符号抽象来支持独立于底层状态空间的推理和规划。

Method: 提出SkillWrapper方法，利用基础模型进行生成式谓词发明，主动收集机器人数据，从RGB图像观察中学习黑盒技能的可解释、可规划表示。

Result: 在仿真和真实机器人上的广泛实验表明，SkillWrapper学习的抽象表示能够使用黑盒技能解决未见的长时程任务。

Conclusion: 提出了生成式谓词发明的形式化理论框架，确保学习到的符号操作符具有可证明的正确性和完备性，支持有效的任务规划。

Abstract: Generalizing from individual skill executions to solving long-horizon tasks remains a core challenge in building autonomous agents. A promising direction is learning high-level, symbolic abstractions of the low-level skills of the agents, enabling reasoning and planning independent of the low-level state space. Among possible high-level representations, object-centric skill abstraction with symbolic predicates has been proven to be efficient because of its compatibility with domain-independent planners. Recent advances in foundation models have made it possible to generate symbolic predicates that operate on raw sensory inputs, a process we call generative predicate invention, to facilitate downstream abstraction learning. However, it remains unclear which formal properties the learned representations must satisfy, and how they can be learned to guarantee these properties. In this paper, we address both questions by presenting a formal theory of generative predicate invention for skill abstraction, resulting in symbolic operators that can be used for provably sound and complete planning. Within this framework, we propose SkillWrapper, a method that leverages foundation models to actively collect robot data and learn human-interpretable, plannable representations of black-box skills, using only RGB image observations. Our extensive empirical evaluation in simulation and on real robots shows that SkillWrapper learns abstract representations that enable solving unseen, long-horizon tasks in the real world with black-box skills.

</details>


### [279] [AFT: Appearance-Based Feature Tracking for Markerless and Training-Free Shape Reconstruction of Soft Robots](https://arxiv.org/abs/2511.18215)
*Shangyuan Yuan,Preston Fairchild,Yu Mei,Xinyu Zhou,Xiaobo Tan*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉、无标记、无需训练的软体机器人形状重建框架，利用机器人自然表面特征作为隐式视觉标记，通过分层匹配策略实现实时形状跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的方法通常依赖复杂相机设置、特定背景或大规模训练数据集，限制了在实际场景中的实用性。需要一种更实用、低成本的软体机器人形状重建方法。

Method: 利用机器人自然表面外观作为隐式视觉标记，采用分层匹配策略，将局部分区对齐与全局运动学优化解耦。仅需初始3D重建和运动学对齐即可工作。

Result: 在连续软体机器人上的实验验证显示，实时操作期间平均尖端误差为2.6%，在实用闭环控制任务中表现稳定。

Conclusion: 该方法在动态现实环境中具有可靠、低成本部署的潜力，展示了基于自然表面特征的视觉形状重建的可行性。

Abstract: Accurate shape reconstruction is essential for precise control and reliable operation of soft robots. Compared to sensor-based approaches, vision-based methods offer advantages in cost, simplicity, and ease of deployment. However, existing vision-based methods often rely on complex camera setups, specific backgrounds, or large-scale training datasets, limiting their practicality in real-world scenarios. In this work, we propose a vision-based, markerless, and training-free framework for soft robot shape reconstruction that directly leverages the robot's natural surface appearance. These surface features act as implicit visual markers, enabling a hierarchical matching strategy that decouples local partition alignment from global kinematic optimization. Requiring only an initial 3D reconstruction and kinematic alignment, our method achieves real-time shape tracking across diverse environments while maintaining robustness to occlusions and variations in camera viewpoints. Experimental validation on a continuum soft robot demonstrates an average tip error of 2.6% during real-time operation, as well as stable performance in practical closed-loop control tasks. These results highlight the potential of the proposed approach for reliable, low-cost deployment in dynamic real-world settings.

</details>


### [280] [APULSE: A Scalable Hybrid Algorithm for the RCSPP on Large-Scale Dense Graphs](https://arxiv.org/abs/2511.18236)
*Nuno Soares,António Grilo*

Main category: cs.RO

TL;DR: APULSE是一种混合标签设置算法，用于高效解决资源受限最短路径问题(RCSPP)，特别针对大规模密集图场景。它结合了A*启发式搜索、脉冲式剪枝和时间分桶策略，在大规模UGV规划场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的RCSPP求解器在处理大规模密集图时面临严重的可扩展性限制，无法满足复杂现实场景（如无人地面车辆任务规划）中的实时规划需求。

Method: APULSE采用混合标签设置算法，集成了A*启发式引导的最佳优先搜索、脉冲式剪枝机制和时间分桶策略，实现有效的状态空间缩减。

Result: 在大规模UGV规划场景的计算研究中，APULSE相比最先进算法能够持续找到接近最优解，速度提升数个数量级，在大型问题实例上表现更加鲁棒。

Conclusion: APULSE在复杂大规模环境中的优越可扩展性使其成为RCSPP的有效解决方案，支持交互式决策和动态重规划能力。

Abstract: The resource-constrained shortest path problem (RCSPP) is a fundamental NP-hard optimization challenge with broad applications, from network routing to autonomous navigation. This problem involves finding a path that minimizes a primary cost subject to a budget on a secondary resource. While various RCSPP solvers exist, they often face critical scalability limitations when applied to the large, dense graphs characteristic of complex, real-world scenarios, making them impractical for time-critical planning. This challenge is particularly acute in domains like mission planning for unmanned ground vehicles (UGVs), which demand solutions on large-scale terrain graphs. This paper introduces APULSE, a hybrid label-setting algorithm designed to efficiently solve the RCSPP on such challenging graphs. APULSE integrates a best-first search guided by an A* heuristic with aggressive, Pulse-style pruning mechanisms and a time-bucketing strategy for effective state-space reduction. A computational study, using a large-scale UGV planning scenario, benchmarks APULSE against state-of-the-art algorithms. The results demonstrate that APULSE consistently finds near-optimal solutions while being orders of magnitude faster and more robust, particularly on large problem instances where competing methods fail. This superior scalability establishes APULSE as an effective solution for RCSPP in complex, large-scale environments, enabling capabilities such as interactive decision support and dynamic replanning.

</details>


### [281] [Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters](https://arxiv.org/abs/2511.18243)
*Eashan Vytla,Bhavanishankar Kalavakolanu,Andrew Perrault,Matthew McCrink*

Main category: cs.RO

TL;DR: 本文探讨了基于物理信息的世界模型学习方法在无人机控制中的应用，与传统RNN模型相比，虽然两种模型在训练数据上表现良好，但在新轨迹上泛化能力不足，导致状态推演快速发散。


<details>
  <summary>Details</summary>
Motivation: 当前无人机控制算法在动态环境和恶劣条件下缺乏鲁棒性，基于模型的强化学习虽然样本效率高，但Dreamer等方法在无人机系统上应用困难，主要由于样本效率低和动力学模型泛化能力差。

Method: 提出物理信息的世界模型学习方法，将四旋翼视为自由体系统，预测作用在其上的净力和力矩，然后通过6自由度龙格-库塔积分器预测未来状态推演。

Result: 虽然物理信息方法和标准RNN模型在训练数据上都表现良好，但两者都无法泛化到新轨迹，导致状态推演快速发散，阻碍策略收敛。

Conclusion: 物理信息的世界模型学习虽然改进了策略性能，但在泛化能力方面仍面临挑战，需要进一步研究解决状态推演发散问题。

Abstract: Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.

</details>


### [282] [Skypilot: Fine-Tuning LLM with Physical Grounding for AAV Coverage Search](https://arxiv.org/abs/2511.18270)
*Zhongkai Chen,Yihao Sun,Chao Yan,Han Zhou,Xiaojia Xiang,Jie Jiang*

Main category: cs.RO

TL;DR: Skypilot是一个基于大语言模型的两阶段框架，通过集成蒙特卡洛树搜索将语言模型物理接地，解决AAV在覆盖操作中的幻觉和可重现性问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在增强自主飞行器智能方面具有潜力，但缺乏物理接地导致空间推理和决策中的幻觉和可重现性问题。

Method: 提出两阶段框架：第一阶段引入多样化动作空间和物理信息奖励函数确保轨迹可行性；第二阶段在23,000个MCTS生成样本上微调Qwen3-4B模型。

Result: 实现了显著的推理加速同时保持解决方案质量，通过数值模拟和真实飞行实验验证了方法的效率和优越性。

Conclusion: Skypilot框架成功解决了LLM在AAV应用中的物理接地问题，为自主飞行器的智能决策提供了可靠解决方案。

Abstract: Autonomous aerial vehicles (AAVs) have played a pivotal role in coverage operations and search missions. Recent advances in large language models (LLMs) offer promising opportunities to augment AAV intelligence. These advances help address complex challenges like area coverage optimization, dynamic path planning, and adaptive decision-making. However, the absence of physical grounding in LLMs leads to hallucination and reproducibility problems in spatial reasoning and decision-making. To tackle these issues, we present Skypilot, an LLM-enhanced two-stage framework that grounds language models in physical reality by integrating monte carlo tree search (MCTS). In the first stage, we introduce a diversified action space that encompasses generate, regenerate, fine-tune, and evaluate operations, coupled with physics-informed reward functions to ensure trajectory feasibility. In the second stage, we fine-tune Qwen3-4B on 23,000 MCTS-generated samples, achieving substantial inference acceleration while maintaining solution quality. Extensive numerical simulations and real-world flight experiments validate the efficiency and superiority of our proposed approach. Detailed information and experimental results are accessible at https://sky-pilot.top.

</details>


### [283] [MicCheck: Repurposing Off-the-Shelf Pin Microphones for Easy and Low-Cost Contact Sensing](https://arxiv.org/abs/2511.18299)
*Steven Oh,Tai Inui,Magdeline Kuan,Jia-Yeu Lin*

Main category: cs.RO

TL;DR: MicCheck是一种低成本、即插即用的声学传感方法，利用现成的蓝牙针式麦克风作为接触传感器，通过3D打印夹持器插入件和标准USB接收器实现，无需定制电子设备或驱动程序。


<details>
  <summary>Details</summary>
Motivation: 机器人操作任务需要丰富的接触信息，但大多数模仿学习方法主要依赖视觉，难以捕捉刚度、粗糙度、滑动等精细交互线索。触觉信号可以弥补这一差距，但现有传感器通常需要昂贵、精密或集成复杂的硬件。

Method: 将现成的蓝牙针式麦克风重新用作低成本接触传感器，麦克风夹入3D打印的夹持器插入件中，通过标准USB接收器传输音频，无需定制电子设备或驱动程序。

Result: 在材料分类中，在四种交互类型（轻敲、敲击、慢压、拖动）的10类基准测试中达到92.9%的准确率。在操作任务中，将针式麦克风集成到模仿学习流程中，将拾取和倾倒任务的成功率从0.40提高到0.80，并实现了可靠的接触丰富技能执行，如拔插和基于声音的排序。

Conclusion: 与高分辨率触觉传感器相比，针式麦克风在空间细节上有所牺牲，但在成本和集成便利性方面具有优势，为在低成本机器人设置中部署声学接触传感提供了实用途径。

Abstract: Robotic manipulation tasks are contact-rich, yet most imitation learning (IL) approaches rely primarily on vision, which struggles to capture stiffness, roughness, slip, and other fine interaction cues. Tactile signals can address this gap, but existing sensors often require expensive, delicate, or integration-heavy hardware. In this work, we introduce MicCheck, a plug-and-play acoustic sensing approach that repurposes an off-the-shelf Bluetooth pin microphone as a low-cost contact sensor. The microphone clips into a 3D-printed gripper insert and streams audio via a standard USB receiver, requiring no custom electronics or drivers. Despite its simplicity, the microphone provides signals informative enough for both perception and control. In material classification, it achieves 92.9% accuracy on a 10-class benchmark across four interaction types (tap, knock, slow press, drag). For manipulation, integrating pin microphone into an IL pipeline with open source hardware improves the success rate on picking and pouring task from 0.40 to 0.80 and enables reliable execution of contact-rich skills such as unplugging and sound-based sorting. Compared with high-resolution tactile sensors, pin microphones trade spatial detail for cost and ease of integration, offering a practical pathway for deploying acoustic contact sensing in low-cost robot setups.

</details>


### [284] [Enhancing UAV Search under Occlusion using Next Best View Planning](https://arxiv.org/abs/2511.18353)
*Sigrid Helene Strand,Thomas Wiedemann,Bram Burczek,Dmitriy Shutin*

Main category: cs.RO

TL;DR: 本文提出了一种在遮挡环境中解决无人机最佳视角选择问题的优化规划策略和高效算法，通过几何启发式和可见性启发式两种新方法提升搜索性能，在模拟和真实森林环境中验证了可见性启发式的优越性。


<details>
  <summary>Details</summary>
Motivation: 在密集森林等遮挡环境中进行搜救任务时，无人机需要优化相机位置和视角来捕捉清晰的地面视图，但现有方法在复杂遮挡环境中的搜索效果有限。

Method: 提出了两种新颖的优化启发式方法：几何启发式和可见性启发式，用于选择最优相机视点，并开发了高效的算法来解决遮挡环境中的最佳视角问题。

Result: 在模拟森林环境中，可见性启发式能够识别超过90%的隐藏物体，比几何启发式提供10%更好的检测率；真实世界实验表明可见性启发式在树冠下具有更好的覆盖效果。

Conclusion: 可见性启发式方法在遮挡环境中具有显著优势，能够有效提升搜救任务的成功率，特别是在密集森林等复杂地形中。

Abstract: Search and rescue missions are often critical following sudden natural disasters or in high-risk environmental situations. The most challenging search and rescue missions involve difficult-to-access terrains, such as dense forests with high occlusion. Deploying unmanned aerial vehicles for exploration can significantly enhance search effectiveness, facilitate access to challenging environments, and reduce search time. However, in dense forests, the effectiveness of unmanned aerial vehicles depends on their ability to capture clear views of the ground, necessitating a robust search strategy to optimize camera positioning and perspective. This work presents an optimized planning strategy and an efficient algorithm for the next best view problem in occluded environments. Two novel optimization heuristics, a geometry heuristic, and a visibility heuristic, are proposed to enhance search performance by selecting optimal camera viewpoints. Comparative evaluations in both simulated and real-world settings reveal that the visibility heuristic achieves greater performance, identifying over 90% of hidden objects in simulated forests and offering 10% better detection rates than the geometry heuristic. Additionally, real-world experiments demonstrate that the visibility heuristic provides better coverage under the canopy, highlighting its potential for improving search and rescue missions in occluded environments.

</details>


### [285] [Explicit Bounds on the Hausdorff Distance for Truncated mRPI Sets via Norm-Dependent Contraction Rates](https://arxiv.org/abs/2511.18374)
*Jiaxun Sun*

Main category: cs.RO

TL;DR: 本文首次建立了截断最小鲁棒正不变集与其无限时域极限之间Hausdorff距离的显式闭式上界，提供了可计算的截断误差量化表达式。


<details>
  <summary>Details</summary>
Motivation: 现有的mRPI近似方法通过几何或范数参数保证渐近收敛，但都没有提供针对给定时域量化截断误差的可计算表达式。

Method: 证明了误差满足d_H(ℰ_N,ℰ_∞) ≤ r_W·γ^(N+1)/(1-γ)，其中γ<1是诱导范数收缩因子，r_W仅依赖于扰动集。该边界完全解析，无需迭代集计算。

Result: 提出的边界完全解析，不需要迭代集计算，直接表征了截断Minkowski级数的衰减率。向量范数的选择可作为设计参数加速收敛。

Conclusion: 数值实验验证了所提出边界的锐度、可扩展性和实际相关性，能够显著收紧鲁棒不变集计算和基于管道的MPC的时域选择。

Abstract: This paper establishes the first explicit and closed-form upper bound on the Hausdorff distance between the truncated minimal robust positively invariant (mRPI) set and its infinite-horizon limit. While existing mRPI approximations guarantee asymptotic convergence through geometric or norm-based arguments, none provides a computable expression that quantifies the truncation error for a given horizon. We show that the error satisfies \( d_H(\mathcal{E}_N,\mathcal{E}_\infty) \le r_W\,γ^{N+1}/(1-γ), \) where $γ<1$ is the induced-norm contraction factor and $r_W$ depends only on the disturbance set. The bound is fully analytic, requires no iterative set computations, and directly characterizes the decay rate of the truncated Minkowski series. We further demonstrate that the choice of vector norm serves as a design parameter that accelerates convergence, enabling substantially tighter horizon selection for robust invariant-set computations and tube-based MPC. Numerical experiments validate the sharpness, scalability, and practical relevance of the proposed bound.

</details>


### [286] [Expanding the Workspace of Electromagnetic Navigation Systems Using Dynamic Feedback for Single- and Multi-agent Control](https://arxiv.org/abs/2511.18486)
*Jasan Zughaibi,Denis von Arx,Maurus Derungs,Florian Heemeyer,Luca A. Antonelli,Quentin Boehler,Michael Muehlebach,Bradley J. Nelson*

Main category: cs.RO

TL;DR: 本文提出通过系统级控制设计显著扩展电磁导航系统(eMNS)的有效工作空间，通过五种关键方法降低实现期望运动所需的电流，包括运动中心控制、能量优化电流分配、实时位姿估计、动态反馈和高带宽组件。


<details>
  <summary>Details</summary>
Motivation: 电磁导航系统的有效工作空间常受功率和热限制严重约束，需要找到方法来扩展工作空间并降低操作电流。

Method: 采用五种系统级控制方法：运动中心扭矩/力目标、能量优化电流分配、实时位姿估计、动态反馈控制、高带宽eMNS组件。通过将场中心策略替换为运动中心方法，并在八线圈OctoMag系统上实现3D倒立摆稳定控制。

Result: 将稳定3D倒立摆所需的电流从8-14A显著降低到0.1-0.2A，在临床导向的Navion eMNS上实现距离线圈50cm处的稳定平衡，并成功同时稳定两个倒立摆。

Conclusion: 反馈控制是实现可扩展、高效且临床相关的磁操纵的实用途径，系统级控制设计能显著扩展电磁导航系统的工作空间。

Abstract: Electromagnetic navigation systems (eMNS) enable a number of magnetically guided surgical procedures. A challenge in magnetically manipulating surgical tools is that the effective workspace of an eMNS is often severely constrained by power and thermal limits. We show that system-level control design significantly expands this workspace by reducing the currents needed to achieve a desired motion. We identified five key system approaches that enable this expansion: (i) motion-centric torque/force objectives, (ii) energy-optimal current allocation, (iii) real-time pose estimation, (iv) dynamic feedback, and (v) high-bandwidth eMNS components. As a result, we stabilize a 3D inverted pendulum on an eight-coil OctoMag eMNS with significantly lower currents (0.1-0.2 A vs. 8-14 A), by replacing a field-centric field-alignment strategy with a motion-centric torque/force-based approach. We generalize to multi-agent control by simultaneously stabilizing two inverted pendulums within a shared workspace, exploiting magnetic-field nonlinearity and coil redundancy for independent actuation. A structured analysis compares the electromagnetic workspaces of both paradigms and examines current-allocation strategies that map motion objectives to coil currents. Cross-platform evaluation of the clinically oriented Navion eMNS further demonstrates substantial workspace expansion by maintaining stable balancing at distances up to 50 cm from the coils. The results demonstrate that feedback is a practical path to scalable, efficient, and clinically relevant magnetic manipulation.

</details>


### [287] [SafeFall: Learning Protective Control for Humanoid Robots](https://arxiv.org/abs/2511.18509)
*Ziyu Meng,Tengyu Liu,Le Ma,Yingying Wu,Ran Song,Wei Zhang,Siyuan Huang*

Main category: cs.RO

TL;DR: SafeFall是一个保护人形机器人免受摔倒损坏的框架，包含轻量级摔倒预测器和强化学习保护策略，在Unitree G1机器人上验证显著降低了硬件损伤。


<details>
  <summary>Details</summary>
Motivation: 人形机器人双足行走容易摔倒，导致昂贵的传感器、执行器和结构部件严重损坏，这是实际部署的关键障碍。

Method: 结合GRU摔倒预测器和强化学习保护策略，预测器持续监测机器人状态，保护策略在预测到不可避免摔倒时激活执行损伤最小化动作。

Result: 在Unitree G1人形机器人上验证，峰值接触力降低68.3%，峰值关节扭矩降低78.4%，消除了99.3%的脆弱部件碰撞。

Conclusion: SafeFall为人形机器人提供了关键安全网，支持更激进的实验并加速在复杂现实环境中的部署。

Abstract: Bipedal locomotion makes humanoid robots inherently prone to falls, causing catastrophic damage to the expensive sensors, actuators, and structural components of full-scale robots. To address this critical barrier to real-world deployment, we present \method, a framework that learns to predict imminent, unavoidable falls and execute protective maneuvers to minimize hardware damage. SafeFall is designed to operate seamlessly alongside existing nominal controller, ensuring no interference during normal operation. It combines two synergistic components: a lightweight, GRU-based fall predictor that continuously monitors the robot's state, and a reinforcement learning policy for damage mitigation. The protective policy remains dormant until the predictor identifies a fall as unavoidable, at which point it activates to take control and execute a damage-minimizing response. This policy is trained with a novel, damage-aware reward function that incorporates the robot's specific structural vulnerabilities, learning to shield critical components like the head and hands while absorbing energy with more robust parts of its body. Validated on a full-scale Unitree G1 humanoid, SafeFall demonstrated significant performance improvements over unprotected falls. It reduced peak contact forces by 68.3\%, peak joint torques by 78.4\%, and eliminated 99.3\% of collisions with vulnerable components. By enabling humanoids to fail safely, SafeFall provides a crucial safety net that allows for more aggressive experiments and accelerates the deployment of these robots in complex, real-world environments.

</details>


### [288] [Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation](https://arxiv.org/abs/2511.18525)
*Samarth Chopra,Jing Liang,Gershom Seneviratne,Yonghan Lee,Jaehoon Choi,Jianyu An,Stephen Cheng,Dinesh Manocha*

Main category: cs.RO

TL;DR: Splatblox是一个用于户外密集植被环境的实时自主导航系统，通过融合RGB图像和LiDAR点云构建可穿越性感知的ESDF，在植被丰富场景中相比现有方法成功率提高50%以上，冻结事件减少40%，路径缩短5%，目标时间加快13%。


<details>
  <summary>Details</summary>
Motivation: 解决户外环境中密集植被、不规则障碍物和复杂地形带来的导航挑战，需要同时处理几何和语义信息来区分可穿越植被和刚性障碍物。

Method: 使用高斯泼溅技术融合分割RGB图像和LiDAR点云，构建在线更新的可穿越性感知欧几里得符号距离场(ESDF)，结合语义推理区分可穿越植被和刚性障碍物。

Result: 在植被丰富场景的现场试验中，相比最先进方法成功率提高50%以上，冻结事件减少40%，路径缩短5%，目标时间加快13%，支持长达100米的远程任务。

Conclusion: Splatblox系统能够有效处理户外复杂环境导航，在四足机器人和轮式平台上均表现出色，为密集植被环境下的自主导航提供了可靠解决方案。

Abstract: We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io

</details>


### [289] [Object-centric Task Representation and Transfer using Diffused Orientation Fields](https://arxiv.org/abs/2511.18563)
*Cem Bilaloglu,Tobias Löw,Sylvain Calinon*

Main category: cs.RO

TL;DR: 该论文提出了一种使用扩散方向场（DOF）的方法，通过平滑变化的局部参考框架来解决曲面物体上的技能迁移问题，将任务迁移简化为建立稀疏关键点对应关系。


<details>
  <summary>Details</summary>
Motivation: 曲面物体在机器人技能迁移中构成根本挑战，因为它们缺乏全局参考框架，使得任务相关方向随位置和几何形状变化，难以在不同形状间迁移面向物体的任务。

Method: 使用扩散方向场（DOF）作为局部参考框架的平滑表示，通过受偏微分方程控制的扩散过程从原始点云数据在线计算DOF，并以关键点为条件。

Result: 在几何、拓扑和定位扰动下评估DOF，成功实现了需要连续物理交互的任务（如检查、切割和剥离）在不同物体间的迁移。

Conclusion: DOF方法通过表达在平滑变化的局部框架中的操作任务，有效解决了曲面物体间的技能迁移问题，将复杂任务迁移简化为建立稀疏关键点对应关系。

Abstract: Curved objects pose a fundamental challenge for skill transfer in robotics: unlike planar surfaces, they do not admit a global reference frame. As a result, task-relevant directions such as "toward" or "along" the surface vary with position and geometry, making object-centric tasks difficult to transfer across shapes. To address this, we introduce an approach using Diffused Orientation Fields (DOF), a smooth representation of local reference frames, for transfer learning of tasks across curved objects. By expressing manipulation tasks in these smoothly varying local frames, we reduce the problem of transferring tasks across curved objects to establishing sparse keypoint correspondences. DOF is computed online from raw point cloud data using diffusion processes governed by partial differential equations, conditioned on keypoints. We evaluate DOF under geometric, topological, and localization perturbations, and demonstrate successful transfer of tasks requiring continuous physical interaction such as inspection, slicing, and peeling across varied objects. We provide our open-source codes at our website https://github.com/idiap/diffused_fields_robotics

</details>


### [290] [An Analysis of Constraint-Based Multi-Agent Pathfinding Algorithms](https://arxiv.org/abs/2511.18604)
*Hannah Lee,James D. Motes,Marco Morales,Nancy M. Amato*

Main category: cs.RO

TL;DR: 本研究通过约束分类指导基于约束的搜索算法在多智能体路径规划(MAPF)和多机器人运动规划(MRMP)中的设计选择，分析了保守型与激进型约束对搜索行为的影响。


<details>
  <summary>Details</summary>
Motivation: 为未来的MAPF和MRMP算法设计提供指导，帮助用户基于约束分类做出合适的选择，重点关注CBS和CBSw/P算法在不同约束类型下的表现。

Method: 将约束分类为保守型和激进型，在混合网格-路线图表示下使用不同分辨率，比较CBS和CBSw/P算法在不同约束类型下的搜索行为。

Result: 激进型(优先级约束)在智能体数量或分辨率增加时能解决更多实例，而保守型(运动约束)在两者都成功时提供更强的解质量。研究结果被整合为决策流程图。

Conclusion: 研究为MAPF和MRMP算法设计提供了实用的约束选择指导，强调在考虑拓扑特征的同时需要综合问题、解和表示特征的重要性。

Abstract: This study informs the design of future multi-agent pathfinding (MAPF) and multi-robot motion planning (MRMP) algorithms by guiding choices based on constraint classification for constraint-based search algorithms. We categorize constraints as conservative or aggressive and provide insights into their search behavior, focusing specifically on vanilla Conflict-Based Search (CBS) and Conflict-Based Search with Priorities (CBSw/P). Under a hybrid grid-roadmap representation with varying resolution, we observe that aggressive (priority constraint) formulations tend to solve more instances as agent count or resolution increases, whereas conservative (motion constraint) formulations yield stronger solution quality when both succeed. Findings are synthesized in a decision flowchart, aiding users in selecting suitable constraints. Recommendations extend to Multi-Robot Motion Planning (MRMP), emphasizing the importance of considering topological features alongside problem, solution, and representation features. A comprehensive exploration of the study, including raw data and map performance, is available in our public GitHub Repository: https://GitHub.com/hannahjmlee/constraint-mapf-analysis

</details>


### [291] [How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints](https://arxiv.org/abs/2511.18606)
*Kensuke Nakamura,Arun L. Bishop,Steven Man,Aaron M. Johnson,Zachary Manchester,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 本文提出LatentCBF方法，解决现有潜在安全过滤器在切换名义策略和安全策略时破坏任务性能的问题。通过梯度惩罚实现平滑边界函数，并混合两种策略数据训练值函数，实现平滑的安全过滤。


<details>
  <summary>Details</summary>
Motivation: 现有潜在安全过滤器采用"最小限制"过滤，在名义策略和安全策略之间离散切换，这会破坏现代视觉运动策略的宝贵任务性能。虽然可达性值函数理论上可以适配为控制屏障函数进行平滑优化过滤，但当前潜在空间学习方法产生的值函数存在根本性不兼容问题。

Method: 提出LatentCBF方法：1）使用梯度惩罚实现平滑边界函数，无需额外标注；2）混合名义策略和安全策略分布数据训练值函数。解决了边界函数饱和导致值函数不连续跳跃的问题，以及强化学习近似仅基于安全策略数据导致名义策略动作值估计不准确的问题。

Result: 在模拟基准测试和基于视觉的机械臂操作硬件实验中，LatentCBF能够实现平滑的安全过滤，同时将任务完成率比先前的切换方法提高了一倍。

Conclusion: LatentCBF成功解决了潜在安全过滤中的两个关键挑战：边界函数平滑性和值函数估计准确性，实现了既安全又高效的任务执行。

Abstract: Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement "least-restrictive" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a "margin function" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.

</details>


### [292] [AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations](https://arxiv.org/abs/2511.18617)
*Litian Gong,Fatemeh Bahrani,Yutai Zhou,Amin Banayeeanzade,Jiachen Li,Erdem Biyik*

Main category: cs.RO

TL;DR: AutoFocus-IL是一种通过视觉语言模型自动识别关键物体并生成时间显著性图来改进视觉模仿学习数据效率和泛化能力的方法，无需昂贵的人工标注监督。


<details>
  <summary>Details</summary>
Motivation: 现有基于显著性正则化的方法需要昂贵的人工监督（如人类注视数据或手动标注），限制了其实际应用。AutoFocus-IL旨在开发一种无需人工监督就能自动识别任务相关特征的方法。

Method: 利用视觉语言模型自动识别和跟踪演示中的关键物体，生成时间显著性图来突出因果视觉信号并抑制干扰因素，然后将这些图用于正则化行为克隆策略。

Result: 在CARLA模拟器和真实机器人操作任务中的实验表明，AutoFocus-IL不仅优于标准行为克隆，还超过了需要人类监督的最先进基线方法。

Conclusion: AutoFocus-IL提供了一种有效且实用的方法，通过自动显著性正则化显著提高了视觉模仿学习的数据效率和泛化能力，无需依赖昂贵的人工监督。

Abstract: AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.

</details>


### [293] [Online Learning-Enhanced Lie Algebraic MPC for Robust Trajectory Tracking of Autonomous Surface Vehicles](https://arxiv.org/abs/2511.18683)
*Yinan Dong,Ziyu Xu,Tsimafei Lazouski,Sangli Teng,Maani Ghaffari*

Main category: cs.RO

TL;DR: 提出了一种结合李群上凸误差状态MPC和在线学习模块的高效控制器，用于在未知扰动下实现海洋车辆轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 自主水面车辆(ASV)容易受到风浪等环境扰动影响，在动态海洋条件下实现精确轨迹跟踪是一个持续挑战。

Method: 将李群上的凸误差状态模型预测控制(MPC)与在线学习模块相结合，实时补偿未知扰动。

Result: 在数值模拟、VRX模拟器和真实世界现场实验中的广泛评估表明，该方法在各种扰动场景下相比现有方法实现了更优越的跟踪精度。

Conclusion: 该设计能够在保持计算效率的同时实现自适应和鲁棒控制。

Abstract: Autonomous surface vehicles (ASVs) are easily influenced by environmental disturbances such as wind and waves, making accurate trajectory tracking a persistent challenge in dynamic marine conditions. In this paper, we propose an efficient controller for trajectory tracking of marine vehicles under unknown disturbances by combining a convex error-state MPC on the Lie group with an online learning module to compensate for these disturbances in real time. This design enables adaptive and robust control while maintaining computational efficiency. Extensive evaluations in numerical simulations, the Virtual RobotX (VRX) simulator, and real-world field experiments demonstrate that our method achieves superior tracking accuracy under various disturbance scenarios compared with existing approaches.

</details>


### [294] [Stable Multi-Drone GNSS Tracking System for Marine Robots](https://arxiv.org/abs/2511.18694)
*Shuo Wen,Edwin Meriaux,Mariana Sosa Guzmán,Zhizun Wang,Junming Shi,Gregory Dudek*

Main category: cs.RO

TL;DR: 提出一种基于多无人机GNSS的海洋机器人跟踪系统，通过视觉检测、多目标跟踪和GNSS三角定位，结合置信度加权的扩展卡尔曼滤波器，实现水面和近水面海洋机器人的实时稳定定位。


<details>
  <summary>Details</summary>
Motivation: 海洋机器人精确定位至关重要，但GNSS信号在水下不可靠，传统方法存在误差累积、计算量大或依赖基础设施等问题。

Method: 结合高效视觉检测、轻量级多目标跟踪、GNSS三角定位和置信度加权EKF，并引入跨无人机跟踪ID对齐算法确保多视角一致性。

Result: 在多样化复杂环境中验证了系统的可扩展性和鲁棒性。

Conclusion: 该系统为海洋机器人提供了一种可扩展、实时的GNSS定位解决方案，克服了传统方法的局限性。

Abstract: Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.

</details>


### [295] [Asynchronous Distributed Multi-Robot Motion Planning Under Imperfect Communication](https://arxiv.org/abs/2511.18703)
*Ardalan Tajbakhsh,Augustinos Saravanos,James Zhu,Evangelos A. Theodorou,Lorenz T. Biegler,Aaron M. Johnson*

Main category: cs.RO

TL;DR: 本文提出了一种延迟感知ADMM（DA-ADMM）方法，用于解决多机器人系统在通信延迟下的协调问题，通过实时调整惩罚参数来提升鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式优化算法对通信延迟敏感，传统的惩罚参数调整方法（如残差平衡和自适应启发式）没有显式考虑延迟，导致性能下降。

Method: 引入延迟感知ADMM变体，基于实时延迟统计调整惩罚参数，使智能体能够降低陈旧信息的权重，在共识和双重更新中优先处理最新更新。

Result: 在2D和3D环境中的广泛仿真表明，DA-ADMM相比固定参数、残差平衡和固定约束基线方法，显著提高了鲁棒性、成功率和解决方案质量。

Conclusion: 性能下降不仅取决于延迟长度或频率，还取决于优化器对延迟信息的上下文推理能力。DA-ADMM在不完美通信条件下为弹性多机器人运动规划提供了原则性和高效的机制。

Abstract: This paper addresses the challenge of coordinating multi-robot systems under realistic communication delays using distributed optimization. We focus on consensus ADMM as a scalable framework for generating collision-free, dynamically feasible motion plans in both trajectory optimization and receding-horizon control settings. In practice, however, these algorithms are sensitive to penalty tuning or adaptation schemes (e.g. residual balancing and adaptive parameter heuristics) that do not explicitly consider delays. To address this, we introduce a Delay-Aware ADMM (DA-ADMM) variant that adapts penalty parameters based on real-time delay statistics, allowing agents to down-weight stale information and prioritize recent updates during consensus and dual updates. Through extensive simulations in 2D and 3D environments with double-integrator, Dubins-car, and drone dynamics, we show that DA-ADMM significantly improves robustness, success rate, and solution quality compared to fixed-parameter, residual-balancing, and fixed-constraint baselines. Our results highlight that performance degradation is not solely determined by delay length or frequency, but by the optimizer's ability to contextually reason over delayed information. The proposed DA-ADMM achieves consistently better coordination performance across a wide range of delay conditions, offering a principled and efficient mechanism for resilient multi-robot motion planning under imperfect communication.

</details>


### [296] [GVD-TG: Topological Graph based on Fast Hierarchical GVD Sampling for Robot Exploration](https://arxiv.org/abs/2511.18708)
*Yanbin Li,Canran Xiao,Shenghai Yuan,Peilai Yu,Ziruo Li,Zhiguo Zhang,Wenzheng Chi,Wei Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于广义Voronoi图(GVD)的拓扑地图更新方法，通过多粒度分层GVD生成、节点聚类与连接机制、以及基于形态学膨胀的前沿提取，实现了实时更新准确且细节丰富的环境拓扑地图，提高了机器人探索效率。


<details>
  <summary>Details</summary>
Motivation: 实时更新准确且细节丰富的环境拓扑地图在机器人探索任务中仍然是一个挑战，现有的拓扑地图方法在效率和准确性方面存在不足。

Method: 1. 基于GVD的拓扑地图更新方法：包括去噪处理、多粒度分层GVD生成、覆盖图维护；2. 具有连通性约束的节点聚类方法和基于切换机制的连接方法；3. 基于形态学膨胀的前沿提取方法；4. 轻量级成本函数实时评估和切换下一个视点。

Result: 通过多粒度分层GVD生成确保了拓扑结构的准确性，增强了细节特征捕捉能力，减少了路径回溯概率，提高了GVD利用效率。节点聚类和连接方法避免了不可达节点和错误节点的生成，提高了探索效率。前沿提取方法有效保证了前沿的可达性。

Conclusion: 该方法能够使机器人在出现路径回溯迹象时快速调整策略，摆脱困境，增加探索灵活性。通过与SOTA方法的对比测试验证了系统在探索任务中的性能优势。

Abstract: Topological maps are more suitable than metric maps for robotic exploration tasks. However, real-time updating of accurate and detail-rich environmental topological maps remains a challenge. This paper presents a topological map updating method based on the Generalized Voronoi Diagram (GVD). First, the newly observed areas are denoised to avoid low-efficiency GVD nodes misleading the topological structure. Subsequently, a multi-granularity hierarchical GVD generation method is designed to control the sampling granularity at both global and local levels. This not only ensures the accuracy of the topological structure but also enhances the ability to capture detail features, reduces the probability of path backtracking, and ensures no overlap between GVDs through the maintenance of a coverage map, thereby improving GVD utilization efficiency. Second, a node clustering method with connectivity constraints and a connectivity method based on a switching mechanism are designed to avoid the generation of unreachable nodes and erroneous nodes caused by obstacle attraction. A special cache structure is used to store all connectivity information, thereby improving exploration efficiency. Finally, to address the issue of frontiers misjudgment caused by obstacles within the scope of GVD units, a frontiers extraction method based on morphological dilation is designed to effectively ensure the reachability of frontiers. On this basis, a lightweight cost function is used to assess and switch to the next viewpoint in real time. This allows the robot to quickly adjust its strategy when signs of path backtracking appear, thereby escaping the predicament and increasing exploration flexibility. And the performance of system for exploration task is verified through comparative tests with SOTA methods.

</details>


### [297] [Autonomous Surface Selection For Manipulator-Based UV Disinfection In Hospitals Using Foundation Models](https://arxiv.org/abs/2511.18709)
*Xueyan Oh,Jonathan Her,Zhixiang Ong,Brandon Koh,Yun Hann Tan,U-Xuan Tan*

Main category: cs.RO

TL;DR: 提出了一种利用基础模型简化机械臂UV消毒表面选择的方法，无需模型训练，通过VLM辅助分割优化减少误分割，在真实实验中达到92%以上的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统UV消毒方法需要大量人工干预定义消毒区域，而基于深度学习的方法需要大量微调和大数据集，且缺乏对部分表面消毒的场景理解，难以避免意外UV暴露。

Method: 利用基础模型简化机械臂UV消毒的表面选择，减少人工参与，无需模型训练；提出VLM辅助分割优化来检测和排除细小非目标物体。

Result: 目标和非目标表面正确分割成功率超过92%，真实机械臂和模拟UV光实验证明了其实际应用潜力。

Conclusion: 该方法有效简化了UV消毒的表面选择过程，减少了人工干预和模型训练需求，具有实际应用价值。

Abstract: Ultraviolet (UV) germicidal radiation is an established non-contact method for surface disinfection in medical environments. Traditional approaches require substantial human intervention to define disinfection areas, complicating automation, while deep learning-based methods often need extensive fine-tuning and large datasets, which can be impractical for large-scale deployment. Additionally, these methods often do not address scene understanding for partial surface disinfection, which is crucial for avoiding unintended UV exposure. We propose a solution that leverages foundation models to simplify surface selection for manipulator-based UV disinfection, reducing human involvement and removing the need for model training. Additionally, we propose a VLM-assisted segmentation refinement to detect and exclude thin and small non-target objects, showing that this reduces mis-segmentation errors. Our approach achieves over 92\% success rate in correctly segmenting target and non-target surfaces, and real-world experiments with a manipulator and simulated UV light demonstrate its practical potential for real-world applications.

</details>


### [298] [Head Stabilization for Wheeled Bipedal Robots via Force-Estimation-Based Admittance Control](https://arxiv.org/abs/2511.18712)
*Tianyu Wang,Chunxiang Yan,Xuanhong Liao,Tao Zhang,Ping Wang,Cong Wen,Dingchuan Liu,Haowen Yu,Ximin Lyu*

Main category: cs.RO

TL;DR: 开发了一种基于模型的地面力估计方法，用于6自由度轮式双足机器人，通过导纳控制算法提高地形适应性，验证了力估计器的实时性能和机器人在不平坦地形上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 轮式双足机器人作为灵活的现场探索平台，但由不平坦地形引起的头部不稳定性会降低机载传感器精度或损坏脆弱载荷。现有研究主要关注稳定移动平台，但忽视了在世界坐标系中主动稳定头部，导致垂直振荡破坏整体稳定性。

Method: 开发了基于模型的地面力估计方法，利用这些力估计实现导纳控制算法以增强地形适应性。

Result: 仿真实验验证了力估计器的实时性能和机器人在穿越不平坦地形时的鲁棒性。

Conclusion: 所提出的方法有效解决了轮式双足机器人在不平坦地形上的头部稳定性问题，提高了整体系统性能。

Abstract: Wheeled bipedal robots are emerging as flexible platforms for field exploration. However, head instability induced by uneven terrain can degrade the accuracy of onboard sensors or damage fragile payloads. Existing research primarily focuses on stabilizing the mobile platform but overlooks active stabilization of the head in the world frame, resulting in vertical oscillations that undermine overall stability. To address this challenge, we developed a model-based ground force estimation method for our 6-degree-of-freedom wheeled bipedal robot. Leveraging these force estimates, we implemented an admittance control algorithm to enhance terrain adaptability. Simulation experiments validated the real-time performance of the force estimator and the robot's robustness when traversing uneven terrain.

</details>


### [299] [SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map](https://arxiv.org/abs/2511.18756)
*Xueyu Du,Lilian Zhang,Fuan Duan,Xincan Luo,Maosong Wang,Wenqi Wu,JunMao*

Main category: cs.RO

TL;DR: 提出了一种基于滤波器的立体视觉惯性导航系统SP-VINS，通过隐式环境地图实现高效闭环约束，结合混合残差滤波框架和在线外参标定，在保持计算效率的同时实现长期高精度定位。


<details>
  <summary>Details</summary>
Motivation: 传统基于滤波器的VINS系统在精度和效率之间取得了良好平衡，但其有限的地图质量限制了长期高精度状态估计。

Method: 1) 提出基于关键帧和2D关键点的隐式环境地图实现高效闭环约束；2) 设计结合地标重投影和射线约束的混合残差滤波框架；3) 在退化环境中将相机-IMU外参融入视觉描述实现在线标定。

Result: 基准测试表明，SP-VINS在保持高计算效率的同时实现了长期高精度定位性能，优于现有最先进方法。

Conclusion: 所提出的SP-VINS系统通过创新的隐式地图表示和混合滤波框架，成功解决了滤波器VINS在长期定位中的精度问题，为移动机器人提供了高效的导航解决方案。

Abstract: Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation. To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints. Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates. Finally, considering the degraded environment, we incorporated the camera-IMU extrinsic parameters into visual description to achieve online calibration. Benchmark experiments demonstrate that the proposed SP-VINS achieves high computational efficiency while maintaining long-term high-accuracy localization performance, and is superior to existing state-of-the-art (SOTA) methods.

</details>


### [300] [MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent](https://arxiv.org/abs/2511.18810)
*Yuxia Fu,Zhizhen Zhang,Yuqi Zhang,Zijian Wang,Zi Huang,Yadan Luo*

Main category: cs.RO

TL;DR: MergeVLA是一个面向合并的视觉-语言-动作模型架构，通过稀疏激活的LoRA适配器和仅交叉注意力块来解决多技能VLA模型合并的挑战，实现了与单独微调专家相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在单一任务上表现良好，但在多技能设置中直接合并不同任务的专家会导致性能急剧下降。本文旨在解决VLA模型在多技能合并中的根本问题。

Method: 提出MergeVLA架构：1）使用任务掩码的稀疏激活LoRA适配器保持参数一致性；2）用仅交叉注意力块替换自注意力块，使专业化保持局部化和可组合；3）测试时任务路由器自适应选择任务掩码和专家头。

Result: 在LIBERO、LIBERO-Plus、RoboTwin和真实SO101机械臂上的多任务实验中，MergeVLA实现了与单独微调专家相当甚至更优的性能，展示了跨任务、具身和环境的鲁棒泛化能力。

Conclusion: MergeVLA通过设计保持可合并性，成功解决了VLA模型在多技能合并中的挑战，为构建通用多技能机器人系统提供了可行方案。

Abstract: Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.

</details>


### [301] [AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion](https://arxiv.org/abs/2511.18857)
*Changsheng Luo,Yushi Wang,Wenhan Cai,Mingguo Zhao*

Main category: cs.RO

TL;DR: AutoOdom是一种新型自回归本体感知里程计系统，通过两阶段训练范式解决腿式机器人在GPS缺失和视觉退化环境中的导航问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决腿式机器人在GPS缺失和视觉退化环境中传统视觉里程计失效的问题，克服现有方法在建模不确定性、累积漂移、仿真到现实迁移困难等方面的局限性。

Method: 采用创新的两阶段训练范式：第一阶段使用大规模仿真数据学习腿式运动的复杂非线性动力学和快速变化的接触状态；第二阶段引入自回归增强机制，使用有限的真实世界数据有效弥合仿真到现实的差距。

Result: 在Booster T1人形机器人上的综合实验验证显示，AutoOdom在所有评估指标上显著优于最先进方法：绝对轨迹误差提升57.2%，Umeyama对齐误差提升59.2%，相对位姿误差提升36.2%。

Conclusion: 自回归训练方法使模型能够从自身预测中学习，发展出对传感器噪声的鲁棒性，在高度动态环境中提高稳健性，为挑战性运动场景下的鲁棒本体感知里程计提供了系统设计见解。

Abstract: Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail. Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection. This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm. Stage 1 employs large-scale simulation data to learn complex nonlinear dynamics and rapidly changing contact states inherent in legged locomotion, while Stage 2 introduces an autoregressive enhancement mechanism using limited real-world data to effectively bridge the sim-to-real gap. The key innovation lies in our autoregressive training approach, where the model learns from its own predictions to develop resilience against sensor noise and improve robustness in highly dynamic environments. Comprehensive experimental validation on the Booster T1 humanoid robot demonstrates that AutoOdom significantly outperforms state-of-the-art methods across all evaluation metrics, achieving 57.2% improvement in absolute trajectory error, 59.2% improvement in Umeyama-aligned error, and 36.2% improvement in relative pose error compared to the Legolas baseline. Extensive ablation studies provide critical insights into sensor modality selection and temporal modeling, revealing counterintuitive findings about IMU acceleration data and validating our systematic design choices for robust proprioceptive odometry in challenging locomotion scenarios.

</details>


### [302] [Accelerating Reinforcement Learning via Error-Related Human Brain Signals](https://arxiv.org/abs/2511.18878)
*Suzie Kim,Hye-Bin Shin,Hyo-Jeong Jang*

Main category: cs.RO

TL;DR: 本研究探索了如何利用隐式神经反馈加速复杂机器人操作任务中的强化学习，通过将脑电图解码的错误相关电位整合到奖励塑造中，在7自由度机械臂的障碍物丰富环境中验证了神经反馈能加速强化学习并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 先前基于脑电图的强化学习研究主要关注导航或低维运动任务，本研究旨在验证神经评估信号是否能在涉及障碍物和精确末端执行器控制的高维操作任务中改善策略学习。

Method: 将离线训练的脑电图分类器解码的错误相关电位整合到奖励塑造中，系统评估人类反馈权重的影响，在7自由度机械臂的障碍物丰富环境中进行实验。

Result: 神经反馈加速了强化学习，根据人类反馈权重的不同，任务成功率有时超过稀疏奖励基线。最佳反馈权重在所有受试者中都能一致加速强化学习，留一受试者评估证实了框架的鲁棒性。

Conclusion: 基于脑电图的强化学习可以扩展到运动任务之外，为人类对齐的操作技能获取提供了可行途径。

Abstract: In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.

</details>


### [303] [An Efficient Closed-Form Solution to Full Visual-Inertial State Initialization](https://arxiv.org/abs/2511.18910)
*Samuel Cerezo,Seong Hun Lee,Javier Civera*

Main category: cs.RO

TL;DR: 提出了一种无需非线性优化的闭式初始化方法，用于恢复完整的视觉-惯性状态，相比基于优化的方法具有更低的初始化误差、更短的初始化窗口和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-惯性初始化方法通常依赖迭代求解器，需要非线性优化，这可能导致数值不稳定和计算成本高的问题。本文旨在开发一种解析、易于实现且数值稳定的初始化方法。

Method: 基于小旋转和恒定速度近似，构建紧凑的公式化方法，同时保持运动与惯性测量之间的基本耦合。进一步提出基于可观测性的两阶段初始化方案，平衡精度与初始化延迟。

Result: 在EuRoC数据集上的广泛实验验证了方法的有效性：相比基于优化的方法，初始化误差降低10-20%，初始化窗口缩短4倍，计算成本降低5倍。

Conclusion: 所提出的闭式初始化方法能够可靠地启动视觉-惯性系统，无需非线性优化，在精度、速度和计算效率方面均优于现有方法。

Abstract: In this letter, we present a closed-form initialization method that recovers the full visual-inertial state without nonlinear optimization. Unlike previous approaches that rely on iterative solvers, our formulation yields analytical, easy-to-implement, and numerically stable solutions for reliable start-up. Our method builds on small-rotation and constant-velocity approximations, which keep the formulation compact while preserving the essential coupling between motion and inertial measurements. We further propose an observability-driven, two-stage initialization scheme that balances accuracy with initialization latency. Extensive experiments on the EuRoC dataset validate our assumptions: our method achieves 10-20% lower initialization error than optimization-based approaches, while using 4x shorter initialization windows and reducing computational cost by 5x.

</details>


### [304] [Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation](https://arxiv.org/abs/2511.18950)
*Juntao Gao,Feiyang Ye,Jing Zhang,Wenjing Qian*

Main category: cs.RO

TL;DR: Compressor-VLA是一个用于视觉-语言-动作模型的混合指令条件令牌压缩框架，通过语义任务压缩器和空间细化压缩器实现高效的任务导向视觉信息压缩，在保持竞争力的同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型中处理冗余视觉令牌带来的巨大计算开销问题，同时需要保留任务关键视觉信息，实现实时机器人部署。

Method: 提出Compressor-VLA框架，包含语义任务压缩器（STC）提取整体任务相关上下文，和空间细化压缩器（SRC）保留细粒度空间细节，通过自然语言指令动态调节压缩过程。

Result: 在LIBERO基准测试中保持竞争力的成功率，同时将FLOPs减少59%，视觉令牌数量减少超过3倍，真实机器人部署验证了模型的仿真到现实迁移性和实际应用性。

Conclusion: Compressor-VLA通过指令引导的动态压缩机制，有效降低了VLA模型的计算负担，同时保持了任务性能，实现了高效的视觉信息处理。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.

</details>


### [305] [End-to-end Autonomous Vehicle Following System using Monocular Fisheye Camera](https://arxiv.org/abs/2511.19011)
*Jiale Zhang,Yeqiang Qian,Tong Qin,Mingyang Jiang,Siyuan Chen,Ming Yang*

Main category: cs.RO

TL;DR: 本文提出了一种仅使用摄像头的车辆跟随框架，通过端到端方法提升整体驾驶性能，解决了现有编队系统依赖车道线和昂贵传感器的限制问题。


<details>
  <summary>Details</summary>
Motivation: 车辆保有量增加导致交通拥堵、事故频发和碳排放上升，车辆编队是解决这些问题的有前景方案，但现有系统依赖车道线和昂贵传感器，限制了广泛应用。

Method: 提出端到端车辆跟随框架，使用语义掩码解决多帧数据融合中的因果混淆问题，并引入动态采样机制精确跟踪前车轨迹。

Result: 在真实车辆闭环验证中，该系统能在各种场景下跟随前车，性能优于传统多阶段算法。

Conclusion: 该方法为成本效益高的自动驾驶车辆编队提供了有前景的解决方案。

Abstract: The increase in vehicle ownership has led to increased traffic congestion, more accidents, and higher carbon emissions. Vehicle platooning is a promising solution to address these issues by improving road capacity and reducing fuel consumption. However, existing platooning systems face challenges such as reliance on lane markings and expensive high-precision sensors, which limits their general applicability. To address these issues, we propose a vehicle following framework that expands its capability from restricted scenarios to general scenario applications using only a camera. This is achieved through our newly proposed end-to-end method, which improves overall driving performance. The method incorporates a semantic mask to address causal confusion in multi-frame data fusion. Additionally, we introduce a dynamic sampling mechanism to precisely track the trajectories of preceding vehicles. Extensive closed-loop validation in real-world vehicle experiments demonstrates the system's ability to follow vehicles in various scenarios, outperforming traditional multi-stage algorithms. This makes it a promising solution for cost-effective autonomous vehicle platooning. A complete real-world vehicle experiment is available at https://youtu.be/zL1bcVb9kqQ.

</details>


### [306] [Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors](https://arxiv.org/abs/2511.19031)
*Haihang Wu,Yuchen Zhou*

Main category: cs.RO

TL;DR: 本文提出首个多智能体单目密集SLAM系统，通过3D重建先验和基于闭环的地图融合机制，在保持精度的同时提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有单目SLAM系统计算成本高，且MASt3R-SLAM仅限于单智能体操作，需要扩展到多智能体场景以实现更高效的协同建图。

Method: 每个智能体使用3D重建先验进行局部SLAM，通过基于闭环的地图融合机制将个体地图融合为全局一致地图。

Result: 在真实世界数据集上的评估表明，相比最先进方法，该方法在保持相似建图精度的同时提高了计算效率。

Conclusion: 成功扩展了MASt3R-SLAM到多智能体场景，实现了首个多智能体单目密集SLAM系统，为协同建图提供了高效解决方案。

Abstract: Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.

</details>


### [307] [Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework](https://arxiv.org/abs/2511.19094)
*David Bricher,Andreas Mueller*

Main category: cs.RO

TL;DR: 提出基于深度学习的人机安全框架(HRSF)，通过动态调整机器人速度来提升协作效率，相比传统安全技术可减少15%的循环时间。


<details>
  <summary>Details</summary>
Motivation: 当前符合ISO/TS-15066标准的协作机器人实现由于保守的速度限制，限制了协作任务的效率。

Method: 开发基于深度学习的人机安全框架，使用四种深度学习方法来提取人体信息：人体识别、人体分割、人体姿态估计和人体部位分割，根据人机分离距离动态调整机器人速度。

Result: 实验证明相比传统安全技术，循环时间最多可减少15%。

Conclusion: 该框架能够区分人体部位与其他物体，实现优化的机器人过程执行，在保证安全的同时显著提高协作效率。

Abstract: Over the last years collaborative robots have gained great success in manufacturing applications where human and robot work together in close proximity. However, current ISO/TS-15066-compliant implementations often limit the efficiency of collaborative tasks due to conservative speed restrictions. For this reason, this paper introduces a deep-learning-based human-robot-safety framework (HRSF) that aims at a dynamical adaptation of robot velocities depending on the separation distance between human and robot while respecting maximum biomechanical force and pressure limits. The applicability of the framework was investigated for four different deep learning approaches that can be used for human body extraction: human body recognition, human body segmentation, human pose estimation, and human body part segmentation. Unlike conventional industrial safety systems, the proposed HRSF differentiates individual human body parts from other objects, enabling optimized robot process execution. Experiments demonstrated a quantitative reduction in cycle time of up to 15% compared to conventional safety technology.

</details>


### [308] [Efficient Optimization of a Permanent Magnet Array for a Stable 2D Trap](https://arxiv.org/abs/2511.19201)
*Ann-Sophia Müller,Moonkwang Jeong,Jiyuan Tian,Meng Zhang,Tian Qiu*

Main category: cs.RO

TL;DR: 本文提出了一种使用永磁体阵列实现稳定2D磁力陷阱的方法，用于控制医疗微型机器人。通过GPU加速优化算法计算磁体阵列的最佳角度，在20-120mm距离范围内实现稳定控制，并验证了算法的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决无缆磁控医疗微型机器人在大距离范围内施加高驱动力的挑战，克服Earnshaw定理限制，实现稳定的3D磁力控制。

Method: 采用GPU加速优化算法，使用均方误差和Adam优化器计算永磁体阵列的最佳角度，通过数值模拟和物理实验验证。

Result: 成功实现微型机器人的稳定捕获和复杂轨迹跟踪，算法具有高可扩展性，可在3秒内优化100个磁体的角度。

Conclusion: 该方法为医疗微型机器人的精确控制提供了有效解决方案，优化工作流程可适应实现所需力矢量场。

Abstract: Untethered magnetic manipulation of biomedical millirobots has a high potential for minimally invasive surgical applications. However, it is still challenging to exert high actuation forces on the small robots over a large distance. Permanent magnets offer stronger magnetic torques and forces than electromagnetic coils, however, feedback control is more difficult. As proven by Earnshaw's theorem, it is not possible to achieve a stable magnetic trap in 3D by static permanent magnets. Here, we report a stable 2D magnetic force trap by an array of permanent magnets to control a millirobot. The trap is located in an open space with a tunable distance to the magnet array in the range of 20 - 120mm, which is relevant to human anatomical scales. The design is achieved by a novel GPU-accelerated optimization algorithm that uses mean squared error (MSE) and Adam optimizer to efficiently compute the optimal angles for any number of magnets in the array. The algorithm is verified using numerical simulation and physical experiments with an array of two magnets. A millirobot is successfully trapped and controlled to follow a complex trajectory. The algorithm demonstrates high scalability by optimizing the angles for 100 magnets in under three seconds. Moreover, the optimization workflow can be adapted to optimize a permanent magnet array to achieve the desired force vector fields.

</details>


### [309] [Reference-Free Sampling-Based Model Predictive Control](https://arxiv.org/abs/2511.19204)
*Fabian Schramm,Pierre Fabre,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.RO

TL;DR: 提出一种基于采样的模型预测控制框架，无需依赖手工步态模式或预定义接触序列，通过优化高层目标自主发现多样化运动模式，包括小跑、疾跑、跳跃等，实现实时控制。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要手工设计步态模式和接触序列，限制了机器人的运动多样性和适应性。本文旨在通过优化高层目标自主发现运动模式，实现更自然和多样化的机器人运动。

Method: 基于模型预测路径积分（MPPI），提出双空间样条参数化方法，在位置和速度控制点上操作。该方法自动适应任务需求，实现接触建立和接触中断策略，仅需有限数量的采样轨迹。

Result: 在Go2四足机器人上验证了各种涌现步态和基本跳跃能力。在仿真中展示了更复杂行为，如后空翻、动态倒立平衡和类人机器人行走，无需参考跟踪或离线预训练。

Conclusion: 该方法能够实现实时控制，无需GPU加速，在标准CPU硬件上运行，展示了通过优化高层目标自主发现多样化运动模式的可行性。

Abstract: We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.

</details>


### [310] [Soft pneumatic grippers: Topology optimization, 3D-printing and experimental validation](https://arxiv.org/abs/2511.19211)
*Prabhat Kumar,Chandra Prakash,Josh Pinskier,David Howard,Matthijs Langelaar*

Main category: cs.RO

TL;DR: 本文提出了一个系统性的拓扑优化框架，用于设计软气动抓取器，明确考虑了驱动载荷的设计依赖性。通过Darcy定律和排水项建模载荷，采用稳健公式将2D软臂单元优化为柔顺机构设计问题，使用MMA算法求解。优化的2D单元性能优于传统矩形设计，并扩展到3D模块组装成软臂，最终3D打印实现SPG并验证其抓取性能。


<details>
  <summary>Details</summary>
Motivation: 传统软气动抓取器设计缺乏对驱动载荷设计依赖性的系统考虑，需要开发能够明确处理这种依赖性的拓扑优化框架，以提高软气动抓取器的性能和适应性。

Method: 使用Darcy定律加排水项建模气动载荷；采用稳健公式将2D软臂单元优化为柔顺机构设计问题；通过min-max优化考虑蓝图和侵蚀设计的输出变形；对蓝图部分施加体积约束，对侵蚀部分施加应变能约束；使用MMA算法求解优化问题；将优化的2D单元挤出为3D模块并组装成软臂。

Result: 有限元分析显示优化的2D单元在气动载荷下性能优于传统矩形设计；成功制造了由10个单元组成的软臂；3D打印了四个软臂并集成支撑结构实现SPG；在不同重量、尺寸、刚度和形状的物体上验证了抓取性能。

Conclusion: 提出的拓扑优化框架能够有效设计高性能软气动抓取器，明确考虑了驱动载荷的设计依赖性，优化设计在多种测试条件下表现出良好的抓取性能。

Abstract: This paper presents a systematic topology optimization framework for designing a soft pneumatic gripper (SPG), explicitly considering the design-dependent nature of the actuating load. The load is modeled using Darcy's law with an added drainage term. A 2D soft arm unit is optimized by formulating it as a compliant mechanism design problem using the robust formulation. The problem is posed as a min-max optimization, where the output deformations of blueprint and eroded designs are considered. A volume constraint is imposed on the blueprint part, while a strain-energy constraint is enforced on the eroded part. The MMA is employed to solve the optimization problem and obtain the optimized soft unit. Finite element analysis with the Ogden material model confirms that the optimized 2D unit outperforms a conventional rectangular design under pneumatic loading. The optimized 2D unit is extruded to obtain a 3D module, and ten such units are assembled to create a soft arm. Deformation profiles of the optimized arm are analysed under different pressure loads. Four arms are 3D-printed and integrated with a supporting structure to realize the proposed SPG. The gripping performance of the SPG is demonstrated on objects with different weights, sizes, stiffness, and shapes.

</details>


### [311] [SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control](https://arxiv.org/abs/2511.19236)
*Yuxuan Wang,Haobin Jiang,Shiqing Yao,Ziluo Ding,Zongqing Lu*

Main category: cs.RO

TL;DR: SENTINEL是一个端到端的语言-动作模型，用于人形机器人全身控制，直接将语言指令和本体感觉输入映射到低级动作，无需中间表示。


<details>
  <summary>Details</summary>
Motivation: 现有的人形控制系统依赖遥操作或模块化生成管道，前者完全由人类驱动，后者语言理解与物理执行分离，缺乏紧密对齐。

Method: 通过使用预训练全身控制器在模拟中跟踪人类运动构建大规模数据集，结合文本注释。模型使用流匹配生成动作块，可通过残差动作头进行细化以适应真实世界部署。

Result: 在模拟和真实世界部署中，该方法在人形机器人上表现出强大的语义理解和稳定执行能力，并支持通过将输入转换为文本来实现多模态扩展。

Conclusion: SENTINEL提供了一个完全端到端的解决方案，实现了语言指令与物理行为之间的紧密对齐，在人形机器人控制方面表现出色。

Abstract: Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.

</details>


### [312] [Rethinking Intermediate Representation for VLM-based Robot Manipulation](https://arxiv.org/abs/2511.19315)
*Weiliang Tang,Jialin Gao,Jia-Hui Pan,Gang Wang,Li Erran Li,Yunhui Liu,Mingyu Ding,Pheng-Ann Heng,Chi-Wing Fu*

Main category: cs.RO

TL;DR: 本文提出了SEAM表示方法，通过将中间表示分解为词汇和语法，设计语义丰富的操作词汇和VLM友好的语法，以处理多样未见任务。同时提出新的开放词汇分割范式和检索增强的少样本学习策略，实现快速细粒度物体部件定位。


<details>
  <summary>Details</summary>
Motivation: 解决使用视觉语言模型将人类指令转换为动作可解析中间表示时，在VLM可理解性和泛化性之间的权衡问题。

Method: 1) 设计SEAM表示，分解为词汇和语法；2) 提出开放词汇分割范式，采用检索增强的少样本学习策略；3) 制定新的动作泛化性和VLM可理解性评估指标。

Result: SEAM在动作泛化性和VLM可理解性方面均优于主流表示方法，在多种设置和任务下展现出最先进的性能，推理时间最短。

Conclusion: SEAM表示方法通过词汇-语法分解有效平衡了VLM可理解性和任务泛化性，结合高效的细粒度定位策略，在机器人操作任务中表现出色。

Abstract: Vision-Language Model (VLM) is an important component to enable robust robot manipulation. Yet, using it to translate human instructions into an action-resolvable intermediate representation often needs a tradeoff between VLM-comprehensibility and generalizability. Inspired by context-free grammar, we design the Semantic Assembly representation named SEAM, by decomposing the intermediate representation into vocabulary and grammar. Doing so leads us to a concise vocabulary of semantically-rich operations and a VLM-friendly grammar for handling diverse unseen tasks. In addition, we design a new open-vocabulary segmentation paradigm with a retrieval-augmented few-shot learning strategy to localize fine-grained object parts for manipulation, effectively with the shortest inference time over all state-of-the-art parallel works. Also, we formulate new metrics for action-generalizability and VLM-comprehensibility, demonstrating the compelling performance of SEAM over mainstream representations on both aspects. Extensive real-world experiments further manifest its SOTA performance under varying settings and tasks.

</details>


### [313] [Deployment Dynamics and Optimization of Novel Space Antenna Deployable Mechanism](https://arxiv.org/abs/2511.19377)
*Mamoon Aamir,Mariyam Sattar,Naveed Ur Rehman Junejo,Aqsa Zafar Abbasi*

Main category: cs.RO

TL;DR: 该论文提出了一种用于空间天线任务的新型三重剪刀可展开桁架机构(TSDTM)，该机构在发射时收起，在轨道上高效展开，提供最大孔径尺寸同时占用最小发射体积。


<details>
  <summary>Details</summary>
Motivation: 随着空间任务对大孔径天线需求的增加，将此类结构装入小型运载火箭的困难促使了可展开天线系统的设计。

Method: 论文涵盖了从几何建模、使用螺旋理论和牛顿方法的运动学分析、通过特征值和仿真方法的动力学分析，到使用SolidWorks验证的整个设计过程。此外，还基于支持向量机编写了优化程序用于LEO环境中的材料选择，以及使用机器学习方法进行几何设置。

Result: 所提出的TSDTM具有增强的结构动力学特性，仿真和分析预测之间具有良好的比较。优化结构证明高度准确，机器学习预测与模拟自然频率之间的偏差仅为1.94%。

Conclusion: 这证明了在空间结构设计中融入基于AI方法的潜力。

Abstract: Given the increasing need for large aperture antennas in space missions, the difficulty of fitting such structures into small launch vehicles has prompted the design of deployable antenna systems. The thesis introduces a new Triple Scissors Deployable Truss Mechanism (TSDTM) for space antenna missions. The new mechanism is to be stowed during launch and efficiently deploy in orbit, offering maximum aperture size while taking up minimal launch volume. The thesis covers the entire design process from geometric modeling, kinematic analysis with screw theory and Newtonian approaches, dynamic analysis by eigenvalue and simulation methods, and verification with SolidWorks. In addition, optimization routines were coded based on Support Vector Machines for material choice in LEO environments and machine learning method for geometric setup. The TSDTM presented has enhanced structural dynamics with good comparison between simulation and analytical predictions. The structure optimized proved highly accurate, with a deviation of just 1.94% between machine learning-predicted and simulated natural frequencies, demonstrating the potential of incorporating AI-based methods in space structural design.

</details>


### [314] [Mixture of Horizons in Action Chunking](https://arxiv.org/abs/2511.19433)
*Dong Jing,Gang Wang,Jiaqi Liu,Weiliang Tang,Zelong Sun,Yunchao Yao,Zhenyu Wei,Yunhui Liu,Zhiwu Lu,Mingyu Ding*

Main category: cs.RO

TL;DR: 本文提出了一种混合视野（MoH）策略来解决视觉-语言-动作模型中动作块长度选择的权衡问题。MoH通过并行处理不同视野长度的动作段，并融合输出，同时实现了长期前瞻性和短期精度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在机器人操作中表现出色，但其性能对训练时使用的动作块长度（视野）敏感。研究发现存在固有权衡：长视野提供更强的全局前瞻性但降低细粒度精度，短视野提高局部控制能力但在长期任务上表现不佳。

Method: 提出混合视野（MoH）策略：将动作块重新排列为几个具有不同视野的段，使用共享动作变换器并行处理，并通过轻量线性门融合输出。该方法支持动态推理和自适应视野选择。

Result: 在基于流的策略π₀、π₀.₅和一步回归策略π_reg上的广泛实验表明，MoH在仿真和真实世界任务中均带来一致且显著的性能提升。在混合任务设置下，π₀.₅结合MoH在仅30k训练迭代后达到LIBERO上99%的平均成功率，创下新纪录。

Conclusion: MoH策略有效缓解了动作块长度选择的权衡问题，在保持最小训练和推理开销的同时，显著提升了模型性能和泛化能力，特别是在复杂任务上的表现。

Abstract: Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\textbf{action chunk length}$ used during training, termed $\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [315] [The use of artificial intelligence in music creation: between interface and appropriation](https://arxiv.org/abs/2511.17507)
*Arnaud Zeller,Emmanuelle Chevry Pebayle*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: By observing the activities and relationships of musicians and sound designers to the activities of creation, performance, publishing and dissemination with artificial intelligence (AI), from two specialized forums between 2022 and 2024, this article proposes a lexicometric analysis of the representations linked to their use. Indeed, the machine, now equipped with artificial intelligences requiring new appropriations and enabling new mediations, constitutes new challenges for artists. To study these confrontations and new mediations, our approach mobilizes the theoretical framework of the Human-AI Musicking Framework, based on a lexicometric analysis of content. The aim is to clarify the present and future uses of AI from the interfaces, in the creation of sound and musical content, and to identify the obstacles, obstacles, brakes and limits to appropriation ``in the fact of making the content one's own and integrating it as a part of oneself'' (Bachimont and Crozat, 2004) in the context of a collaboration between musician and machine.

</details>


### [316] [Beyond Awareness: Investigating How AI and Psychological Factors Shape Human Self-Confidence Calibration](https://arxiv.org/abs/2511.17509)
*Federico Maria Cau,Lucio Davide Spano*

Main category: cs.HC

TL;DR: 本研究通过两个实验探讨人类自我信心校准、认知需求水平和积极开放思维对AI辅助决策的影响，发现这些因素显著影响决策准确性和元认知感知，并提出了针对个体特质的AI系统设计建议。


<details>
  <summary>Details</summary>
Motivation: 人类与AI协作的效果很大程度上取决于人类自我信心的校准，这影响对AI建议的依赖或抵制。研究旨在了解自我信心校准、认知需求水平和积极开放思维如何影响决策准确性、自我信心适当性和元认知感知。

Method: 研究一：识别良好校准用户的方法，比较不同认知需求和积极开放思维水平下的决策准确性和自我信心适当性。研究二：在AI辅助决策（无AI、两阶段AI和个性化AI）中检验校准自我信心的效果，同时考虑不同认知需求和积极开放思维水平。

Result: 结果显示人类自我信心校准和心理特质在AI辅助决策系统设计中的重要性。校准良好的自我信心和特定心理特质组合能提高决策准确性和元认知感知。

Conclusion: 研究强调了在设计AI辅助决策系统时考虑人类自我信心校准和个体心理特质的必要性，提出了解决自我信心校准挑战和支持个性化、以用户为中心的AI的设计建议。

Abstract: Human-AI collaboration outcomes depend strongly on human self-confidence calibration, which drives reliance or resistance toward AI's suggestions. This work presents two studies examining whether calibration of self-confidence before decision tasks, low versus high levels of Need for Cognition (NFC), and Actively Open-Minded Thinking (AOT), leads to differences in decision accuracy, self-confidence appropriateness during the tasks, and metacognitive perceptions (global and affective). The first study presents strategies to identify well-calibrated users, also comparing decision accuracy and the appropriateness of self-confidence across NFC and AOT levels. The second study investigates the effects of calibrated self-confidence in AI-assisted decision-making (no AI, two-stage AI, and personalized AI), also considering different NFC and AOT levels. Our results show the importance of human self-confidence calibration and psychological traits when designing AI-assisted decision systems. We further propose design recommendations to address the challenge of calibrating self-confidence and supporting tailored, user-centric AI that accounts for individual traits.

</details>


### [317] [First Contact with Dark Patterns and Deceptive Designs in Chinese and Japanese Free-to-Play Mobile Games](https://arxiv.org/abs/2511.17512)
*Gloria Xiaodan Zhang,Yijia Wang,Taro Leo Nakajima,Katie Seaborn*

Main category: cs.HC

TL;DR: 研究探索了中国和日本免费移动游戏中的暗黑模式，识别了独特模式、DP组合策略和增强器，并开发了分类本体论。


<details>
  <summary>Details</summary>
Motivation: 移动游戏因可访问性而广受欢迎，但其中存在暗黑模式和欺骗性设计。本研究旨在探索中国和日本免费移动游戏中的这些模式。

Method: 分析中国和日本免费移动游戏的入门体验，识别暗黑模式，绘制相对流行度，并开发分类本体论。

Result: 发现了独特的暗黑模式、DP组合策略和增强器，创建了欺骗性游戏设计模式的分类系统。

Conclusion: 研究增进了对欺骗性游戏设计模式的理解，为未来文化维度和伦理游戏设计研究提供了见解。

Abstract: Mobile games have gained immense popularity due to their accessibility, allowing people to play anywhere, anytime. Dark patterns and deceptive designs (DPs) have been found in these and other gaming platforms within certain cultural contexts. Here, we explored DPs in the onboarding experiences of free-to-play mobile games from China and Japan. We identified several unique patterns and mapped their relative prevalence. We also found that game developers often employ combinations of DPs as a strategy ("DP Combos") and use elements that, while not inherently manipulative, can enhance the impact of known patterns ("DP Enhancers"). Guided by these findings, we then developed an enriched ontology for categorizing deceptive game design patterns into classes and subclasses. This research contributes to understanding deceptive game design patterns and offers insights for future studies on cultural dimensions and ethical game design in general.

</details>


### [318] [Motivational Climate Effects on Communications, Emotional-Social States, and Performance in Collaborative Gaming Environment](https://arxiv.org/abs/2511.17513)
*Omer Eldadi,Yarin Dekimhi,Gershon Tenenbaum*

Main category: cs.HC

TL;DR: 研究探讨了动机氛围对协作游戏环境中沟通特征、情绪状态、集体效能和表现的影响，发现积极支持性氛围能显著提升团队表现、沟通效率和情绪韧性。


<details>
  <summary>Details</summary>
Motivation: 探索在协作游戏环境中，不同的动机氛围如何影响团队沟通、情绪状态、集体效能和表现表现，为理解团队协作机制提供实证依据。

Method: 40名无游戏经验参与者被随机分配到20个性别匹配的三人团队，分为积极支持性和中性无支持两种动机氛围条件。通过三个难度递增的Overcooked! 2游戏关卡，观察并编码沟通内容、情绪反应、集体效能和表现结果。使用混合设计MANOVA和ANOVA分析动机氛围和任务难度的影响，卡方分析检验沟通内容差异。

Result: 积极支持性团队在低难度任务中表现显著更好，但随着任务复杂性增加优势减弱。积极支持性团队使用更多行动导向、事实性和情感/激励性陈述，而中性无支持团队使用更多不确定性和非任务相关沟通。积极支持性团队成员保持更积极的情绪状态，集体效能信念在所有难度水平上都更高。

Conclusion: 积极动机氛围能显著增强团队沟通有效性、情绪韧性和在挑战性协作环境中的表现结果，为团队协作干预提供了重要启示。

Abstract: The study explores the effects of motivational climate on communication features, emotional states, collective efficacy, and performance in collaborative gaming environments. Forty participants with no prior gaming experience were randomly assigned to 20 gender-matched teams of three (including one confederate) across two motivational climates: positive-supportive (PS) or neutral-unsupported (NU) (10 teams per condition). Team members completed three progressively difficult levels of Overcooked! 2 during which communication contents, emotional responses, collective efficacy, and performance outcomes were observed and coded. Mixed-design MANOVAs and ANOVAs were employed to examine the effects of motivational climate and task difficulty on communication patterns, emotions, collective efficacy, and performance. Chi-square analyses were performed to test communication content differences between conditions. Results revealed that PS team members significantly outperformed NU teams at lower task difficulty level, but this advantage diminished as task complexity increased. Communication analysis revealed that PS team members utilized significantly more action-oriented, factual, and emotional/motivational statements, while NU team members used more statements of uncertainty and non-task-related communication. The percentage of the talk time increased with difficulty across both climate conditions. PS team members maintained more positive emotional profiles throughout, with higher excitement and happiness scores and lower anxiety, dejection, and anger compared to NU team members. Furthermore, PS team members reported consistently higher collective efficacy beliefs across all difficulty levels. These findings reveal that positive motivational climate enhances team communication effectiveness, emotional resilience, and performance outcomes in challenging collaborative environments.

</details>


### [319] [Embedding Generative AI into Systems Analysis and Design Curriculum: Framework, Case Study, and Cross-Campus Empirical Evidence](https://arxiv.org/abs/2511.17515)
*Mahmoud Elkhodr,Ergun Gide*

Main category: cs.HC

TL;DR: SAGE框架通过将生成式AI嵌入课程设计，教导学生如何负责任地编排AI，包括何时接受、修改或拒绝AI建议。研究发现学生能够发展选择性判断能力，但在识别AI和人类分析都忽略的差距方面存在能力上限。


<details>
  <summary>Details</summary>
Motivation: 当前教学缺乏系统方法来教导负责任地编排AI，学生存在盲目接受AI建议的风险，需要培养批判性思维同时满足教育成果。

Method: 在四所澳大利亚大学的18个学生小组中实施SAGE框架，将生成式AI嵌入课程设计，训练学生评估AI建议的适当性。

Result: 84%的学生小组超越了被动接受，展现出选择性判断；但无人能主动识别AI和人类分析都忽略的差距。55%的小组难以识别AI对系统边界的错误分类，45%遗漏数据管理错误，55%忽略异常处理缺失。

Conclusion: 教育者应要求学生记录接受/修改/拒绝AI建议的原因，在每个开发阶段嵌入可访问性提示，并让学生在使AI前创建自己的规范进行比较，以识别差距。

Abstract: Systems analysis students increasingly use Generative AI, yet current pedagogy lacks systematic approaches for teaching responsible AI orchestration that fosters critical thinking whilst meeting educational outcomes. Students risk accepting AI suggestions blindly or uncritically without assessing alignment with user needs or contextual appropriateness. SAGE (Structured AI-Guided Education) addresses this gap by embedding GenAI into curriculum design, training students when to accept, modify, or reject AI contributions. Implementation with 18 student groups across four Australian universities revealed how orchestration skills develop. Most groups (84\%) moved beyond passive acceptance, showing selective judgment, yet none proactively identified gaps overlooked by both human and AI analysis, indicating a competency ceiling. Students strong at explaining decisions also performed well at integrating sources, and those with deep domain understanding consistently considered accessibility considerations. Accessibility awareness proved fragile. When writing requirements, 85\% of groups explicitly considered elderly users and cultural needs. Notably, 55\% of groups struggled identifying when AI misclassified system boundaries (what belongs inside versus outside the system), 45\% missed data management errors (how information is stored and updated), and 55\% overlooked missing exception handling. Three implications emerge for educators: (i) require students to document why they accepted, modified, or rejected each AI suggestion, making reasoning explicit; (ii) embed accessibility prompts at each development stage because awareness collapses without continuous scaffolding; and (iii) have students create their own specifications before using AI, then compare versions, and anchor to research or standards to identify gaps.

</details>


### [320] [A Dynamic Take on Window Management](https://arxiv.org/abs/2511.17516)
*Rohit Chouhan*

Main category: cs.HC

TL;DR: 本研究评估动态窗口管理器与传统浮动窗口系统的可用性，开发了支持可配置布局和工作区管理的原型系统，发现动态窗口管理器在多窗口工作流中显著提升任务完成时间37.83%。


<details>
  <summary>Details</summary>
Motivation: 传统窗口管理器依赖手动调整窗口，在小屏幕设备上效率低下，频繁的窗口调整会中断工作流程并增加任务完成时间。动态窗口管理虽然具有潜在可用性优势，但仍是小众技术。

Method: 开发了包含可配置布局和工作区管理的原型动态窗口管理器，并进行了启发式评估和统计测试来评估其有效性。

Result: 动态窗口管理器在多窗口工作流中显著改善任务完成时间达37.83%。

Conclusion: 动态窗口管理作为传统浮动窗口系统的可行替代方案具有潜力，为更广泛的人机交互领域提供了基于证据的见解。

Abstract: On modern computers with graphical user interfaces, application windows are managed by a window manager, a core component of the desktop environment. Mainstream operating systems such as Microsoft Windows and Apple's macOS employ window managers, where users rely on a mouse or trackpad to manually resize, reposition, and switch between overlapping windows. This approach can become inefficient, particularly on smaller screens such as laptops, where frequent window adjustments disrupt workflow and increase task completion time. An alternative paradigm, dynamic window management, automatically arranges application windows into non-overlapping layouts. These systems reduce the need for manual manipulation by providing intelligent placement strategies and support for multiple workspaces. Despite their potential usability benefits, dynamic window managers remain niche, primarily available on Linux systems and rarely enabled by default. This study evaluates the usability of dynamic window managers in comparison to conventional floating window systems. We developed a prototype dynamic window manager that incorporates configurable layouts and workspace management, and we conducted both heuristic evaluation and statistical testing to assess its effectiveness. Our findings indicate that dynamic window managers significantly improve task completion time in multi-window workflows by 37.83%. By combining cognitive heuristics with empirical performance measures, this work highlights the potential of dynamic window management as a viable alternative to traditional floating window systems and contributes evidence-based insights to the broader field of human-computer interaction (HCI).

</details>


### [321] [Digital Diasporas: How Origin Characteristics and Host-Native Distance Shape Immigrants' Online Cultural Retention](https://arxiv.org/abs/2511.17756)
*Aparup Khatua,David Jurgens,Ingmar Weber*

Main category: cs.HC

TL;DR: 基于Facebook广告数据研究移民文化保留倾向，发现更大的东道国-原籍国文化距离与更高的在线文化保留相关，而原籍国背景影响相对较小。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要关注移民的文化适应（熔炉假说），本研究试图识别文化马赛克假说的前因，即增强或削弱移民文化保留倾向的因素。

Method: 使用来自8个国家的美国移民的Facebook广告数据进行分析。

Result: 研究发现，更大的东道国-原籍国文化距离与更高的在线文化保留相关，原籍国背景虽然统计显著但影响相对较小。

Conclusion: 文化距离是影响移民在线文化保留倾向的重要因素，支持文化马赛克假说而非传统的熔炉假说。

Abstract: Immigrants bring unique cultural backgrounds to their host countries. Subsequent interplay of cultures can lead to either a melting pot, where immigrants adopt the dominant culture of the host country, or a mosaic, where distinct cultural identities coexist. The existing literature primarily focuses on the acculturation of immigrants, specifically the melting pot hypothesis. In contrast, we attempt to identify the antecedents of the mosaic hypothesis or factors that enhance (or diminish) the propensity for cultural retention among immigrants. Based on Facebook advertising data for immigrants from 8 countries residing in the USA, our findings suggest that greater host-native distance is linked to higher online cultural retention, and while origin country context is statistically significant, its impact is generally smaller.

</details>


### [322] [AnimAgents: Coordinating Multi-Stage Animation Pre-Production with Human-Multi-Agent Collaboration](https://arxiv.org/abs/2511.17906)
*Wen-Fan Wang,Chien-Ting Lu,Jin Ping Ng,Yi-Ting Chiu,Ting-Ying Lee,Miaosen Wang,Bing-Yu Chen,Xiang 'Anthony' Chen*

Main category: cs.HC

TL;DR: AnimAgents是一个人类-多智能体协作系统，通过核心智能体和专门智能体协调动画前期制作的复杂多阶段工作流程，显著优于单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI工具在动画前期制作中相互孤立，创作者需要在多个系统间手动协调，面临碎片化输出、信息量大以及保持阶段间连续性和创意控制的挑战。

Method: 基于对12位专业创意总监和独立动画师的调研，开发了AnimAgents系统，该系统通过核心智能体和专门智能体协调四个主要前期制作阶段，提供阶段感知编排、阶段特定输出管理和元素级细化功能。

Result: 在16位专业创作者的受试者内总结性研究中，AnimAgents在协调性、一致性、信息管理和总体满意度方面显著优于配备先进并行图像生成的单智能体基线（p < .01）。

Conclusion: AnimAgents提供了一个端到端的工作流程，有效支持专业动画前期制作实践，在实际项目中展现出良好的效果。

Abstract: Animation pre-production lays the foundation of an animated film by transforming initial concepts into a coherent blueprint across interdependent stages such as ideation, scripting, design, and storyboarding. While generative AI tools are increasingly adopted in this process, they remain isolated, requiring creators to juggle multiple systems without integrated workflow support. Our formative study with 12 professional creative directors and independent animators revealed key challenges in their current practice: Creators must manually coordinate fragmented outputs, manage large volumes of information, and struggle to maintain continuity and creative control between stages. Based on the insights, we present AnimAgents, a human-multi-agent collaborative system that coordinates complex, multi-stage workflows through a core agent and specialized agents, supported by dedicated boards for the four major stages of pre-production. AnimAgents enables stage-aware orchestration, stage-specific output management, and element-level refinement, providing an end-to-end workflow tailored to professional practice. In a within-subjects summative study with 16 professional creators, AnimAgents significantly outperformed a strong single-agent baseline that equipped with advanced parallel image generation in coordination, consistency, information management, and overall satisfaction (p < .01). A field deployment with 4 creators further demonstrated AnimAgents' effectiveness in real-world projects.

</details>


### [323] [Exploring Multiview UI Layouts and Placement Strategies for Collaborative Sensemaking in Virtual Reality](https://arxiv.org/abs/2511.17919)
*Tamzid Hossain,Md. Fahimul Islam,Farida Chowdhury*

Main category: cs.HC

TL;DR: 研究探讨了在远程沉浸式工作空间中，文档类型（图表、图像）和配对动态对协作布局形成的影响。通过20名参与者的用户研究发现，用户常采用半圆形布局进行协作，不同文档类型在不同任务中降低认知负荷，复杂比较任务中易产生窗口选择冲突。


<details>
  <summary>Details</summary>
Motivation: 沉浸式技术为协作感知和视觉分析提供了新可能，但文档类型和配对动态在协作布局形成中的作用尚未被充分探索。研究旨在了解用户如何在远程沉浸式工作空间中组织多视图窗口。

Method: 进行了包含20名参与者的用户研究，观察用户在搜索、比较和分类等任务中如何组织多视图窗口，分析文档类型（图表、图像）和配对动态的影响。

Result: 用户常采用半圆形布局进行协作；图像+文本文档在比较任务中降低心理和时间需求；图表在分类任务中降低任务负荷；复杂比较任务中易产生窗口选择冲突，困难任务中频繁讨论和重新组织。

Conclusion: 基于研究发现，提出了支持VR协作和头脑风暴的多视图系统设计指南，强调需要考虑文档类型和任务特性来优化协作体验。

Abstract: Immersive technologies expand the potential for collaborative sense-making and visual analysis via head-worn displays (HWDs), offering customizable, high-resolution perspectives of a shared visualization space. In such an immersive environment, window/view management is crucial for collaborative sense-making tasks. However, the role of document types (graphs, images) and pair dynamics in collaborative layout formation has rarely been explored. We conducted a user study with 20 participants to explore how pair of users organize multiview windows in remote immersive workspaces during tasks such as search, comparison, and classification. Findings show that users often arrange windows in a semi-circular layout for pair collaboration. Image+text documents reduce mental and temporal demand in comparison tasks, while graphs lower task load for classification. Conflicts in window selection arise mainly in complex comparisons, with frequent discussion and reorganization during difficult tasks. Based on these insights, we propose design guidelines for multiview systems that support VR collaboration and brainstorming.

</details>


### [324] [Typing Reinvented: Towards Hands-Free Input via sEMG](https://arxiv.org/abs/2511.18213)
*Kunwoo Lee,Dhivya Sreedhar,Pushkar Saraf,Chaeeun Lee,Kateryna Shapovalenko*

Main category: cs.HC

TL;DR: 使用注意力架构的表面肌电图(sEMG)系统，用于在空间计算和VR中实现肌肉驱动的实时文本输入，显著优于现有卷积基线。


<details>
  <summary>Details</summary>
Motivation: 为下一代人机交互开发非侵入式输入方式，特别是在空间计算和虚拟现实中，传统键盘不实用。

Method: 采用基于注意力的架构，结合轻量级解码流程和基于语言模型的校正。

Result: 在线通用CER从24.98%降至20.34%，离线个性化CER从10.86%降至10.10%，同时保持完全因果性。

Conclusion: 证明了基于肌肉活动的准确实时文本输入在未来可穿戴和空间界面中的可行性。

Abstract: We explore surface electromyography (sEMG) as a non-invasive input modality for mapping muscle activity to keyboard inputs, targeting immersive typing in next-generation human-computer interaction (HCI). This is especially relevant for spatial computing and virtual reality (VR), where traditional keyboards are impractical. Using attention-based architectures, we significantly outperform the existing convolutional baselines, reducing online generic CER from 24.98% -> 20.34% and offline personalized CER from 10.86% -> 10.10%, while remaining fully causal. We further incorporate a lightweight decoding pipeline with language-model-based correction, demonstrating the feasibility of accurate, real-time muscle-driven text input for future wearable and spatial interfaces.

</details>


### [325] [Clinician-Directed Large Language Model Software Generation for Therapeutic Interventions in Physical Rehabilitation](https://arxiv.org/abs/2511.18274)
*Edward Kim,Yuri Cho,Jose Eduardo E. Lima,Julie Muccini,Jenelle Jindal,Alison Scheid,Erik Nelson,Seong Hyun Park,Yuchen Zeng,Alton Sturgis,Caesar Li,Jackie Dai,Sun Min Kim,Yash Prakash,Liwen Sun,Isabella Hu,Hongxuan Wu,Daniel He,Wiktor Rajca,Cathra Halabi,Maarten Lansberg,Bjoern Hartmann,Sanjit A. Seshia*

Main category: cs.HC

TL;DR: 本研究评估了一种使用大型语言模型（LLMs）将临床医生的运动处方自动转化为干预软件的数字干预范式，结果显示相比基于模板的基准方法，个性化处方的软件实现比例增加了45%，且生成软件能正确执行99.78%的指令。


<details>
  <summary>Details</summary>
Motivation: 当前数字健康干预通常作为预编程软件在临床就诊前开发，只能选择模块和调整有限参数，无法反映就诊期间出现的患者特定需求（如独特的运动限制和家庭环境）。

Method: 采用前瞻性单臂可行性研究，20名持证物理和职业治疗师与标准化患者创建了40个个性化上肢运动程序（398条指令），通过LLM自动翻译为可执行软件。

Result: LLM生成软件正确执行了99.78%（397/398）的处方指令，性能监测准确率达88.4%（352/398）；90%治疗师认为与患者交互安全，75%表示愿意采用该技术。

Conclusion: 这是医疗保健领域首次对临床医生指导的LLM干预软件生成进行前瞻性评估，证明了可行性，并激励开展更大规模试验以评估真实患者群体中的临床有效性和安全性。

Abstract: Digital health interventions are increasingly used in physical and occupational therapy to deliver home exercise programs via sensor equipped devices such as smartphones, enabling remote monitoring of adherence and performance. However, digital interventions are typically programmed as software before clinical encounters as libraries of parametrized exercise modules targeting broad patient populations. At the point of care, clinicians can only select modules and adjust a narrow set of parameters like repetitions, so patient specific needs that emerge during encounters, such as distinct movement limitations, and home environments, are rarely reflected in the software. We evaluated a digital intervention paradigm that uses large language models (LLMs) to translate clinicians' exercise prescriptions into intervention software. In a prospective single arm feasibility study with 20 licensed physical and occupational therapists and a standardized patient, clinicians created 40 individualized upper extremity programs (398 instructions) that were automatically translated into executable software. Our results show a 45% increase in the proportion of personalized prescriptions that can be implemented as software compared with a template based benchmark, with unanimous consensus among therapists on ease of use. The LLM generated software correctly delivered 99.78% (397/398) of instructions as prescribed and monitored performance with 88.4% (352/398) accuracy, with 90% (18/20) of therapists judged it safe to interact with patients, and 75% (15/20) expressed willingness to adopt it. To our knowledge, this is the first prospective evaluation of clinician directed intervention software generation with LLMs in healthcare, demonstrating feasibility and motivating larger trials to assess clinical effectiveness and safety in real patient populations.

</details>


### [326] [The Evaluation for Usability Methods of Unmanned Surface Vehicles: Are Current Usability Methods Viable for Unmanned Surface Vehicles? Insights from a Multiple Case Study Approach to Human-Robot Interaction](https://arxiv.org/abs/2511.18561)
*Zitian Peng,Shiyao Zhang,Shanliang Yao,Xiaohui Zhu,Mengjie Huang,Prudence Wong,Yong Yue*

Main category: cs.HC

TL;DR: 本研究通过访谈和实地观察，探讨了无人水面艇（USV）操作中的实际可用性挑战，特别关注新手操作员的困难及其应对策略，为海事人机交互设计提供新见解。


<details>
  <summary>Details</summary>
Motivation: 随着USV在环境监测、安全巡逻等领域的广泛应用，完全自主性仍无法在所有场景实现，远程人工干预在动态复杂环境中至关重要。相比无人机和自动驾驶车辆领域的人机交互研究，USV特定的人机交互考虑仍显著不足。

Method: 通过对9名工程师和用户进行深度访谈，并结合实地观察，研究USV操作中的可用性挑战，特别关注新手操作员的困难和应对策略。

Result: 发现当前USV系统在动态内陆和离岸环境中不适合新手操作，操作员需在不确定性下及时决策、管理复杂空间意识并适应变化的环境条件。识别了三个代表性用例中的关键操作模式：有害藻华检测、水下隐蔽管道检查和施工后水文测量。

Conclusion: 研究揭示了新手操作员的可用性问题、心智模型和适应策略，为未来以用户为中心的USV系统设计提供了依据，为新兴的海事人机交互领域贡献了新见解，并总结了应指导未来海事人机交互设计的关键交互约束。

Abstract: Unmanned Surface Vehicles (USVs) are increasingly utilised for diverse applications, ranging from environmental monitoring to security patrols. While USV technology is progressing, it remains clear that full autonomy is not achievable in all scenarios, and remote human intervention is still crucial, particularly in dynamic or complex environments. This continued reliance on human intervention highlights a range of Human-Robot Interaction (HRI) challenges that remain unresolved. Compared to the extensive body of HRI research in domains such as unmanned aerial vehicles and autonomous vehicles, HRI considerations specific to USVs remain significantly underexplored. Addressing this gap, our study investigates real-world usability challenges in USV operation through in-depth interviews with 9 engineers and users, supported by field observations. We focus especially on the difficulties beginner operators encounter and their coping strategies. Our findings reveal existing usability issues, mental models, and adaptation strategies of beginners that inform future user-centered design of USV systems, contributing new insights to the emerging field of maritime HRI. Based on these findings, we argue that current USV systems are poorly suited for beginner operation in dynamic inland and offshore environments, where operators must make timely decisions under uncertainty, manage complex spatial awareness, and adapt to changing environmental conditions. Furthermore, we identify key operational patterns in three representative use cases-harmful algal bloom detection, underwater concealed pipe inspection and post-construction hydrographic survey, and summarise key interaction constraints that should inform future maritime HRI design efforts.

</details>


### [327] [REFLECTing SPERET: Measuring and Promoting Ethics and Privacy Reflexivity in Eye-Tracking Research](https://arxiv.org/abs/2511.18965)
*Susanne Hindennach,Mayar Elfares,Céline Gressel,Andreas Bulling*

Main category: cs.HC

TL;DR: 本文开发了REFLECT定性问卷和SPERET定量心理测量量表两个工具，用于评估眼动追踪研究中的隐私和伦理反思性，发现研究社区关注用户隐私并具有不断发展的伦理责任感。


<details>
  <summary>Details</summary>
Motivation: 眼动追踪技术在高风险领域的广泛应用凸显了伦理意识的重要性，但关于研究人员如何反思自身工作的实证研究仍然稀缺，需要填补这一空白。

Method: 通过70多位研究人员的参与，开发了REFLECT定性问卷和SPERET定量心理测量量表，系统评估眼动追踪研究中的隐私和伦理反思性。

Result: 研究发现眼动追踪研究社区关注用户隐私，认识到方法限制（如样本偏差），并具有随着项目成熟而发展的细致伦理责任感。

Conclusion: 这些工具和分析为眼动追踪研究中的反思性提供了系统检查，并展现了希望的前景，促进了更具隐私和伦理意识的实践。

Abstract: The proliferation of eye tracking in high-stakes domains - such as healthcare, marketing and surveillance - underscores the need for researchers to be ethically aware when employing this technology. Although privacy and ethical guidelines have emerged in recent years, empirical research on how scholars reflect on their own work remains scarce. To address this gap, we present two complementary instruments developed with input from more than 70 researchers: REFLECT, a qualitative questionnaire, and SPERET (Latin for "hope"), a quantitative psychometric scale that measures privacy and ethics reflexivity in eye tracking. Our findings reveal a research community that is concerned about user privacy, cognisant of methodological constraints, such as sample bias, and that possesses a nuanced sense of ethical responsibility evolving with project maturity. Together, these tools and our analyses offer a systematic examination and a hopeful outlook on reflexivity in eye-tracking research, promoting more privacy and ethics-conscious practice.

</details>


### [328] [LLM Chatbots in High School Programming: Exploring Behaviors and Interventions](https://arxiv.org/abs/2511.18985)
*Manuel Valle Torre,Marcus Specht,Catharine Oertel*

Main category: cs.HC

TL;DR: 本研究通过设计研究循环优化LLM在高中编程教育中的应用，发现无引导下学生倾向于执行性查询，这与考试成绩负相关。教学干预成功改变了学生行为，但未显著提升成绩，表明仅改变工具使用策略不足以弥补基础知识差距。


<details>
  <summary>Details</summary>
Motivation: 识别高中编程教育中LLM使用的潜在问题：无引导环境下学生倾向于执行性查询，这与考试成绩呈负相关，且这种行为模式不会自我纠正。

Method: 采用设计研究方法，通过对比组分析发现问题，在教学组实施中期教学干预，教授工具性求助策略，并进行后续评估。

Result: 干预成功减少了执行性查询，转向更高效的学习工作流程，但考试成绩未出现统计学显著改善。

Conclusion: LLM的教育价值依赖于支持求助的教学方法，但这只是复杂学习过程的一部分，仅改变工具使用策略不足以克服基础知识差距。

Abstract: This study uses a Design-Based Research (DBR) cycle to refine the integration of Large Language Models (LLMs) in high school programming education. The initial problem was identified in an Intervention Group where, in an unguided setting, a higher proportion of executive, solution-seeking queries correlated strongly and negatively with exam performance. A contemporaneous Comparison Group demonstrated that without guidance, these unproductive help-seeking patterns do not self-correct, with engagement fluctuating and eventually declining. This insight prompted a mid-course pedagogical intervention in the first group, designed to teach instrumental help-seeking. The subsequent evaluation confirmed the intervention's success, revealing a decrease in executive queries, as well as a shift toward more productive learning workflows. However, this behavioral change did not translate into a statistically significant improvement in exam grades, suggesting that altering tool-use strategies alone may be insufficient to overcome foundational knowledge gaps. The DBR process thus yields a more nuanced principle: the educational value of an LLM depends on a pedagogy that scaffolds help-seeking, but this is only one part of the complex process of learning.

</details>


### [329] [Facilitating the Integration of LLMs Into Online Experiments With Simple Chat](https://arxiv.org/abs/2511.19123)
*R. Bermudez Schettino,A. Dasmeh,L. Brinkmann*

Main category: cs.HC

TL;DR: Simple Chat是一个开源研究工具，简化了在Qualtrics等调查平台中集成大型语言模型的技术难度，提供实时对话体验和实验控制。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型日益普及，理解人机交互成为心理学研究的重点，但现有调查平台集成LLM存在技术障碍，难以实现生态有效的实时对话体验。

Method: 开发开源聊天界面Simple Chat，支持商业和开源模型，提供流式响应、统一参与者体验和管理界面，可精细控制提示词和界面功能。

Result: 成功创建了一个降低技术门槛、标准化界面、改善参与者体验的工具，通过两个案例研究展示了其效用。

Conclusion: Simple Chat通过减少技术障碍、标准化界面和改进参与者体验，有助于推进人机交互研究的发展。

Abstract: As large language models (LLMs) become increasingly prevalent, understanding human-LLM interactions is emerging as a central priority in psychological research. Online experiments offer an efficient means to study human-LLM interactions, yet integrating LLMs into established survey platforms remains technically demanding, particularly when aiming for ecologically valid, real-time conversational experiences with strong experimental control. We introduce Simple Chat, an open-source, research-focused chat interface that streamlines LLM integration for platforms such as Qualtrics, oTree, and LimeSurvey, while presenting a unified participant experience across conditions. Simple Chat connects to both commercial providers and open-weights models, supports streaming responses to preserve conversational flow, and offers an administrative interface for fine-grained control of prompts and interface features. By reducing technical barriers, standardizing interfaces, and improving participant experience, Simple Chat helps advance the study of human-LLM interaction. In this article, we outline Simple Chat's key features, provide a step-by-step tutorial, and demonstrate its utility through two illustrative case studies.

</details>


### [330] [Human-AI Teaming Under Deception: An Implicit BCI Safeguards Drone Team Performance in Virtual Reality](https://arxiv.org/abs/2511.19312)
*Christopher Baker,Stephen Hinton,Akashdeep Nijjar,Riccardo Poli,Caterina Cinel,Tom Reed,Stephen Fairclough*

Main category: cs.HC

TL;DR: 本研究开发了一种基于脑机接口的神经解耦团队方法，在AI提供误导性反馈的高认知负荷环境下，通过使用决策前的脑电信号来保护团队免受AI错误的影响，相比传统行为聚合方法显著提高了团队决策准确性。


<details>
  <summary>Details</summary>
Motivation: 人类-AI团队在面对AI错误反馈时容易发生灾难性失败，特别是在高认知负荷下。传统团队聚合方法容易受到AI错误的影响，导致群体决策错误率增加。

Method: 在VR无人机监控任务中，比较传统基于行为的团队策略与纯神经解耦团队方法。神经解耦团队仅使用基于决策前脑电信号的BCI置信度分数，而不依赖行为决策。

Result: 在AI欺骗条件下，基于行为的团队准确率崩溃至44%，而神经解耦团队保持了98%的准确率，显著优于团队中最佳个体表现者。

Conclusion: 隐式BCI通过适应其神经策略提供弹性，在简单条件下依赖高效自动处理信号，在认知冲突时转向解释费力审议的特征，从而在高风险环境中防御AI引发的错误。

Abstract: Human-AI teams can be vulnerable to catastrophic failure when feedback from the AI is incorrect, especially under high cognitive workload. Traditional team aggregation methods, such as voting, are susceptible to these AI errors, which can actively bias the behaviour of each individual and inflate the likelihood of an erroneous group decision. We hypothesised that a collaborative Brain-Computer Interface (cBCI) using neural activity collected before a behavioural decision is made can provide a source of information that is decoupled from this biased behaviour, thereby protecting the team from the deleterious influence of AI error. We tested this in a VR drone surveillance task where teams of operators faced high workload and systematically misleading AI cues, comparing traditional behaviour-based team strategies against a purely Neuro-Decoupled Team (NDT) that used only BCI confidence scores derived from pre-response EEG. Under AI deception, behaviour-based teams catastrophically failed, with Majority Vote accuracy collapsing to 44%. The NDT, however, maintained 98% accuracy, a statistically significant synergistic gain over even the team's best individual performer (p < .001). This was explained by a neuro-behavioural decoupling, where the BCI's predictions remained highly accurate while the operator's subjective confidence became an unreliable signal. We conclude that an implicit BCI provides resilience by learning to adapt its neural strategy, shifting from relying on signals of efficient, autopilot processing in simple conditions to interpreting signatures of effortful deliberation when confronted with cognitive conflict. This demonstrates a system that leverages the context of the neural signal to defend against AI-induced error in high-stakes environments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [331] [SALPA: Spaceborne LiDAR Point Adjustment for Enhanced GEDI Footprint Geolocation](https://arxiv.org/abs/2511.17600)
*Narumasa Tsutsumida,Rei Mitsuhashi,Yoshito Sawada,Akira Kato*

Main category: eess.IV

TL;DR: SALPA是一个多算法优化框架，用于校正星载LiDAR系统的地理定位误差，仅使用全球可用的数字高程模型和大地水准面数据，在复杂地形和平坦地形上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 星载LiDAR系统（如GEDI）的地理定位不确定性（通常5-15米）会系统性地传播到衍生产品中，影响森林剖面估计和碳储量评估。现有校正方法存在关键限制：波形模拟方法需要高分辨率LiDAR数据，而基于地形的方法使用确定性网格搜索可能错过连续解空间中的最优解。

Method: SALPA是一个多算法优化框架，集成了三种优化范式（梯度基、进化、群体智能）和五种距离度量。仅使用全球可用的数字高程模型和大地水准面数据，通过连续解空间探索进行地理定位校正。

Result: 在对比站点验证中：日本日光（复杂地形）和法国兰德斯（平坦地形），SALPA相比原始GEDI位置提高了15-16%，相比最先进的GeoGEDI算法提高了0.5-2%。L-BFGS-B与基于面积的度量实现了最佳精度-效率权衡，而基于群体的算法在复杂地形中表现优异。

Conclusion: 该平台无关的框架便于适应新兴星载LiDAR任务，为通用地理定位校正提供了可推广的基础，对于可靠的全球森林监测和气候政策决策至关重要。

Abstract: Spaceborne Light Detection and Ranging (LiDAR) systems, such as NASA's Global Ecosystem Dynamics Investigation (GEDI), provide forest structure for global carbon assessments. However, geolocation uncertainties (typically 5-15 m) propagate systematically through derived products, undermining forest profile estimates, including carbon stock assessments. Existing correction methods face critical limitations: waveform simulation approaches achieve meter-level accuracy but require high-resolution LiDAR data unavailable in most regions, while terrain-based methods employ deterministic grid searches that may overlook optimal solutions in continuous solution spaces. We present SALPA (Spaceborne LiDAR Point Adjustment), a multi-algorithm optimization framework integrating three optimization paradigms with five distance metrics. Operating exclusively with globally available digital elevation models and geoid data, SALPA explores continuous solution spaces through gradient-based, evolutionary, and swarm intelligence approaches. Validation across contrasting sites: topographically complex Nikko, Japan, and flat Landes, France, demonstrates 15-16% improvements over original GEDI positions and 0.5-2% improvements over the state-of-the-art GeoGEDI algorithm. L-BFGS-B with Area-based metrics achieves optimal accuracy-efficiency trade-offs, while population-based algorithms (genetic algorithms, particle swarm optimization) excel in complex terrain. The platform-agnostic framework facilitates straightforward adaptation to emerging spaceborne LiDAR missions, providing a generalizable foundation for universal geolocation correction essential for reliable global forest monitoring and climate policy decisions.

</details>


### [332] [Robust Detection of Retinal Neovascularization in Widefield Optical Coherence Tomography](https://arxiv.org/abs/2511.17744)
*Jinyi Hao,Jie Wang,Kotaro Tsuboi,Liqin Gao,Tristan T. Hormel,Yukun Guo,An-Lun Wu,Min Gao,Christina J. Flaxel,Steven T. Bailey,Thomas S. Hwang,Yali Jia*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的方法，用于在宽场OCT/OCTA图像上进行视网膜新生血管的诊断和分期，将RNV识别重新定义为直接的二元定位任务，无需多层视网膜分割。


<details>
  <summary>Details</summary>
Motivation: 视网膜新生血管是糖尿病视网膜病变中威胁视力的发展，及时干预可预防视力丧失。宽场OCTA技术可改善RNV早期检测，但现有算法仅适用于窄视场图像。

Method: 将RNV识别重新定义为直接二元定位任务，使用完全自动化的深度学习模型，在来自多个设备和诊所的589张宽场扫描图像上进行训练和验证。

Result: 在RNV诊断方面，设备依赖的AUC范围为0.96-0.99；在分割方面，平均IOU范围为0.76-0.88；还展示了纵向监测病变生长的能力。

Conclusion: 基于深度学习的宽场OCTA图像分析可为改善RNV筛查和管理提供有价值的手段。

Abstract: Retinal neovascularization (RNV) is a vision threatening development in diabetic retinopathy (DR). Vision loss associated with RNV is preventable with timely intervention, making RNV clinical screening and monitoring a priority. Optical coherence tomography (OCT) angiography (OCTA) provides high-resolution imaging and high-sensitivity detection of RNV lesions. With recent commercial devices introducing widefield OCTA imaging to the clinic, the technology stands to improve early detection of RNV pathology. However, to meet clinical requirements these imaging capabilities must be combined with effective RNV detection and quantification, but existing algorithms for OCTA images are optimized for conventional, i.e. narrow, fields of view. Here, we present a novel approach for RNV diagnosis and staging on widefield OCT/OCTA. Unlike conventional methods dependent on multi-layer retinal segmentation, our model reframes RNV identification as a direct binary localization task. Our fully automated approach was trained and validated on 589 widefield scans (17x17-mm to 26x21-mm) collected from multiple devices at multiple clinics. Our method achieved a device-dependent area under curve (AUC) ranging from 0.96 to 0.99 for RNV diagnosis, and mean intersection over union (IOU) ranging from 0.76 to 0.88 for segmentation. We also demonstrate our method's ability to monitor lesion growth longitudinally. Our results indicate that deep learning-based analysis for widefield OCTA images could offer a valuable means for improving RNV screening and management.

</details>


### [333] [Generative MR Multitasking with complex-harmonic cardiac encoding: Bridging the gap between gated imaging and real-time imaging](https://arxiv.org/abs/2511.17847)
*Xinguo Fang,Anthony G. Christodoulou*

Main category: eess.IV

TL;DR: 提出了一种统一的图像重建框架Generative Multitasking，使用条件变分自编码器(CVAE)和复杂谐波心脏坐标，将门控和实时心脏MRI统一在单一自由呼吸、非ECG门控采集中。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的图像重建框架，弥合实时和门控心脏MRI之间的差距，包括定量MRI，避免单独的门控和实时扫描。

Method: 使用条件变分自编码器(CVAE)学习隐式神经时间基和可解释的潜在空间，心脏运动建模为复杂谐波，相位编码时间和潜在振幅捕捉心跳间功能变异性。

Result: Generative Multitasking提供了灵活的心脏运动表示，能够重建典型的心脏相位分辨电影（类似门控）以及显示心跳间变异性的时间分辨序列（类似实时成像）。对于定量映射，与传统Multitasking相比，T1和T2的变异系数显著降低（T1：0.13 vs 0.31；T2：0.12 vs 0.32；p<0.001）。

Conclusion: Generative Multitasking使用CVAE和复杂谐波心脏坐标，在单一自由呼吸、非ECG门控采集中统一门控和实时CMR，允许灵活的心脏运动表示，抑制轨迹相关伪影，并改善T1和T2映射。

Abstract: Purpose: To develop a unified image reconstruction framework that bridges real-time and gated cardiac MRI, including quantitative MRI. Methods: We introduce Generative Multitasking, which learns an implicit neural temporal basis from sequence timings and an interpretable latent space for cardiac and respiratory motion. Cardiac motion is modeled as a complex harmonic, with phase encoding timing and a latent amplitude capturing beat-to-beat functional variability, linking cardiac phase-resolved ("gated-like") and time-resolved ("real-time-like") views. We implemented the framework using a conditional variational autoencoder (CVAE) and evaluated it for free-breathing, non-ECG-gated radial GRE in three settings: steady-state cine imaging, multicontrast T2prep/IR imaging, and dual-flip-angle T1/T2 mapping, compared with conventional Multitasking. Results: Generative Multitasking provided flexible cardiac motion representation, enabling reconstruction of archetypal cardiac phase-resolved cines (like gating) as well as time-resolved series that reveal beat-to-beat variability (like real-time imaging). Conditioning on the previous k-space angle and modifying this term at inference removed eddy-current artifacts without globally smoothing high temporal frequencies. For quantitative mapping, Generative Multitasking reduced intraseptal T1 and T2 coefficients of variation compared with conventional Multitasking (T1: 0.13 vs. 0.31; T2: 0.12 vs. 0.32; p<0.001), indicating higher SNR. Conclusion: Generative Multitasking uses a CVAE with complex harmonic cardiac coordinates to unify gated and real-time CMR within a single free-breathing, non-ECG-gated acquisition. It allows flexible cardiac motion representation, suppresses trajectory-dependent artifacts, and improves T1 and T2 mapping, suggesting a path toward cine, multicontrast, and quantitative imaging without separate gated and real-time scans.

</details>


### [334] [A Versatile Optical Frontend for Multicolor Fluorescence Imaging with Miniaturized Lensless Sensors](https://arxiv.org/abs/2511.17860)
*Lukas Harris,Micah Roschelle,Jack Bartley,Mekhail Anwar*

Main category: eess.IV

TL;DR: 本文分析了使用光纤板(FOP)的无透镜荧光成像系统，通过优化FOP的数值孔径(NA)来平衡收集效率、分辨率、厚度和荧光激发效率之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 传统薄膜干涉滤光片对入射角敏感，在无透镜成像系统中难以有效阻挡激发光而通过荧光发射。需要开发角度不敏感的光学前端来改进无透镜荧光传感器。

Method: 使用光纤板(FOP)吸收离轴光，同时在干涉滤光片两侧放置滤光片以解决散射问题。通过仿真优化两种不同NA(8.3°和45.7° FWHM)的光学前端设计。

Result: 高NA设计(520μm厚)的荧光灵敏度提高59倍，分辨率仅下降3.2倍；低NA设计可实现三色荧光成像，在1mm工作距离下达到110μm分辨率。

Conclusion: 开发了一种多功能光学前端，适用于不同荧光团、照明配置和无透镜成像技术，为体内成像和低成本诊断应用提供了紧凑的解决方案。

Abstract: Lensless imaging enables exceptionally compact fluorescence sensors, advancing applications in \textit{in vivo} imaging and low-cost, point-of-care diagnostics. These sensors require a filter to block the excitation light while passing fluorescent emissions. However, conventional thin-film interference filters are sensitive to angle of incidence (AOI), complicating their use in lensless systems. Here we thoroughly analyze and optimize a technique using a fiber optic plate (FOP) to absorb off-axis light that would bleed through the interference filter while improving image resolution. Through simulations, we show that the numerical aperture (NA) of the FOP drives inherent design tradeoffs: collection efficiency improves rapidly with a higher NA, but at the cost of resolution, increased device thickness, and fluorescence excitation efficiency. To illustrate this, we optimize two optical frontends with full-width at half maximums (FWHMs) of 8.3° and 45.7°. Implementing these designs, we show that angle-insensitivity requires filters on both sides of the FOP, due to scattering. In imaging experiments, the 520-$μ$m-thick high-NA design is 59$\times$ more sensitive to fluorescence while only degrading resolution by 3.2$\times$. Alternatively, the low-NA design is capable of three-color fluorescence imaging with 110-$μ$m resolution at a 1-mm working distance. Overall, we demonstrate a versatile optical frontend that is adaptable to a range of applications using different fluorophores, illumination configurations, and lensless imaging techniques.

</details>


### [335] [INT-DTT+: Low-Complexity Data-Dependent Transforms for Video Coding](https://arxiv.org/abs/2511.17867)
*Samuel Fernández-Menduiña,Eduardo Pavez,Antonio Ortega,Tsung-Wei Huang,Thuong Nguyen Canh,Guan-Ming Su,Peng Yin*

Main category: eess.IV

TL;DR: 本文提出了一种低复杂度数据依赖变换框架DTT+，通过将图基可分离变换与离散三角变换结合，实现了计算效率和编码性能的平衡。提出的INT-DTT+整数近似方法显著降低了计算和内存复杂度，在VVC标准中实现了超过3%的BD-rate节省。


<details>
  <summary>Details</summary>
Motivation: 传统离散三角变换在视频编码中计算效率高但性能有限，而数据依赖变换如KLT和GBSTs虽然能量压缩性能更好但计算复杂度高。本文旨在设计既能适应信号统计特性又能保持低计算复杂度的数据依赖变换。

Method: 提出DTT+框架，基于DTT图的秩一更新构建GBSTs；开发图学习算法联合估计行列图的秩一更新；利用DTT+的渐进结构将核分解为基础DTT和结构化柯西矩阵；通过低复杂度整数DTT和稀疏化柯西矩阵构建INT-DTT+整数近似。

Result: INT-DTT+显著降低了计算和内存复杂度，相对于可分离KLT性能损失最小；在VVC标准的MTS框架中集成，实现了超过3%的BD-rate节省，计算复杂度与整数DCT-2相当。

Conclusion: DTT+框架成功弥合了传统DTT和数据依赖变换之间的差距，INT-DTT+在保持高性能的同时显著降低了复杂度，为视频编码中的变换设计提供了新的解决方案。

Abstract: Discrete trigonometric transforms (DTTs), such as the DCT-2 and the DST-7, are widely used in video codecs for their balance between coding performance and computational efficiency. In contrast, data-dependent transforms, such as the Karhunen-Loève transform (KLT) and graph-based separable transforms (GBSTs), offer better energy compaction but lack symmetries that can be exploited to reduce computational complexity. This paper bridges this gap by introducing a general framework to design low-complexity data-dependent transforms. Our approach builds on DTT+, a family of GBSTs derived from rank-one updates of the DTT graphs, which can adapt to signal statistics while retaining a structure amenable to fast computation. We first propose a graph learning algorithm for DTT+ that estimates the rank-one updates for rows and column graphs jointly, capturing the statistical properties of the overall block. Then, we exploit the progressive structure of DTT+ to decompose the kernel into a base DTT and a structured Cauchy matrix. By leveraging low-complexity integer DTTs and sparsifying the Cauchy matrix, we construct an integer approximation to DTT+, termed INT-DTT+. This approximation significantly reduces both computational and memory complexities with respect to the separable KLT with minimal performance loss. We validate our approach in the context of mode-dependent transforms for the VVC standard, following a rate-distortion optimized transform (RDOT) design approach. Integrated into the explicit multiple transform selection (MTS) framework of VVC in a rate-distortion optimization setup, INT-DTT+ achieves more than 3% BD-rate savings over the VVC MTS baseline, with complexity comparable to the integer DCT-2 once the base DTT coefficients are available.

</details>


### [336] [Spectral Super-Resolution Neural Operator with Atmospheric Radiative Transfer Prior](https://arxiv.org/abs/2511.17895)
*Ziye Zhang,Bin Pan,Zhenwei Shi*

Main category: eess.IV

TL;DR: 提出SSRNO框架，将大气辐射传输先验融入数据驱动的光谱超分辨率方法中，通过上采样、重建和精炼三阶段实现物理一致的光谱重建。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法忽视物理原理，导致光谱重建不真实，特别是在大气影响波段。需要结合物理先验来提高光谱重建的物理一致性。

Method: 三阶段框架：1) 上采样阶段利用先验信息扩展多光谱输入；2) 重建阶段使用神经算子学习光谱域的连续映射；3) 精炼阶段对输出施加硬约束消除色彩失真。采用GMP方法和SAC层。

Result: 实现了连续光谱重建和零样本外推，各种实验验证了方法的有效性和泛化能力。

Conclusion: SSRNO通过结合神经算子和大气辐射传输先验，能够产生物理一致的光谱超分辨率结果，优于传统数据驱动方法。

Abstract: Spectral super-resolution (SSR) aims to reconstruct hyperspectral images (HSIs) from multispectral observations, with broad applications in remote sensing. Data-driven methods are widely used, but they often overlook physical principles, leading to unrealistic spectra, particularly in atmosphere-affected bands. To address this challenge, we propose the Spectral Super-Resolution Neural Operator (SSRNO), which incorporates atmospheric radiative transfer (ART) prior into the data-driven procedure, yielding more physically consistent predictions. The proposed SSRNO framework consists of three stages: upsampling, reconstruction, and refinement. In the upsampling stage, we leverage prior information to expand the input multispectral image, producing a physically plausible hyperspectral estimate. Subsequently, we utilize a neural operator in the reconstruction stage to learn a continuous mapping across the spectral domain. Finally, the refinement stage imposes a hard constraint on the output HSI to eliminate color distortion. The upsampling and refinement stages are implemented via the proposed guidance matrix projection (GMP) method, and the reconstruction neural operator adopts U-shaped spectral-aware convolution (SAC) layers to capture multi-scale features. Moreover, we theoretically demonstrate the optimality of the GMP method. With the neural operator and ART priors, SSRNO also achieves continuous spectral reconstruction and zero-shot extrapolation. Various experiments validate the effectiveness and generalization ability of the proposed approach.

</details>


### [337] [Diverse Instance Generation via Diffusion Models for Enhanced Few-Shot Object Detection in Remote Sensing Images](https://arxiv.org/abs/2511.18031)
*Yanxing Liu,Jiancheng Pan,Jianwei Yang,Tiancheng Chen,Peiling Zhou,Bingchen Zhang*

Main category: eess.IV

TL;DR: 本文提出了一种利用扩散模型生成多样化遥感图像实例的框架，通过切片生成和语义对齐技术，显著提升少样本目标检测在遥感图像中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像少样本目标检测中实例多样性不足的问题，特别是在濒危物种监测和灾害评估等应用中。

Method: 1. 使用预训练扩散模型生成实例级切片；2. 通过切片到切片模块将切片嵌入全尺度图像；3. 开发类无关图像反演模块将遥感实例切片反演到语义空间；4. 引入对比损失实现语义对齐。

Result: 在多个数据集和各种方法上实现了平均4.4%的性能提升，消融实验验证了反演模块和对比损失的有效性。

Conclusion: 所提出的框架能够有效利用扩散模型生成多样化的遥感实例，显著提升少样本目标检测性能，为遥感应用提供了有效的解决方案。

Abstract: Few-shot object detection (FSOD) aims to detect novel instances with only a limited number of labeled training samples, presenting a challenge that is particularly prominent in numerous remote sensing applications such as endangered species monitoring and disaster assessment. Existing FSOD methods for remote sensing images (RSIs) have achieved promising progress but remain constrained by the limited diversity of instances. To address this issue, we propose a novel framework that can leverage a diffusion model pretrained on large-scale natural images to synthesize diverse remote sensing instances, thereby improving the performance of few-shot object detectors. Instead of directly synthesizing complete remote sensing images, we first generate instance-level slices via a specialized slice-to-slice module, and then embed these slices into full-scale imagery for enhanced data augmentation. To further adapt diffusion models for remote sensing scenarios, we develop a class-agnostic image inversion module that can invert remote sensing instance slices into semantic space. Additionally, we introduce contrastive loss to semantically align the synthesized images with their corresponding classes. Experimental results show that our method hasachieved an average performance improvement of 4.4% across multiple datasets and various approaches. Ablation experiments indicate that the elaborately designed inversion module can effectively enhance the performance of FSOD methods, and the semantic contrastive loss can further boost the performance.

</details>


### [338] [Linear Algebraic Approaches to Neuroimaging Data Compression: A Comparative Analysis of Matrix and Tensor Decomposition Methods for High-Dimensional Medical Images](https://arxiv.org/abs/2511.18197)
*Jaeho Kim,Daniel David,Ana Vizitiv*

Main category: eess.IV

TL;DR: 本文评估了Tucker分解和奇异值分解(SVD)在神经影像数据压缩中的表现。Tucker分解在保持多维关系方面表现更优，而SVD在极端压缩情况下表现更好但会牺牲保真度。


<details>
  <summary>Details</summary>
Motivation: 评估不同分解方法在神经影像数据压缩中的性能，以确定最适合保持数据结构和时间关系的方法。

Method: 使用Tucker分解和奇异值分解(SVD)对神经影像数据进行压缩，比较两种方法在重建保真度和感知相似性方面的表现。

Result: Tucker分解在保持多维关系方面表现优越，具有更好的重建保真度和感知相似性；SVD在极端压缩情况下表现更好，但会牺牲保真度。

Conclusion: Tucker分解更适合需要保持结构和时间关系的应用场景，而SVD适用于极端压缩需求但可接受保真度损失的情况。

Abstract: This paper evaluates Tucker decomposition and Singular Value Decomposition (SVD) for compressing neuroimaging data. Tucker decomposition preserves multi-dimensional relationships, achieving superior reconstruction fidelity and perceptual similarity. SVD excels in extreme compression but sacrifices fidelity. The results highlight Tucker decomposition's suitability for applications requiring the preservation of structural and temporal relationships.

</details>


### [339] [Equivariant Deep Equilibrium Models for Imaging Inverse Problems](https://arxiv.org/abs/2511.18667)
*Alexander Mehta,Ruangrawee Kitichotkul,Vivek K Goyal,Julián Tachella*

Main category: eess.IV

TL;DR: 本文提出了一种模块化的反向传播方法，用于训练具有复杂等变成像损失的深度平衡模型，简化了训练过程并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 等变成像允许在没有真实数据的情况下训练信号重建模型，但训练具有复杂EI损失的深度平衡模型需要隐式微分，实现较为困难。

Method: 展示了反向传播可以模块化实现，简化了训练过程。使用隐式微分训练深度平衡模型。

Result: 实验表明，使用隐式微分训练的DEQs在性能上优于使用雅可比自由反向传播和其他基线方法训练的模型。

Conclusion: EI训练的DEQs近似于不变先验的近端映射，模块化反向传播简化了训练并提升了模型性能。

Abstract: Equivariant imaging (EI) enables training signal reconstruction models without requiring ground truth data by leveraging signal symmetries. Deep equilibrium models (DEQs) are a powerful class of neural networks where the output is a fixed point of a learned operator. However, training DEQs with complex EI losses requires implicit differentiation through fixed-point computations, whose implementation can be challenging. We show that backpropagation can be implemented modularly, simplifying training. Experiments demonstrate that DEQs trained with implicit differentiation outperform those trained with Jacobian-free backpropagation and other baseline methods. Additionally, we find evidence that EI-trained DEQs approximate the proximal map of an invariant prior.

</details>


### [340] [Evaluation of Hardware-based Video Encoders on Modern GPUs for UHD Live-Streaming](https://arxiv.org/abs/2511.18686)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: eess.IV

TL;DR: 评估多代NVIDIA、Intel GPU和Qualcomm Snapdragon移动SoC中硬件视频编码器的率失真性能、编码速度和功耗，并与软件编码器进行比较，包括最新的H.266/VVC编解码器。


<details>
  <summary>Details</summary>
Motivation: 随着直播视频内容的兴起，如VTuber、游戏直播和现场活动广播，对GPU中高效硬件编码器的需求增加，特别是处理4K/8K超高清实时视频编码任务。

Method: 使用PSNR、SSIM和基于机器学习的VMAF等多种指标，评估和比较硬件编码器的率失真性能、编码速度和功耗。

Result: 现代GPU硬件编码器在实时编码场景中能够匹配软件编码器的率失真性能，虽然新硬件的编码速度有所提高，但硬件代际间的率失真性能改进大多可忽略不计。

Conclusion: 计算了每个硬件编码器匹配YouTube转码质量所需的比特率，表明硬件编码器在实时视频编码中具有竞争力。

Abstract: Many GPUs have incorporated hardware-accelerated video encoders, which allow video encoding tasks to be offloaded from the main CPU and provide higher power efficiency. Over the years, many new video codecs such as H.265/HEVC, VP9, and AV1 were added to the latest GPU boards. Recently, the rise of live video content such as VTuber, game live-streaming, and live event broadcasts, drives the demand for high-efficiency hardware encoders in the GPUs to tackle these real-time video encoding tasks, especially at higher resolutions such as 4K/8K UHD. In this paper, RD performance, encoding speed, as well as power consumption of hardware encoders in several generations of NVIDIA, Intel GPUs as well as Qualcomm Snapdragon Mobile SoCs were evaluated and compared to the software counterparts, including the latest H.266/VVC codec, using several metrics including PSNR, SSIM, and machine-learning based VMAF. The results show that modern GPU hardware encoders can match the RD performance of software encoders in real-time encoding scenarios, and while encoding speed increased in newer hardware, there is mostly negligible RD performance improvement between hardware generations. Finally, the bitrate required for each hardware encoder to match YouTube transcoding quality was also calculated.

</details>


### [341] [Neural B-Frame Coding: Tackling Domain Shift Issues with Lightweight Online Motion Resolution Adaptation](https://arxiv.org/abs/2511.18724)
*Sang NguyenQuang,Xiem HoangVan,Wen-Hsiao Peng*

Main category: eess.IV

TL;DR: 提出轻量级分类器预测下采样因子，解决B帧编解码器中因训练和测试GOP大小不匹配导致的域偏移问题，在保持编码性能的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 分层时间预测的B帧编解码器存在域偏移问题，由于训练和测试时GOP大小不匹配，导致大运动估计不准确。传统方法需要昂贵的率失真优化来确定最佳下采样因子。

Method: 提出三种轻量级分类器：二元分类器（Bi-Class）使用Focal Loss选择高低分辨率；多类分类器（Mu-Class）使用基于率失真成本的软标签训练；协同分类器（Co-Class）结合多类分类器的预测能力和二元分类器的选择性搜索。

Result: 实验结果表明，这些分类器方法在保持与穷举搜索方法相当的编码性能的同时，显著降低了计算复杂度。

Conclusion: 提出的轻量级分类器能有效预测下采样因子，无需重新训练编解码器即可与现有B帧编解码器无缝协作，在计算复杂度和编码性能之间取得良好平衡。

Abstract: Learned B-frame codecs with hierarchical temporal prediction often encounter the domain-shift issue due to mismatches between the Group-of-Pictures (GOP) sizes for training and testing, leading to inaccurate motion estimates, particularly for large motion. A common solution is to turn large motion into small motion by downsampling video frames during motion estimation. However, determining the optimal downsampling factor typically requires costly rate-distortion optimization. This work introduces lightweight classifiers to predict downsampling factors. These classifiers leverage simple state signals from current and reference frames to balance rate-distortion performance with computational cost. Three variants are proposed: (1) a binary classifier (Bi-Class) trained with Focal Loss to choose between high and low resolutions, (2) a multi-class classifier (Mu-Class) trained with novel soft labels based on rate-distortion costs, and (3) a co-class approach (Co-Class) that combines the predictive capability of the multi-class classifier with the selective search of the binary classifier. All classifier methods can work seamlessly with existing B-frame codecs without requiring codec retraining. Experimental results show that they achieve coding performance comparable to exhaustive search methods while significantly reducing computational complexity. The code is available at: https://github.com/NYCU-MAPL/Fast-OMRA.git.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [342] [Comprehensive Multimodal and Multiscale Analysis of Alzheimer Disease in 5xFAD Mice: Optical Spectroscopies, TEM, Neuropathological, and Behavioral Investigations](https://arxiv.org/abs/2511.18320)
*Dhruvil Solanki,Ishmael Apachigawo,Sazzad Khan,Santanu Maity,Fatemah Alharthi,Samia Nasim,Mohammad Alizadeh Poshtiri,Fnu Sweety,Jiangfeng Xiao,Mohammad Moshahid Khan,Prabhakar Pradhan*

Main category: physics.med-ph

TL;DR: 本研究使用5xFAD小鼠模型，通过光散射技术检测阿尔茨海默病早期纳米级结构变化，发现脑组织中结构异质性和质量密度波动显著增加，这些变化与淀粉样蛋白β聚集和神经炎症相关。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病是美国主要死因之一且无有效治疗方法，理解其神经病理机制对于识别早期可靠生物标志物和开发有效疗法至关重要。

Method: 采用5xFAD小鼠模型，使用部分波谱学和逆参与比光散射技术检测脑组织、核成分和线粒体的纳米级结构变化，并辅以行为学和病理组织学分析。

Result: 5xFAD小鼠脑部结构异质性和质量密度波动显著增加，行为测试显示记忆受损，病理分析发现海马和皮层中淀粉样蛋白β斑块增多和微胶质细胞激活。

Conclusion: PWS和IPR衍生的指标可作为阿尔茨海默病早期细胞和亚细胞破坏的敏感生物物理指标，有望成为检测和监测疾病进展的定量生物标志物。

Abstract: Alzheimer disease (AD) is considered one of the leading causes of death in the United States, and there is no effective cure for it. Understanding the neuropathological mechanisms underlying AD is essential for identifying early, reliable biomarkers and developing effective therapies. In this paper, we report on a comprehensive multimodal study of AD pathology using the 5xFAD mouse model. We employed light-scattering techniques, Partial Wave Spectroscopy (PWS) and Inverse Participation Ratio (IPR), to detect nanoscale structural alterations in brain tissues, nuclear components, and mitochondria. To support the light-scattering experiments, behavior, and histopathological studies were conducted. These analyses revealed significant increases in structural heterogeneity and mass density fluctuations in the brains of 5xFAD mice compared with Non-transgenic controls. Behavioral assessment demonstrated memory impairment in 5xFAD mice, reflected by a reduced recognition index. Histopathological analysis further revealed increased amyloid beta plaques and microglia activation in the hippocampus and cortex of 5xFAD mice compared with Non-transgenic controls. An increase in structural disorder within brain tissues can be attributed to higher mass density fluctuations, likely arising from macromolecular rearrangement driven by amyloid beta aggregation and neuroinflammatory responses as the disease progresses. Our findings suggest that PWS and IPR-derived metrics provide sensitive biophysical indicators of early cellular and subcellular disruption, offering potential as quantitative biomarkers for the detection and progression of AD.

</details>


### [343] [Towards Integrated Clinical-Computational Nuclear Medicine](https://arxiv.org/abs/2511.18547)
*Faraz Farhadi,Shadi A. Esfahani,Fereshteh Yousefirizi,Monica Luo,Pedro Esquinas Fernandez,Arkadiusz Sitek,Hamid Sabet,Babak Saboury,Arman Rahmim,Pedram Heidari*

Main category: physics.med-ph

TL;DR: 本文综述了临床计算核医学领域的最新进展，重点介绍了AI、示踪动力学建模、影像组学和信息学技术在改善成像质量、自动化病变检测和个性化放射性药物疗法方面的应用。


<details>
  <summary>Details</summary>
Motivation: 推动临床计算核医学领域的发展，通过整合先进计算技术来提升核医学成像和治疗的效果，同时强调临床监督在确保准确性和患者安全方面的重要性。

Method: 采用多种计算技术，包括人工智能、示踪动力学建模、影像组学、信息学集成、基于生理学的药代动力学建模、体素级剂量测定、工作流自动化和自然语言处理。

Result: 这些技术显著改善了成像质量，实现了病变检测的自动化，并支持了个性化放射性药物疗法的发展，同时通过工作流自动化和NLP提高了操作效率。

Conclusion: 计算创新在核医学领域具有巨大潜力，但成功实施和采用需要临床监督，以确保准确性、可解释性和患者安全，临床医生指导的评估对于塑造精准成像和治疗的未来至关重要。

Abstract: The field of Clinical-Computational Nuclear Medicine is rapidly advancing, fueled by AI, tracer kinetic modeling, radiomics, and integrated informatics. These technologies improve imaging quality, automate lesion detection, and enable personalized radiopharmaceutical therapy through physiologically based pharmacokinetic (PBPK) modeling and voxel-level dosimetry. Workflow automation and Natural Language Processing (NLP) further enhance operational efficiency. However, successful implementation and adoption of these tools require clinical oversight to ensure accuracy, interpretability, and patient safety. This paper highlights key computational innovations and emphasizes the critical role of clinician-guided evaluation in shaping the future of precision imaging and therapy.

</details>


### [344] [On the Appropriateness of Linear Stress Recovery in Biomechanical Analysis of Abdominal Aortic Aneurysm](https://arxiv.org/abs/2511.18741)
*Alastair Catlin,Mostafa Jamshidian,Adam Wittek,Karol Miller*

Main category: physics.med-ph

TL;DR: 该研究验证了线性应力恢复方法在腹主动脉瘤（AAA）破裂风险评估中的可靠性，发现成像相位对线性应力恢复影响很小，且线性恢复与非线性分析结果高度一致，支持线性方法在临床单相静态成像中的使用。


<details>
  <summary>Details</summary>
Motivation: 腹主动脉瘤壁应力是潜在的破裂风险标志物，但通常基于单相图像计算，成像相位未知。线性应力恢复方法已在静态应力估计中得到验证，但其对未知成像相位的鲁棒性尚未探索。

Method: 分析两个患者特异性AAA病例（来自公共4D-CTA队列），比较舒张期和合成收缩期几何形状下的线性应力恢复，并与非线性分析结果进行对比，评估99百分位应力、应力分布和3D应力差异。

Result: 线性恢复在舒张期与合成收缩期几何形状下的99百分位应力差异很小（8.6%和3.5%），在线性恢复与非线性分析在脉压下的99百分位应力几乎完全一致（0%和1.1%差异），应力分布高度相似。

Conclusion: 线性应力恢复方法适用于临床单相静态成像的AAA分析，提供了计算效率高的替代方案，且不损害准确性或需要患者特异性壁特性。

Abstract: Abdominal aortic aneurysm (AAA) wall stress is a candidate rupture risk marker but is typically computed from single-phase images without known cardiac phase. Linear stress recovery methods, which solve a single geometrically linear equilibrium problem on the imaged, already-loaded geometry, have been validated for static stress estimation, but their robustness to unknown imaging phase remains unexplored. We investigated whether imaging phase materially biases 99th percentile stress recovered linearly, and whether linear recovery agrees with non-linear analysis under matched loads. Two patient-specific AAAs from a public 4D-CTA cohort (Case 1: 5.5% strain; Case 2: 4.5% strain) were analyzed. For each, we analyzed diastolic and synthetic systolic geometry, the latter generated by warping the diastolic mesh via displacements from non-linear hyperelastic analysis. Linear stresses were recovered on both geometries under systolic pressure and compared via 99th-percentile maximum principal stress, stress distributions, and 3D stress differential contours. Linear stresses under pulse pressure were compared against non-linear stresses. 99th-percentile stresses from linear recovery on diastolic vs synthetic systolic geometries under systolic pressure differed by 8.6% (Case 1) and 3.5% (Case 2), within segmentation uncertainty. 99th-percentile stresses from linear recovery and non-linear analysis under pulse pressure agreed closely: 0% difference (Case 1) and 1.1% (Case 2), with nearly identical distributions. These findings support linear stress recovery for patient-specific AAA analysis in clinical settings with static single-phase imaging, offering a computationally efficient alternative without compromising accuracy or requiring patient-specific wall properties.

</details>


### [345] [Measuring thermal contact resistances between metallic rods using laser spot heating and infrared thermography. Part 1](https://arxiv.org/abs/2511.18898)
*Thomas Lahens,Alain Sommier,Marie-Marthe Groz,Jean-Christophe Batsale*

Main category: physics.med-ph

TL;DR: 该论文提出了一种使用激光加热和红外热成像技术测量圆柱体间热接触电阻的方法，通过简化热模型和分析数学处理，能够表征颗粒簇或纤维介质的接触特性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够准确测量圆柱体间热接触电阻的方法，用于模拟颗粒簇、纤维介质特性以及检测热阻裂纹，为复杂介质研究提供基础验证。

Method: 采用激光在横截面上加热，通过红外热成像测量温度响应，利用简化的热模型（将温度场视为等温）和分析数学方法（线性系统松弛），结合矩阵对数处理温度场数据。

Result: 该方法在2-3个圆柱体情况下得到验证，能够有效表征接触特性并检测热阻，为研究更复杂介质（大量圆柱体、颗粒介质）奠定了基础。

Conclusion: 提出的热接触电阻测量方法在简化模型下具有可行性，为复杂介质的热特性研究提供了有效的初步验证工具。

Abstract: Estimation of thermal contact resistances between cylinders can be achieved using heating on the cross sections by a laser spot and measurement of the temperature response by IR thermography. This type of measurement makes it possible to characterise contacts in clusters of cylinders to simulate clusters of grains, or the properties of fibrous media in the transverse direction or, under certain conditions, to detect emerging thermal resistances (cracks perpendicular to the observation plane) by IR thermography. Here, the thermal model is simplified because the temperature field in relaxation on either side of the resistance can be considered isothermal with a separation of transfers in the plane and along the thickness. The mathematical model is then analytical (relaxation of a linear system), and the processing of the temperature field can be inversed thanks to the consideration of the logarithm of matrices, useful when studying the propagation of measurement noise. The method proposed in the case of two or three cylinders is a first step and a validation for applications to the study of more complex media (large number of cylinders, granular media).

</details>


### [346] [In-vivo imaging with a low-cost MRI scanner and cloud data processing in low-resource settings](https://arxiv.org/abs/2511.19226)
*Teresa Guallart-Naval,Robert Asiimwe,Patricia Tusiime,Mary A. Nassejje,Leo Kinyera,Lemi Robin,Maureen Nayebare,Luiz G. C. Santos,Marina Fernández-García,Lucas Swistunow,José M. Algarín,John Stairs,Michael Hansen,Ronald Amodoi,Andrew Webb,Joshua Harper,Steven J. Schiff,Johnes Obungoloch,Joseba Alonso*

Main category: physics.med-ph

TL;DR: 在非洲建造和运营的低成本低场强MRI扫描仪实现了体内成像，通过系统硬件和软件改进解决了资源匮乏环境中的主要操作限制问题。


<details>
  <summary>Details</summary>
Motivation: 展示低成本低场强MRI扫描仪在资源匮乏环境中的可行性，并证明通过系统改进可以克服电磁噪声和电网不稳定性等主要限制因素。

Method: 对位于乌干达的46 mT Halbach扫描仪进行全面升级，包括重新组织接地和屏蔽、安装新控制电子设备和开源用户界面软件，使用三维RARE序列采集体内脑图像，并通过基于云的重建实现畸变校正。

Result: 升级后系统噪声水平降至热极限的三倍以下，实现多日稳定运行，成功获取三维T1和T2加权脑图像，并通过远程GPU重建实现畸变校正和近实时可视化。

Conclusion: 低成本MRI系统在妥善处理电磁噪声和电网不稳定性后可以达到临床相关图像质量，该工作突出了在资源匮乏环境中可持续MRI开发的可行性，并将稳定电力供应和本地能力建设确定为临床转化的关键下一步。

Abstract: Purpose: To demonstrate in-vivo imaging with a low-cost, low-field MRI scanner built and operated in Africa, and to show how systematic hardware and software improvements can mitigate the main operational limitations encountered in low-resource environments. Methods: A 46 mT Halbach scanner located at the Mbarara University of Science and Technology (Uganda) was upgraded through a complete reorganization of grounding and shielding, installation of new control electronics and open-source user-interface software. Noise performance was quantified using a standardized protocol and in-vivo brain images were acquired with three-dimensional RARE sequences. Distortion correction was implemented using cloud-based reconstructions incorporating magnetic field maps. Results: The revamped system reached noise levels routinely below three times the thermal limit and demonstrated stable operation over multi-day measurements. Three-dimensional T1- and T2-weighted brain images were successfully acquired and distortion-corrected with remote GPU-based reconstructions and near real-time visualization through the user interface. Conclusions: The results show that low-cost MRI systems can achieve clinically relevant image quality when electromagnetic noise and power-grid instabilities are properly addressed. This work highlights the feasibility of sustainable MRI development in low-resource settings and identifies stable power delivery and local capacity building as the key next steps toward clinical translation.

</details>


### [347] [A primer on treatment planning aspects for temporally modulated pulsed radiation therapy](https://arxiv.org/abs/2511.19329)
*Christian Velten,Jiayi Huang,Wolfgang A. Tomé*

Main category: physics.med-ph

TL;DR: 本文研究了时间调制脉冲放疗(TMPRT)技术，该技术通过时间分离的低剂量脉冲(<30 cGy)提供常规分割剂量，利用肿瘤辐射超敏性。开发了VMAT和3D-CRT计划技术，评估了计划的可实施性和准确性。


<details>
  <summary>Details</summary>
Motivation: 利用肿瘤辐射超敏性现象，开发能够提供低剂量率(约6.7 cGy/min)的TMPRT技术，以改善放疗效果。

Method: 开发了VMAT和3D-CRT计划技术，遵循NRG CC-017试验指南。通过体模测量评估吸收剂量准确性，使用EPID进行等剂量验证。

Result: VMAT仅单弧计划可接受，动态适形弧需使用部分弧。现代LINAC在低剂量率下传递准确，传统LINAC表现中等。VMAT在均匀性、适形性和器官保护方面更优。

Conclusion: VMAT是实现TMPRT优化的首选方法，而3D-CRT可提高TMPRT在更多患者和诊所的可用性。

Abstract: Temporally modulated pulsed radiotherapy (TMPRT) delivers conventional fraction doses of radiation using temporally separated pulses of low doses (<30 cGy) yielding fraction-effective dose rates of around 6.7 cGy/min with the goal to exploit tumor radiation hypersensitivity, which was observed in both, preclinical models and in human clinical trials. To facilitate TMPRT, volumetric modulated arc therapy (VMAT) and 3D-CRT planning techniques were developed following the guidelines of the proposed NRG CC-017 trial. Plans were evaluated with respect to homogeneity, conformality, and adherence to dose constraints. Deliverability of plans was assessed using in-phantom measurements for absorbed dose accuracy at low dose rates and using EPID for isodose verification. For VMAT only single arc plans were found to be acceptable due to otherwise unacceptably heterogeneous field doses, while for dynamic conformal arcs machine limtations on the number of monitor units per degree require the use of partial arcs for each pulse. Delivery of plans at low dose rates (< 100 MU/min) was accurate with high Gamma pass rates on modern LINACs and moderate pass rates on legacy LINACs, in line with their general performance. Generally, VMAT is preferred to achieve optimal homogeneity, conformality, and organ-at-risk sparing, while the use of 3D-CRT can increase the availability of TMPRT for more patients and clinics.

</details>
